[{content={parts=[{text=Okay, here are 35 flashcards based on the provided text, formatted as requested.

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a variable-length code over a non-empty set A?
--InteriorSeparator--
A subset C ⊆ A+ such that C* is a free sub-monoid of A*.
--InteriorSeparator--
Easy
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are equivalent conditions for C to be a code over A?
--InteriorSeparator--
(right) Any code sequence W ∈ C* can be uniquely decomposed into code words.
(wrong) C must be finite.
(right) For any u1,...,um,v1,...,vn ∈ C, u1---um=v1---vn => u1=v1.
(wrong) C must contain all elements of A.
--InteriorSeparator--
Medium
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a prefix code?
--InteriorSeparator--
A code where no code word is a prefix of any other code word.
--InteriorSeparator--
Easy
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following is true regarding the sets Ci defined in the Sardinas-Patterson theorem?
--InteriorSeparator--
(right) C1 = {x ∈ A+ | (∃c ∈ C)(cx ∈ C)}
(wrong) Ci only contains elements from C
(right) Ci+1 = {x ∈ A+ | (∃c ∈ Ci)(cx ∈ C) ∨ (∃c ∈ C)(cx ∈ Ci)} for any i ≥ 1.
(wrong) Ci is always a subset of C.
--InteriorSeparator--
Medium
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
According to the Sardinas-Patterson Theorem, when is C a code over A?
--InteriorSeparator--
If and only if C ∩ Ci = ∅, for any i ≥ 1.
--InteriorSeparator--
Hard
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the output of the Sardinas-Patterson Algorithm if C is not a code?
--InteriorSeparator--
0
--InteriorSeparator--
Easy
--InteriorSeparator--
7
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Who proposed Huffman codes?
--InteriorSeparator--
David Huffman
--InteriorSeparator--
Easy
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are characteristics of Huffman Codes?
--InteriorSeparator--
(right) They are prefix codes
(right) Associate shorter code words to highly probable symbols
(wrong) They are suffix codes
(wrong) They always use the same length code words.
--InteriorSeparator--
Medium
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define an information source (IS).
--InteriorSeparator--
A couple IS = (A, π), where A is a non-empty and at most countable set (source alphabet), and π is a probability distribution on A.
--InteriorSeparator--
Easy
--InteriorSeparator--
10
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the formula for calculating the average length of a homomorphism h with respect to an information source IS = (A, π)?
--InteriorSeparator--
Lh(IS) = Σ |h(a)|π(a) for all a ∈ A
--InteriorSeparator--
Medium
--InteriorSeparator--
10
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What makes an encoding 'h' a Huffman encoding of IS?
--InteriorSeparator--
C = {h(a) | a ∈ A} is a prefix code, and h has minimum length among all prefix codes of IS.
--InteriorSeparator--
Medium
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the Huffman Algorithm, what happens when n = 2?
--InteriorSeparator--
h(a1) = 0 and h(a2) = 1 (or vice-versa) is a Huffman code for IS.
--InteriorSeparator--
Easy
--InteriorSeparator--
13
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the Huffman Algorithm, how is a reduced source IS' computed when n ≥ 3?
--InteriorSeparator--
A' = {a1, a2, ..., an-2, an-1,n} and π' = {p1, p2, ..., pn-2, pn-1,n} where pn-1,n = pn-1 + pn.
--InteriorSeparator--
Medium
--InteriorSeparator--
13
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
If h' is a Huffman code for IS', how is the Huffman code h for IS constructed?
--InteriorSeparator--
h(x) = h'(x) if x ∉ {an-1, an}; h(x) = h'(x)0 if x = an-1; h(x) = h'(x)1 if x = an.
--InteriorSeparator--
Medium
--InteriorSeparator--
14
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is involved in data compression using Huffman Codes?
--InteriorSeparator--
(right) Parse the text to compute the number of occurrences of each symbol
(wrong) Always replace symbols with binary code words of equal length.
(right) Compute a Huffman code for the information source.
(wrong) Using lossy compression techniques exclusively.
--InteriorSeparator--
Medium
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the difference between lossless and lossy data compression?
--InteriorSeparator--
Lossless allows the exact original data to be reconstructed, while lossy does not.
--InteriorSeparator--
Easy
--InteriorSeparator--
18
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What limits lossless data compression?
--InteriorSeparator--
The entropy of the information source.
--InteriorSeparator--
Medium
--InteriorSeparator--
18
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define the entropy of an information source S with n symbols and probabilities p1, ..., pn.
--InteriorSeparator--
H(S) = Σ pi * log2(1/pi) for i = 1 to n (mathematical convention: 0 * log(1/0) = 0).
--InteriorSeparator--
Medium
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which statement are true regarding the entropy H(p1, . . . , pn)?
--InteriorSeparator--
(wrong) H(p1, . . . , pn) ≤ 0
(right) 0 ≤ H(p1, . . . , pn) ≤ log n
(right) H(p1, . . . , pn) = 0 if pi = 1 for some i
(wrong) H(p1, . . . , pn) = log n if pi = 1 for some i
--InteriorSeparator--
Hard
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
If S1 and S2 are two information sources, how is their product S1 o S2 defined?
--InteriorSeparator--
S1 o S2 = ({(ai, bj) | 1 ≤ i ≤ n, 1 ≤ j ≤ m}, (pi * qj | 1 ≤ i ≤ n, 1 ≤ j ≤ m)).
--InteriorSeparator--
Hard
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the entropy of Sk, where S is an information source and k is the number of times S is composed with itself?
--InteriorSeparator--
H(Sk) = kH(S)
--InteriorSeparator--
Hard
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
State Shannon's noiseless coding theorem (part 1).
--InteriorSeparator--
H(S) ≤ Lh(S), for any code h of S.
--InteriorSeparator--
Hard
--InteriorSeparator--
22
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which statement is true regarding Shannon's noiseless coding theorem?
--InteriorSeparator--
(wrong) For any code h of S, H(S) ≥ Lh(S)
(right) H(S) ≤ Lh(S) < H(S) + 1, for any Huffman code h of S
(wrong) The theorem places an upper bound on the minimum length of an encoding of a source S
(right) lim k->∞ Lmin(Sk)/k = H(S), where Lmin(S’) is the average length of some Huffman code for S’.
--InteriorSeparator--
Hard
--InteriorSeparator--
22
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Why might adaptive Huffman coding be preferred over standard Huffman coding for large inputs?
--InteriorSeparator--
It requires only one parsing of the input, making it less time-consuming.
--InteriorSeparator--
Medium
--InteriorSeparator--
23
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the first step in Huffman encoding for an input W E Σ+
--InteriorSeparator--
Determine the frequency of occurrences of each letter a in W
--InteriorSeparator--
Easy
--InteriorSeparator--
23
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the tree representation of binary codes, what do the code words correspond to?
--InteriorSeparator--
The sequences of labels collected from the root to leaves.
--InteriorSeparator--
Medium
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the initial step in adaptive Huffman coding?
--InteriorSeparator--
Start with a Huffman tree T0 associated to the alphabet A, where each symbol of A has frequency 1.
--InteriorSeparator--
Medium
--InteriorSeparator--
25
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
In the sibling transformation, what happens if the immediate successor has frequency k + 1 or greater?
--InteriorSeparator--
(right) The nodes are still in sorted order
(right) There is no need to change anything
(wrong) The nodes need to be swapped with their parent
(wrong) The nodes need to be swapped with the last successor
--InteriorSeparator--
Hard
--InteriorSeparator--
26
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the sibling transformation, after comparing a to its successors, what is the next step?
--InteriorSeparator--
Increment the frequency of 'a' (from k to k+1).
--InteriorSeparator--
Medium
--InteriorSeparator--
26
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
After incrementing the frequency of 'a' in the sibling transformation, what happens next?
--InteriorSeparator--
If 'a' is the root, the loop halts; otherwise, the loop repeats with the parent of 'a'.
--InteriorSeparator--
Medium
--InteriorSeparator--
26
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Is Huffman adaptive a variable-length code?
--InteriorSeparator--
No.
--InteriorSeparator--
Easy
--InteriorSeparator--
28
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Is Huffman adaptive a time-varying code?
--InteriorSeparator--
Yes
--InteriorSeparator--
Easy
--InteriorSeparator--
28
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Where can you find more details regarding time-varying codes?
--InteriorSeparator--
[2] Ferucio Laurentiu Tiplea, Erkki Makinen, Dragos Trinca, and Costel Enea. Characterization results for time-varying codes. Fundamenta lnformaticae, 53(2):185—198, may 2002.
--InteriorSeparator--
Medium
--InteriorSeparator--
28
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What pages should you read from textbook [1]?
--InteriorSeparator--
Pages 235-267
--InteriorSeparator--
Easy
--InteriorSeparator--
30
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Who is considered the father of information theory?
--InteriorSeparator--
(right) Claude Shannon
(wrong) David Huffman
(wrong) Newton Faller
(wrong) Jeffrey Scott Vitter
--InteriorSeparator--
Easy
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following can be used to compress data?
--InteriorSeparator--
(right) Huffman Codes
(wrong) The Sardinas-Patterson Algorithm
(wrong) Sibling Transformation
(wrong) Time-varying Codes
--InteriorSeparator--
Easy
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What does the compression ratio measure?
--InteriorSeparator--
(right) The ratio of the size of the original data to the size of the compressed data.
(wrong) The size of the original data added to the size of the compressed data.
(wrong) The size of the compressed data to the size of the original data.
(wrong) The rate of compression.
--InteriorSeparator--
Easy
--InteriorSeparator--
17
--FlashCardSeparator--
}], role=model}, finishReason=STOP, avgLogprobs=-0.12059710072536095}]