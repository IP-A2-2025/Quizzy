[{content={parts=[{text=Okay, here are 35 flashcards based on your instructions:

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the main focus of the chapter regarding Reinforcement Learning?
--InteriorSeparator--
The Q learning algorithm, which acquires optimal control strategies from delayed rewards, even without prior knowledge of action effects.
--InteriorSeparator--
easy
--InteriorSeparator--
2
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the characteristics of control learning?
--InteriorSeparator--
(right) Training examples are not provided as <s, π(s)>.
(wrong) The trainer provides specific actions to take in each state.
(right) Learner faces the problem of temporal credit assignment.
(wrong) The current state is always fully observable.
--InteriorSeparator--
medium
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does the discount factor (γ) represent in Reinforcement Learning?
--InteriorSeparator--
The discount factor for future rewards, a value between 0 and 1.
--InteriorSeparator--
easy
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are possible definitions for the total reward in Reinforcement Learning?
--InteriorSeparator--
(right) The discounted cumulative reward.
(wrong) The most recent reward.
(right) The final horizon reward.
(wrong) The initial reward.
--InteriorSeparator--
medium
--InteriorSeparator--
7
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does V*(s) represent?
--InteriorSeparator--
The optimal value function, representing the maximum achievable cumulative reward starting from state s.
--InteriorSeparator--
medium
--InteriorSeparator--
7
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Why is the Q function useful?
--InteriorSeparator--
(right) It allows the agent to choose the optimal action even without knowing the state transition function.
(wrong) It simplifies the calculation of immediate rewards.
(right) It represents the expected utility of taking a specific action in a specific state.
(wrong) It eliminates the need for exploration.
--InteriorSeparator--
medium
--InteriorSeparator--
11
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the core idea behind the Q-learning update rule?
--InteriorSeparator--
To iteratively refine the Q-value estimate based on the immediate reward and the best possible future Q-value.
--InteriorSeparator--
medium
--InteriorSeparator--
12
--FlashCardSeparator--


}], role=model}, finishReason=STOP, avgLogprobs=-0.03489670755075379}]