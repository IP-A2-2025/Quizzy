[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What is the main focus of the chapter regarding Reinforcement Learning?
--InteriorSeparator--
The Q learning algorithm, which acquires optimal control strategies from delayed rewards, even without prior knowledge of action effects.
--InteriorSeparator--
easy
--InteriorSeparator--
2
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the characteristics of control learning?
--InteriorSeparator--
(right) Training examples are not provided as <s, π(s)>.
(wrong) The trainer provides specific actions to take in each state.
(right) Learner faces the problem of temporal credit assignment.
(wrong) The current state is always fully observable.
--InteriorSeparator--
medium
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does the discount factor (γ) represent in Reinforcement Learning?
--InteriorSeparator--
The discount factor for future rewards, a value between 0 and 1.
--InteriorSeparator--
easy
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are possible definitions for the total reward in Reinforcement Learning?
--InteriorSeparator--
(right) The discounted cumulative reward.
(wrong) The most recent reward.
(right) The final horizon reward.
(wrong) The initial reward.
--InteriorSeparator--
medium
--InteriorSeparator--
7
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does V*(s) represent?
--InteriorSeparator--
The optimal value function, representing the maximum achievable cumulative reward starting from state s.
--InteriorSeparator--
medium
--InteriorSeparator--
7
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Why is the Q function useful?
--InteriorSeparator--
(right) It allows the agent to choose the optimal action even without knowing the state transition function.
(wrong) It simplifies the calculation of immediate rewards.
(right) It represents the expected utility of taking a specific action in a specific state.
(wrong) It eliminates the need for exploration.
--InteriorSeparator--
medium
--InteriorSeparator--
11
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the core idea behind the Q-learning update rule?
--InteriorSeparator--
To iteratively refine the Q-value estimate based on the immediate reward and the best possible future Q-value.
--InteriorSeparator--
medium
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a "full interval" in the context of Q-learning convergence proof?
--InteriorSeparator--
An interval during which each state-action pair <s, a> is visited at least once.
--InteriorSeparator--
hard
--InteriorSeparator--
16
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the conditions necessary for the Q-learning algorithm to converge in the deterministic case?
--InteriorSeparator--
(right) The system is deterministic.
(wrong) Rewards are unbounded.
(right) Every state-action pair is visited infinitely often.
(wrong) The discount factor must be 1.
--InteriorSeparator--
hard
--InteriorSeparator--
15
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How is the exploration-exploitation trade-off managed in Q-learning, as suggested in the provided text?
--InteriorSeparator--
By using a probability distribution P(a|s) based on the Q-values, controlled by a parameter K, where high K favors exploitation and low K favors exploration.
--InteriorSeparator--
medium
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Why might it be beneficial to store and periodically retrain on past state-action transitions?
--InteriorSeparator--
If a Q-value for a predecessor state receives a large update, it is likely that the current state's Q-value should also be updated.
--InteriorSeparator--
medium
--InteriorSeparator--
18
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What adjustments are made to the Q-learning algorithm when dealing with a non-deterministic environment?
--InteriorSeparator--
(right) Taking the expected values of rewards and next states.
(wrong) Ignoring the uncertainty in the environment.
(right) Using a learning rate αn that decreases with the number of visits to a state-action pair.
(wrong) Keeping Q values constant for increased stability.
--InteriorSeparator--
hard
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the key difference in how Q-values are updated in the non-deterministic case compared to the deterministic case?
--InteriorSeparator--
Revisions to Q-values are made more gradually, using a learning rate that decreases with the number of visits to the state-action pair.
--InteriorSeparator--
hard
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the main idea behind Temporal Difference (TD) Learning?
--InteriorSeparator--
To blend estimates of Q-values calculated over different numbers of steps, reducing the discrepancy between successive estimates.
--InteriorSeparator--
medium
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
How does the parameter λ influence Temporal Difference Learning?
--InteriorSeparator--
(right) As λ increases, more emphasis is put on more distant future rewards.
(wrong) As λ increases, more emphasis is put on immediate rewards.
(wrong) A value of λ = 1 is equivalent to only considering the immediate reward.
(right) A value of λ = 0 is equivalent to Q(1).
--InteriorSeparator--
hard
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What are some potential ways to further develop Reinforcement Learning techniques, as suggested in the document?
--InteriorSeparator--
Replacing the Q-table with a neural network, handling partially observable states, designing optimal exploration strategies, extending to continuous action and state spaces, and learning models of the environment.
--InteriorSeparator--
medium
--InteriorSeparator--
23
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
When are dynamic programming algorithms generally more efficient than Q-learning?
--InteriorSeparator--
(right) When the state transition function (δ) is known.
(wrong) When the reward function (r) is unknown.
(right) When both the state transition function (δ) and the reward function (r) are known.
(wrong) When the environment is non-deterministic.
--InteriorSeparator--
medium
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the Markov assumption in the context of Reinforcement Learning?
--InteriorSeparator--
The future state and reward depend only on the current state and action.
--InteriorSeparator--
easy
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What does argmax represent in the context of Reinforcement Learning?
--InteriorSeparator--
(right) The argument that maximizes the function.
(wrong) The average value of a state.
(right) A method of finding the optimal policy.
(wrong) The minimum expected reward.
--InteriorSeparator--
medium
--InteriorSeparator--
7
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the purpose of the exploration strategy in Q-learning?
--InteriorSeparator--
To discover new state-action pairs and improve the Q-value estimates by trying different actions.
--InteriorSeparator--
medium
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the role of the reward function r(s,a)?
--InteriorSeparator--
It specifies the immediate reward received after taking action a in state s.
--InteriorSeparator--
easy
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is temporal credit assignment?
--InteriorSeparator--
(right) The problem of determining which actions are responsible for a delayed reward.
(wrong) Allocating computational resources to different parts of the Q-table.
(right) Determining which states are most valuable.
(wrong) Assigning probabilities to different state transitions.
--InteriorSeparator--
medium
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In Q-learning, what does Q(s, a) represent?
--InteriorSeparator--
The estimated value of taking action *a* in state *s* and following the optimal policy thereafter.
--InteriorSeparator--
easy
--InteriorSeparator--
11
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Why is it important for an agent to perform space exploration, especially in continuous spaces?
--InteriorSeparator--
To discover new states and improve the accuracy of the Q-value estimates by trying different actions in different states.
--InteriorSeparator--
hard
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
If an agent's current state is only partially observable, what must the learner consider?
--InteriorSeparator--
(right) Previous observations.
(wrong) The future state.
(right) Improve the current observability.
(wrong) Ignore non-relevant information.
--InteriorSeparator--
hard
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does 5(s,a) represent in Markov Decision Processes?
--InteriorSeparator--
The next state after taking action a in state s.
--InteriorSeparator--
easy
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is the relationship between V*(s) and Q(s,a)?
--InteriorSeparator--
(right) V*(s) = maxₐ Q(s, a).
(wrong) Q(s,a) = maxₐ V*(s).
(wrong) Q(s,a) = V*(s) + r(s,a).
(right) V*(s) represents the maximum Q-value attainable from state s.
--InteriorSeparator--
hard
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the training goal for the autonomous agent (robot)?
--InteriorSeparator--
To perform actions, observe consequences and learn a control strategy.
--InteriorSeparator--
easy
--InteriorSeparator--
2
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What kind of issues may lead to the usage of Temporal Difference Learning?
--InteriorSeparator--
(right) Q Learning reduces discrepancy between Q successive estimates.
(wrong) V*(s) = maxₐ Q(s, a).
(wrong) It requires little computation power to calculate.
(right) n-steps calculation are not convenient.
--InteriorSeparator--
hard
--InteriorSeparator--
21
--FlashCardSeparator--
}], role=model}, finishReason=STOP, avgLogprobs=-0.19401469179221037}]