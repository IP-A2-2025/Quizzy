***************Beginning Page***************
***************page number:1**************
1
MACHINE LEARNING
Liviu Ciurtuz
Department of CS, University of Ia§i, Rom-Snia

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2
What is Machine Learning?
0 ML studies algorithms that improve with experience.
_,_/
learn [ruin

Torn lvlitchell’s Deﬁnition of the [general] learning problem:
“A computer program is said to learn from experience E with respect
m some class uf tasks T and performance measnm P, ifits perfbrlllallce
01! tasks iii T, as measured by P, lliiproves with experience E."

u Examples of [speciﬁc] learning problems (see next slide)

I [Liviu Ciortuz:] ML is data-driven programming

0 [Liviu Ciortuzz] ML gathers a number of Well-deﬁned sub-
domains/disciplines, each one oftheni aiming to solve in its
own way the above-formulated [general] learning problemv

***************Ending Page***************

***************Beginning Page***************
***************page number:3**************
‘a
What is Machine Learning good for?
I natural language (text 86 speech) processing
I genetic sequence analysis
0 robotics
0 customer (ﬁnancial risc) evaluation
I terrorist threat detection
I compiler optimisation
I semantic web
0 computer security
0 software engineering
l computer vision (image processing)
u ctc.

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
Related courses at FII 4
0 Artiﬁcial Intelligence
0 Geneiie Algorithms
a Artiﬁcial Neural Neiwerks
0 Probabilistic programming
o Special Chapters of Maenine Learning
a speeial Chapters (if Artiﬁcial Intelligence
0 Special Chapters of Artiﬁcial Neural Networks
0 Data Mining
0 Nature-inspired computing methods
a Big Data Analytics
a Image Processing
0 Cumputel' Vision
o Bieincerrnaiies

***************Ending Page***************

***************Beginning Page***************
***************page number:5**************
A multl-domaln Vlew
Algnrilhms
Maihemalics
Aniiigial “muse
lmelligence Sysiems
imncepl lamina! ‘ (Knowiedge Discovery
\ Muchlne in Bauhaus!
inning ‘
Stallsncal Pattern
/Y Learning Recognition \
Slalislics Enginsering
(model iimng)

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
r»
The Machine Learning Undergraduate Course:
Plan
0. Introduction to Machine Learning (T. Mitohell. ch. 1)
l. Probabilities Revision (Ch. lVIzlnning & H. Schlitze, cll. 2)
2. Decision “eels (T. Mitnhell. (:h. a)
s. Bayesian Learning (T. Mitchell, ch. 0)
(and the relationship with Logistic Regression]
4. lnstzmce»b'<lsed Learning (T. Mitchell, ch. s)
5. Clustering Algorithms (Ch. hlanning KL H. Schiitzc, Ch. 14)

***************Ending Page***************

***************Beginning Page***************
***************page number:7**************
7
The Machine Learning Master Course:
Tentative Plan
l. Doeision limes: Boosting
2. Support Vector lVIachines (N, Cristianini (Q J. Shawe-Taylor. 2000)
a. Computational Learning Theory (T. Mitelieu, en. 7)
Probabilities Revisinn (on. Manning 3t H. Schl'itze, ch. 2)

4. Gaussian Bayesian Learning
5. The EM algorithmic schemata (T. Miteliell, on. 6.12)
5. Hidden Marknv Models (cn. Manning at H. Schiitze, ch. 9)

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
8
Bibliography
O. “Exercigii do Invﬁian) autolnalﬁ"
L. Ciortuz. A. Munteanu E4 Bﬁdﬁriu.
Iaq'i, Romania. 2023
www.info.liaic.rn/~cinﬂuz/NIL.eX-hnok/editia-ZUZIif/eX-hnnk.205ept2023.pdf
1. “Machine Learning"
Tom Mitchell. McGraw'Hiil, 1997
2. “Blacilinc Learning Foundations"
Tcahu Ju. Springer, 2021
a. “Deep Niachine Learning Foundations“
Teaho Jo. Springer, 202:1
4. “Founda'inns nr Staﬁscical Natural Language Protessing"
Chris'npher Manning, Hinrinh thii'ze. [\{IT Press, 2002
5. "Support Vector Niachines and DthEr kernel'baserl learning methods"
Nelln Cristianini, John Shawellhylor. Cambridge University Press, 2000.

***************Ending Page***************


***************Beginning Page***************
***************page number:9**************
f)
A general schema for machine learning methods

tesi/generalizaiion
delta
training machine [earning —> data
data algorithm model

predicted
classification
“We m drawning m information but starved f0?‘ knowledge.”
1...... Mimi... --Meu..,...d.~~ hm, 1m

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
10
Basu: ML Termlnology

1t instance 1_ instanee set x

eoneept I g X, or l- 1 X A (0,1;

example (labeled instance): (r. (1(1)); positive examples, neg. examples
2i hypotheses ll : X e {0,1}

hypotheses representation language

hypotheses set 11

hypotheses consistent with the eoneept e m r) I [my cxamplu (1.r(r))

version space
s. learning I train + test

supervised learning (classiﬁcation), unsupervised learning (clustering)
4t erron, : HT e X. hm 74 mm

training error, test error

aeeuraey, precision, reeall
5t validation set, development set

11-fold eress-valid-ation, leave-one-out cross-validation

overﬁtting

***************Ending Page***************

***************Beginning Page***************
***************page number:11**************
11
The Inductive Learning Assumption
Any hypothesis found to conveniently approximate the
target function over a sufficiently large set of training
examples
will also conveniently approximate the target function
over other unobserved examples.

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
12
Inductive Bias
Consider
- a concept learning algorithm L
n the instances X, and the target concept r
- the training examples u‘ a (<J,l,(l,))}.
l Let L(l,,.D() denote the classiﬁcation assigned to the instance 1,, by L
after training on data up
Deﬁnition:
The inductive bias nf L is any minimal set of assertinns I? such
that
(v.1; E X)[(B v 0‘ v n) P Lha- 0(11
for any target concept 1 and corresponding training examples up
(.4 , 13 means .4 logically entails B)

***************Ending Page***************

***************Beginning Page***************
***************page number:13**************
13
iluumveivm
Nxmlhm 232555.352?
Tmmnwmuu mu“ WWW‘
Bimini
New mum Um vaum
SW H
Inductive systems
can be modelled by ewuui Mum mun
equivalent deductive
systelns Tmnml; WHIP!“ Chmﬁcaﬂnn mi
new mm m
“mm kmw“
New mum mm PM:
[MAUI]?! bm:
mad: zxplwir
Mum “Hm-m
e‘ (ml emu»

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
14
Evaluation measures in Machine Learning
Mmmy- A” I 4t%
H H H ipiinifpiﬁl
c h preciszon: P I tp ‘ff’,
‘an ywrnll (nr: .sensitivmty): R : Til’)?
v i 2 I’ >< R
m r-mmm. r a W
spemﬂmiy: sp : Till];
/,, , true pﬂiiLiVBS ~ i
1,, , false positives f””’“‘*' i 4%" +
m i Lruu negative:
[H false negatives Mathews COWllli'ion Uﬂﬁﬁ'iﬂirnt:
MCC: a
\ (w + Ma" + WWW + ln/Wm + I»)

***************Ending Page***************

***************Beginning Page***************
***************page number:15**************
11'!
Lazy learning vs. eager learning algorithms
Eager: generalize hefure seeing query
Q ID3. Backpropagation, Naivo Bayes, Radial basis function not-
works, . v.
0 Must create global approximation
Lazy: wait for query before generalizing
o k-Nearest Neighbor, Locally weighted regression, Case based rea—
soning
0 Can ereate lnany loeal approximations
Does it matter?
If they use the sarne hypothesis spaee 11, lazy learners ean represent
rnore complex functions
Ego, a lazy Backpropagation algorithm can learn a NN which is dif-
ferent for eaeh query point, compared to the eager version of Back_
propagation.

***************Ending Page***************


***************Beginning Page***************
***************page number:16**************
1!»
Basic Machine Learning Algorithms

***************Ending Page***************

***************Beginning Page***************
***************page number:17**************
17
ID3 algorithm: a simpliﬁed version
Ross Quinlan, 1979, 1936
START
create the root mull’;
assign all exalnples t0 the rout node;
Main loop:
14 A <~ the “best” decision attribute for the ncxt "(NIH
24 for each value of A, create a new descendant of muli';
sl sort training examples to leaf nudes:
4l if training examples are perfectly classiﬁed, tllen
STOP;
else iterate over the new leaf nodes

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
AdaBoost algorithm [Yoav neurid, Robert Schnpire, 1996, 1997, 1999] 18
Consider 7H training exemples s: ([1 run) v (in nil), Where in c X and I/l I i l l1)~
Siipposi‘ we how o weak looming algorithm A whieii prouures n hypothesii h 1 s lei +l}
given any disiribuiion I) oi examples. \

s Begin with a uniiorm disiribuiion vim e w , e l, .m.
. At eachiteraliou l : I, ,T, "'
. l'un lne weak learning alga i on the disiribuiion n, and produce ine hypothsis In;
NW (1): Sinrp ,4 is a weak learning algoriunn, the pmducPr] liypriinesis in at rounrl i, is
only slightly better ilisn random guessing, ssy. by a margin 1i:
|
e, : would : meow? mil] : 3 es,
Notv (2); ii oi s certain iierslion i, g T the weak olsssisler 4 cannut produce s hypoinesis
beiier insn random guessing (Len q, : n) or it produces a iiyponiesis ipr which s, : u.
llien llie AdaBoust algorithm sliould be slopped.
- updsie ilie distributiun
l ,
ppm]: ulli) ("'"l" "ll forl:l ,m (l)
zl
wliere in ':" :lu L V :‘ . and z, is [he normalizer.
o in ilio end, deliver ll, \XVVK(ZYL\IMII1) us ilio luarnuil liypoiliosis, wliioli will act es u
“wiglilml vvuijoy'n'ty mite.

***************Ending Page***************

***************Beginning Page***************
***************page number:19**************
19
AdaBoost as an instance of a more general stepwise algm-ithm
Inpnti s, L71‘ ti, where
S I (tn. w) “(mm i/m) is the training datasetT with
1/, e (441)
T is the nnniher nr itenetinns tn he exeented,
H is e set of “hypotheses'K
(wt/'1 is u “hiss” / mist“ / "risk" functiun:
Prucedure:
\Vvlllahle the dassiller by iskmg Mi] : n (the constant nmsnon 0]‘
and um) : i/m for t : 1. m
For I 1 la T do
1 Compute
(Mm) i argmHM-e'i heft ZZZ, WW‘ fieibrt) + tirt[r,)y
2 Uvdate (he dassiller
M’) i item>+mrim1
(ompyle D, t i
end for
retum me dassiller S!gll[_/y (1))
Note: At each step. the algorithm greedily adds h hypothesis h e at to the
current combinad hypothesis to minimize the O-loss.

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
, t 20
A Generalized AdaBoost Algoritlun
inn-r, 2003 fall, Tommi Jaakkola, Hw4, pt. 2.14;}
Initialization: i1‘,"':i,/mnnd ruin) :u for t : i. mt
Loop: rot-1:1 to l'do:
Step 1: Find n himiiit-i my 9.) pol-forming hhttnt thnh chm-mu Wit tho, wright-d mining hm“:
K, i Z u, y,h\4,,5)ii(lizu‘ V/t/A(l, 1%))
. MM, t,‘ , I ‘
Nuk Minimixing 5, it minim h, ﬁnding it tt..t Minn-"m Ni ltm MW‘ m m» wnm
1,1,. (1,; 7 %Xtti\_<v‘/. .ww/w hit‘ my
Stzp 2: Set the vanes m fur the new ColnpDnEnL by minimizing the uverall empirical 1055:
(t, : nignnti m. 0,1
Slap s: Recolnpute the nurmalized weights fur the next iterALiun accnrding tn
ii,""‘:it, amt /, ,(t,1+y,,t,h(it9,>) min, .m,
_,—/
M1,.‘
where t, is chosen so that 2;" , ii'k“ i 1.
Output: {,-

***************Ending Page***************

***************Beginning Page***************
***************page number:21**************
21
The Naive Bayes Classiﬁer
- Assume chm mp aﬂrihums Q 1.‘ _,.,, > that describe inshlnces are mndmnmny
indeandent w“. m the given classiﬁcatinn:
PM, 0-1,..11,,\w,) : H mam)
- Training prnnadure:
[Y’\l\'i.,B’\‘1imlA-I1|!N(t‘1r|mp](‘§)
for each value w, of the output attribute
PM) H estimate PM)
for each value u‘ ofeach input attribute a
PM ,) ‘ estimate PM”)
o The decin'on rule ofthe Naive Bayes classiﬁer is:
‘40' 1-,0' PM "2 u")
‘ M,
: Argnmxl’[rn.ﬂg 11,, mm”; I‘ugnmxnP(11‘\A'J]P(1r,) : W8

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
22
. . . Piy:lx:ty:n4:)=
Loglstlc Regresslon m :me,>:1 net where
Given the dataset “Wm/<1»). Lewd/m)“. . (n">.m“1)), wry H'W e ‘It,
when: each vuctur t"! rnte 11 feature! / attributes, and e _
W e (0,1) [or 1 : in...“ its complete log-likelihood is: t "1' meg“, ‘:‘ w I Wm-
tn, mvmwml:\n1[7%,"! ,/"1:hvllm WM)“ ww '>> mmnmgmj
n nﬁn “v/“WWv+mlllrnﬂww[Iv/+1, g t,
Nete thnt I‘ does nnt depend nn tne paralneter u’.
It can he shnwn that the conditional Ingnlikelihnnd function ['(ur) enn he; written as:
‘t Nnt. tnnt nun) n n cvnni-enlrnpy,
. m t w , m , t m
4,1 Sta-"ﬁnd, 1015 n“, ‘new N8, "we
u'Laqu I ntgmnmn) I Mgnnnkaw» (2) p,_ 5c‘

***************Ending Page***************

***************Beginning Page***************
***************page number:23**************
23
See the analogy with the sigmoidal perceptron
CMU, 2011 fall. Eric Xing, HWL pr. 3.3
‘I n, “Fl
n 1+,

***************Ending Page***************


***************Beginning Page***************
***************page number:24**************
24
The k-Nearest Neighbor Argorithm
Evelyn Fix. Joseph Hudges, 1951; Thomas Cuver, Peter Hart, 1961
'lYaining:
Store all training examplﬁs.
Classiﬁcation:
Given a query/test instance I,”
ﬁrst locate um nearest training examples I, v. m
mun cstilnutc 1m):
- Lake a vote among its 1.- nearest neighbors
A
j<1,,) H @an 1m. F.»
M p‘
when: 1( , i> the well-known imlicutur 1-Way".

***************Ending Page***************

***************Beginning Page***************
***************page number:25**************
2r,
The Bottom-up Hierarchical Clustering Algorithm
Given: a m X : (n, HM} ul'ubjcuta
a function >irn: mx) >< P(X) » R
for I :1 n do
y, i (M) and

0 (1|. .m}
J : H +1
while \r >1

(WAN!) : (“WWW h “(W sim(1,,,¢,)

1, :(m mw

(‘:I7\(Ivr,-w)‘~(u)

I I / i l

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
2n
The k-Means Algorithm
S. P. Lloyd, 1957
Given: a set X , iii. .1“) g 11'",
a distance measure ,1 on R'”,
a lunetiun [or eunipuling Lhe mean ,l 'PUU") e 72'".
liuilt I. elusters sn as tn sntisly n certain (“Stopping”) rriterian (e.g., maxi-
mization of group-average similarity).
Procedure:
Select (arbitrarily) i initial eenters A, /k in 71"‘:
while the shipping eriterinn is uni satisﬁed
for allclustersr-,doueiu\Vf,1I(|,.f/)Sll[ii.[1]iend
for all means I, do /, Him end

***************Ending Page***************

***************Beginning Page***************
***************page number:27**************
21
K-Means algorlthm revnslted (I)

. s. .......|......> ... mod m"... cmmvﬂll Mm“... ... .1.....-... Rm... w... . v...» [\i
“h... J‘. 1- ~= .. <. Z u- K). relormulal] ca .. “1...... .¢ 09...... mm... mm...
A | | I .1 ducmnznxprwondavmu

. um. u... ... v. m... "m... .i .. . . . . .

.L... . ... m. é ,.. n .. v 0..-... .... .c... d... .......... .. 1...... 9.....- m.
" d ‘ " P “t .mom (Indirect) mum... Inna-chute“.
p... 1.
CMclllean w “Mel: H 2
7\L/‘l*2H11*/M.H .
w k {. .... ...,...J..1:...,r..,.u1 w“ ‘L.
L , “pm... 1......“ v... mm“... mum... C...-
1. um 4. .. .1"... ale e .. ..... arbi- ,
,m my, (“gm (4....» *gm “my,” ._ m... .. m... v... ...... we mm...“ m...
.9"... X. P... .-
Pm, ,. m"... .. “K... gang!» m. .1...“ L . 1......“
- I. cluster», C... ...........,.,..,. M... Minn»:
mm...“ .1 MM... mm... . m...
in... p... g.
7...... .w... j é C, 4.... 211.». > m PM"... .5......._1._.X_....,5...... m. “I.-. pm...
mlgncull ' .. pa... c... .. ............. ML ....
n.........,mm D... |....... ... e... W... m"...
>1. m. .1 mi... m... “in... d... n... L ... -. mm...“ ..
u. » z. .. r...“ ... ............ mm... .. m... 1. m... “ma...
'7] I ('lbrmmarﬁ), allfcl w rI-‘Dclﬁ dc la Pasul L
A... mm... mam... c“W... ,.,. 1......“- R...n....; L .. ..

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
2x
K-Means algorithm revisited (II)
L T
wzwlg ,u 15 —>-a§:"‘=.v. 314m." _.‘ 1:5 a c““‘=ammm MM";
c
“I mll'l 10:, m m mm aw, H)
M _ 2 Y 11"’ X‘ ‘ > ‘
u‘ 7 W ,0" '=Irgr;va(U" ', m

***************Ending Page***************

***************Beginning Page***************
***************page number:29**************
29
The General EM Problem
Approach
Start with W“ ah arbitrarily/eohveuleutly
Given ehoseu value of h,
Repeatedly
e observed data x e (I... ml
independently generated “sing 1. Use the observed data x eud the current
“m pmmmmizcd damn“, hypothesis ll"! to estimate [the proba-
ﬁlm/hypumm5 h» MM bllitles associated to the values oq the
unobserved variables z. and further oh
0 unobserved dutu z : (1r. Hem) compute their expectations, E[Z_

determine 2. The expected values uf the unobserved
varlehles z ere used to calculate an im-
/~ that (locally) maximizes Proved hypothesis h" ‘L based oh max-
l’(M/l)~ imizing the rheau 0f a log-likelihood
furretlour Ellumylmlx W], where y :
(tr, Hun) ls the eomplete (observed
and unobserved) data, Le. 1/, : (I, 2,),

fnr / i l m.

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
3U

The EM algorithmic Schema
l=l)

¢

hm a E[Z | x, hm]

“1|; ln P(X|h)
h“‘"=argmax 5mm hm’ [In P(X,Zlh)]
,. .

***************Ending Page***************


***************Beginning Page***************
***************page number:31**************
ADMINISTRATIVIA

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
32
Who is Liviu Ciortuz?
O Diploma (maths and CS) from UAIC. 1215i, Romania, 1985
PhD in cs from Unlverslté de Lille, France, 1996
0 programmer:
Bacﬁu, Romania (1955-1957)
a full-Limo rcscarchcr:
Germany (DFKI. Saarbrﬁcken. 1997-2001),
UK (Univl of Yurk and Univl uf Aberystwyth, 2901-2093),
ﬁance (INRIA, Rollncs. 20124013)
s assistant. lecturer, and then associate professor:
Univ‘ of Ia§i, Romania (19901997, 2003-2012, 201340113”

***************Ending Page***************

***************Beginning Page***************
***************page number:33**************
:13
Teaching assistants for the ML undergraduate course 2023
(fall semester)
. Com: dri Anca Ignat (H. Image processing)
hups:/,/profs info uaic ro/'~ancai/ML/
- Sebastian Ciobann (PhD; Amazon)
hupsz//'sites.google.mm/'view/semi'narm\
I Andi Munteanu
(PhD student at UAIC, and research assistcnt at Univ. 0f Cambridge)
- Cristian Simioncscu (PhD student; Nexus)
0 Ramona Albert (PhD student: Amazon)
0 Stefan Pangiru (NISC; Mambu)
. Corina Diuiitriu (MSc student)

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
an
Grading standards for the ML undergraduate course 2023
Obiectiv: Invégare pe tot parcursul semestrului!
Puncta]
Tl T2 S P1 P2
Test: 5p Test: sp Seminar: 8p Ex.panial:10p Ex.panial:10p
Minim:1.5p Minim:1.Sp Minim: 2p Minim: 2.5;, Minim: 2.5;,
Prelenls la curs: recomandsla!
Prezema la seminar: obligatorie!
Penallzare: u.2p pentru llecare absenla de la a doua Incolo!
Nma=(lﬂ+T1 +T2+S+P1 +P2)/5
Pamm pmmovare: Nola >= 4.5 <=>T1 +12 + s + F1+ P2>=12.5

***************Ending Page***************

***************Beginning Page***************
***************page number:35**************
:15
REGULI generale pentru cursul de Invigare automati
de la licengzva
Regulila de organizare a mmnm de invawe Amnmais (2,151., Ivlarhine Learning, ML),
Semi I, sum speciﬁcate in ﬁw discipliuei
http://profs.info.uaic.ro/~cionuz/ﬁsa-disciplinei.pdf
- Bibliugruﬁc minimum: vczi alidl: #8
. Planiﬁrarea materiei, pentru ﬁecare sspiamana (curs + seminar):
http://prof§vinfo.uaicvro/~ciortuz/what-you-should-knowipdf
- Prezenia la curs: recomandati!
- Rzgula o: Prezenga la seminar: obligatorie!
Pemnl ﬁecare absenga Ia seminar‘ incepénd de la a doua absenga incolm se aplicé o
pcnalizare/dcpunctarc do 0.2 punctc. (vm [onnulu do IlOLiu'C.)
Regulile se aplic§ inclusiv studenﬁlor reinmatriculaﬂi.
. Sépléménal margea, mm orele 18 20, in sale (1309 se va ﬁne “n
seminar :uplimentar‘ dcsmm pemru acci studenﬁ care sum foal-Lo imeresugi dc acest
domcniu gi cﬁruru 1i.- pluc dumunmugmc lnuturnuﬁcc. (vmqi sccgiunilc "Advanced
issues“ din ducurnentul http://profsiinfu.uniuiro/Ncim".uz/whatryou'should'know,pdf.)

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
an
REGULI generalc pcntru cursul dc Inv5§arc automaté dc la liccnﬁ:
(conL)

Regula 1: Prntrn snminariL nu n nnmu lnutﬁri nln mmnmilnr dn la n grupﬁ la aha, drciﬂ
in mum! grupelur care an acela§i zuistenl. / Fromm respunaquil Lle selninar.

Regula 2: Nu se m echivam-i d2 punctaje pentru studengii care nu Bu promuval cursnl in
anii prccrdcnﬁ.

Regula 3: Profcsnrul vcspuniabil pcntru acrst curs, Liviu 00mm
NU Va réspunde 18 email-uri care pun intrebih'i pentru care raspunsul a fast deja dat
i ﬁe in arcstc ilidvuri,

ﬁe pe siL+>ul Piazza dedicat Henna curs:
m» ,//piazza com/Info uachcNaHZUZCi,/m\2023l/humew
ﬁe 18 @an.

Recomandare importanté (1) La ﬁuran' an» g1 seminar, ,mnnnngii vnr avra rungerEa
<12 Easemlﬁn d1 mam nutomntﬁ" (de L. cam-n et Bl) i v5 recomand§m sé imprimagi
cnpiwlelc Clanﬁmm bavw'nnd, may“ unwra 1w wmmw, Arbon' dc dmm si ("U/5'
[211'an gi evemual slide-“rile indicaLe in slide-u! nnnmor.

Recomemdare importanté (2) Consultagi sﬁptﬁménal docnmemul
whairyuu'shomd'know paf din pmnn dc Rcsursc, dc no sittrul Piazza dcdicat arcstui curs.

***************Ending Page***************

***************Beginning Page***************
***************page number:37**************
:17
REGULI generale pentru cursul de inviQare automaté de la licenﬁ:
(conL)

- Slide-uri de imprimat (in acaxali urdine qi, <1: preferal, COLOR):
thpl//profs‘infuvuaic,m/~ciortuz/SLlDES/l'oundationsvpdf
thps://pml's.inl'o.uaicvro/~ciortuz/ML.ex-bouk/SLIDES/LIL.ex-book.SLlDES.Pmbstat.pdf
https://profs.info,uaicao/~cioﬂuz/Mme-bnnk/SLIDES/ML-cX-booksLIDES-DT-pdf
thpsz//pml's.info.uaicvro/~ciox'Luz/ML.ex-bouk/SLIDES/LIL.ex-book.SLlDES.Bayes.pd!
https://Drufs.info.uaic.ro/\cionuz/MLex'bouk/SLIDES/ML‘ex'book.SLlDES.IBL.pdl'
[lupu//prul'.~.inl'u.uaic.ru/~ciurtuz/ML.ex-buuk/SLIDES/ML.ex-buuk.SLIDES.Cluster.pdf
(Amman; acrst set do slidvuri mm ﬁ acumlim Pl‘ nammul semestrului!)
- De inlprilnal. (ALB-NEGRU):
h“p://prof)‘.infn.uair.rn/~rinﬂuz/SLIDES/rnlﬂ.prlf
thp://pru{§.inlu.uaicJu/Nciurtuz/SLIDES/mlled!
thp=//profs‘infuvuaic,ru/~ciortuz/SLlDES/ml6‘pdf
thp://pruf§.infu.uniuJu/~ciurtuz/SL[DES/ml!.pd{
http://profs.infn.uaic.rn/~ciortuz/SL[DES/clusv.er.pdf

***************Ending Page***************

