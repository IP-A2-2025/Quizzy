***************Beginning Page***************
***************page number:1**************
1.
Mathematical Foundations
— Probabilities, Information Theory
and ()ptlmlzatlon Methods —
for Machine Learning
Contains:
Random Variables: ex. 9, 10, 101, 19
Probabilistic Distributions: ex. 25, 112, 32, 20, 34, 38, 40, 114, 29
Estimating the parameters of some probability distributions: ex. 43, 124,
44,50,134,51,52,53
Information Theory: ex. 56, 57, 61, 55, 141, 142, 143, 144, 64, 63, 58, 146, 147, 62
Kernel Functions: ex. 70, 74, 75
Optimization Methods: ex. 79, 80, 82, 87, 88
from the 2023f version of the ML exercise book by L. Ciortuz et al.

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2.
Random Variables
Some proofs

***************Ending Page***************


***************Beginning Page***************
***************page number:3**************
3.
E[X + Y] I E[X] + E[Y]
where X arid Y are rariderh variables 0f the same type (i.e. either discrete er cont.)
The discrete case:
E[X +Y] I 220%’) +Y(W)) ~ PW)
I WEXW) - PW) + ZYW) ' PW) I ELK] + EV]
The continuous case: w w
E[X + Y] I L/yﬂv +y)pxy(w,y)dydw

I L fy wpxy<ili>y>dydw+ L fy ypxy(w,y)dydw

I Lw/l/pXi/(wiymydw+/yy/wpxy(w,y)dwdy

z / mew + / yin/(wdy : Em + Em

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
4.
X and Y are independent :> E [X Y] : E [X ] - E [Y],
X and Y being random variables of the same type (i.e. either discrete or continuous)
The discrete case:
E[XY] I Z Z myP(X I 33,3’ I y) I Z Z xyP(X I 1B) - P(Y I y)
mEVaKX) yGVaKY) mEVaKX) yEVal(Y)
I Z a:P(X I a3) Z yP(Y I y) I Z 95P(X I @Em I E[X] -E[Y]
wEVal(X) yGVal(Y) mGVaKX)
The continuous case:
E[XY] I //wy p(X I w,Y I y)dydw If/wy P(X I w) 'p(Y I y)dydw
a: y w y
I /$p(X : x) (fyp(Y I y)dy) dx I /$p(X :$)E[Y]da:
iB y x
I E[Y] for p(X I my; I E[X] -E[Y]

***************Ending Page***************


***************Beginning Page***************
***************page number:5**************
5.
Discrete random variables:
independence, conditional independence, conditional probabilities
CMU, 2015 spring, T. Mitchell, N. Balcan, HWZ, pr. 1.d

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
6.
Let X, Y, and Z be random variables taking values in {0,1}.
The following table lists the probability of each possible assign-
ment of 0 and 1 to the variables X, Y, and Z:
-——
-----
Y:0 1/15 1/15 4/15 2/15
Y : 1 1/10 1/10 8/45 4/45
For example,
P(X :0,Y I 1,Z: 0) I 1/10 and P(X:1,Y:1,Z:1):4/45.
a. Is X independent of Y? Why or why not?
b. Is X conditionally independent of Y given Z ‘.7 Why or why not?
c. Calculate P(X z 0 | X + Y > 0).

***************Ending Page***************


***************Beginning Page***************
***************page number:7**************
7.
Answer
a. N0.
P(X I 0) I 1/15 + 1/10+ 4/15 + 8/45 I 11/18,
P(Y Z 0) z 1/15 + 1/15 + 4/15 + 2/15 z 8/15,
and P X O Y 0 1 15 4 15 5
P(X:0,y:0)zgzgzn
P(Y I 0) 8/15 8
Since P(X I 0) does not equal P(X I O|Y I 0), X is not independent of Y.

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
8.
b. For all pairs y,z € {0,1}, we need to check that P(X : OlY : y,Z : z) : P(X : O|Z : z).
That the other probabilities are equal follows from the law of total probability.
1/15
PXIOYIOZIO : —:1 2
I I ’ I 1/15+1/15 /
1/10
PX:0Y:1 Z:() : —:1 2
I I ’ I 1/10+1/10 /
4/15
PX:0Y:0Z:1 : —:2 3
I I ’ I 4/15-l-2/15 /
8/45
PX:0Y:1 Z:1 : —:2 3.
I I 7 I 8/45+4/15 /
and
1 15 1 10
P(X:0|Z:0) : #:1/2
1/15+1/15+1/10+1/1O
4 15 8 45
P(X :0|Z: 1) : $ I 2/3.
4/15+2/15+8/45+4/45
This shows that X is independent of Y given Z.
c' 1/10 8/45
+
PX:0X Y>0 :—:5 12.
< | + > 1/15+1/10+1/10+2/15+4/45+8/45 /

***************Ending Page***************


***************Beginning Page***************
***************page number:9**************
9.
The correlation coejﬁcz'ent 0f two random variables:
two properties
Sheldon Ross
A First Course in Probability, 5th ed., Prentice Hall, 1997 , pag. 332

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
10.
Pentru doué variabile aleatoare oarecare X §i Y avénd Var (X) 75 O §i Var (Y) 75
0, coeﬁcz'entul de corelatz'e se deﬁne§te astfel:
def. Cov X, Y)
O'X O'Y
unde 0X “25' \/ Var(X) §i 0y nét' \/ VGT(Y),
a. Sé se demonstreze c5 —1 g p(X, Y) g 1.
Consecinjéi: C021 (X, Y) € [—0X Uy, +0X 0y].
b. Sé se arate c5 dacéi ,0(X, Y) : 1, atunci Y : aX —|— b, cu a : Uy/UX > 0.
Similar, dacé p(X,Y) I —1, atunci Y : aX + b, cu a : —0y/0X < O.
Observaﬁe: A§adar, coeﬁcientul de corelaigie reprezinté 0 ,,m€isurii“ a gradului
de ,,dependen§5 liniarii“ dintre X §i Y.

***************Ending Page***************


***************Beginning Page***************
***************page number:11**************
11.
Indicagii

1. La punctul a, pentru a demonstra inegalitatea p(X, Y) Z —1 vii sugerém

X Y
sé dezvoltagi expresia Va?"(— —|— —) folosind urmétoarele douéi proprietfilgi,

(7X (TY
valabile pentru orice variabile aleatoare X §i Y:

Var(X —|— Y): Var(X) + VLZT(Y) —|— 2000(X, Y)
§i
0011(aX, bY) I abCov(X, Y), pentru orice a, b € 1R.
. . . . A . X Y v
Ap01 ve§1 proceda $1m11ar, dezvoltand expresm Var(— — —), ca sa
O'X O'Y

demonstraﬁ inegalitatea p(X, Y) g 1.
2. La punctul b, veigi tine cont de faptul 05 pentru 0 variabilii aleatoare
oarecare X, avem Va?" (X) : O dacé §i numai dacé variabila X este constantéi.
(Mai precis, existéi c E R astfel incét P(X I c) I 1, unde P este distribufgia de
probabilitate consideraté la deﬁnirea variabilelor din enun§ul problemei.)

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
12.
Raspuns
a. Pentru a demonstra inegalitatea p(X, Y) Z —1, procediim conform Indicatiei
1:
X Y X Y X Y
Var(— + —) : Var<—> + Var(—) + 20022 <—, —)
0X UY UX UY UX UY
1 1 1 1
: —2 VCLT(X) —|— —2 VGT<Y) —|— 2——C011(X,Y)
0X UY 0X 0y

I 1+1+2p(X,Y>:2[1+p<X,Y>1-
A A . . . v v v X
Intrucat Var(X) Z O pentru orlce varlablla aleatoare X, rezulta ca Var(— +

(7X

Y
—) 2 0, deci 1 + p(X, Y) 3 0, adicé p(X, Y) 2 _1.
(TY
1n mod similar, putem s5 arétém c5

X Y

Var(— _ _) I 2p _ p(X, Y)] 2 0,

O'X O'Y

deci p(X, Y) g 1.

***************Ending Page***************


***************Beginning Page***************
***************page number:13**************
13.
b. Dacéi p(X, Y) : —1, atunci din primul calcul de la punctul a va rezulta c5
X Y _ v A A v v X Y . . v
Var <— + —) I O. Stim ca aceasta se intampla daca — + — este 0 variabila
O'X O'Y UX UY
v o v o o o v / A A X Y /
aleatoare constanta, adica, mai preCIS, eX1sta a E R astfel incat — + — I a
O'X O'Y
cu probabilitate 1. Prin urmare, putem scrie Y I day — U—YX. Rezulté c5
O'X
0
Y:aX+b, unde a:——Y <0 §i bIa'ay.
5X
in mod similar, dacé p(X, Y) I 1, atunci din a1 doilea calcul de la punctul a va
X Y X Y
rezulta c5 Va?" (— — —> I 0, deci existé a” E R astfel incét — — — I a” cu
O'X UY (7X UY
0
probabilitate 1. Agadar, Y I —a”0y + —YX. Renotémd, obginem Y : aX + b,
(7X
cu a: U—Y >0 §i b:—a”0y.
0X

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
14.
Probabilistic Distributions
Some properties

***************Ending Page***************


***************Beginning Page***************
***************page number:15**************
l5.
B‘ ' ld' ‘b ' . b . diff-Cr 7" n—r
1nom1a lstr1 utlon. (7", n, p) - n p (1 — p)

Signiﬁcance: b(’r; n, p) is the probability of drawing r heads in n independent
flips of a coin having the head probability p.
b(r; n, p) indeed represents a probability distribution:

o b(r;n,p) I C£pT(1—p)"_r Z 0 for all p € [0,1], n € N and r € {0,1,...,n},

o 22:0 b(r;n,p) I 1:

(1 — p)” + Cirﬂ — NH + - - - + CZ_1P”_1(1— p) +29" I [p + (1 — pH" :1

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
16.
Binomial distribution: calculating the mean
de. TL
Elbtrmmﬂ =f Zr . w: mp) =
TIO
= 1'Cip(1 — p)"_1 + 2 - (131920 — MM + - ' ' + (n — 1) 'CZ_1P"_1(1— P) + n ' p"
= p [C310 — MM + 2 ‘ CZPU — MM + ' ' ' + (n — 1) 'CZ_1P"_2(1— p) + n mm]
I np [(1 — pYH + Ci_1p(1 — PYH + - - - + CZi12P”_2(1 — P) + 02:11PM] (1)
I np[p + (l — p)]”_1 I np I n E[Bern0ullz' (10)] (2)
For the (1) equality we used the following property:
k0: I k n! I n! I n(n—l)!
k!(n—k)! (k—1)!(n—k)! (k—1)!(n—1—(k—l))!
I nCSiNk I l,...,n.

***************Ending Page***************


***************Beginning Page***************
***************page number:17**************
l7.
Binomial distribution: calculating the variance
following WWW.proofwiki.org/vviki/Variance_of_Binomia|_Distribution, which cites
“Probability: An Introduction”, by Geoffrey Grimmett and Dominic Welsh,
Oxford Science Publications, 1986
We will make use of the formula Var[X] I E[X2] — E2[X].
By denoting q : 1 — p, it follows:
d f n n n(n l) (n 7“ + 1)
2 , 3- 2 rrn—r_ 2 — — T'n-T’
Elb (737149)] — Zr Cnp q —21" —¢~! p q
r20 TIO
n TL
(n_1)"'(n—7n+1)rn—r_ 'I“—1 Tn—7“
I ZTanq —2TnCn_1pq
r21 T21
'TL
1":1

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
18.
Binomial distribution: calculating the variance (cont’d)
By denoting j I r — 1 and m I n — 1, we’ll get:
EleO‘; MM I "PZU + 1) Gin Mm”
3:0
I npl ZjCilqum” +20%ijWj]
j=0 j=0
_,—/ W
Elb(r;n—1,p)17 Cf-(Q) 1
Therefore,
Elbzﬁ“; MM I WK” — 1)}? + ll I H2192 — 71292 + Hp-
Finally,
Varle I Elb2(r; MPH — (EWT; n,p)l)2 I 712292 — 71292 + np — H2292 I 7110(1 — p)

***************Ending Page***************


***************Beginning Page***************
***************page number:19**************
19.
Binomial distribution: calculating the variance
Another solution

o se demonstreaza relativ usor ca orice variabila aleatoare urmand
distributia binomiala b(r;n,p) poate ﬁ vazuta ca o suma de n vari-
abile independente care urmeaza distributia Bernoulli de parametru
29;“

0 stim (sau, se poate dovedi imediat) ca varianta distributiei Bernoulli
de parametru p este p(1 — p);

0 tinand cont de proprietatea de liniaritate a variantelor — Var[X1 +
X2 . .. + Xn] I VaﬂXﬂ + Var[X2] . .. + Var[Xn], daca X1,X2, . . .,Xn sunt
variabile independente —, rezulta ca Var[X] : np(l — p).

a Vezi www.proofwiki.org/wiki/Bernoul|i_Process_as_Binomia|_Distributi0n, care citeaza de asemenea ca sursa “Prob-

ability: An Introduction” de Geoﬁrey Grimmett si Dominic Welsh, Oxford Science Publications, 1986.

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
20.
The categorical distribution:
Computing probabilities and ewpectations
CMU, 2009 fall, Geoff Gordon, HWl, pr. 4

***************Ending Page***************


***************Beginning Page***************
***************page number:21**************
21.
Suppose we have n bins and m balls. We throw balls into bins independently
at random, so that each ball is equally likely to fall into any of the bins.
a. What is the probability of the ﬁrst ball falling into the ﬁrst bin?
b. What is the expected number of balls in the ﬁrst bin?
Hint (1): Deﬁne an indicator random variable representing whether the i-th
ball fell into the ﬁrst bin:
X‘ _ 1 if i-th ball fell into the ﬁrst bin;

Z — O otherwise.
Hint (2): Use linearity of expectation.
c. What is the probability that the ﬁrst bin is empty?
d. What is the expected number of empty bins? Hint (3): Deﬁne an indicator
for the event “bin j is empty” and use linearity of expectations.

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
22.
Answer
a. It is equally likely for a ball to fall in any of the bins, so the probability that ﬁrst
ball falling into the ﬁrst bin is 1/11.
b. Deﬁne X3 as described in Hint 1. Let Y be the total number of balls that fall into
ﬁrst bin: Y I X1 —|— . . . —|— Xm. The expected number of balls is:
E[Y] : ZE[X.3] : 21-13%- : 1) Im-l/nI m/n
1:1 1:1
c. Let Y and X,- be the same as deﬁned at point b. For the ﬁrst bin to be empty none
of the balls should fall into the ﬁrst bin: Y I 0.
_ 1 m
P(Y I O) I P(Xl I 0, . . . ,Xm I 0) I H21P(X3 I O) I (1 — 1/71)m I (n7)
d. For each one of the n bins, we deﬁne an indicator random variable Y]- for the event
“bin j is empty”. Let Z be the random variables denoting the number of empty bins.
Z I Y1 —|— . . . + Yn. Then the expected number of empty bins is:
n n n n n — 1 m n — 1 m
E121IE12Y11IZE1Y11 :21-P<1@>:Z T In T
3:1 3:1 3:1 3:1

***************Ending Page***************


***************Beginning Page***************
***************page number:23**************
23.
The univariate Gaussian distribution:
1 _M
_ 2
Nu,0(~’17)— We 2”
The plot, when ,u I O:
1 e (—:c2)
— X —
0% p 202
0
34% 34%
0.1% 2% 14% 14% 2% 0.1%
:13
—30 —20 —0 0 20 30
Source: http://www.texamp|e.net/tikz/examples/standard-deviation/

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
24.
Proving that NW, is indeed a p.d.f.
1. NMUQU) Z 0 Van 6 R (true)
2. If; NH,U($)da: : 1
Note: Concerning the second property, it is enough to prove it for the stan-
dard case (,u I O, 0 I 1), because the non-standard case can be reduced to
this one:
Using the variable transformation v : U Will imply w : 011+,u and dx : adv,
so: U
00 00 1 _($—#)2 1 00 _(w—u)2
NaiEdiEI/ —e 202 dsc:—/ e 202 dx
/—oo M’ ( > m:_oo \/27TU \/27TU $:_OO
112 02
—1/OO _5d —1 foo _5d —1/OO N ()d
: e 0 1) I 0 6 v I $ $
\/27T0' U:_OO \/27TO' U:_OO \/27T $:—oo 0,1

***************Ending Page***************


***************Beginning Page***************
***************page number:25**************
The standard case: proving that N071 is indeed a p.d.f.: 25'
f e2dv I / e2da: -/ e2dy I/ / e 2 dyda:
v:—oo 1;:—oo y:—oo ac:—oo y:—oo
$2 +y2
I // e 2 dydzr
R2
By switching from x,y to polar coordinates 736 (see the Note below), it follows:
oo U_2 2 oo 271' i oo i 27r oo i
f e 2 dv I/ f e 2 (rdrdd):/ re 2 (/ d9) drz/ re 2 Qﬁﬂdr
v:—oo T'IO 19:0 r20 6:0 r20
7,2 7n2 0° ,02 U2
I 27T/ re—2dr:27r(—e—2) :27T(1—O):27T:>/ e—2dv:\/27r:>/ Le—2clv:1.
T20 v:—oo 11:—oo V 27-‘-
0
Note: a: : rcosG and y : rsinQ, with r Z O and 6 6 [0,21r). Therefore, x2 + y2 I r2, and the
J acobian matrix is
8:1: (9:1:
— — cos6 —r sinQ
M : 8T 89 : I rcos2 6 —|— rsin2 9 I r Z O. So, dxdy I rdrdd.
803 6) @ @ sin 6 r cos 6
(9r 86

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
26.
Calculating the mean
df OO 1 00 _($—#)2
ENUw i'/ w/Vamdaz:—/ m-e 202 da:
Using again the variable transformation v : U Will imply:
0
v2 v2 112
1 OO __ 0_ OO __ OO __
EX I — 011+ 620dU:— 0/ ve 2dv+/62dv
[l @U/Q m <>¢§U< m Hm
v2 v2 v2 OO v2
1 /w( )_5d + foo _5d 1 _5 + /OO _5d
I — —0 —ve v e v :— —0'€ e v
\/27T —OO ILL _OO \/27T ,u _OO
‘,2’;
U2 :0
u °° —— M
I — e 2dUI—\/27T:

***************Ending Page***************


***************Beginning Page***************
***************page number:27**************
27.
Calculating the variance
We will make use of the formula Var[X] I E[X2] — E2[X].
OO 1 00 _ (w — #02
EX2I/ a32NU$dx:—/ x2-e 202 dzr
l l _OO H,() Tm _OO
Again, using the transformation v : U will imply 50 I 011+” and dzv I adv. Therefore,
0
112
ElXQl I —/1—/OO (WHOZ [5mm
271'0' —oo
@2
U OO 2 2 2 _—
I — 0 1) +20 11+ e 2 d1)
Tm /_OO( u u )
112 112 v2
I L 02 /OO v2 e—5dv+20u/OO 2) e_5dv+,u2/OO e_€dv
\/ 27l- —OO —OO —OO
222 v2

Note that we have already computed If; v6 2 d1) I O and If; e 2 d2) I \/ 27T-

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
28.
Calculating the variance (Cont’d)
Therefore, we only need to compute
f v26 2dr) : / (—v) —ve 2 dvz/ (—1)) e 2 d2)
U2 0° 00 U2 00 U2
I (—v)e 2 —/ (-1)e 2dv:0+/ e 2dv:\/21r.
Here above we used the fact that
112 1 v2
lim we 2 I lirn LQZHQZW lim —2:O: lirn we 2
v—>oo v—>oo 1) v—>oo 'U v—>—oo
6 2 q) 6 2
So, E[X2] I ﬁ (0'2\/27T —|— 20p - O —|— ,LLQVZW) I 02 —|— #2.
And, ﬁnally, Var[X] I E[X2] — (E[X])2 I (02 —|— #2) — ,LL2 I 02.

***************Ending Page***************


***************Beginning Page***************
***************page number:29**************
29.
Vectors of random variables.
A property:
The covariance matrix E corresponding to such a vector is
symmetric and positive semi-deﬁnite
Chuong Do, Stanford University, 2008
[adapted by Liviu Ciortuz]

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
30.

Fie variabilele aleatoare X1, . . .,Xn, cu X,- : Q % R pentru
i I 1,...,n. Matricea de covariantd a vecto'rului de variabile
aleatoa're X I (X1, . . . , Xn) este 0 matrice pﬁitraticii de dimensiune
n >< n, ale céirei elemente se deﬁnesc astfel: [Cov(X)]Z-j déf' C021<X¢,Xj),
pentru orice i,j € {1, . . .,n}.

Aréitagi c5 E 712' C0v(X) este matrice simetricii §i pozitiv semi-
deﬁnitﬁ, cea de-a doua proprietate insemnénd (:5 pentru orice
vector z E R” are loc inegalitatea ZTEZ Z O. (Vectorii z € R” sunt
consideraigi vectori-coloanﬁ, iar simbolul T reprezintii operaigia de
transpunere de matrice.)

***************Ending Page***************


***************Beginning Page***************
***************page number:31**************
31.

Cov(X)Z-7j dif' Cov(XZ-,Xj), for all i,j € {1, . . .,n}, and
COU(XZ'7XJ') déf' EKXi — E[X¢])(Xj — EPQD] I EKXJ' — EPQ'DUQ — EWD] I COU(XJ,X@)»
therefore Cov(X) is a symmetric matrix.
We will show that zTZz Z 0 for any Z € R” (seen as a column-vector):
ZTEZ I ZZZ(Z EZ-jzj) : Z 2(zZ-EZ-jzj) I Z 2(25 C0v[XZ-,Xj] zj)

I Z Xvi EMXZ- — E[X¢1><Xj — E[Xj1>1zj> I E [Z Z ZZ- (Xi — E[X¢1><Xj — EM) a]

I E [<2 Z1 (X2- — E[X1])) (20% — E[Xj]) 141)]

I E [(2%- — EM > (20:7- — Em H I E[((X — E[X])T - a2] 2 0

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
32.
Multi-variate Gaussian distributions:
A property:
When the covariance matrix of a multi-variate (cl-dimensional) Gaussian
distribution is diagonal, then the p.d.f. (probability density function) of
the respective multi-variate Gaussian is equal to the product of d
independent uni-variate Gaussian densities.
Chuong Do, Stanford University, 2008
[adapted by Liviu Ciortuz]

***************Ending Page***************


***************Beginning Page***************
***************page number:33**************
33.
Let’s consider X : [X1...Xd]T, ,u € Rd and Z] G Si, where Si is the set of symmetric
positive deﬁnite matrices (which implies |E| 3i O and (x — u)TZ_1(x — ,u) > 0, therefore
—%(m — M)TE_1($ — ,u) < O, for any :15 E Sd, as 75 H)-
The probability density function of a multi-variate Gaussian distribution of parameters
,u and E is:
1 1
at; ,E:—e ——a:— TE_1x— ,
p( u ) (Md/ZIEll/Z Xp< 2( u) ( M)

Notation: X N NW, Z).
Show that when the covariance matrice E is diagonal, then the p.d.f. (probability den-
sity function) of the respective multi-variate Gaussian is equal to the product of d
independent uni-variate Gaussian densities.
We will make the proof for d : 2 Note: It is easy to show that if E e Si is diagonal, the
(generalization t0 d > 2 Will be easy): elements on the principal diagonal E are indeed strictly

positive. (It is enough to consider z = (1,0) and respec-
LU _ l 331 1 _ l ,Ull 1 E _ l 0% 0 1 tively z : (0,1) in formula for pozitive-deﬁniteness of E.)

— M — — 2
£62 [Ll/2 0 02 This is why we wrote these elements of a as a? and 0;.

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
34.
. I .
- l l
.- _-|-. I.‘ ‘Ii-h. ‘I -\. '
114 L 1.. _ 1
a /
.[]. H - I‘); 3 Frill-.113]
“u {115
1 “a 2
1L1
k 1
.1']: 173
Rh 5 - Hr“
rs “ax n
u
4-1

***************Ending Page***************


***************Beginning Page***************
***************page number:35**************
35.
—1
1 1 $1-M1 T (7% 0 [ail-#1]
MW” I Tap 3 952-112 0 a; WW
51
1
— 0
: —eX —— _
271'0'10'2 p 2 $2—M2 0 _2 £132 H2
U2
1
T —2($1—#1)
0
—1 eXp ll x1 —'LL1 I 1
I —2 SE — 2 1
271'0102 2 H —2($2—H2)
U2
1 1 2 1 2
I —e —— $1-M1) ——($2—M2))
271'0'10'2 Xp< 20%< 20%
I p(w1;#1,0%)29(w2;#2,0§)-

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
36.
Factorizing positive deﬁnite matrices, using eigenvectors
Sebastian Ciobanu,
following Chuong D0 (from Stanford University),
The Multivariate Gaussian Distribution, 2008

***************Ending Page***************


***************Beginning Page***************
***************page number:37**************
37.
Fie o variabiléi aleatoare X : Q —> Rd. in cele ce urmeazéi elementele din Rd vor ﬁ
considerate vectori-coloané. Vorn nota cu Si multimea matricelor simetrice pozitiv
deﬁnite de dimensiune d >< d.
Deﬁnitie: Fie A E RdXd. Se numegte valoare proprie a matricei A un numiir complex
A € (C pentru care existé un vector nenul x € Cd pentru care are loc egalitatea Ax I >00.
Acest vector 90 se nume§te "vector propriu asociat valorii proprii A.
a. Demonstrati céi pentru orice matrice E E Si existﬁ o matrice B € RdXd astfel incét E
$5 poaté ﬁ factorizatéi sub forma urmétoare: E I BBT.
Observatie: Factorizarea aceasta nu este unicé, adicé existéi mai multe posibilitéti de a
scrie matricea E drept E : BBT.
Indicatie: V5 puteti folosi de urmétoarele proprietéiti:
i. Orice matrice A E RdXd care este simetricéi poate ﬁ scriséi astfel: A I UAUT, unde
U € RdXd este o matrice ortonormalé continénd vectorii proprii (pentru care impunem
sii aibé norrna 1) ai lui A drept coloane, iar A este matricea diagonalii continénd valorile
proprii ale lui A in ordinea corespunzétoare coloanelor (adicé, a vectorilor proprii) din
matricea U .(Faptul c5 U este matrice ortonormaléi se poate scrie in mod formal astfel:
U T U I U U T I I.)
ii. Fie A € RdXd o matrice simetricéi. A este pozitiv deﬁnitéi dacéi §i numai dacéi toate
valorile sale proprii sunt (reale §i) pozitive.
"iii. (AB)T : BTAT, pentru orice matrice A,B G RdXd.

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
Solugie 38.
Stim, prin ipotezéi, 05 matricea Z 6 Si, deci este simetricé. Conform proprietéiigii (m),
putem scrie urmétoarea factorizare pentru E:
2 I UAUT,
unde U este matricea ortonormaléi conﬁnfind vectorii proprii (cu norma 1) ai lui Z
drept coloane, iar A € RdXd este matricea diagonaléi conginénd valorile proprii ale lui E,
iAn ordinea corespunzétoare coloanelor matricei U.
Intrucét E Q Si este matrice pozitiv deﬁnitéi, conform proprietéigii (ariz') rezultéi c5 toate
valorile proprii ale lui Z sunt pozitive. Prin urmare, existé matricea
\/)\1 O
A1/2 I : -. : € Rd”.
O \/)\d
Observayii:
(1) Este imediat faptul céi putem factoriza / descompune matricea A in felul urméitor:
A1/2 - A1/2 I A. (3)
(2) A1/2 este matrice diagonalé, deci simetricé. Agadar,
(Al/2)T : A1/2. (4)

***************Ending Page***************


***************Beginning Page***************
***************page number:39**************
39.
Acum putem relua factorizarea matricei E : U AU T. Elaborénd — adicé, factorizénd
matricea A — in partea dreapté a acestei egalitégi, vom putea obtgine 0 nouﬁi factorizare
pentru matricea Z in felul urmétor:
2: I UAUT
(i) UA1/2A1/2UT g) UA1/2(A1/2)TUT
age. UA1/2<(A1/2)TUT) (aim) UA1/2(UA1/2>T
I BBT, unde B “211A”? (5)
Observajiz':
(3) O altéi modalitate de a factoriza / descompune matricea E sub forma E I BBT
este datéi de factorizarea Cholesky: dacé A este 0 matrice simetricéi §i pozitiv deﬁnité,
atunci existii 0 unicé matrice L G RdXd, inferior triunghiularé, astfel incét A I LLT. (See
https://en.wikipedia.org/Wiki/Positive-deﬁnitematrix.)
(4) Factorizarea (5) este de fapt valabiléi §i pentru matrice pozitiv semideﬁnite. inséi
proprietatea de inversabilitate (vedegi punctul c) nu este valabilé deceit pentru matrice
pozitiv deﬁnite.

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
40.
b. La acest punct vom face 0 ewemplz'ﬁcare pentru chestiunile prezentate la punctul a.
Considerénd matricea
E _ 1 0.5
— 0.5 1 ’
(deci, d I 2) determinaﬁ 0 matrice B € RQXZ astfel incét E I BBT.
Indicatie: Folosind notatiile din Deﬁniﬁa daté pentru valorile proprii §i vectorii proprii,
vom avea:
A51: I MIMI) (AId —A)a: I 0,513 75 O<I>det()\1d —A) I O,
unde 0 este vectorul coloanéi nul d-dimensional, iar Id este matricea identitate d-
dimensionalii.

***************Ending Page***************


***************Beginning Page***************
***************page number:41**************
41.
Solugie
Mai intéi vom calcula valorile proprii ale matricei E din QHUHL'. Fie deci A G R §i a: G Rd,
a: 75 0. Trebuie s5 rezolvém ecua§ia Ex I Ax §i, conform Indicajiez' din enuntj, aceasta
revine la a rezolva ecuagia det()\Id — E) : O.
_ A — 1 —O.5 _ 2 2 _
det()\ld—§]) _O (I) det ([_0_5 A_1]>—0<:>()\—1)—0.5 —O
(I) (A—1—0.5)()\— 1+O.5) 204:) (A—1.5)()\—0.5) :0
Deci valorile proprii ale matricei Z sunt A1 I 0.5 §i A2 I 1.5.
Se observéi c5 A1,)\2 > 0. Agadar, conform proprietéiigii (a,z'z'), matricea E datﬁ in enuni;
este pozitiv deﬁnité. Conform punctului a, §tim acum 05 existé o matrice B astfel incét
matricea E séi poatéi ﬁ factorizaté sub forma Z I BBT. Pentru a determina matricea B,
trebuie sa calculém vectorii proprii ai matricei E. Ii vom ob§ine rezolvénd urmétorul
sistem, care este compatibil nedeterminat (pentru c5 det()\ld — E) I O):
0.5m
( )1) u v A — 1 .
—0.5v —|— (A — 1)u I O

***************Ending Page***************

***************Beginning Page***************
***************page number:42**************
Deci, vectorii proprii sunt de forma: 42'
0.5
a: I (mu) I (mum) ,u E R.
in mod concret,
A1 I 0.5 :> 51:1 I (-u,u),u 6 R
§i
A2 I 1.5 i $2 I (u,u),u € R.
Deoarece vrem ca vectorii 3:1 §i x2 $5 aibéi norma 1, ii vom ,,n0rmaliza“. Mai intéi,
H551H2 \/2u2 ﬂ|u|
§i, considerénd u > 0, deci |u| : u, rezultéi c5
$1 _(—u,u)_( 1 1)
H111H2 ﬂu \/§’ \/§ '
Apoi, in mod similar obginem
||$2||2 \/§7 \/§ '

***************Ending Page***************


***************Beginning Page***************
***************page number:43**************
43.
De la punctul a §tim 05 putem alege B I UAl/Z, deci
1 1 1 0
U I \/§ \/§ A1/2 I ” 0'5 0 I \/§
L L 0 \/1.5 0 Q
\/§ \/§ V5
1 1 1
__ _ _ 0 _1 Q
B I \/§ \/§ \/§ I 2 2
i i 0 L5 1 ﬂ '
\/§ ﬂ \/§ 5 7
Veriﬁcfim 5 intr-adeviir E I BBT:
1 \/§ _} 1 1 + 3 1 + 3
BBTIZQ 22:44 44:10-5:E
5 7 7 7 4 4 4 4

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
44.
Observaﬁe

in ton cu Observatia din enuntj, dacéi in rela§ia (6) vom considera u < 0, vom obii'ine incéi
0 solutie pentru factorizarea matricei E:

Lizm:(i _L)I_L

Hill/1H2 Jim \/§7 \/§ H131H2

Lézwz(_i _L>:_i

H$§H2 \/§\UI \/§7 \/§ ||112||27
deci

U’: —U, B’: U’A1/2 I _UA1/2 I —B
§i, in consecingé,
B’(B’)T : (_B)(_B)T : BBT : 2.

***************Ending Page***************


***************Beginning Page***************
***************page number:45**************
45.
c. Demonstraﬁ c5 matricea B care satisface proprietatea de la punctul a este inversabilé.
Indicatie: V5 puteigi folosi de urmétoarele proprietéigi:
i. Matricea A € RdXd este inversabilé dacé §i numai dacé det(A) 51$ O.
ii. det(AB) I det(A) det(B), pentru orice matrice A, B € RdXd.
iii. det(A) I det(AT), unde A E RdXd.
iv. Orice matrice pozitiv deﬁnitéi este inversabiléi (iar inversa ei este de asemenea ma-
trice pozitiv deﬁnité).

***************Ending Page***************

***************Beginning Page***************
***************page number:46**************
46.
Soluigie
De la punctul a rezultii 05 pentru orice matrice Z € Si existéi o descompunere / factor-
izare de forma Z I BBT, deci
det(E) I det(BBT) (2”) det(B) det(BT) (62” det(B)2.

Prin urmare,

:> det(B) I :|:\/det(E). (7)
Pe de altii parte, din faptul c5 E este matrice pozitiv deﬁnité, rezulté conform pro-
prietéﬁgii (0.2%)) c5 E este matrice inversabiléi. Mai departe, conform proprietéigii (0.2),
vom avea det(E) 75 0. Coroborélnd aceasta cu rela§ia (7), rezultéi c5 det(B) 75 O §i in
consecinigé (din nou, conform proprietéiigii (c.z')) c5 matricea B este inversabilé (ceea ce
era de demonstrat).

***************Ending Page***************


***************Beginning Page***************
***************page number:47**************
47.
The Gaussian Multivariate Distribution:
Its density function is indeed a p.d.f.
Sebastian Ciobanu,
following Chuong Do (from Stanford University),
The Multivariate Gaussian Distribution, 2008

***************Ending Page***************

***************Beginning Page***************
***************page number:48**************
48.
Deﬁnitie: Notém cu Si multjimea matricelor simetrice pozitiv deﬁnite de dimensiune
d >< d. Spunem (:5 variabila aleatoare vectorialé X : Q —> Rd, reprezentatéi sub forma
X I (X1...Xd)T, urmeazé 0 distribuigie gaussiané multivariate, avénd media ,u € Rd si
matricea de covarianigii E € Si, dacé functia ei de densitate [de probabilitate] are forma
analiticé urmétoare:
< E) 1 1< FEW >
av; , I — eX —— x — x — ,
pX ” (27v)d/2det(2)1/2 p 2 ” ”
unde nota§ia det(E) desemneazﬁ determinantul matricei E, iar eXp( ) desemneazé functgia
exponenQialéi avénd baza e. Pe scurt, vom nota aceastéi proprietate de deﬁnitie a lui X
sub forma X ~ N(,u, E).
Observaiie (1): La problema am demonstrat (:5 pentru orice matrice Z G Rd simet-
ricé §i pozitiv deﬁnité existéi (inséi nu neapérat in mod unic) 0 matrice B € RdXd cu
proprietatea céi E se poate ,,fact0riza“ sub forma E : BBT. Mai mult, am demonstrat
(:5 orice matrice B care satisface aceasté proprietate este in mod necesar inversabilé.
a. Demonstrati 05 dacé deﬁnim Z : B_1(X—,u), atunci Z ~ N(0, Id), unde O este vectorul
coloanéi nul d-dimensional, iar Id este matricea identitate d-dimensionaléi.
Observajie (2): Proprietatea aceasta este 0 generalizare a metodei de ,,standardizare“
pe care am intélnit-o deja la problema CMU, 2004 fall, T. Mitchell, Z. Bar-Joseph,
HWl, pr. 3, aplicaté in cazul distribu§iilor gaussiene univam'ate.

***************Ending Page***************


***************Beginning Page***************
***************page number:49**************
Indicajie (1): V5 putegi folosi de urméitoarele proprietéti: 49-
i. Fie X I (X1...Xd)T € Rd 0 variabilii aleatoare de tip vector, cu distribu§ia comunﬁ datﬁ
de functia de densitate pX :Rd —> R. Dacﬁ Z I H(X) € Rd, unde H este func§ie bijectivé §i
diferengiabiléi, atunci Z are distribugia comunéi datéi de functjia de densitate pZ : Rd —> R, unde
8x1 8$1
@—zl 8—Zd
pz(z) I px(x)- det .
8$d 8$d
(9—21 8—Zd
6$1 axl
8—Zl 8—Zd
Matricea ' . _ se nume§te matricea jacobiand a lui m in raport cu z §i se noteazé,
8$d 8$d
8—21 3—zd
A (9x
1n acest caz, cu —.
82
211% I A, unde 513,1) € Rd, A G RdXd, iar A §i b nu depind de :13.
m‘. (AB)_1 : B_1A_1, unde A,B e Rd”.
iv. (AB)T I BTAT, unde A,B G RdXd.
'v. det(AB) I det(A)det(B), unde A,B € RdXd.
'v'i. det(A) I det(AT), unde A € RdXd.

***************Ending Page***************

***************Beginning Page***************
***************page number:50**************
50.
Solugie
Sunt imediate urmétoarele relagii:
z:B_1(:c—u)ng:aJ—p:>;c:Bz+/L (8)
(9a: (9(Bz + ,u) (aid)
_ : — : B
(92 (92 (9)
(d6t(2))1/2 I MdquBT) (‘2’) mew?) det(BT) (“é”) \/(det(B))2 I \det(B)|. (10)
Vom rescrie acum expresia care constituie argumentul functiei eXp() din deﬁniﬁa p.d.f.-
ului distribu§iei gaussiene multivariate (vedeti enun§ul), in func§ie de vectorul z.
(w — MTYWJ — u)
(8) _ _
I (BZ + H — #)T(BBT) 1(BZ + H — H) I (BZ)T(BBT) 1(52)
(am) _ _
I (BZ)T((BT) lB 1)(BZ)
(aé'v) (zTBT)((BT>—1B—1)(BZ)
(18:00 zT(BT(BT)—1)(B_1B)z:zT IdIdZIZTZ. (11)

***************Ending Page***************


***************Beginning Page***************
***************page number:51**************
51.
Aplicémd acum proprietatea (m), vom putea calcula p.d.f.—ul distribugiei gaussiene aso-
ciate variabilei Z:
(9
192(2) I pX($) - ‘det (6%)‘
_ 1 1 T _1 (9x
— <2w>d/2det<2>1/2 eXp ($36”) E (“H”) 1011(5)‘
(9) (12) (11) 1 1 T
_ —(27T)d/21/dﬁt/€BT[€XP (—§z z)j/deft/EB§T
I — —— : — __ I
(Md/2 eXp( ZZ > <2w>d/2<det<1d>>1/2 6Xp( Qz dz)

***************Ending Page***************

***************Beginning Page***************
***************page number:52**************
52.

b. Aratagi ca functia pX (as; ,u, E) care a fost data in enuni; este intr-adevar funcigie den-
sitate de probabilitate (p.d.f.).
Indicajie (2): V51 puteti folosi de urmatoarele proprietati:
i. Pentru cazul d : 1, funcgia pX(x;p, 02) este func§ie densitate de probabilitate.
ii. In cazul in care matricea Z este diagonala,

0% . . . O ,ul

0 . . . 0'62], ,ud
expresia func§iei de densitate gaussiana multivariata este identica cu produsul a d functgii
de densitate de tip gaussian, univariate §i independente, prima funcgie avand media ,ul
§i varianta of, a doua functie avand media p2 §i varianta 0;, . . . , a d-a functie avand
media ,ud §i varianiga 03.

***************Ending Page***************


***************Beginning Page***************
***************page number:53**************
Soluljie 53'
pX (at; ,u, Z) este funcigie densitate de probabilitate (p.d.f.) dacfi:
' px(w;#73) 2 07W € Rd
0 I n2” If; If; . . . If; pX(aj;,u, E)dxd . . . dwgdxl I 1.
Prima conditie este satisfécutéi, pentru c5 numitorul frac§iei [care este factorul de nor-
malizare] din deﬁnitjia func§iei de densitate de probabilitate pX(x;/1, E) este pozitiv, iar
@Xp(y) > O pentru orice y E R.
In continuare vom veriﬁca a doua condiigie.
In integrala I facem substituigia (schimbarea de variabilé): z I B_1(50 — u), unde E I
BBT,B € RdXd. Matricea B existéi §i este inversabiléi dupéi cum s-a precizat in enun§
(vedeti Observatia (1)).
De la punctul a rezulté imediat c5:
<b-_M> °° . °° . °° . (w _
_ pZ1(z1,0,1)dzl pZ2(22,0,1)d22 pZd(zd,O,1)dzd _ 1-11 _ 1.
Deci, §i a doua conditgie este satisfiicuté, ceea ce inseamné c5 functia pX(a:; nil) este
intr-adevéir funcigie densitate de probabilitate (p.d.f.).

***************Ending Page***************

***************Beginning Page***************
***************page number:54**************
54.
Bi-variate Gaussian distributions. A property:
The conditional distributions Xlng and X2|X1 are also
Gaussians.
The calculation of their parameters
Duda, Hart and Stork, Pattern Classiﬁcation, 2001,
Appendix A.5.2
[adapted by Liviu Ciortuz]

***************Ending Page***************


***************Beginning Page***************
***************page number:55**************
55.
Fie X 0 variabiléi aleatoare care urmeazé 0 distribuigie gaussiané bi-variaté
de parametri ,u (vectorul de medii) §i E (matricea de covariantéi). Agadar,
,li I (M1,,LL2) € R2, iar E E M2><2(R)-
Prin deﬁnitjie, E : Cov(X,X), unde X nét' (X1,X2), a§adar EU : C01)(Xi,Xj)
pentru 2',j G {1,2}. De asemenea, Cov(XZ-,XZ-) I VaﬂXi] “it 01-2 Z O pentru
1L € {1,2}, in vreme ce pentru 2' 75 j avem Cov(XZ-,Xj) I 0012(Xj,XZ-) “it Uij-
in sfér§it, dacé introducem ,,coeﬁcientul de corelare“ p déf' g, rezultii cii
0'10'2
putem scrie astfel matricea de covariantgéz
2
Z I l “1 p015” I . (12)
pO'lUQ 0'2

***************Ending Page***************

***************Beginning Page***************
***************page number:56**************
n if;
g a
Demonstrati c5 ipoteza X ~ N(,a, E), im- {gm
plicé faptul c5 distributia conditionalé J #ﬁﬁii
X2|X1 este de tip gaussian, §i anume “~_
X2|X1 I 1B1 ~N(/12|1,U§|1),
X2
_ Q _ . 2 _ 2 1_ 2 _,
cu M2|1 — M2+PU ($1 M1) §1 02|1 — U2< P )-
1 “2H . (ff
Observatie: Pentru X1|X2, rezultatul H/ ‘Li _
este similar: X1|X2 : x2 ~ N(,u1|2,0%,2), cu (ff-'2" j:
0'1 . fin?’
M1|2IH1+p—($2—/~L2) §10¥|2:0%(1—p2). A /
0'2 X]
Source: xl
Pattern Classiﬁcation, 2nd ed., Appendix A.5.2,
R. Duda, P. Hart and D. Stork, 2001

***************Ending Page***************


***************Beginning Page***************
***************page number:57**************
57.
Answer
def- PX1 X2 (5'11W2)
PX X $25131 I ’—» 13
2| 1< | > 19ml) < >
where
< > 1 ( 1< FEW >> '
a: ,x I —eX —— a; — a; — 1
1 1 2
I __ _ _ 14
PX1(IE1) @01 exp< 20%(5131 M1) > ( )
From (12) it follows that |El I 0%og(1 — p2). In order that \/|E\ and Z_1 be deﬁned, it
follows that p € (—1,1). Moreover, since 01, 02 > O, we will have \/|E| : 0102\/l — p2.
2-1 I 1 2* I 1 6% —pU1U2
Ufagﬂ — p2) 0%030 — p2) —p0102 0%
1 p
_ 1 0% 0102
— (1 — P2) _L i
0'10'2 0';

***************Ending Page***************

***************Beginning Page***************
***************page number:58**************
58.
So,
pX1,X2(7J1,332) I
1 p
Iéexp —;(5131 _ H1 $2 _ H2) 0% 0102 < x1 — p1 >
27T0'10'2\/1 — p2 2(1 — p)2 7 _ p i 932 — H2
0162 0'5
_ 1
27T0'10'2\/1 — p2
2 2
1 $1—M1 £171—,M1 £192—,M2 $2—M2
—— — - 2 — — — 15
exp< 2(1—p2)[< 01 > p< 01 >< 02 + 02 ()

***************Ending Page***************


***************Beginning Page***************
***************page number:59**************
59.
By substitution (14) and (15) in the deﬁnition (13), we will get:
P($21$1) I PX1,X2($1,$2)
PX1 (IE1)
2 2
_ 1 1 $1—,M1 $1—H1 $2—/12 $2—M2
_ —-eXp ——2 — —2p — — + —
27TO'10'2\/1—,02 2(1—p) 01 01 U2 (72
2
1 _
@2019,pr (M) >
2 01
_ 1 1 (332,112 $1,111)2
— —eXp ——2 — —p—
\/27TO'2\/1—p2 2(1—,0) U2 01
0'2 2
1 1 372- [#2+p0—1($1 —lu1)i
I — exp —— —
\/27TO'2\/1—p2 2 02\/1—p2
Therefore,
. not. U not.
X2\X1 I $1 ~N(#2|1,U§|1)W1th u2|1 I #2 +p0—i($1 — #1) and Ug|1 I 05(1 — p2)-

***************Ending Page***************

***************Beginning Page***************
***************page number:60**************
60.
Using the Central Limit Theorem (the i.i.d. version)
to compute the real error of a classiﬁer
CMU, 2008 fall, Eric Xing, HW3, pr. 3.3

***************Ending Page***************


***************Beginning Page***************
***************page number:61**************
61.
Chris recently adopts a new (binary) classiﬁer to ﬁlter email
spams. He wants to quantitively evaluate how good the classi-
ﬁer is.
He has a small dataset of 100 emails on hand which, you can
assume, are randomly drawn from all emails.
He tests the classiﬁer on the 100 emails and gets 83 classiﬁed
correctly, so the error rate on the small dataset is 17%.
However, the number on 100 samples could be either higher or
lower than the real error rate just by chance.
With a conﬁdence level of 95%, what is likely to be the range of
the real error rate? Please write down all important steps.
(Hint: You need some approximation in this problem.)

***************Ending Page***************

***************Beginning Page***************
***************page number:62**************
62.
Notations:
Let Xi, 2': 1,. . .,n I 100 be deﬁned as:
X1- : 1 if the email 2' was incorrectly classiﬁed, and 0 otherwise;
Ele'l nét. H n2 ereal ; leXi) Hg <72
no , X . . . Xn
6sample :t $ I 0.17
n
X . . . Xn — .
Zn I M (the standardlzed form of X1 + . . . + Xn)
ﬁ 0
Key insight:
Calculating the real error of the classiﬁer (more exactly, a symmetric interval
around the real error p ngt' ,u) With a “conﬁdence” of 95% amounts to ﬁnding a > 0
sunch that P(|Zn| g a) Z 0.95.

***************Ending Page***************


***************Beginning Page***************
***************page number:63**************
63.
Calculus:
X—|—...+X —n X—|—...+X —n a
ﬁ a n0 \/H
X . . . X — n a0 X . . . X a0
4:) M s _ (I) $ _ H g _
<:> I | < CLO" <:> | | < CLO'
6 — 6 — 6 — 6 —
sample real _ W real sample _ W
<:> CEO" < < CLO'
ﬁ _ real sample _ m
<:> (10' < < + CLO'
6 — — 6 6 —
sample ﬁ _ real _ sample ﬁ
¢> € 0,0‘ + CLO‘
6 6 — — 6 —
real sample W a sample m

***************Ending Page***************

***************Beginning Page***************
***************page number:64**************
Important facts: 64.
The Central Limit Theorem: Zn —> N(0; 1)
Therefore, P(\Zn| g a) w P(|X\ g a) I @(a) — <1>(—a), where X ~N(0; 1)
and (I) is the cumulative function distribution of N(0; 1).
Calculus:
<I>(—a) —|— CI>(a) I 1 :> P(| Zn ‘g a) I @(a) — <I>(—a) I 2<I>(a) — 1
P(| Zn |§ a):0.95<I>2(I>(a) — 1:0.95<I><l>(a):0.975 (I) a 2 1.97 (see (I) table)
02 nét' Varreal : emalﬂ — 67ml) because Xi are Bernoulli variables.
Futhermore, we can approximate ereal With esemple, because
1
E[esample] I 67ml and Varsample I — Varreal —> 0 for n —> +00,
n
cf. CMU, 2011 fall, T. Mitchell, A. Singh, HW2, pr. 1.ab.
Finally:
WG-17 1 — 0.17
:“ie1.97-—( >200?
ﬁ M100
lama; — 65ample| g 0.07 (I) ‘ereal — 0.171 g 0.07 (I) —0.07 g ereal — 0.17 g 0.07
(I) ereal 6 [0.10, 0.24]

***************Ending Page***************


***************Beginning Page***************
***************page number:65**************
65.
Exemplifying
a mixture 0f Bernoully distributions;
Bayes’ formula
MIT, 2016 fall, R. Barzilay, S. Sra, Weekly exercises, week 2, pr. 3

***************Ending Page***************

***************Beginning Page***************
***************page number:66**************
66.
You have just purchased a two-sided die, which can come up either 1 or 2.
You want to use your crazy die in some betting Z~Bern0ulli
games with friends later this evening, but ﬁrst you 1
waht to know the probability that it will roll a 1. A“
You know it came either from factory O or factory 1, factory 1 factoryo
but not which. Factory 0 produces dice that roll a X~Bemoulli1 X~Bem°ulli0
1 with probability 90. Factory 1 produces dice that e 1'91 60 1-9
roll a 1 with probability 61. You believe initially 1 °
that with probability 7T that it came from factory 1. 1 2
a. Without seeing any rolls of this die, what would be your predicted proba-
bility that it would roll at 1?
b. If we roll the die and observe the outcome, what can we infer about where
the die was manufactured?

***************Ending Page***************


***************Beginning Page***************
***************page number:67**************
67.

c. More concretely, let’s assume that:

60 : 1: dice from factory O always roll a 1;

61 : 0.5: dice from factory 1 are fair (roll at 1 with probability 0.5);

7r : 0.7: we think with probability 0.7 that this die came from factory 1.
Now we roll it, and it comes up 1! What is your posterior distribution on
which factory it came from?
d. You roll it again, and it comes up 1 again. Now, what is your posterior
distribution on which factory it came from?
e. Instead, what if it rolls a 2 on the second roll?
f. In the general case (not using the numerical values we have been using)
prove that if you have two observations, and you use them to update your
prior in two steps (ﬁrst conditioning on one observation and then conditioning
on the second), that no matter which order you do the updates in you will
get the same result.

***************Ending Page***************

***************Beginning Page***************
***************page number:68**************
68.
Solution:
a. Deﬁne Z as a binary random variable which is one if the die came from
factory 0 and X as the random variable associated with a dice roll. Then by
conditional probability, we have
P(X I 1) I P(X I 1|Z I 0)P(Z : 0) + P(X : 1|Z : 1)P(Z I 1) I (90(1 — 7T) —|— ler.
b. Having observed an outcome at, we can apply Bayes’ rule.
PX: Z:1PZ:1 @9514} H’
P(Z:1|X:$)I%I%-
P(X : x) 8f (1 — 61)1_~"L‘ 1r + 6% (1 — 60)1_<E (1 — 7T)
where 5L" ng' 151:1}.
Note: In the second equality, we used the “exponentiation trick” as a way to
select amongst the two possible choices in general.
c. 0.5 - 0.7
PZ:1X:1 :—%0.54
( I ) 0.5-0.7+1-0.3

***************Ending Page***************


***************Beginning Page***************
***************page number:69**************
69.

d.
PZ:1X :1X :1 : —

< '1 ’2 > P(X1:1,X2:1)

m_d. P(X1 I 1|Z I 1) - P(X2 : 1|Z : 1) - P(Z : 1)

_ P(X1 : 1|Z : 1) -P(X2 : 1\Z : 1) - P(Z : 1) —|—P(X1 : llZ z 0) - P(X2 : 1|Z I 0) - P(Z I O)

0.5-0.5-0.7
: — @037
0.5-0.5-0.7+1-1-O.3
e‘ 05 05 07
p(Z:1‘X1:17X2:2):;:1'
O.5-O.5-O.7+1-0-0.3

So, now we know for sure where this die came from!

***************Ending Page***************

***************Beginning Page***************
***************page number:70**************
70.
f. Let us denote our 2 observations by 33a, :cb.
Pemwb) I ZP($a,wb|z I z)P(Z I Z)
MIG” ZPWZ I Z)P<xb|z I z)P(Z I Z)
I ZPthIZ I Z)P(wa|z I Z)P<z I Z) deep' Z Pm, wa|Z I Z)P(Z I Z)
I P(xb,ma).
Thus, the probability of observing ma, xb does not change with the order in
which they appear. By Bayes’ Rule,
P(Z I Z|$m33b> I P(a:a,a:b|Z I Z)P(Z I z) I P(a:a|Z I z)P(:Bb|Z I z)P(Z I z)
P<LCa7ZUb) P<£aa£b>
_ P(xb|Z I z)P(xa|Z I z)P(Z I z) _ P(xb,xa|Z I z)
_ P(a:b,a:a) _ P(ZL'b,£L‘a)
: P(Z : zbe,xa).

***************Ending Page***************


***************Beginning Page***************
***************page number:71**************
71.
Exemplifying
a mixture of categorical distributions;
how to compute its expectation and variance
CMU, 2010 fall, Aarti Singh, HWl, pr. 2.2.1-2

***************Ending Page***************

***************Beginning Page***************
***************page number:72**************
72.

Suppose that I have two six-sided dice, one is fair and the other one is
loaded — having:

1

— a: I 6

13(33): { 21

— 1 2 4
I will toss a coin to decide which die to roll. If the coin ﬂip is heads I will
roll the fair die, otherwise the loaded one. The probability that the coin
flip is heads is p € (0,1).
a. What is the expectation of the die roll (in terms of p).
b. What is the variation of the die roll (in terms of p).

***************Ending Page***************


***************Beginning Page***************
***************page number:73**************
73.
Solution:
Bernoulli
H W1”
Graphical representation of the fair die loaded die
,,miXture“ of the two categorical Categoricall Categoricalz
distributions: 1/6 1/6 1/10 1/10
1/6 1,6
1/10 1/2
6 1 2 5 6
a. E[X] : 22' - [P(z'\faz'r) ~p —|— P(z'\loaded) - (1 —p)]
1:1
6 6
I [2/ - Pam/)1 p-l- [2/ - P(z'|l0aded)] (1 —p)
1:1 1:1
7 9 9
: — — 1 — I — —
2p + 2( p) 2 p

***************Ending Page***************

***************Beginning Page***************
***************page number:74**************
74.
b. Recall that we may write Var/(X) I E[X2] — (E[X])2, therefore:
6
E[X2] I 212- [P(z'\faz'7“) -p + P(z'|loaded) - (1 _ 19)]
6:1
6 6
I [212 - P611666] p+ [262 - P(i|loaded)] (1 _ p)
i:1 6:1
91 36 55
I _ _ _ 1 _
6p+(2 +10)( p)
_ 47 25
_ 2 3 p
Combining this with the result of the previous question yields:
141 5O 9
VWX) I E1X21—<E1X1>2 I 7 - 61% <5 -p>2
141 50 81
I _ _ _ _ _ - 9 2
6 6p (4 P+P)
141 81 50
I _ _ _ _ _ _ 9 - 2
< 6 4 > < 6 >p p
_ 13 + 2 2
— 4 3p p

***************Ending Page***************


***************Beginning Page***************
***************page number:75**************
Estimating the parameters of some
probability distributions:
Exempliﬁcations

***************Ending Page***************

***************Beginning Page***************
***************page number:76**************
76.
Estimating the parameter of the Bernoulli
distribution:
the MLE and MAP approaches
ClVlU, 2015 spring, Tom Mitchell, Nina Balcan, HWZ, pr. 2

***************Ending Page***************


***************Beginning Page***************
***************page number:77**************
77.

Suppose we observe the values of n i.i.d. (independent, identi-

cally distributed) random variables X1, . . . , Xn drawn from a single
Bernoulli distribution with parameter 8. In other words, for each

Xi, we know that

P(XZ-:l):9 and P(XZ-:O):1—6.
Our goal is to estimate the value of <9 from the observed values of
X1, . . . 7Xn.

***************Ending Page***************

***************Beginning Page***************
***************page number:78**************
78.
Reminder: Maximum Likelihood Estimation

For any hypothetical value é, we can compute the probability of
observing the Aoutcome X1, . . . ,Xn if the true parameter value d
were equal to 6.
This probability of the observed data is often called the data likeli-
hood, and the function L(H) that maps each (9 to the corresponding
likelihood is called the likelihood function.
A natural way to estimate the unknown parameter (9 is to choose
the d that maximizes the likelihood function. Formally,

éMLE I argmaXLwA).

é

***************Ending Page***************


***************Beginning Page***************
***************page number:79**************
79.
a. Write a formula for the likelihood function, L(§).
Your function should depend onAthe random variables X1, . . . ,Xn
and the hypothetical parameter 6.
Does the likelihood function depend on the order of the random
variables?
Solution:
Since the Xi are independent, we have
L<é> I P§(X1,. . .,Xn) I 1139(ng 1-1in - (1 _ é)1—X¢)
1:1 1:1
I é#{X¢:1} . (1 _ (9A)#{Xi:0}’
where #{} counts the number of Xi for which the condition in
braces holds true.
Note that in the third equality we used the trick XZ- : I{XZ.:1}.
The likelihood function does not depend on the order of the data.

***************Ending Page***************

***************Beginning Page***************
***************page number:80**************
80.
b. Suppose that n I 10 and the data _
- - Solutlon:
set contams s1x ls and four Os.
. MLE; n = 10, six 1s, four Os
erte a short computer program 0 0012
that plots the likelihood function of '
this data. 0-001
For the plot, theAzr-axis should be é, 0'0008
and the y-aXis L09). Scale your y-axis 0.0006
so that you can see some varlatlon 1n 0000 4
1ts value.
A 0.0002
Estimate (9MLE by marking on the x- 0 ‘
the likelihood. 6

***************Ending Page***************


***************Beginning Page***************
***************page number:81**************
81.
c. Find a closed-form formula for éMLE, the MLE estimate of é. Does the
closed form agree with the plot?
Solution:
Let’s consider HQ) I ln(L(€)). Since the ln function is increasing, the é
that maximizes the log-likelihood is the same as the 6’ that maximizes the
likelihood. Using the properties of the ln function, we can rewrite [(8) as
follows: A A A A A
l(9) I ln(€”1 - (l — 6W“) I n11n(6)+ n01n(l — 9).
where n1 nIt' #{XZ- I 1}, iar n0 n2 #{XZ- I 0}.
Assuming that 3 75 O and 9 I l, the ﬁrst and second derivatives of l are
given by
z’(é) I 7i} — l and l”(é) I —$ - Ll
d l — d (92 (l — Q2
Since l”(é) is always negative, the l function is concave, and we can ﬁnd its
maximizer by solving the equation l’(€) I 0.
The solution to this equation is given by éMLE I L.
n1 + 720

***************Ending Page***************

***************Beginning Page***************
***************page number:82**************
82.
d. Create three more likelihood plots: one Where n = 5 and the
data set contains three 1s and two 0s; one where n = 100 and the
data set contains sixty 1s and fourty 0s; and one where n I 10 and
there are ﬁve 1s and ﬁve Os.
Solution:
MLE; n = 5, three ts, two Os MLE; n = 100, sixty 1s, fourty Os
0.04 7e-30
0.035 66-30
0-03 5e-30
0.025 4e_30
0.02 3 30
0.015 e‘
001 2e-30
0.005 19-30
0 0
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
0 0

***************Ending Page***************


***************Beginning Page***************
***************page number:83**************
83.
e. Describe how the likelihood
Solution (to part d.): functions and maximum likelihood
estimates compare for the different
MLE; n = 10, five 1s, five Os data sets.
0.001
00008 Solution (to part e.):
00006 The MLE is equal to the propor-
' tion of 1s observed in the data, so
0.0004 for the ﬁrst three plots the MLE is
always at 0.6, While for the last plot
0'0002 it is at 0.5.
0 As the number of samples n in-
O 0'2 0'4 0'6 0'8 1 creases, the likelihood function gets
6 more peaked at its maximum value,
and the values it takes on decrease.

***************Ending Page***************

***************Beginning Page***************
***************page number:84**************
84.
Reminder: Maximum a Posteriori Probability Estimation

In the maximum likelihood estimate, we treated the true parameter value 6 as a fixed
(non-random) number. In cases where we have some prior knowledge about 9, it is
useful to treat 6 itself as a random variable, and express our prior knowledge in the
form of a prior probability distribution over d.
For example, suppose that the X1, . . . ,Xn are generated in the following way:

— First, the value of 6 is drawn from a given prior probability distribution

— Second, X1, . . . ,Xn are drawn independently from a Bernoulli distribution using

this value for 6.
Since both 9 and the sequence X1,...,Xn are random, they have a joint probability
distribution. In this setting, a natural way to estimate the value of 6 is to simply choose
its most probable value given its prior distribution plus the observed data X1, . . . ,Xn.
Deﬁnition: A A
9MAP I argmaxP(6 : (9|X1, . . . ,Xn).
é

This is called the maximum a posteriori probability (MAP) estimate of 9.

***************Ending Page***************


***************Beginning Page***************
***************page number:85**************
85.
Reminder (cont’d)
Using Bayes rule, we can rewrite the posterior probability as follows:
A P(X1,...,Xn\€:§)P(6:§)
( I 1’ ’ ) P(X1,...,Xn)
Since the probability in the denominator does not depend on é, the MAP estimate is
given by
éMAP I argmaXP(X1, . . . ,an I é)P(e I é)
é
I argmaXL(§)P(9 I é).

é
In words, the MAP estimate for 9 is the value é that maximizes the likelihood function
multiplied by the prior distribution on 6.

***************Ending Page***************

***************Beginning Page***************
***************page number:86**************
86.

We will consider a Beta(3, 3) prior distribution for d, which has the density

A 02 1—é 2
function given by p(€) : W, where B(d, B) is the beta function and
B(3, 3) z 0.0333.
f. Suppose, as in part b, that Solution:
n I 10 and we observed siX 1s and MAP; n: 10, six 1s, fourOs; Beta(3,3)
four Os. 0.0025
Write a short computer program 0002
that plots the function 3 |—§
L(€)p(6) for the same values of 6 0'0015
as in part b. 0.001
Estimate éMAP by marking on the (10005
x-axis the value of <9 that maXi- 0
mizes the function. 0 0.2 0.4 0.6 0.8 1

f)

***************Ending Page***************


***************Beginning Page***************
***************page number:87**************
87.

g. Find a closed form formula for HAMAP, the MAP estimate of é.
Does the closed form agree with the plot?
Solution:
As in the case of the MLE, we will apply the ln function before
ﬁnding the maximizer. We want to maximize the function

w) I 1mm) w» I 1n<ém+2 - <1 — or”) —1n<B<3,3>>-
The normalizing constant for the prior appears as an additive con-
stant and therefore the ﬁrst and second derivatives are identical
to those in the case of the MLE (except with n1 + 2 and n0 + 2
instead of n1 and n0, respectively).
It follows that the closed form formula for the MAP estimate is
given by

MAP— n1+n0+4'

This formula agrees with the plot obtained in part f.

***************Ending Page***************

***************Beginning Page***************
***************page number:88**************
88.
h. Compare the MAP estimate to the MLE computed from the
same data in part b. Brieﬂy explain any signiﬁcant difference.
Solution:
The MAP estimate is equal to the MLE with four additional vir-
tual random variables, two that are equal to 1, and two that are
equal to O. This pulls the value of the MAP estimate closer to the
value 0.5, which is why QMAP is smaller than QMLE.
i. Comment on the relationship between the MAP and MLE es-
timates as n goes to inﬁnity, while the ratio #{X,; : l}/#{X,; I 0}
remains constant.
Solution:
It is obvious that as n goes to inﬁnity, the inﬂuence of the 4 vir-
tual random variables diminishes, and the two estimators become
equaL

***************Ending Page***************


***************Beginning Page***************
***************page number:89**************
89.
The MLE estimator for the parameter 0f
the Bernoulli distribution:
the bias and [an example of] inadmissibility
ClVlU, 2004 fall, Tom Mitchell, Ziv Bar-Joseph, HW2, pr. 1

***************Ending Page***************

***************Beginning Page***************
***************page number:90**************
90.
Suppose X is a binary random variable that takes value () With probability p and value
1 with probability 1 —p. Let X1, . . . ,Xn be i.i.d. samples of X.
a. Compute an MLE estimate ofp (denote it by 13).
Answer:
By way of deﬁnition,
15 I argmaXP(X1, . . . ,Xn|p) i'iz'd' argmaxn P(X,-|p) I argmaxpkﬂ — p)”_k
P P 2-:1 P
Where k is the number of O’s in x1, . . .,xn.
Furthermore, since 1n is a monotonic (strictly increasing) function,
15 : argmax lnpk(1 — p)”_k : argmax(klnp + (n — k) ln(1 — p))
P P
Computing the ﬁrst derivative of klnp + (n — k) ln(1 —p) w.r.t. p leads to:
d k n — k
— kl — [<3 l 1 — I _ _ —_
ap<ﬁ19+<n )n( P)) p 1_p
Hence,
(9 k n — k k:
—kl —kl 1— :0<:>—:—<I>A:—.
8p( nP+(n )n( P)) p 1_p P n

***************Ending Page***************


***************Beginning Page***************
***************page number:91**************
91.
b. Is 15 an unbiased estimate of p? Prove the answer.
Answer:
Since k: can be seen as a sum of n [independent] Bernoulli variables
of parameter p, we can write:
E11 E k 112m 1E1 in 1< imp
I — I — I — n — 7; I — n — i
p n n n 2,:1 n 1:1

1 n 1 1

n(n Z; 29)) RM n( 10)) nnp p
Therefore, 15 is an unbiased estimator for the parameter p.

***************Ending Page***************

***************Beginning Page***************
***************page number:92**************
92.
c. Compute the expected square error of 13 in terms of p.
Answer:
A A A E k2
El<p—p)2l I Elp2l—2E[plp+p2: %—2p2+p2
Vark+E2k npl—p+np2 p
I (>2 [LPZI < >2 < >_p2:_(1_p)
n n n
We used the fact that Vadk) : E[k2] — E2[k], and also Var(k) :
np(1 — p) because, as already said, k can be seen as a sum of n
independent Bernoulli variables of parameter p.
Note that Em] I p (cf. part b), therefore
A A A A P
EKP —P)2l I EKP — EMF] I Vadpl I 5(1—p)-
This implies that Var [15] —> 0 for n —> oo.

***************Ending Page***************


***************Beginning Page***************
***************page number:93**************
93.
d. Prove that if you know that p lies in the interval [1/4;3/4] and you are
given only n I 3 samples of X, then 15 is an inadmissible estimator of p when
minimizing the expected square error of estimation.
Note: An estimator c) of a parameter (9 is said to be inadmissible when there
exists a different estimator 5’ such that R(6, 5’) g R(6, 6) for all 9 and R(6, 5’) <
RM, 5) for some 9, where R(6, 5) is a risk: function and in this problem it is the
expected square error of the estimator.
Answer:
Consider another estimator, 15 I 1/2.
E[(25 — 29)21:(1/2 — 29)2-
For p I 1/2 we have E[(]5—p)2] I 0 < E[(]5—p)2] I 1/12.
We now need to show that E[(15 —p)2] g E[(15 —p)2] over p € [1/4;3/4].
2
1 1 1 4 4
E”— 2—E A— 2: —— ——- 1— :——— —2.
[(29 29) l [(29 29) l <2 29> 3 29( 29) 4 329+329
This is a parabola going up, so we need to show that it lies below or equal to
zero for p € [1/4;3/4].
It is equivalent to showing that it is below or equal to O at boundary points.
1 4 4
In fact it is: 4 — 510+ §p2 : O for both p : 1/4 and p : 3/4.

***************Ending Page***************

***************Beginning Page***************
***************page number:94**************
94.
Estimating the parameters of the categorical
distribution:
the MLE approach
CMU, 2009 spring, Ziv Bar-Joseph, HWl, pr. 2.3

***************Ending Page***************


***************Beginning Page***************
***************page number:95**************
. 95.
In this problem we will derive the lVlLE for the parameters of a categorical
distribution where the variable of interest, X, can take on k values, namely
a1,a2, . . .,a;,, the probability of seeing an event of type j being d,- for j :
1, . . . , k.
a. Given data describing n independent identically distributed observa-
tions of X, namely d1, . . . , dn, each of which can be one of k values, express
the likelihood of the data given k — 1 parameters for the distribution over
X. Let n,- represent the number of times X takes on value i in the data.
Answer:
The verosimility of the data is:
L(€) I P(d1,...,dn\6)i'i:'d'1_[?:1 zle<e,1d,:,,) (1 is the indicator function)
k m- not. n
I Hi:16i (n, I 23:1 [djzaii
I<3—1
I (1 — Z 9,)”k H7211 @Z” (since the thetas sum to one)
13:1
W
6k

***************Ending Page***************

***************Beginning Page***************
***************page number:96**************
A 96.
b. Find 62-, the MLE for 99-, one of the k — 1 parameters, by setting the
partial derivative of the likelihood in part a with respect to 93- equal to zero
and solving for it.
Hint: You may want to start by ﬁrst taking the log of the likelihood from
part a before taking its derivative.
Answer:
k—1 k—1
dlnL(9) nk n-
lnL(8):nkln(l— (90+ nianiz>—:—T+—J
g 2:; ‘961 1 — ZZZ-:11 §¢ 91'
81 L € . .
g_g():0@_#H+izo@izAn—lﬁ/H
J 1 — 91' — Ewém 9i 91' 91' 1 — 91' — 217% 91'
k—1
¢¢j,l<;
n- k—1
@é-:—Jl— 92- forall'é 1,...,k—l.

***************Ending Page***************


***************Beginning Page***************
***************page number:97**************
97.
c. At this point you should have k — 1 equations describing MLEs
of different parameters. Show how those equations imply that the
MLE for a parameter (93- representing the probability that X takes
on value j is equal to E.
n

Hint: In order to remove the k-th parameter from the likelihood
in part a you had to represent it with an equation, dk : f ( ). At
this point you may ﬁnd it helpful to replace all occurrences of
f ( ) with 6k. After replacing f ( ) with @k you can substitute all
occurrences of each other parameter in f ( ) with its MLE from
part b. This should allow you to solve for the MLE of 6k, which
can then be used to simplify all of the other equations.

***************Ending Page***************

***************Beginning Page***************
***************page number:98**************
98.
Answer
As the likelihood function is uniquely optimal for the vector 6’, the last
equation in part b can be written as:
n- k:—1
e; I —~1 (1 _ Z é)
Tbj + 71k; i¢j,k:
n- k—1
(I) §-:—Jé-+é becauseé :1— @7;
(i, @j(1_i) Iigméjlziék
nj+nk nj+nk nj+nk nj+nk
(1)9} I ﬁék for alle {1,...,k—1}.
"k

***************Ending Page***************


***************Beginning Page***************
***************page number:99**************
99.
Finally,
ék:1-91-...-ék_1:1-Eék-...-%ék
71k nk
_,—/
n
iéj:&-%:E for allj€{l,...,k—1}.

71k; n TL
Note: Even though [here] we can go from the non-hatted (91) to the hatted
form (6'1) of the equation in the ﬁrst step of c, this will generally not be
possible. To solve for a maximum likelihood criterion under additional
constraints like the thetas summing to one, a generic and useful method
is the method of Lagrange multipliers.

***************Ending Page***************

***************Beginning Page***************
***************page number:100**************
100.
The Gaussian [uni-variate] distribution:
estimating ,u when 02 is known
CMU, 2011 fall, Tom Mitchell, Aarti Singh, HW2, pr. 1
CMU, 2010 fall, Ziv Bar-Joseph, HW1, pr. 1.2-3

***************Ending Page***************


***************Beginning Page***************
***************page number:101**************
101.
Assume we have n samples, 3:1, . . . ,ain, independently drawn from a normal
distribution with known variance 02 and unknown mean ,u.
a. Derive the MLE estimator for the mean ,u.
Solution:
Px,...,:12n I sz- : —e 202
n 1 ($1 — M2)
:>lnP:13,...,a:n I ln———
< 1 m ;( 7m 202
:> 212(33 w | ) _ Zn: w
6/1 17"‘) n/“L—Z.:1 02
QPW :EIn):O<:>Zn:u:0<i>2n:(m—u):0@i:mznn
a,“ 17 ' ' '7 TL i:1 02 1:1 'L iIl z
:> ,LLMLE I —ZZ_1
n
Remark: It can be easily shown that 1n P(a;1, . . . ,ainl/r) indeed reaches its maximum for
,u : HMLE-

***************Ending Page***************

***************Beginning Page***************
***************page number:102**************
b. Show that EMMLE] I M. 102-
Solution:
The sample x1, . . . ,xn can be seen as the realization of n independent ran-
dom variables X1, . . . ,Xn of Gaussian distribution of mean ,u and variance
02. Then, due to the property of linearity for the expectation of random
variables, we get:
X+...+Xn EX +...+EX,, n
n n n
Therefore, the ,LLMLE estimator is unbiased.
c. What is VarLuMLE]?
Solution:
1 n (*) 1 n mld. 1 02
VarLuMLE] I Var [n ;X,-] I F g Var[X,-] I n; Var[X1] I I
Therefore, Var [MMLEl —> 0 as n —> oo.
(*) Remember that VaﬂaX] I a2 Var[X].

***************Ending Page***************


***************Beginning Page***************
***************page number:103**************
103.
d. Now derive the MAP estimator for the mean ,u. Assume that
the prior distribution for the mean is itself a normal distribution
with mean u and variance $2.

***************Ending Page***************

***************Beginning Page***************
***************page number:104**************
104.
Solution 1:
T- Ba es P($17"'7$R‘M)P(H)
P(H|$1,--->$n) :y W (16)
1 (an — [02 1 _(,U/ _ W2
H711 —6_ 202 -—e 262
Z— \/27r0 \/27r6
I + (17)
where C n25. P(x1,...,xn).
:>1P( _Zn:1‘/2 M 1~/26MIC
n p\a:1,...,:cn)_— n 7TO'—|— 202 —n 7T — 262 —n
1;:1
:> 211(1P(|:c x)_i—a:i—p_—p—u
8H H 17"‘: n —i:1 02 62
5 n xZ-—,u ,u—u 1 n Eyzlxi 1/
02v + [32 217:1 £131-
:> NMAP—W

***************Ending Page***************


***************Beginning Page***************
***************page number:105**************
105.
Solution 2:
Instead of computing the derivative of the posterior distribution
P(,u|.231,. ..,a3n),
we will ﬁrst show that the right hand side of (17) is itself a Gaussian, and
then we will use the fact that the mean of a Gaussian is where it achieves its
maximum value.
1 n 1 (xi-mg 1 _M
P x,...,:vn : — —@— 202 -—6 252
(Ml 1 ) C g \/ 27T0' \/ 27Tﬁ
_ 2?:1(xi — #02 _ (M — V)2
I const -e 202 262
_52 ELM — M2 + 02W — W2
I const -e 20262
n62 + 02 2 ﬂZ E111 my; —|— uaz ﬁ2 214:1 $12 —|— V202
_—/J +—H_—
I const -e 20252 U252 25252

***************Ending Page***************

***************Beginning Page***************
***************page number:106**************
106.
PM|231,...,a:n): n 2 2 2
< 2 522?:133i+w72 + 5221:1931‘ +V U
,0 _2'LL n62+02 n62+02
I const-exp —ﬁ
n52+02
n 2 2 n 2 2 2
2 TL 33-+u02 622iz1$i+w72 6 Zizlxz +u 0
6 2 1 Z 2 + —2
_ Z— ) _ 2 2 nﬂ2+0
(,0 nﬁ2+02 n3 +0
cons p 2 06 2
nﬁ2+0 2
2 n 2 2
2 7.2 56- V02 2 522122136¢+u02 _6221:1$i+7/ 0
6 Z_ Z+ 2 2
_ 2_1 2 2 n6 +0
,u 2 2 n6 +0
n5 +0 -eXp 262
I const-exp —0+62 —2 02 2
217152+02 n5 +0
n 2 2
62 21:1 93i+v0 >
(,0- nﬂ2+02
I constl-exp —W
7152+O'2

***************Ending Page***************


***************Beginning Page***************
***************page number:107**************
107.
The exp term in the last equality being a Gaussian of mean
2 n 2 2 2
M and varlance i, 1t follows that 1ts max1mum
n52 + 02 n52 + 02
. . 52 27L Im- + W2
IS obtamed for I # I .
ILL n62 + 02 ILLMAP

***************Ending Page***************

***************Beginning Page***************
***************page number:108**************
108.
e. Please comment 0n what happens to the MLE and MAP esti-
mators for the mean ,u as the number of samples n goes t0 inﬁnity.
Solution:
_ 2?:1 $1
HMLE —
TL
_ w _ l + @22—:1w
'“MAP _ 02+n52 _ 02+n62 02+n62
1 n
I l + 52;”; I l + M
02+n62 1+ 0.2 02+n52 1+ 0.2
n52 n32
2 2
O‘ U U

***************Ending Page***************


***************Beginning Page***************
***************page number:109**************
109.
The Gaussian [uni-variate] distribution:
estimating 02 when ,u I O
CMU, 2009 spring, Ziv Bar-Joseph, HWl, pr. 2.1

***************Ending Page***************

***************Beginning Page***************
***************page number:110**************
llO.
Let X be a random variable distributed according to a Normal
distribution with 0 mean, and 02 variance, i.e. X ~ N(O, 02).
a. Find the maximum likelihood estimate for 02, i.e. Uqu-
Solution:
Let X1,X2,...,Xn be drawn i.i.d. ~ N(0,02). Let f be the density function
corresponding to X. Then we can write the likelihood function as:
L012) I P(X1,X27---7Xn|62):Hf(Xi;/i:07a2)
izl
1 n n X1- — 0 2 1 n T‘ X2
I (f) HGXP(——< 2)): (f ) eXp(__2@:12 I)
27m 1.:1 20 27m 20
:> lnL : constant — Elna2 — i ng
2 202 2:1 Z
dlnL n 1 n 2 dlnL 2 l n 2
:> W I —ﬁ + ﬁgXi. Therefore, W 204:)0MLE: thl
1Note: It can be easily shown that L(X1,X2, . . .,Xn|02) indeed reaches its maximum for a2 :
E 2?:1 X?-

***************Ending Page***************


***************Beginning Page***************
***************page number:111**************
111.
b. Is the estimator you obtained biased?
Solution:
It is unbiased, since:
Eﬁ 217:1 X22] = QE[X2] since i.i.d.
n
I VaﬂX] —|— (E[X])2
I VGT[X]:O'2 since E[X] :0

***************Ending Page***************

***************Beginning Page***************
***************page number:112**************
112.
The Gaussian [uni-variate] distribution:
estimating 02 (without restrictions on ,u)
CMU, 2010 fall, Ziv Bar-Joseph, HWl, pr. 2.1.1-2

***************Ending Page***************


***************Beginning Page***************
***************page number:113**************
113.
Let X 712' (2:1, . . . ,xn) be observed i.i.d. samples from a Gaussian distribution
2
N (xi/1,0 )-
a. Derive 514w» the MLE for 02.
Solution: 2
1 _ (a? — H)
The p.d.f. for N(a:\p,02) has the form f(x) I —e 202 .
\/27TU
The log likelihood function of the data X is:
n n 1 (33¢ — #02
2 2:113 2:1 1-: -_122_—
(M) n (Xma) ngfw) Z( 2 n< w) 202
_ n 2 1 n 2
2:1
86 2 1 n
The partial derivativeacgf Z v:.r.t. 02: % : —2:% + g 22-21(xi — m2.
1
Solving the equation % I 0, we get: 03MB I — 2?:1($¢ — MMLEV.
0 n
Note that we had to take into account the optimal value of ,u (see problem CMU, 2011
fall, T. Mitchell, A. Singh, HW2, pr. 1)

***************Ending Page***************

***************Beginning Page***************
***************page number:114**************
114.
2 n _ 1 2
n
Solution:
2 1 n 2 2 1 n 2
E10MLE1 Z E 5 2W — HMLE) Z E1(951 — HMLE) 1: E (1E1 — g 251%)
2:1 izl
1 n 1:1 Z n2 1:1 Z
2 2 n 1 n 2 2
1:1 1:1 i<j
2 1 n 2 2 n 2
I E[;1:1] + F Emmi] _ E Z E2112] + F 2121:6262]
1:1 1:1 i<j
1 2 2 2 n(n — 1)
n — 1 n — 1
: —E 2 _ —E
n [9311 n [$1162]

***************Ending Page***************


***************Beginning Page***************
***************page number:115**************
115.
02 I WWW) I Elwil — (E[$11)2 I Eli] — M2 i 191%]: U2 + H2
Because 3:1 and x2 are independent, it follows that Cov<x1,a:2) : 0.
Therefore,
0 I 0011mm) I El<$1 — El$1l><$2 — 1911321)] I 51(371 — H)(1112 — Ml
I E[:v1:v2] — MElivl + 51321 + M2 I E[w1:v2] — H(E19511+ El$21)+ M2
I E[:c1a;2] — MZH) + M2 I E[w1w2] — #2
So, E[a:1332] I #2.
By substituting Emil I 02 + ,u2 and E[:131:132] I #2 into the previously obtained
— 1 — 1
equality (E[0MLE] I "TEL/1%] _ "Tmelegp, we get:
n — 1 n — 1 n — 1
E 2 z 2 2 _ 2 Z 2
[UMLEl n (U 4-,“) n ll n U

***************Ending Page***************

***************Beginning Page***************
***************page number:116**************
116.
c. Find an unbiased estimator for 02.
Solution:
1
It can be immediately proven that —1 221(901- — ,LLMLE)2 is an un-
n —
biased estimator of 02.

***************Ending Page***************


***************Beginning Page***************
***************page number:117**************
117.
The Gamma distribution:
Maximum Likelihood Estimation of parameters
Liviu Ciortuz, 2017

***************Ending Page***************

***************Beginning Page***************
***************page number:118**************
g 118.
The Gamma distribution 0f parameters r > O and ﬁ > 0 25 I iiiﬁiﬁiiﬁ
has the following density function: m I ligjgjﬁiijg
o' r=0.5,[3=1.0
a" §
Gamma(a:|r [5’) I ;$T—16_E for all x > 0 g
where the P symbol designates Euler’s Gamma function. g i,
Notes: 1 X
1. In the above deﬁnition, m is the so-called normalization factor, since it does
T r
a"
not depend on x, and [5:300 xT_1e 5 dzv I 67‘ P(r).
2. Euler’s Gamma function is deﬁned as follows: F(r) I f0+<>o tT—1e_tdt, for all r G R,
except for the negative integers. Starting from the deﬁnition of F, it can be easily
shown that F(r+ 1) : rF(r) for any r > O, and also F(1) I 1. Therefore, P(r+ 1) : r-F(n) :
r- (r- 1) -I‘(r— 1) I . .. : r- (r— 1) - . . .-2-1 I rl, which means that the P function generalizes
the factorial function.
3. The exponential distribution is a member of the Gamma family of distributions.
(Just set r t0 1 in Gamma’s density function.)

***************Ending Page***************


***************Beginning Page***************
***************page number:119**************
119.
Consider 11:1, . . . ,acn € R+, all of them [having been] generated by one compo-
nent of the above family of distributions.
Find the maximum likelihood estimation of the parameters 2" and 6.
Solution
o The verosimility function:
def. 2.2.d. n
2mg) I Pm, . . . ,xnlr,ﬂ) z Haw-‘235)
2:1
TL I'd-1 1 2n
I 5_T”(F(r))_n (H xi) e 6 Z:
2:1
o The log-verosimility function:
def. n 1 n
Knﬂ I lnLr,ﬁ :—2"nlnﬂ—nlnl“2* + 2"—1 lnzvi—— 5131-.
( ) ( ) ( ) ( >2; 6 2:;

***************Ending Page***************

***************Beginning Page***************
***************page number:120**************
120.
Now we Will calculate the partial derivative of 6(7", B) W.r.t. 5, and then equate
it t0 O:
(9 a 6) Tn + 1 Zn: 1 Zn: 6
— 7", :—— — xi:— azi—7“n
8B 6 62 1:1 62 1:1
a A 1 n
—Z I I — 1‘ .
8601,13) 0:16 111§x>0
By substituting B into £035), we will get:
A A n 1 n
[(7416) : —rnlnﬁ — nlnl‘(r) + (7“ — 1) Elnati — 52:102-
1:1 1:1
I Tnlnﬁn) — TnanxZ- — nlnF(r) —|— (7“ — 1);:1115132- — 11— ~2331-
1:1 1:1 21:1 xi 1:1
I Tnlnhm) — 'r'n (anati + 1) — nlnl‘(7") —|— (r — 1);:111131'
1:1 1:1

***************Ending Page***************


***************Beginning Page***************
***************page number:121**************
121.
Therefore, by computing the partial derivative of £0", B) with respect to "I", and
then equating this derivative to 0, we will get:
6 A n r’ n
ganﬁ) : 0<:>nln(m“) +n—n (Ingxi —|— 1) —n- $ +gln93i I (Hi)
n(ln7“ — ¢(7“)) I —nlnn — Zlnajz- —|— nanxi (I)
1:1 1:1
1 7L TL
l — I —l — — l i l i.
n?" 1M1") nn nizzlna: + nizzla:
The solution of the last equation is f, the maximum likelihood estimation of
the parameter 7".

***************Ending Page***************

***************Beginning Page***************
***************page number:122**************
122.
The Gaussian multi-variate distribution:
ML estimation of
the mean and the precision matriaz, A
(A is the inverse of the covariance matriw, E)
CMU, 2010 fall, Aarti Singh, HWl, pr. 3.2.1

***************Ending Page***************


***************Beginning Page***************
***************page number:123**************
123.
The density function of a d-dimensional Gaussian distribution is
as follows:
_1 exp (—%(¢v — mT/m — M)
NW l m A ) I —,
(2W)d/2\/l1\_1l

Where A is the inverse of the covariance matrix, or the so-called
precision matrix. Let {$1,£L‘2,...,£Un} be an i.i.d. sample from a
d-dimensional Gaussian distribution.
Suppose that n >> d. Derive the MLE estimates ﬁr and A.

***************Ending Page***************

***************Beginning Page***************
***************page number:124**************
Hint 124-
You may ﬁnd useful the following formulas (taken from Matriw Identities, by
Sam Roweis, 1999):
1
(2b) |A_1! I —
IAI

(2e) Tr(AB) I Tr(BA);a
more generally, Tr(ABC. . .) : Tr(BC. . .A) : Tr(C . . .AB) : ...

d (9

b —T XA :—T AX :AT

<3>8Xr< > @Xr<>

8
4b —l X : X_1T: XT_1
( ) 8X nl l ( ) ( )
(5c) laTXb : abT

(9X

d
(5g) 8—X(Xa + b)TC(Xa —|— b) I (C —|— CT)(Xa + b)aT
Tr(A), the trace of an n-by-n square matrix A, is deﬁned as the sum of the
elements on the main diagonal (the diagonal from the upper left to the lower
right) of A, i.e., 6111 + . . . —|— am.

a See Theorem 1.3.d from Matrix Analysis f0?" Statistics, 2017, James R. Schott.

***************Ending Page***************


***************Beginning Page***************
***************page number:125**************
Given the $1,---151311 data, the log-likelihood function is: 125-
mm I 1nHN<11111N1> I ZlnN<11|1N1>
1:1 1:1
"d 111(2 ) 11 111 \A—1| 1 ﬁg; )TA(11 )
I —— 7T — — — — 1' — 1; —
2 2 2 2,21 I‘ M
(23) "d1 111 A 1 n TA
— —? n(27T)+§ Ill l— §Z($1—M) ($1—#)-
1:1
For any ﬁxed positive deﬁnite precision matrix A, the log-likelihood is a quadratic
function of ,u with a negative leading coefﬁcient, hence a strictly concave function of ,u.
We then solve
V l(,u A) I 0 g —1(A+AT)ZTL:(33,~—,11)(—1) I O (I) AZnXx, —,u) I 0 4:> Aim,- : nA/1.
M 7 2 1:1 1:1 1:1
(18)
From (18) we get, by the assumption that A is invertible, the following estimate of ,u:
ll : 2?:1 $7,,
n
which coincides with the sample mean E and is constant w.r.t. A.

***************Ending Page***************

***************Beginning Page***************
***************page number:126**************
126.
Now that we have
[(M, A) g l(;l,A) \m e Rd, A being positive deﬁnite,
we continue to consider A by ﬁrst plugging ,[i back in the log-likelihood function (18):
1 TL
m1, A) I 117d 111(271) + gln |A| _ 5 gm _ 5;)TA($Z- - a?) (19)
(22> _%l ln(27r) + gﬂn |A| _ Tr(SA)), (20)
l
Where S is the sample covariance matriazz S I — 2?:1(xi — E)(xZ — 5:)T.
n
Emplanatz'on:
(x1- — E)TA($Z- — E) is a 1-by-1 matrix, therefore (x1- — E)TA(xZ- — f) I Tr((xi — E)TA(xZ- — f»,
and using the (2e) rule, it can be further written as Tr((a:i — 5:)(5131 — EVA).
Using another simple rule, Tr(A —|— B) I Tr(A) —|— Tr(B) (which can be easily proven), it
follows that
2m- _ 5:)TA(05¢ _ a?) z Z mm- _ WAW _ 92)) z Z Tr((xz- _ a?)(a;i _ WA)
¢:1 n 1;:1 n 1:1
Z Tr( 2((952- _ 5;)(351- _ Wm) z Tr((2(a:¢ _ @(wz- _ 03)T>A> z Tr((nS)A) Z nTr(SA).
1:1 izl

***************Ending Page***************


***************Beginning Page***************
***************page number:127**************
127.
By the fact that ln |A| is strictly concave on the domain of positive deﬁnite
A,a and that Tr(SA) is linear in A, we are able to ﬁnd the maximum of the
expression (20) by solving
V Al (i1, A) I O,
which can be provenb to be equivalent to
A—1 — S I 0. (Therefore, ii I A—1 I S.)
Since n >> d, we can safely assume that S is invertible and get the following
estimate: A
A I S_1.
a See, for example, Section 3.1.5, Convex Optimization: http://www.stanford.edu/~boyd/cvxbook/.
b Applying (4b) and (3b) on (20) you’ll get (AT)—1 — ST. Then (AT)—1 — ST I O <=> (A—1)T I ST ¢> A—1 I S.

***************Ending Page***************

***************Beginning Page***************
***************page number:128**************
128.
Notes
1. In the above derivation, we have ensured that the estimates ,u and A
are in the parameter space and satisfy
MM) g up, A) g KM) v” e Rd, A being positive deﬁnite,
so they are the MLE estimates.
2. Instead of using the relation (20), i.e., working with the Tr functional,
one could directly compute the partial derivative of l(,[L, A) in (19):
A (4b),(5c) n _ 1 n A A n _ l n A A
VAZUMA) I 5(AT) 1 — 5 Z<$i — HXiEi — WT I 5A 1 — 5 2(331— HXZ'Z' — WT
1:1 1:1
n n
: —A_1 — —S.
2 2
So,
A A 1 n
VlﬂA :0eA—1”2"2:s”2'_ FA FAT.
A (H ) n 2C1? Ma? H)

***************Ending Page***************


***************Beginning Page***************
***************page number:129**************
129.
Elements of Information Theory:
Some examples and then some useful proofs

***************Ending Page***************

***************Beginning Page***************
***************page number:130**************
130.
Computing entropies and speciﬁc conditional entropies
for discrete random variables
CMU, 2012 spring, R. Rosenfeld, HWZ, pr. 2

***************Ending Page***************


***************Beginning Page***************
***************page number:131**************
131.
On the roll of two six-sided fair dice,
a. Calculate the distribution of the sum (S) of the total.
b. The amount of information (or surprise) when seeing the outcome at
1
for a random variable X is deﬁned as log2 m I —log2 P(X I x). How
I a;
surprised are you (in bits) to observe S I 2, S I 11, S I 5, S I 7?
c. Calculate the entropy of S [as the expected value of the random variable
— log2 P(X I 5c)].
d. Let’s say you throw the die one by one, and the ﬁrst die shows 4. What
is the entropy of S after this observation? Was any information gained / lost
in the process? If so, calculate how much information (in bits) was lost or
gained.

***************Ending Page***************

***************Beginning Page***************
***************page number:132**************
132.
a.
---------uuu
P(S) 1/36 2/36 3/36 4/36 6/36 6/36 6/36 4/36 3/36 2/36 1/36
b.
InformatioMS I 2) I —10g2(1/36) I 10g2 36 I 210g2 6 I 2(1 +10g2 3)
I 5169925001 bits
Information(S I 11) I —10g2 2/36 I 10g2 18 I 1 + 210g2 3 I 4169925001 bits
Information(S I 5) I —10g2 4/36 I 10g2 9 I 210g2 3 I 3169925001 bits
Information(S I 7) I —10g2 6/36 I 10g2 6 I 1 +10g2 3 I 2.584962501 bits

***************Ending Page***************


***************Beginning Page***************
***************page number:133**************
133.
C.
11(5) I —Zpi10g2pi
izl
1 1 2 2 3 3 4 4
I _ 2-_1 _ 2-_1 — 2-—1 — 2-—1 —
< 36 Og236+ 36 Og236+ 36 Og236+ 36 Og236+
5 5 6 6
2-—1 — —1 —
36 Og236+36 Og236>
1 36
: %(210g2 36 + 410g2 18 + 610g2 12 + 810g2 9 + 1010g2 6 + 610g2 6)
1 62
I %(210g262+410g26-3+610g26-2+810g232+1010g2€+610g26)
1
: $(4010g264-2010g23-l-6-1010g25)
1
: %(6010g2 3 —|— 46 — 1010g2 5) I 3274401919 bits.

***************Ending Page***************

***************Beginning Page***************
***************page number:134**************
134.
d.
“III-MEI
P(S\---) nun-I-I-qu
. . 1 1 _
H(S|Fzrst-dze-sh0w5-4) I —6 - 610g2 6 I 10g2 6 I 2.58 blts,
IG(S; First-die-shows-4) I H(S) — H(S\First-dz'e-sh0ws-4) I 3.27 — 2.58 I 0.69 bits.

***************Ending Page***************


***************Beginning Page***************
***************page number:135**************
135.
Computing entropies and average conditional entropies
for discrete random variables
CMU, 2012 spring, Roni Rosenfeld, HWZ, pr. 3

***************Ending Page***************

***************Beginning Page***************
***************page number:136**************
136.
A doctor needs to diagnose a person having cold (C). The primary factor
he considers in his diagnosis is the outside temperature (T). The random
variable C takes two values, yes / no, and the random variable T takes 3
values, sunny, rainy, snowy. The joint distribution of the two variables is
given in following table.
———-

C I no 0.30 0.20 0.10

C I yes 0.05 0.15 0.20
a. Calculate the marginal probabilities P(C), P(T).
Hint: Use P(X I m) I ZY P(X I x;Y I y). For example,
P(C I no) I P(C I no, T I sunny) —l- P(C I no, T I rainy) —l- P(C I no, T I snowy).
b. Calculate the entropies H(C), H(T).
c. Calculate the average conditional entropies H (C \T), H (TlC).

***************Ending Page***************


***************Beginning Page***************
***************page number:137**************
137.
a. PC I (0.6, 0.4) §i PT I (0.35, 0.35, 0.30).
b.
5 5 .
H(C) I 0.610g2 g + 0.410g2 5 I 10g2 5 — 0.610g2 3 — 0.4 I 0.971 blts
20 10
H(T) I 2 - 0.3510g2 7 + 0.310g2 g
I 0.7(2 +10g2 5 —10g2 7) + 0.3(1 +10g2 5 —10g2 3)
I 1.7 +10g2 5 — 0.710g2 7 — 0.310g2 3 I 1.581 bits.

***************Ending Page***************

***************Beginning Page***************
***************page number:138**************
133.
C.
H(C|T)d§' Z P(T:t)-H(C|T:t)
t€Val(T)

: P(T:sunny)-H(C|T:sunny)+P(T:miny)-H(CIT:mz'ny)+

P(T I snowy) - H(C|T I snowy)

0.30 0.05 0.20 0.15
I 0.35-H —,— 0.35-H —,—
<0.30+0.05 0.30+0.05)+ (0.20+0.15 0.20+0.15)+
.1 .
030.}; Lji
0.10+0.20 0.20+0.10

7 6 1 7 4 3 3 1 2
:—-H—— —-H—— —-H——

20 (Mm <7’7>+10 (313)

7 610 7+110 7 —l—7 410 7+310 7 +3 11 3-1-21 3

20 7g267g2 20 7g247g23 10 3g2 3g22

7 6 6 7 3 3 3 2
I _-1 -_-_1 _-1 -_-_1 _-1 -_

20 (6&7 7 70g23)+20 (0.627 7 7Og23)+10 (6&3 3)

7 3 4 2 6 3 3 7 3 9
z _1 7- — — — - — ——_ .1 :_1 __1 __

10 Og2 (10+10+10) (20+20 10) Og23 10 Og27 20 Og23 10
I 0.32715 bits.

***************Ending Page***************


***************Beginning Page***************
***************page number:139**************
139.
H(T|C)d§f' Z P(C:c)-H(T|C:c)
c€Val(C)
I P(C I n0) - H(T|C I n0) + P(C I yes) - H(T|C I yes)
. .2 .1
: 0.60.}1 030 7 0 0 7 0 0 +
0.30 —|— 0.20 —|— 0.10 0.30 + 0.20 —|— 0.10 0.30 + 0.20 —|— 0.10
0.40 _ H 0.05 7 0.15 7 0.20
0.05 —|— 0.15 —|— 0.20 0.05 —|— 0.15 —|— 0.20 0.05 + 0.15 —|— 0.20
1 1 1
I 9H +2.1 13,1
5 2 3 6 5 8 8 2
3 1 1 1 2 1 3 1
:___1 _11 __-__1 _
5(2+3 0g23+6( +0g23)>+5<8 3+8(3 0g23)+2)
3 2 1 2 3
I _ _ _1 _ 2__1
5 (3+2 0g23)+5( 8 0g23>
6 3 .

***************Ending Page***************

***************Beginning Page***************
***************page number:140**************
140.
Entr0pie, entropie comuné,
entropie condi§i0nalfi, cé§tig de informatie:
deﬁnigii §i proprietéiigi imediate
CMU, 2005 fall, T. Mitchell, A. Moore, HWl, pr. 2

***************Ending Page***************


***************Beginning Page***************
***************page number:141**************
141.
Deﬁnigii
o Entropia variabilei X:
H(X) déf' _ 22. P(X z $¢>10gP(X : 331-) "2' EX[—10gP(X)].
o Entropia conditionalé speciﬁcé a variabilei Y in raport cu valoarea wk a variabilei X:
de .
H(Y | szk) :f -§ij(Y:yj )X :xk)10gP(Y:yj ) szk)
"2‘ Ey|szk[—10gP(Y | X I 5%)].
o Entropia condiigionalé medie a variabilei Y in raport cu variabila X:
de . no .
H(Y | X) :f ZkP(X:wk)H(Y l XIwk) :t Ex[H(Y l XM-
o Entropia comunﬁ a variabilelor X §i Y:
def.
H(X,Y) I — 2123- P(X I 11:1,Y I yj)10gP(X : 33¢,Y I yj)
“2' EXX [_ 10g P(X, Y)].
o Cégtigul de informagie a1 variabilelor X §i Y, numité de asemenea informajia mutuald
a variabilei X in raport cu variabila Y (sau invers):
IG(X; Y) “2' MI(X, Y) dif' H(X) _ H(X ) Y) I H(Y) _ H(Y | X)
(Observaigie: ultima egalitate de mai sus are loc datorité rezultatului de la punctul
c de mai jos.)

***************Ending Page***************

***************Beginning Page***************
***************page number:142**************
142.
a.
H(X) Z 0.
1
H(X) I _ ZP(X I 1132-) 10gP(X I 351-): ZP(X I 1131-)10g m 2 0
'L 'L
20 _/—’ZO
Mai mult, H (X) I 0 dacéi §i numai dacéi variabila X este constantii:
1
,,I>“ Presupunem (:5 H(X) I O, adicéi ZZ- P(X I xiﬂog m I O. Datorité faptului
c5 ﬁecare termen din aceastﬁ sumﬁ este mai mare sau egal cu 0, rezultéi c5 H (X ) I 0
1
doar dacé pentru Vi, P(X I x1) I O sau logw I 0, adicéi dacéi pentru Vi,
I $2‘
P(X I xi) I O sau P(X I $2) I 1. Cum insé 2iP(X I xi) I 1 rezultéi c5 existii 0
singurii valoare x1 pentru X astfel incét P(X I 3:1) I 1, iar P(X I a3) I 0 pentru
orice a: I £131. Altfel spus, variabila aleatoare discretéi X este constantéi.
,,I“ Presupunem c5 variabila X este constantéi, ceea ce inseamné céi X ia 0 singuré
valoare x1, cu probabilitatea P(X I 501) I 1. Prin urmare, H(X) I —1 ~10g 1 I 0.

***************Ending Page***************


***************Beginning Page***************
***************page number:143**************
143.
b.
H(Y|X)I—ZZP(X:$¢,Y:yj)10gP(Y:yj|X::137;)
i j
H<YyX) I ZP(X:xi)H(Y\X:,/ci)
I ZP<X:11¢)[—ZP(Y%XwﬂlogPWwaQI
1L J‘
I _ZZP(X:£C¢)P(Y:%|XIZIF¢)1OgP(Y:yjVXIW
. . @—/
7' J :P(X:a:,L-,Y:yj)
I —ZZP(X:$¢7Y:yj)1OgP(§/:yj‘XIZU
i j

***************Ending Page***************

***************Beginning Page***************
***************page number:144**************
144.

c. H(X, Y) I H(X) + H(Y | X) I H(Y) + H(X | Y)
H(X, Y) I — Z 2pm yj) logpm yj)

= —i:ip(wi) My) l w@)10g[p(IZ-) My) I 5%)]

I _ Z ip(q;)) -p(yj | $i)[10gp($i) +10gp(yj \ IE0]

I _ Z ip<x0 -p(yj | $1)10gp($¢) — Z 229(1)) ~p(yj l 58010ng I $1)

I 5310295)) 10%;?(110 - 22M]- | 1111') —ZZP(9$@) 219% | xi)10gp(yj I $1‘)

T
I H(X) + Zp(xZ-)H(Y ) x; $1.): H(X) + H(Y | X)

***************Ending Page***************


***************Beginning Page***************
***************page number:145**************
145.
Mai general (regula de inlénguire):
H<X1,.. .,Xn> = H<X1> + H<X2 | X1) + - - - + H<Xn | X1, . . . ,Xn_1>
H(X X ) E [1 1 I
,..., n I 0 —
I —Ep($17___7$n) [10g p(x1, . . . ,xn) ]
W
I _Ep(w1)[10gp($1)] _ Ep(w1,w2)[10gp(332 I 11)] _ - -'
—Ep<w1,...,$n>[10gp(wn l 111, - - - ,wn_1)]

***************Ending Page***************

***************Beginning Page***************
***************page number:146**************
146.
Proving [in a direct manner] that
the Information Gain is always positive or 0
and that ]G(X, Y) I 0 <:> X is independent of Y
(an indirect proof is made at CMU, 2007 fall, Carlos Guestrin, HWl, pr. 1.2)
Liviu Ciortuz, 2017

***************Ending Page***************


***************Beginning Page***************
***************page number:147**************
147.

Deﬁniigia cdgtz'gului de informatie (sau: a informatiez' mutuale) a1 unei vari-
abile aleatoare X in raport cu o alta variabila aleatoare Y este

1G (X, Y) I H(X) _ H(X | Y) I H(Y) _ H(Y | X).
La CMU, 2007 fall, Carlos Guestrin, HWl, pr. 1.2 s-a demonstrat — pentru cazul in care X
§i Y sunt discrete — ca IG(X, Y) I KL(PX’y||PX Py), unde KL desemneaza entropic, relatimi (sau:
divergenja Kullback-Leibler), PX si Py sunt distribuigiile variabilelor X si, respectiv, Y, iar PX’y este
distribulgia comuna a acestor variabile. Tot la CMU, 2007 fall, Carlos Guestrin, HWl, pr. 1.2 s-a
aratat ca divergenga KL este intotdeauna ne-negativa. in consecinga, IG(X7 Y) Z 0 pentru orice X
§i Y.
a. Aici v5 cerem sa demonstraﬁ inegalitatea IG(X, Y) Z O in maniera directa,
plecand de la prima deﬁniﬁe data mai sus, far-a a [mai] apela la divergenta
Kullback-Leibler.
b. Aratatji tot intr-o maniera directa ca IG(X , Y) : O daca si numai daca
X si Y sunt independente. (Intr-o maniera indirecta, acest rezultat a fost
demonstrat la CMU, 2007 fall, Carlos Guestrin, HWl, pr. 1.2.0.)

***************Ending Page***************

***************Beginning Page***************
***************page number:148**************
148.
Sugestie: Pute§i folosi urmatoarea forma a inegalitafgii lui Jensen:
n TL
unde baza logaritmului se considera supraunitara, az- Z O pentru 2' I 1, . . . ,n §i
2?:1 az- I 1. Egalitatea are loc daca §i numai daca x1 : : :13”.
Observajie: Avantajul la aceasta problema, comparativ cu CMU, 2007 fall, Carlos Guestrin,
HWl, pr. 1.2.a, este ca aici se lucreaza cu 0 singura distribugie (p), nu cu doua distribugii (p §i q).
Totu§i, demonstragia de aici va ﬁ mai laborioasa.
Answer (in Romanian)
a. Presupunem ca valorile variabilei X sunt £131,332, . . . ,xn, iar valorile variabilei
Y sunt y1,y2, . . . ,ym. Avem:
def.
IG(X,Y) I H(X) — H(X|Y)
d f n m TL
6 .
I E —P(fv¢)10g2 PW) — E Fwy‘) E (-PWyj) logz P($¢\yj))
1:1 3:1 1:1

***************Ending Page***************


***************Beginning Page***************
***************page number:149**************
149.
—1G(X1Y): Z 13(931') 10g2 P(f'31)— 213%) ZPWM) 10g2 PWM)
1:1 3:1 1:1
def. n m m n
prob- mam Z (Z P(f'11,y1)) 10%2 H111) — ZP(y1)ZP(w1-ly1)10g2 P(iv1ly1)
1:1 3:1 3:1 1:1
distrib.~,+ n m m n
I Z 213(5131111/3‘) 1082 P(IU1') — Z 2P(y3)P($1|y3)10g2 P($1\yj)
1:1 3:1 3:11:1
def. n m m n
prob. 00nd. Z Z P(a:i,yj)10g2 P(x3) — Z Z P(a:7;,yj)10g2 P($7;\yj)
1:1 3:1 3:11:1
dist'rl'b.~,+ n m
— Z ZP(I11y1)(10g2 P(w1)—10g2 P(~"v1\y1))
1:1 3:1
p721). n m P( > T69._d8 n m P( >
— m — m
log. P(a:3,y-)10g —Z multipl. P(:ci|y-)P(y1)10g —Z
distrib.~,+ m n P(ZU1')
I PW) P(fv1|y')10g —

***************Ending Page***************

***************Beginning Page***************
***************page number:150**************
150.
Intrucét pe de 0 parte P(xZ-|yj) Z 0 §i pe de alté parte 2?:1 P(xZ-|yj) I 1 pentru
ﬁecare valoare yj a lui Y in parte, putern aplica inegalitatea lui Jensen pentru
cea de-a doua sumii din ultima expresie de mai sus — mai exact, pentru
ﬁecare valoare a indicelui j in parte — §i obtinern:
P(:Jci|y-)10g — g 10g P(:1:Z) I O.
_/—/
1
A§adar,
—1'G(X, Y) I ZP(yj)10g2 2P(wilyj)P— S 213(ij0 I 0
Prin urmare, [C(X, Y) Z O.
b. D1n relatla IG(X, Y) : —Zj:1P(yj)ZZ-:1P(xZ-\yj)10g2 P(—\) care a fost
55¢ yj

obtinutﬁ pe slide-ul precedent, decurge imediat urmétoarea consecinté:
atunci cénd X este independent de Y, intrucélt P(;cz) : P(a:i|yj) pentru orice 2'

" “V1 —:0d'IGXY:0.
§1 j, urmeaza ca 0g2 P(£13i|yj) , e01 ( , )

***************Ending Page***************


***************Beginning Page***************
***************page number:151**************
Invers, vom demonstra acum c5 151_
IG(X, Y) I O implicé faptul c5
X este independent de Y. Aplicénd pentru fiecare valoare a
Pentru aceasta vom folosi teorema lui Jensen. indicelui J in parte [ultima Part9
din] teorerna lui Jensen — ceea
Intr-adev5r, din egalitatea ce este posibil, intrucét a,- Z O §i
m n P< ) 2211 al- I 1 pentru ﬁecare valoare
‘mi _ v 0 - v v
—IG(X, Y) = Z P(yj)ZP(rm!yj)10g2 W ﬁxata a lu1 j —, rezulta ca
9:1 £21 1 J
*,—/
50 P 3;.
A : 04 (c0nst.),Vz' I 1,. ..,n.
avem P<£Ellyj>
Este insé adevérat §i 05 PC“) I OZPCEZIyJ) V2 I 1’ ' ' ' ,71.
n Pm) n _ insuménd aceste egalitéitji pentru
1 P Z -—:1 P i:0,V:1,..., . , v
0g2 gww 0%; (m) ‘7 m z I 1, . . .,n, rezulta 217:1 P(a:2) I
1“ 04272-121 P(xi|yj), deci 1 I 04.
Prin urmare, DGCi P($i) I P(ZUZ'|Z/j), V7, I 1, . . . ,7’),
[§i pentru ﬁecare j I 1, . . . ,m],
ZP($i|yj)1Og2 m :10g2 Z P(q;i|yj)m I 07V]- I 17 _ _ -,m- deci X este independent de Y.
1-le P(~’Bi|yj) 1-21 P(~’I»‘¢|yj)

***************Ending Page***************

***************Beginning Page***************
***************page number:152**************
152.
Corollary
Since
H(X,Y) : H(X) +H(Y|X) : H(Y) +H(X\Y),
and also due to the deﬁnition of the Information Gain,
IG(X,Y) I H(X) — H(X|Y) I H(Y) — H(Y|X),
it follows that
X independent of Y (I) H(X) : H(X|Y)
(I) H(Y) : H(Y|X)
¢> H(X,Y) : H(X) +X(Y)

***************Ending Page***************


***************Beginning Page***************
***************page number:153**************
153.
An upper bound for the entropy of a discrete distribution
CMU, 2003 fall, T. Mitchell, A. Moore, HWl, pr. 1.1

***************Ending Page***************

***************Beginning Page***************
***************page number:154**************
154.

Fie X 0 variabilﬁ aleatoare discretél care ia n y
valori §i urmeazé distribugia probabilisté P. X4
Conform deﬁniigiei, entropia lui X este

n

H(X) z _ Z P(X I xi) 10g2 P(X z xi).
1:1
_ v 0 1 x

Ariitagl ca H(X) g 10g2 n.
Sugestie: Puteigi folosi inegalitatea lnx g :U—1,
care are 10c pentru orice x > O. |n(x)

***************Ending Page***************


***************Beginning Page***************
***************page number:155**************
Answer 155-
1 n
H(X) I E <—ZP(X I $¢)1I1P(X _~’IJ¢))
A§adar, 1:1
1 TL
H(X)§1og2n <:> E —ZP(X:a;Z-)1nP(X:xZ-) 310an
1:1
<I> —ZP($¢)1nP(xi)§lnn
1:1
TL 1 TL
— — 1
n 1 TL
n 1
<:> gPQZ) (111% —1nn>§ O
<:> im )1 1 < 0
2' n— _
1:1 x nP(at7;)

***************Ending Page***************

***************Beginning Page***************
***************page number:156**************
156.

. A . . 1
Apllcand megalltatea 1nm g a: — 1 pentru x : —, vom avea:

Z ($1) n Mm) — Z W (an) ) Z n Z W

2:1 2:1 1:1 1:1

1

Observajie: Aceasté margine superioaré chiar este ,,atins5“. De exemplu,
in cazul in care 0 variabiléi aleatoare discretéi X avénd n valori urmeazii
distribulgia uniforméi, se poate veriﬁca imediat céi H(X) : 10g2 n.

***************Ending Page***************


***************Beginning Page***************
***************page number:157**************
157.
A doua solu§iez folosind inegalitatea lui Jensen
Prezentém acum o alto’ demonstratie pentru inegalitatea H (X ) g log2 n, pornind de la
inegalitatea lui Jensen in formé probabilisté (vedeij'i pr. CMU, 2003 fall, T. Mitchell,
A. Moore, HWl, pr. 1.1.c):
Dacéi X este o variabiléi aleatoare si f este o funcgie convexé, atunci f(E[X]) g E[f(X)],
iar dacé f este funcigie concavéi, atunci f(E[X]) Z E[f(X)].
Considerénd $1, . . . ,xn é R si inlocuind in a doua forméi a inegalitéiﬁi lui Jensen datéi mai
1
sus functia f cu funcigia concavé 10g2x si variabila X cu variabila aleatoare W,
: ac
ob§inem inegalitatea:
n 1 n 1
logg (Ewe-41$ —) 2 ZP<X I ‘xi-Moe —,
2.:1 M 1,21 P(X I xi)
_r—/
1
care se rescrie imediat sub forma 10g2 n Z H (X )
Folosind Observajia 1 din enungul pr. CMU, 2003 fall, T. Mitchell, A. Moore, HWl, pr. 1.1 (observatie valabilé'i
si pentru cazul funcgiilor concave) rezulté c5. log2 n = H(X) dacé §i numai dacé m = = m,
adicé P(X = 33%) I l pentru i = 1, . . . ,n.
TL

***************Ending Page***************

***************Beginning Page***************
***************page number:158**************
158.
A treia s01u§ie: folosind metoda multiplicatorilor lui Lagrange
Conform enuntului, ﬁe X 0 variabilé aleatoare discreté n-aré ale cérei valori
sunt luate cu probabilitfitile p1, . . . ,pn respectiv.
. v t. d f.
Stlm ca H(X) n; H(p1, . . . ,pn) é 2?:1 —pi10g2pi.
Vom aréita céi H (X ) g 10g2 n rezolvénd urméitoarea problemd de optimiza're cu
restricj;ii, pe care 0 vom nota cu (P):
maXPiZO H(p1> - - - 7pm) minpizo —H(p1,. . . ,pn)
<I>
Zipi I 1 2119i I 1
Facem observatia c5 am léisat de 0 parte restrictjiile pi Z O pentru i : 1, . . . ,n,
intrucét dupéi rezolvarea problemei [simpliﬁcatii astfel] va rezulta 051 aceste
restricigii (gi, de asemenea, restricigiile pi g 1 pentru 2' I 1, . . . ,n) sunt satisfécute
de cétre solutia 0b§inutéL

***************Ending Page***************


***************Beginning Page***************
***************page number:159**************
159.
Conditiile Ka/Push-Kuhn- Tucker pentru problema (P) se scriu astfel:
fezabilitate primalé: Zpi : 1;
i
statlonarltate / optlmalltate: 3—Lp()\,p1,...,pn) I O pentru z I 1, . . . ,n,

Pi

unde L P este functia lagrangeanci, deﬁnitéi astfel:
Lp()\,p1,---,pn) I ZP¢10g2P¢+ M2291‘ — 1)-

Conform teo're'mei Karush-Kuhn- Tucker, dacé A*,p>1k,...,p;'; este solulgie a sis-
temului format de condigiile Karush-Kuhn-Tucker enunigate mai sus [iar —H
este func§ie convexﬁ, ceea ce vom demonstra mai jos], atunci p1‘, . . . ,p; este
solugie a problemei (P).

***************Ending Page***************

***************Beginning Page***************
***************page number:160**************
160.
Rezolvénd conditiile de stagionaritate / optimalitate enungate mai sus, vom
avea:
8 L (A ) 1 + 1 1 + A 1 + 1 + A
8LP 1 v . .
— I O (i) 10g2 p1- : —)\ — —, constanta care nu deplnde de z.
61p,- 1102
A 1
Prin urmare, am ob§inut pi“ : : p; : 2 1H2 (constant). Cum 22p: : 1,
1
rezulté 05 pf I — pentru 2' I 1,...,n (§i toate condijiz'le de mai sus sunt
n
satisféicute) .
1
Este imediat c5 H(p*1‘, . . . ,pjfb) : n- —10g2 n I 10g2 n.
n

***************Ending Page***************


***************Beginning Page***************
***************page number:161**************
161.

v A v . v v v . 1 . .

Ramane sa ma1 aratam ca solutla p1‘ I I p; I — corespunde max1mulu1
n

functiei H (p1,..., pn). Pentru aceasta, vom aréita 05 —H este functie strict
convexéi in raport cu argumentele p1, . . . , pn:
(9(—H) 1 1n pl- 1
—: -10 -’:10 -+—:—+—.

(919i (P1 g2 P1) g2 pl 1n2 1112 1n2

82 —H 1 1 @2H
Prin urmare, :> ¥ I — - — > 0, iar — I O pentru orice 2' 75 j.
8191- 1112 pl- 8103-8191-
A§adar, matricea hessiané a functiei —H este [simetricii §i] pozitiv deﬁnitii,
ceea ce justiﬁcé faptul c5 —H este functie strict convexé,“ deci solutia pf :
1
I p; I — corespunde unicului punct de minim a1 ei. Altfel spus, p1‘ I I
n
1
p; I — corespunde unicului punct de maxim a1 functiei H, entropia variabilei
n
aleatoare X.
a Vedeti Observajia de la ﬁnalul rezolvérii punctului a de 1a problema CMU, 2015 fall, A. Smola, B. Poczos, HWl,
pr. 3.1.

***************Ending Page***************

***************Beginning Page***************
***************page number:162**************
162.
The particular form of the chain rule for entropies
when X and Y are independent random variables:
H(X,Y) : H(X) +H(Y)
CMU, 2012 spring, R. Rosenfeld, HWZ, pr. 7.b

***************Ending Page***************


***************Beginning Page***************
***************page number:163**************
Prove that if X and Y are independent randorn variables, the following property holds: 163'
H(X, Y) I H(X) + H(Y).
Answer:
H(X,Y) I —ZP(X I :1:,Y I y)10gP(X I 33,)’ I y)
Mi Z P<X I r>P<Y I y)10g(P(X I r>P<Y I y))
I _ i P(X I r)P(Y I y)(10gP(X I x) +1ogP<Y I 14))
I (i P(X I r)P(Y I y)10gP(X I 513)) _ <2P<X I cc)P(Y I y)1OgP(Y I w)
I<;P(Y I 1”)ng I x)10gP(X I x)) _ (;P(X I :6);P(Y I y)10gP(Y I w)
:—(ZP(Y I 11)) - (ZP(X I x)10gP(X I M) — (ZHX I w)) - (ZNY I y)10gP(Y I w)
I _ imx I x)10gP(; I x) - ZPQ/ I y)10gP(Y :y) y
I 11(2) + H(Y) y

***************Ending Page***************

***************Beginning Page***************
***************page number:164**************
164.
If X , Y are continue and independent random variables, then
PX,Y(X I w>Y I y) I PX(X I 5win/(Y I y)
for any x and y, Where p denotes the p.d.f. corresponding to the [c.d.f. of] P.
Therefore,
de .
H(X,Y) If —/ /pX,Y(X I SQY Iy)10gpx,y(X ISM’ Iy)d$dy
X Y
inde .
Ip - f / pX<X I WW I y><10ng<X I w) +10gpy(Y I y» dwdy
X Y
"I - f / pX<w>pY<y><1ong<w> +1ogpy<y>>dwdy
X Y
I —/ / PY(y)PX($)1OgPX($)d$ dy —/ / PX($)PY(Z/)1OgPY(Z/)d$ dy
X Y X Y
I - / pX<x>1ong<w> (f My) dy) da: - f W) (f py<y>1ogpy<y> dy) 61$
X Y X Y
_,—/ \_/—/
1 H(Y)
I - f pX<w>10ng<w>dw + H<Y> (f pX<w>dw)
X X
\_,—/
I H(X) + H(Y) 1

***************Ending Page***************


***************Beginning Page***************
***************page number:165**************
165.
The conditional form 0f
the simplest case 0f the chain rule for entropies (n : 2):
H(X,Y\A) : H(X\A) + H(Y]X,A)
CMU, 2012 spring, Roni Rosenfeld, HW2, pr. 4.b

***************Ending Page***************

***************Beginning Page***************
***************page number:166**************
166.
H(X,YlA)
a. Prove that for any 3 discrete ran-
dom variables X , Y and A, the follow-
ing property holds:
H(X,Y|A) I H(X|A) + H(Y|X, A) H00 H”)
I H(Y|A) + H(X|Y,A).
Answer: H(A)
H(X,Y|A) I H(X,Y,A) —H(A) IH(X,Y,A) —H(Y,A) +H(Y,A) —H(A)
Z (H(X,Y,A) — H(Y,A)) + (HQ/7A) — HUD) Z H(X|Y>A) + H(Y|A)
I H(Y|A) + H(X|Y, A).
We’ve used the fact that H(X, Y) I H(X|Y) —|— H(Y) (see CMU, 2005 fall, T.
Mitchell, A. Moore, HWl, pr. 2).

***************Ending Page***************


***************Beginning Page***************
***************page number:167**************
167.
b. [LC]
Ariitalgi céi varianta generalizatéi a regule'i de inldnjuz're pentru entropii (care
a fost demonstratii deja la problema CMU, 2005 fall, T. Mitchell, A. Moore,
HWl, pr. 2)
poate ﬁ dedusfi [in manieré alternativii] pornind de la relatia H (X, Y) I H (X )—|—
H (Y | X) — varianta de bazé a regulei de inlﬁniguire pentru entropii — §i
Qinénd cont de proprietatea care a fost demonstraté la punctul a.

***************Ending Page***************

***************Beginning Page***************
***************page number:168**************
168.
Réispuns
Pentru n I 2, rela§ia este identicé cu varianta de bazé a regulei de in15n§uire,
deci este adevéiratéi. Pentru n : 3, putem scrie:
H(X1>X2,X3) I H(X1) + H(X2,X3|X1)
é H(X1) + H(X2|X1) + H(X3\X1, X2).

Pentru n oarecare, facem demonstragia prin recursie:
H(X1,...,Xn) : H(X1) +H(X2,...,Xn\X1)

é H(X1) + H(X2|X1) + H(X3, . . . ,Xn|X1, X2)

g H(X1) + H(X2|Xl) + H(X3|X17 X2) + H(X47 ' ' ' 7XTLIX17 X27 X3)

g H<X1> + H<X2|X1> —|— H(X3|X1,X2) —|— . . . + H<Xn‘X1,.. . ,Xn_1).

***************Ending Page***************


***************Beginning Page***************
***************page number:169**************
169.
Derivation of entropy deﬁnition,
starting from a set of desirable properties
CMU, 2005 fall, T. Mitchell, A. Moore, HWl, pr. 2.2

***************Ending Page***************

***************Beginning Page***************
***************page number:170**************
170.
n
Remark: The deﬁnition we gave for entropy — 22-21 pi 10g pi is not very intuitive.
Theorem:
If @bn (p1, . . . ,pn) satisﬁes the following axioms
A0. [LCz] ¢1(1) : 0 because we view 7,0,, as a measure of disorder / uncertainty,
and in the case n I 1 there is no disorder / uncertainty;
A1. 1b,, should be continuous in p,- and symmetric in its arguments;
A2. if p,- : 1/n then 1b,, should be a strictly increasing function of n;
(If all events are equally likely, then having more events means being
more uncertain.)
A3. if a choice among N events is broken down into successive choices, then
the entropy should be the weighted sum of the entropy at each stage;
then @bn(p1, . . . ,pn) : —Kzzp,-logp,- where K is a positive constant.
(As we’ll see, K depends however on 7,08 (1,. . . , l) for a certain, ﬁxed s € N*\{1}).
S S
Remark: We will prove the theorem ﬁrstly for uniform distributions (p7; : 1/11), secondly for
p,- € Q+, and ﬁnally for p; E R+ such that Zip,- I 1.

***************Ending Page***************


***************Beginning Page***************
***************page number:171**************
171.
Example for the axiom A3:
/1/2 1/2\
//2 1/3 1/6 /2/3 1/3\‘
a i \c b c
Encoding 1 Encoding 2
1 1 1 1 1 1 1 1 1 1 2 1
H——— :—l2—1 —1:——12 ——1:——1
(2,3,6) 20g +30g3+60g6 (2+6) 0g +(3+6) 0g3 3+20g3
1 1 1 2 1 1 2 3 1 1 2 2 1
H—— —H—— :1 ——l — —1 :1 —1 —— :— —1
<2’2)+2 (3’3) +2<3 Og2+30g3) +2(Og3 3) 3+2Og3

***************Ending Page***************

***************Beginning Page***************
***************page number:172**************
172.
Case 1: pi: 1/n for z:1,...,n
We will denote A(n) “it ¢n(1/n,1/n,. ..,1/n), and for parts b-d we will consider t G N*,
and subsequently s € N*, s 75 1 and m 6 N such that
5m g tn g sm+1.
[LCz We could set s : 2, just to be [more] concrete. However, the reasoning which will be
developed below doesn’t depend on the particular value chosen for 5.]
a. Show that A(sm) I mA(s) for any s,m € N*. (1)
b. Show that l m — 10—gt lg l. (2)
n logs n
c. Due to A2 it follows (immediately)
1 1 1 1 1 1
Sm —,...,— g n —,...,— g 5m —,...,—
77b (Sm Sm) wt (tn tn) w +1 (Sm+1 Sm+1>
A( > i.e. A(sm) g AU”) g A(sm+1).
m t 1
ShOWthatlg—m ISE for 5751. (3)
_ _ , _ _ A t lo t 2

d. Comblnlng (2) —l- (3) 1mmed1ately glves l ﬁg) — é lg E for s 75 1 (4)

Show that this inequation implies A(t) I Klogt with K > O (due to A2). (5)

***************Ending Page***************


***************Beginning Page***************
***************page number:173**************
173.
Proof
a.
nivel1
1/s 1/s 1/s
nivel2 ' ' ' de-s-o-ri ' ' '
l l l 1/s 1/s 1/s
S Sm Sm - - -
desori
de smori
nivelm
1/s 1/s 1/s
dens-o-ri
Applying the axion A3 0n the right encoding from above gives:
m 1 2 1 m—1 1
I A(s) —|— A(s) —|— A(s) —|— . . . —|— A(s) I mA(s)
ﬁ,—/
m times

***************Ending Page***************

***************Beginning Page***************
***************page number:174**************
174.
Proof (cont’d)
b.
5m g tn g sm+1 :> mlogs g nlogt g (172+ l)logs :>
mglogtgm+1I>OSlogt_mgl :> logt_@ £1
n logs n n logs n n logs n n
c.
A m n m+1 (1) 5751
(s )§A(t)§A(s )imA(s)§nA(t)§(m+1)A(s):>
m A(t) m 1 A(t) m 1 A(t) m 1
_<—<_ _:>0<—__<_ :> —__ <_
n_A(s)_n+n _A(5) n_n A(s) n_n
d. Consider again s'm g t" g sm+1 with s, t ﬁxed. If m —> oo then n —> oo and
At 1 7f 2 At 1 t
from l - i g _ it follows that l - ﬁ _> 0.
A(s) logs n A(s) logs
Therefore E — 10_gt : O and so E I 10_g15.
A(s) logs A($) logs
A A
Finally, A(t) : Q logt I Klogt, where K I E > 0 (Where s € N*\{1}).
logs logs

***************Ending Page***************


***************Beginning Page***************
***************page number:175**************
175.
Case 2: 192-6 Q+ for i:1,...,k, such that 2119i: 1
Let’s consider a set of N Z 2 equiprobable ran- level 1
dom events, and 73 I ($1, SQ, . . . , Sk) a partition
of this set. Let’s denote p,- :| Si | /N. lSll/N lSkl/N
A “natural” two-step ecoding (as shown in the ISZIM lsil/N
nearby ﬁgure) leads to A(N) :¢k(p1,...,pk)+ $1 $2 Si S
zipiAQ Si |), based on the axiom A3. I I2 k
. . . . . . eve . . .
Finally, using the result A(t) I Klogt, gives:
1/lSil 1/lSil 1/lSil
KlogN I ¢k(p1,- - - ,Pk) + Kim- log | Si l
:> ¢k(P1,---,P/<) I KlIOgN—ZP110g|Si|l
Si
I Kl(1OgN)ZPi— 2291105; | Si | l I —KZP¢10g‘N—| I —K2Pi10gpi

***************Ending Page***************

***************Beginning Page***************
***************page number:176**************
. 176.
Case 3: p,- € R+ for z : 1,...,k, such that 2,19,: 1
v ~ (n) + - (n) _ , - _ _ (n) dif'
Let s cons1der {ql- M21 C Q such that 11n"1,,_,o0 q,- _ p,, for z _ 1,. ..,l<: 1, and qk _
1 — 22:11 qgn). (It can be easily shown that there is n0 E N such that qlgn) E Q+ Vn Z n0.)
It follows that lirn gig”) I 1 — 21:11 liIn qgn) I 1 — 21:11 p,- : p1,. Therefore, lirn,,_>00 qgn) I
pi, Vi: 1,...,]€.
Since qgn) G Q+ for z' : 1,. ..,k, it follows (see Case 2) that qugn)’ . . .,q,gn)) I
—K2¢q£n)10gq§n)7 Vn Z 1. Since log is a continuous function, it follows that
lirn ¢k(q(n) q(n)) I lirn (— Kqun) log (#1)) : —KZ lirn qw) log (107’) : —K2p‘ logp'. (21)
n—>oo 1 7 j k n—>oo ' Z Z ' n—>oo Z z ‘ z z
On the other hand, we know from A1 that the function w], must be continuous in each
of its arguments, so
11m who“) 61””) 21m 11m d”) 11m W) 21mm pk» (22>
n—>oo 1 7 7 k n—>oo 1 7 ’n—>oo k 7 ’
From (21) and (22), we can conclude that
¢k(p1,...,pk) : —KZp,-logp,, for Vp, € [0,1], s.t. 219,-: 1.

***************Ending Page***************


***************Beginning Page***************
***************page number:177**************
177.
Using Information Gain / Mutual Information for doing
Feature Selection
CMU, 2009 spring, Ziv Bar-Joseph, HW5, pr. 6

***************Ending Page***************

***************Beginning Page***************
***************page number:178**************
178.

Given the following observations for input binary features X1,X2,X3,X4,X5,
and output binary label Y, we would like to use a ﬁlter approach to reduce
the feature space of {X1,X2,X3,X4,X5}.

0 1 1 0 1 0

1 O O O 1 O

0 1 O 1 0 1

1 1 1 1 0 1

O 1 1 0 O 1

0 0 O 1 1 1

1 0 0 1 0 1

1 1 1 0 1 1
a. Calculate the mutual information M I (Xi, Y) for each i.
b. Accordingly choose the smallest subset of features such that the best
classiﬁer trained on the reduced feature set will perform at least as well as
the best classiﬁer trained on the whole feature set. Explain the reasons behind
your choice.

***************Ending Page***************


***************Beginning Page***************
***************page number:179**************
Answer 179.

a. As shown at CMU, 2007 fall, Carlos

Guestrin, HWl, pr. 1.2, the mutual infor-

mation can be calculated as: ——-

def. P($)P(y) P(X1 25131) .-
x y 7 P(X3 2953)

The marginal probabilities, estimated (in P<X4 I $4)

the MLE sense) from the given data are P<X5 :x5>

shown in the nearby tables.

The joint probabilities P(XZ' I 387;,Y I y), also estimated from the data, are:
———--
———-—-
———-—-

Note: Columns 3 and 5 can be easily computed using columns 2 and respectively 4

(and also the ﬁrst table from above), as P(wi I 0,3; I 1) I P(a:i I O) — P(:1:Z- I 0,34 I O) and

P(:I:Z- I 1,3; I l) I P(:I:Z- I l) —P(a§i I Ly I O).

***************Ending Page***************

***************Beginning Page***************
***************page number:180**************
180.
Using these probabilities and the given formula, the mutual informations
for each of these features w.r.t. Y are: MI(X1,Y) I 0, MI(X2,Y) I 0.01571,
MI(X3,Y) : 0, MI(X4,Y) I 0.3113 and MI(X5,Y) I 0.3113.
Note: One could easily check, using the previous tables, that X1 is indepen-
dent of Y, and X3 is also independent of Y (so, we can determine in this way
too that MI(X1,Y) and MI(X3,Y) are 0).
b. In order to select a set of features, we can prioritize the ones with more
mutual information because they are less independent to Y.
By looking at the results of the previous question, we can see that X5 and
X4 are the features with more mutual information (0.3113), followed by X2
(0.0157) and, ﬁnally, X1 and X3 that do not have mutual information with Y.
By inspection of the data, we can see that if we select X5, X4 and X2 there
are two samples of different classes with the same features (X2 : 1, X4 I 0,
X5 : 1). To avoid this problem, we can add X1 as an extra feature.
Note: Although the mutual information of X1 with Y is zero, it does not
mean that the combination of X1 with other features will also have zero
mutual information [LCz W.r.t. Y].

***************Ending Page***************


***************Beginning Page***************
***************page number:181**************
181.
Exponential p.d.f.
3 — x=o.5
— i=1
Computing the entropy of the f3
exponential distribution §
CMU, 2011 spring, R. Rosenfeld, g
HWZ, pr. 2.0
O 1 2 3 4 5

***************Ending Page***************

***************Beginning Page***************
***************page number:182**************
182.

Pentru 0 distribugie de probabilitate continuéi P, entropia se deﬁnegte ast-
fel:

+00 1
Calculatgi entropia distributiez' continue ewponentiale de parametru A > 0.
Deﬁni§ia acestei distribuﬁi este urmétoarea:

A5”, dacﬁ 55 Z O;

PW) — { O, dacﬁ 55 < O.

Indicatie: Dacéi P(.:1:) : 0, veti presupune c5 —P(x)10g2 P(x) : 0.

***************Ending Page***************


***************Beginning Page***************
***************page number:183**************
183.
Answer
H(P) f0 P( )1 1 d +/OOP( )1 1 d
I o — 0 —
m $ gZPw) @- O x g2131710”:
dei'P /O 010 0da:+/OO)\6_M10 de—/OOA@—*$10 Law
— -<>O g2 0 g2 A64“ — 0 g2 Ae—>‘$
_,—/
O
1 00 1 1 OO 1 1
HP z — A-M1—d:— A—*m1_1—d
:> () InZ/O 6 nAe—>‘~'” w 1n2/0 e (nA+ne—>‘x) x
1 OO —>\33 Aw
I E/o A6 (—1n)\+lne )da:
1 ‘>0 _A
I _ 96-1
1n2/0 A6 ( n)\+>\x)dx
— i/OOA—W 11m +i/OO>\_M>\ d
— 1n2 0 e n a: 1n2 0 e :1; x
—1n)\ OO A OO
1n2 f0 e :E-i-mZ/O e x x
1A 0° A 0°
I n—/ (e_’\m)l d$——/ (6_>‘m)lscda:.
1n2 0 1n2 0

***************Ending Page***************

***************Beginning Page***************
***************page number:184**************
184.
Prima integraléi se rezolvii foarte u§0rz
OO / OO
/ (e_>‘x) dzc I e_>‘$|0 I e_°° — 60 I O — 1 I —1.
0
Pentru a rezolva cea de-a doua integralﬁ se poate folosi formula de integrare prin pdrjz':
OO / OO OO OO OO
/ (e_>‘m) 93de e_>‘mx,0 —/ e_>‘$a:’da:: e_>‘mx,0 —/ 6”‘:C dai.
0 0 0
Integrala deﬁnitii e_>‘a’x|go nu se poate calcula direct (din cauza conﬂictului 0 - 00 care
se produce atunci cénd lui x i se atribuie valoarea-limité 00), ci se calculeazé folosind
'regula lui l’H6pz'tal:
a; a3’ 1 1
lim aceﬁw I lirn T I lim —, I lim — I — lim 64“ I e_°o I O,
a:—>oo a:—>oo 6 :1: a1—>oo (eAfE) x—>oo Aekm A x—>oo
deci
OO
@—*%\0 z 0 - 0 z 0.

***************Ending Page***************


***************Beginning Page***************
***************page number:185**************
185.
Integrala fooo 6”“ dzc se calculeazé ugor:
0° 1 0° 1 1 00 1 1
—)\;l: _ —>\:r: _ —>\w _ _
/06 d$——X/O (e )dx_—Xe ‘0_—X(0—1)_X.
Prin urmare,
OO —}\:17 I 1 1
d I O — — I ——
f0 (6 ) a: x A x
ceea ce conduce la rezultatul ﬁnal:
111A A 1 111A 1 1 —1n)\
HP:—— —-—:—— —:—.
( ) 1n2+1n2 A ln2+1n2 1n2

***************Ending Page***************

***************Beginning Page***************
***************page number:186**************
186.
Cross-entropy (CH):
deﬁnition, some basic properties, and exempliﬁcations
CMU, 2011 spring, Roni Rosenfeld, HW2, pr. 3.c

***************Ending Page***************


***************Beginning Page***************
***************page number:187**************
187.
Cross-entropy, CH ( p,q), measures the average number of bits needed to en-
code an event from a set of possibilities, if a coding scheme is used based on
a given probability distribution q, rather than the “true” distribution p.
For discrete random variables p and q this means
CHmW I — 2W?) 10g C1(1')
CL‘
The situation for continuous random variable distributions is analogous:
CHOW) I —/ 19(56) 10s (1(1‘) d11-

X

a. Can cross-entropy be negative? Either prove or give a counter-example.

***************Ending Page***************

***************Beginning Page***************
***************page number:188**************
188.
Answer
a. No. Here follows the proof:
For every probability value p(x) and q(x), we know from defnition that 0 g
p(at) g 1 and O g q(az) g 1. From q(m) g 1, we conclude that log q(x) g 0. Given
that 0 g p(a:), and —logq(a:) Z O, we conclude that 0 § —p(a:) log q(x) Thus, the
sum of these terms will also be greater or equal to O, so cross-entropy is never
negative.
Note, however that unlike entropy, cross-entropy is not bounded, so it can
grow to inﬁnity if for an x, q(w) is zero and p(w) is not zero.

***************Ending Page***************


***************Beginning Page***************
***************page number:189**************
189.
b. In many experiments, the quality of different hypothesis models are com-
pared on a data set. Imagine you derived two different models to predict the
probabilities of 7 different possible outcomes, and the probability distribu-
tions predicted by the models are ql, and q2 as follows:
ql I (0.1,0.1,0.2,0.3,0.2,0.05,0.05) and qg I (0.05,01,015,035,0.2,0.1,0.05).
The experiments are done on a data set with the following empirical distri-
bution:
pampmcal I (0.05, 0.1, 0.2, 0.3, 0.2, 0.1, 0.05).
Compute the cross-entropies, CH(p€mp,T,cal,q1) and CH(pempmcal,q2).
Which model has a lower cross-entropy? Is this model guaranteed to be a
better one? Explain your answer.

***************Ending Page***************

***************Beginning Page***************
***************page number:190**************
Answer 190'
b. Using the cross-entropy formula we see that:
CH(pemp,~m~cal,q1) I —(0.05 log 0.1 + 0.1 log 0.1 + 0.2 log 0.2 —|— 0.3 log 0.3 —|—
0.2 log 0.2 —|— 0.1 log 0.05 —|— 0.05 log 0.05) I 2.596 bits
CH(pemp,-m-0al, (12) I —(0.05 log 0.05 —|— 0.1 log 0.1 —|— 0.2 log 0.15 + 0.3 log 0.35 +
0.2 log 0.2 + 0.1 log 0.1 + 0.05 log 0.05) : 2.56 bits.
Also,
1 1 1 3 10
H(pemp,-m-C) : 2- ﬁlogQ 20 + 2- ElegQ 10 —|— 2- 510g2 5 —|— E log2 g
I 0.7 + 0.910g2 5 — 0.3 log2 3 I 2.314 bits.
It can be seen that the pemecal distribution has a lower cross-entropy with the model
q2, so it is reasonable to say that q2 is a better choice.
However, it is not guaranteed that this model is always the better model, because we
are working with an “empirical” distribution here, and the “true” distribution might
not be reﬂected in this empirical distribution completely. Usually, sampling bias and
insufficient training data samples will widen the gap between the true distribution and
the empirical one, so in practice when designing the experiment, you should always
have that in mind, and if possible use techniques that minimize these risks.

***************Ending Page***************


***************Beginning Page***************
***************page number:191**************
191.
Relative entropy a.k.a. the Kulback-Leibler divergence,
and the [relationship to] information gain;
some basic properties
CMU, 2007 fall, C. Guestrin, HWl, pr. 1.2
[adapted by Liviu Ciortuz]

***************Ending Page***************

***************Beginning Page***************
***************page number:192**************
192.

The relative entropy — also known as the Kullback-Lez'bler (KL) divergence
— from a distribution p to a distribution q is deﬁned as

d f. WI)

KL<P||Q> é — Em’) 10g
(EGX WY)

From an information theory perspective, the KL-divergence speciﬁes the num-
ber of additional bits required on average to transmit values of X if the values
are distributed with respect to p but we encode them assuming the distribu-
tion q.

***************Ending Page***************


***************Beginning Page***************
***************page number:193**************
193.
Notes
1. KL is not a distance measure, since it is not symmetric (i.e., in general
KL<P||Q> 75 KL<QI|P))-
1
Another measure, which is deﬁned as JSD(p]|q) : 5(KL(p|\(p+Q)/2) + KL(q|\(p-|—
p)/2)), and is called the Jensen-Shannon divergence is symmetric.
2. The quantity
d(X,Y) dezf' H(X,Y) — IG(X,Y) I H(X) —|—H(Y) — 21G(X,Y)

I H(X | Y)+H(Y | X)
known as variation of information, is a distance metric, i.e., it is non-negative,
symmetric, implies indiscernability, and satisﬁes the triangle inequality.

***************Ending Page***************

***************Beginning Page***************
***************page number:194**************
194.

a. Show that KL(p||q) Z 0, and KL(p\|q) : 0 iﬁ' p(w) : q(:v) for all x.
(More generally, the smaller the KL-divergence, the more similar the two distributions.)
Indicatie:
Pentru a demonstra punctul acesta puteti folosi ine-
galitatea lui Jensen: fO’) x
Daca f : R —> R este 0 functie convexa, atunci pentru #(ﬁ-tl-JSi-FIO-fgg) a
orice t € [0,1] si orice what; € R urmeaza f(x)

f(t$1 + (1 — @152) S WW1) + (1 — t)f($2)- x | y
Daca f este functie strict convexa, atunci egalitatea tx+(1—t)y
are loc doar daca 331 I x2.
Mai general, pentru orice a, Z O, 2': 1,. ..,n cu 21a, 75 O si orice at, E R, 2': 1,. ..,n, avem
f — g —.

2]‘ aj 23' aj

Daca f este strict convexa, atunci egalitatea are loc doar daca x1 : : at”.
Evident, rezultate similare pot ﬁ formulate si pentru functii concave.

***************Ending Page***************


***************Beginning Page***************
***************page number:195**************
195.
Answer
Vom dovedi inegalitatea KL(p\|q) Z 0 folosind inegalitatea lui Jensen, in eX-
presia ciireia vorn inlocui f cu funcigia convexii —10g2, pe ai cu p(a:1) si pe 33¢ cu
PW)
(Pentru conveniengé, in cele ce urmeazé vom renun§a la indicele variabilei 5c.)
Vom avea:
def- (N)
KL p q : — p w 10g—
< 1| > Z < > W)
Jensen q($) _ _ _
2 — 10s<2p<11¢>m> — —10g(2q(iv)) — —10g1 — 0
m m 1
Asadar, KL (p H q) Z O, oricare ar ﬁ distribuigiile (discrete) p §i q.

***************Ending Page***************

***************Beginning Page***************
***************page number:196**************
196.
Vom demonstra acum c5 KL(p]|q) : 0 (I) p : q.
,’<:“
. _ . . v q(w) _ . q(w) _ .
Egalltatea 10(90) - q(a:) Impllca W _ 1, dec1 logﬁ - O pentru orlce as, de unde
p 5U p QC
rezultéi imediat KL(p||q) I 0.
,’:“
Stim 05 in inegalitatea lui Jensen are 10c egalitatea doar in cazul in care 51:2- : 33]-
pentru orice 2' §i j.
in cazul de fagé, aceasté condiigie se traduce prin faptul 05 raportul % este acela§i
p 5c
(a) pentru orice valoare a lui at.
Tinénd cont c5 Z p(x) : 1 §i Z q(x) : Z p(m)@ : 1 rezulté c5 oz : @ : 1
’ ‘I’ w x 29(1)?) 7 29(113) ’
deci p(a:) I q(x) pentru orice :13, ceea (:e inseamnéi 05 distribugiile p §i q sunt identice.

***************Ending Page***************


***************Beginning Page***************
***************page number:197**************
197.
b. We can deﬁne the information gain as the KL-divergence from the
observed joint distribution of X and Y to the product of their observed
marginals:
def. PXWWHZJ)
[G X>Y I KL PX,Y PXPY :— pX,Y 3379 log (—
< > < |\( >> ZZ < > mm,
"2* 52mm 10g (1W) (23>
l, y may)
Prove that this deﬁnition of information gain is equivalent to the one given
at CMU, 2005 fall, T. Mitchell, A. Moore, HWl, pr. 2. That is, show that
IG(X, Y) I H[X] — H[X|Y] I H[Y] — H[Y|X], starting from the deﬁnition in
terms of KL-divergence.
Remark:
From (23) it follows that
_ 19(511 I y) _
1G(X,Y) — Zp(y)zp($ | y)10gW — Zp(y)KL(PX|Y ll PX)
y a: y
I EYlKL(PX|Y l|PX>l

***************Ending Page***************

***************Beginning Page***************
***************page number:198**************
198.
Answer
5y making use of the multiplication rule, namely p(a:, y) I p(:c | y)p(y), we will
KLQQXX H (PX PH)

def._KL _ x 0 P(w)p(y)
— 2363;“ W gi m1»)
I _ 22mg) 10g (105%)) I _ ggpmwogm —10gp<w | w]
I _ Z 229W’ y) logp(w) — <— Z ZP<$>ZD1OgP<$ | W)
z _ 210gp(33) 21m, y) —H[X | Y] : 21166) logptv) — HiX I Y]

a; y m
:p(1v)

I H[X]—H[X\Y]:IG(X,Y)

***************Ending Page***************


***************Beginning Page***************
***************page number:199**************
199.
c.
A direct consequence of parts a. and b. is that I G(X , Y) Z 0 (and therefore
H(X) Z H(X|Y) and H(Y) Z H(Y]X)) for any discrete random variables X and
Y.
Prove that IG(X, Y) I 0 iff X and Y are independent.
Answer:
This is also an immediate consequence of parts a. and b. already proven:
_ g _ (a) -
IG(X, Y) _ 0 KL(pX7y\|pX py) _ O 41> X and Y are 1ndependent.

***************Ending Page***************

***************Beginning Page***************
***************page number:200**************
Remark (in Romanian) 200'
Putem (re)dem0nstra inegalitatea IG(X, Y) Z O, folosind (doar!) rezul-
tatul de la punctul b. (nu §i cel de la punctul a.), §i anume c5 IG(Y,X) :
— 2:1; 2y p(a:, y) 10g (W) . Ideea este s51 aplicéim inegalitatea lui Jensen intr-
0 formii u§0r generalizatii, §i anume:
— in locul unui singur indice, se vor considera doi indici (a§adar in loc de
az- §i 93¢ vom avea aZ-j §i respectiv x15); . .
— vorn lua f I —10g2 iar aZ-j <—p(x¢,yj) §i xZ-j <— 2W;
— in ﬁne, vom §ine cont c5 ZZ- ij(:cZ-,yj) : 1. p whxj
Prin urrnare,
pxi-py' P331‘ ‘Py'
IG<X§Y> I _22p(xijyj)1ogw I ZZPWW [ _ 10g M]
P $1‘ ‘P y-
2 —10g (zgpwhyﬁw) I _10g (22m m»)
I —10g( 219W) - 219%) ) I —1Og1 I 0
1
in concluzie, IG(X,Y) Z 0.

***************Ending Page***************


***************Beginning Page***************
***************page number:201**************
201.
Remark (c0nt’d)
in ce privergte echivalenga IG(X, Y) I O (I) X §i Y sunt independente:
,,¢ ‘4
Dacé X §i Y sunt variabilele independente,
atunci p(xZ-,yj) I p(wZ-)p(yj) pentru orice 2' §i j.
in consecinii'é, togi logaritmii din partea dreaptii a primei egalitéiﬁ din calculul de
mai sus sunt O §i rezultéi IG(X,Y) I 0.
,,:>“
Invers, presupunénd cii I G(X , Y) I O, vom tine cont de faptul c5 putem exprima
c€1§tigul de informagie cu ajutorul divergentei KL §i vom aplica un rationament
similar cu cel de la punctul a.
m. .
Rezultﬁ 05 W I 1 §i deci p($i)p(yj) I p($Z-, yj) pentru orice 2' §i j.
Aceasta echivaleazéi cu a spune c5 variabilele X §i Y sunt independente.

***************Ending Page***************

***************Beginning Page***************
***************page number:202**************
202.
Conditional KL divergence;
The KL divergence of two common distributions: the chain rule;
The conditional KL divergence for a vector of
variables which are mutually independent w.r.t. another variable
Stanford, 2015 fall, Andrew Ng, HWS, pr. 5.b

***************Ending Page***************


***************Beginning Page***************
***************page number:203**************
203.
Remember that the Kullback-Leibler (KL) divergence between two discrete-
valued distributions P(X), Q(X) is deﬁned as followsza
13(11)
KL(P|\Q) I P<w>1og —.
E Q<x>
For notational convenience, we assume P(;c) > O and Q(x) > O, Vm. Sometimes,
we also write the KL divergence as KL(P|\Q) : KL(P(X)|\Q(X)).
a If P and Q are densities for continuous-valued random variables, then the sum is replaced by an integral, and
everything stated in this problem works ﬁne as well. But for the sake of simplicity, in this problem we’ll just work
with this form of KL divergence for probability mass functions / discrete-valued distributions.

***************Ending Page***************

***************Beginning Page***************
***************page number:204**************
204.
The KL divergence between two conditional distributions P(X \Y), Q(X|Y) is
deﬁned as follows:
KL<P<X|Y> |!Q(X|Y)) — P P 1 ML”)
—Z (y) Z (wly) Og— -
y w Q<wly>
This can be thought of as the expected KL divergence between the corre-
sponding conditional distributions on 50 (that is, between P(X|Y I y) and
Q(X\Y : y)), where the expectation is taken over the random y.
a. Prove the following chain rule for KL divergence:
KL(P(X,Y) ||Q(X,Y)) I KL(P(X) ||Q(X))+KL(P(Y|X) llQ<YlX))
I KL(P(Y) llQ(Y)) +KL(P(XlY) l|Q(X|Y))- (24)

***************Ending Page***************


***************Beginning Page***************
***************page number:205**************
205.
Solution
Fwy) P(I)P(ylw)
KLPX,Y QX,Y I P33,ylog—: Px,ylog—
< ( )H < D Z < > my) Z ( > Q(w)62(ylw)
PW) P(y|$)
: Px,ylog—+Pa:,ylog—
;< < ) 62(11) < > mm)
PW) P(y|111)
: leog—+ PxPyxlog—
E U Q<w> $2 U m Q(y|fv))
PW) P(y|fll)
: leog—+ Pa: Pyxlog—
E U Q<w> Z U; <|> Q(y|w))
I KL(P(X)|\Q(X))+KL(P(YlX)||Q(Y|X)),
where we applied (in order): the deﬁnition of KL, the deﬁnition of conditional
probability, log of product is sum of logs, splitting the summation, 2y P(:c, y) :
P(x), and the deﬁnition of KL.
One can prove the second equality in (24) in a similar way.

***************Ending Page***************

***************Beginning Page***************
***************page number:206**************
206.
b. Prove that if X I (X1,...,Xn) and for every 2' 51$ j it follows that X1- is
conditionally independent of Xj w.r.t. Y, then
KL(P(XlY) || Q(X\Y)) I ZKMPQQIY) ll Q(X¢|Y))- (25)
¢:1
Solution [by Liviu Ciortuz]
KL(P(X\Y)\|Q(X|Y)) déf' Em Mogw
’ wa)
idep. cdt. n P<£Uily) n P(a3i|y)
I P(w,y) 10g— I P(w,y)10g—
g 2:; Qtvily) 2:2; Manly)
dZ-swwg. n _ P<$ily> _ ” , Pei-w)
— g gP<M>1og QWM — 22:; 2y: ;P(w1,y)10g Manly)
de . n
If ZKL<P<X1|Y>|\Q<XZ'|Y>>-
1:1

***************Ending Page***************


***************Beginning Page***************
***************page number:207**************
207.
The maximization of the (log-)verosimility function
is equivalent to the minimization of
the KL divergence / relative entropy
Stanford, 2015 fall, Andrew Ng, HW3, pr. 5.c

***************Ending Page***************

***************Beginning Page***************
***************page number:208**************
208.
Consider a density estimation problem, and suppose we are given
a training dataset {Mini = 17 . . . ,m}. Let the empirical distribution
be 1
P(x) : — forz':l,...,m.

m
(P is just the uniform distribution over the training set; i.e., sam-
pling from the empirical distribution is the same as picking a ran-
dom example from the training set.)
Suppose we have some family of distributions P9 parametrized by
9. (If you like, think of P9(a:) as an alternative notation for P(x; @).)
Prove that ﬁnding the maximum likelihood estimate for the pa-
rameter Q is equivalent to ﬁnding P9 with minimal KL divergence
from P. I.e., prove:

arg min KL(15\ 1P9) I argmax Z ln P@(w(i))
6 9 2:1

***************Ending Page***************


***************Beginning Page***************
***************page number:209**************
209.
Answer
arg main KL(P|\P9) : arg main (21PM) log 15(x) — 15(33) log P9(:v))>
: arg main <Z(—15($) log Pg<$>>> : arg meax (2(15(x) log P9(x)))
1 m
m 1
I argmeax (22 gusts} 10g PM»)
1 m . m .
I ar maX— lo P9 33(2) I ar max lo P6 113(2) ,
gem;g()g6;s()
Where we used in order: the deﬁnition of KL, leaving out terms independent
of 6, ﬂip sign and correspondingly ﬂip min-max, deﬁnition of P, switching
order of summation, deﬁnition of P9, and simpliﬁcation.

***************Ending Page***************

***************Beginning Page***************
***************page number:210**************
210.
Remark
Consider the relationship between the prezent exercise and multi-variate
Bernoulli Naive Bayes parameter estimation. In the Naive Bayes model we
assumed P9 is of the following form: P9(a§,y) : p(y) H?:1p(wi\y). By the chain
rule for KL divergence (see (24) and (25) from Stanford, 2015 fall, Andrew
Ng, HW3, pr. 5.b), we therefore have:
KL(P||P@) I KL(P(y)llp(y)) + Z KL(P(wily)llp(w¢!@/))-
1:1
This shows that ﬁnding the maximum likelihood / minimum KL-divergence
estimate of the parameters decomposes into 2n + 1 independent optimization
problems: one for the class prior distributions p(y) and one for each of the con-
ditional distributions Maul-lg) for each feature xi given each of the two possible
labels for y.
Speciﬁcally, ﬁnding the maximum likelihood estimates for each of these prob-
lems individually results in also maximizing the likelihood of the joint dis-
tribution. (A similar remark applies to parameter estimation for Bayesian
networks.)

***************Ending Page***************


***************Beginning Page***************
***************page number:211**************
211.
If K (:13, z) is a kernel function
then am”) (with a > 1) is also a kernel function
Stanford, 2009 fall, Andrew Ng, practice midterm exam, pr. 3.a

***************Ending Page***************

***************Beginning Page***************
***************page number:212**************
212.
[In Romanian]
Consider-5m K 0 functie-nucleu deﬁnitéi pe Rd >< Rd. Arétati 05 functia compuséi
K6 deﬁnité prin relatia K4110, z) I eK($’Z) este de asemenea functie-nucleu.
Sugestz'e: Putegi folosi dezvoltarea sub forméi de serie Taylor a lui ex:
OO n 2 3 n
a; _ $_ _ ‘L ii ‘L
e _Z n! _1+;1;+ 2! + 3! +...+ n! +...,
nIO
precum §i faptul c5 pentru orice §ir de numere nenegative {6%th dacii existé.
a ng' limn_>00 an, atunci a Z O.
Consecinjd’: Pentru orice numéir real a > 1, avem lna > 0, deci functia
(lna)K($,z) este §i ea functie-nucleu (vedeli'i CMU, 2012 fall, T. Mitchell,
Z. Bar-Joseph, ﬁnal exam, p1‘. 7.a.1, punctul a.2'). Apoi,
aK(:E,z) I (elna)K(ac,z) : 6(1na)K(:c,z).
Prin urmare, va rezulta céi §i functia aK(rc,z) este functie-nucleu, pentru orice
a > 1.

***************Ending Page***************


***************Beginning Page***************
***************page number:213**************
213.

Solutie
Observajie:
in rezolvarea care urmeaza, vom demonstra proprietatea din enunt nu in
forma data (generala, adica emmrzl), ci intr-un caz particular care este simplu,
dar sernniﬁcativ. In rnod concret, vorn arata ca functia ew'z de variabile x §i z
din Rd este functie-nucleu. (Evident, functia x - z este functie-nucleu.)
Dupa aceea, a arata ca 6K0”) este functie-nucleu revine la a urma exact ﬁrul
demonstratiei de mai jos — inlocuind x - z cu KW, z) I (Mas) - (Maj) §i respectiv
£132- - 569- cu K(a:i,:1:j) —, ﬁindca justiﬁcarile in baza carora se asigura validitatea
rationamentului in cazul simplu pe care l-am ales mai sus raman valabile §i
pentru cazul general.

***************Ending Page***************

***************Beginning Page***************
***************page number:214**************
214.
Vom demonstra ca ew'z este functie-nucleu nu in mod direct — adica, dovedind
existenta unei functii de mapare qb astfel incat em'z I (M33) - $(z) —, ci folosind
teorema de ,,caracterizare“ a lui Mercer (a se vedea CM U, 2008 fall, Eric
Xing, ﬁnal exam, pr. 2).
Conform acestei teoreme, pentru a dovedi ca em'z este functie-nucleu este
suﬁcient sa aratam ca pentru orice m € N §i pentru orice elemente x1, . . . , 1cm €
Rd, matricea G (Gram), care este de tip m >< m §i are elementul generic de
forma G(z',j) n25. emi'mj pentru 2',j : 1,m, este simetrica §i pozitiv semideﬁnita.
Fie deci un numar natural m E N, ﬁxat, precum §i numerele $1, . . . ,xm E Rd,
de asemenea ﬁxate. Folosind dezvoltarea lui exi'mj ca serie Taylor, avem:
OO TL l
.. . (xi-m)" def. . (1'- w')
m1 ac _ J _ —Z J
e J — Z n! _ #520 Z Z!
11:0 1:0
Cu aceasta, devine evident ca putem scrie matricea G ca ﬁind limita unui §ir
de matrice, {Gn}n€N, elementul generic a1 matricei Gn ﬁind
not n ($2 'xjy —
Gn(2,]) : Z T pentru 2,] : 1,m.
1:0

***************Ending Page***************


***************Beginning Page***************
***************page number:215**************
215.
Matricea Gn este §i pozitiv semideﬁnitéi. J ustiﬁcarea provide din faptul c5 func§ia 33-2: de
variabile :13 §i z din Rd este functie-nucleu §i din aplicarea proprietéili'ilor de ,,c0nstruc§ie“
de noi nuclee: K1+K2, K1K2 §i 0K1 cu c > O sunt func§ii-nucleu dacéi K1 §i K2 sunt funcgii-
nucleu (a se vedea problemele referite anterior)“ Réméne de ariitat c5 prin trecere la
limitii (Gn —> G) se péistreazéi proprietatea de ,,p0zitiv semideﬁnire“ a matricelor.
Fie f € Rm, véizut ca vector-coloané. Urmeazéi:
fTGf I fT<n1ggo Gn>f I nlgggﬂenf) z 0.
20

Cea de-a doua egalitate de mai sus are 10c datoritii proprietﬁgii de liniaritate a limitei
de §iruriz limita combinagiei liniare a m >< m §iruri este c0rnbina§ia liniaré a limitelor
§irurilor respective. Inegalitatea limnnOJfTGw/f) Z O are loc ﬁindcéi fTan Z O pentru
orice n (datoritéi faptului (:5 matricea Gn este pozitiv semideﬁnitéi, dupéi curn am arétat
mai sus) §i datoritii proprietéiﬁi de piistrare a pozitivitéigii prin trecerea la limitfi (vedegi
cea de-a doua sugestz'e din enung).
A§adar, matricea G este simetricéi §i pozitiv semideﬁnitéi. in concluzie, functjia e:M este
func§ie-nucleu.

a Se ‘gine cont de rela§ia de recurenigzi intre matricele-nucleu Gn I Gn_1 +$ [(wi - asj)"]i,j:17—m, cu G1 I I, matricea
identitate de ordin d. Este imediat c5 (:c - z)”, unde n Z 2, este funcgie-nucleu, intrucét a: - z este funcgie-nucleu.

***************Ending Page***************

***************Beginning Page***************
***************page number:216**************
216.
Observai'ie:
Din demonstratia de mai sus rezulté imediat c5 printr-un raigionament similar
se poate dovedi urmétoarea proprietate de tip ,,const'r'uc§z'e“:
Dacii avem un §ir de funcigii-nucleu K1, K2, . . . , Kn, . .. deﬁnite pe Rd >< Rd —> R
§i existii limn_>00 Kn nit K,
atunci K este §i ea funcigie-nucleu.
(Cf. https://en.wikipedia.org/Wiki/Positive-definite_kerne|, accesat la data 30.03.2022.)

***************Ending Page***************


***************Beginning Page***************
***************page number:217**************
217.
RBF is a kernel function
Stanford, 2009 fall, Andrew Ng, practice midterm exam, pr. 3.b

***************Ending Page***************

***************Beginning Page***************
***************page number:218**************
218.
[In Romanian]
1 \l Hg
—— m-z

Ariitatji c5 functia cu baza radialﬁi e 202 , de variabile 50,2’ 6 Rd, este
intr-adevér func§ie-nucleu.
Observajie: in acest exerciigiu nu se cere sii se giiseascé efectiv funcgia de
mapare ¢ pentru nucleul RBF. Este suﬁcient s5 se dovedeascé existenta ei,
folosind de exemplu teorema lui Mercer sau proprietéitjile de ,,c0nstruc§ie“ de
noi nuclee. La CMU, 2011 fall, T. Mitchell, A. Singh, HW6, pr. 2.2 se aréta
cii existii 0 funcigie de ,,mapare“ gb pentru nucleul RBF care asociazii ﬁecérei
instange w un ,,vect0r“ Mat) a cﬁrui dimensiune este inﬁnité!

***************Ending Page***************


***************Beginning Page***************
***************page number:219**************
. 219.
Solutle
Mai intéi vorn pune expresia de deﬁnitie pentru functia cu baza radialéi sub o formé
mai convenabilii:
H$—z||2 (a3—z)(w—z) (x2—293-z+2'2) x2 x-z Z2
e_ 202 z @_ 202 z @_ 202 z @_ 202 e 02 @_202
x - z
I f(w) 6 02 f(Z),
x2
unde prin f(a‘) am notat expresia e 202 .
Acum putem aplica diverse proprietéiti de ,,constructie“ a nucleelor, pornind de la un
nivel simplu spre un nivel din ce in ce mai complex. Evident, functia x - z este functie-
nucleu. La fel §i functia L22 (vedeti CMU, 2012 fall, T. Mitchell, Z. Bar-Joseph, ﬁnal
U a" - z
emam, pr. '7.a.1, punctul a.i). A demonstra faptul cii functia e 202 este functie-nucleu
revine la a reproduce linia demonstratiei de la Stanford, 2009 fall, Andrew Ng, practice
midterm exam, pr. 3.a (péné la un factor constant). In sfér§it, datorité proprietétii
de la CMU, 2012 fall, T. Mitchell, Z. Bar-Joseph, ﬁnal exam, p'r'. 7.a.1, punctul a.ii,
x - z 1
—— . . ——|! — H2 .
rezulté c5 f(w) e 202 f(z) — dec1 §1 e 202 x Z — este functle-nucleu.

***************Ending Page***************

***************Beginning Page***************
***************page number:220**************
220.
Inﬁnite feature spaces
CMU, 2011 fall, T. Mitchell, A. Singh, HW6, pr. 2.2

***************Ending Page***************


***************Beginning Page***************
***************page number:221**************
221.
[In Romanian]
Pentru date reale de tipul w G R putem sii ne géndim la 0 transformare
(,,mapare“) a tréiséturilor (ﬁn : R1 —> Rn+1 deﬁnitii intr-o manieréi ceva mai
complicatéi decét in mod 0bi§nuit:
2 i n
2 2 2 33 2 £6 2 £13
gb x I {ex /2, eﬂc /2a:, e_x /2—, ..., (Tm /2—, ..., 6”” /2—}.
Presupunem acum 05 n % 00 §i deﬁnim 0 nouéi ,,mapare“ a tréiséturilor, care
produce un vector inﬁnit:
¢ < ) { —:n2/2 —ac2/2 —ac2/2 232 —:1c2/2 $2. } (26)
x: e ,e 23,6 —,...,e —,....
°° \/2 W

***************Ending Page***************

***************Beginning Page***************
***************page number:222**************
222.

Observajie: Se poate arata — vedegi de exemplu CMU, 2010 fall, Zi'v Bar-
Joseph, HW4, pr. 1.3-5 §i CMU, 2012 spring, Ziv Bar-Joseph, HW3, pr. 3.2
— ca putem exprima ecuatjia unui separator / clasiﬁcator liniar folosind doar
produse scalare de vectori (anumite instanige de antrenament) intr-un spagiu
oarecare (sau in aga-numitul spalgiu de ,,trasaturi“).
Ne punem problema daca n-am putea sa folosim cumva spatiul de trasaturi
obtinut prin ,,maparea“ gboo. Totu§i, pentru aceasta ar trebui sa putem calcula
produsul scalar de [imagini de] instange in acest spa§iu de ,,trasaturi“ inﬁnit.
Deﬁnim produsul scalar dintre doi vectori inﬁniti a I {a1,...,aZ-,...} §i b I
{(91, . . . ,bi, . . .} ca ﬁind suma inﬁnita

oo d f n

6 . .
a - b I Z aibz- I nlgngo Z aibi. (27)

1:1 1:1
Putem oare sa calculam in mod explicit $90 (a) - $000))? Altfel spus, care este
expresia explicita pentru functia-nucleu K (a, b) mg. ¢Oo(a) - $000))?
Sugestz'e: Folosiii'i dezvoltarea in serie Taylor a lui 6:0:

x _ - _

6 _ $13010 Z; z! ' (28)

1:

***************Ending Page***************


***************Beginning Page***************
***************page number:223**************
223.
Soluigie
Putem scrie imediat:
OO —a2/2 i —b2/2 i
no. (27)(26) e a e b
Ka,b It¢OOa-¢Oob I —-—
< > < > < > 2:) m m
2 b2 OO b 2' 2 b2 _ b 2
Observatie: Ultima expresie care a fost obginuta aici corespunde unei funcgii-
nucleu cu baza radiala, ale carei argumente (a §i b) sunt numere reale.
Demostragia pentru cazul ceva maxi general a1 nucleului RBF K(a,b) dif'
exp(—*y(a — (9)2), unde 'y € R1, iar a,b € R, urmeaza indeaproape linia aces-
tei demonstratii (vedeigi CMU, 2014 fall, Eric Xing, Barnabas Poczos, HW2,
pr. 3.2).
Agadar, pentru fuaniile-nucleu de tip RBF cu argumente reale, ,,maparea“
qb ia valori intr-un spagiu de dimensiune inﬁnita.

***************Ending Page***************

***************Beginning Page***************
***************page number:224**************
224.
Jensen’s inequality
and some consequences
Liviu Ciortuz, 2020

***************Ending Page***************


***************Beginning Page***************
***************page number:225**************
225.
Dacéi f : R —> R este 0 functie convewd, atunci,
conform deﬁnitiei, pentru orice t € [0,1] §i orice f(y)
£171,332 € R urmeazé
tf(x)+(1—1t)f(y) g
f<m1+<1 —t>w2> g tfwl) + <1 —t>f<w2>. (29> ﬁx“ ﬂjf)’ I’
Dacé f este funcljie strict convexéi, atunci egali-
v _ x | y
tatea are loc doar daca 331 _ x2.
tx+(1—t)y

***************Ending Page***************

***************Beginning Page***************
***************page number:226**************
226.

a. Folosind deﬁnitia de mai sus, demonstrati inegalz'tatea lui Jensenza
Pentru orice ai Z O, 2': 1,...,n cu 216%‘: 1 §i orice xi € R, 2': 1,...,n, dacé f
este functie convexé, atunci

1' i
Mai general, pentru orice a; Z O, cu 2' : 1, . . . ,n §i 21a; 75 O avem

/ . _ /_ .

Zj aj 21 a,-
Observatiz':
1. Dacii f este strict convexé, atunci in relatiile de mai sus egalitatea are 10c
doar dacé 331 I ...: yen.
2. Evident, rezultate similare cu cele de mai sus pot ﬁ formulate §i pentru
functii concave, inlocuind in relatiile (30) §i (31) semnul g cu Z.

a Johan Jensen, inginer §i maternatician danez (1859*1925).

***************Ending Page***************


***************Beginning Page***************
***************page number:227**************
227.
b. Demonstraﬁ inegalitatea mediz'lo'r folosind inegalitatea lui Jensen:
w Z \n/331H332muilfn pentru orice 3:1- 20, 2': 1,...,n.
n
c. in contextul teoriei probabilitéigilor, inegalitatea lui Jensen este exprirnatéi
astfel: dacé X este 0 variabiléi aleatoare §i f este 0 functie convexéi, atunci
f(E[X]) g E[f(X)]. (Similar, dacii f este funcgie concavfi, atunci f(E[X]) Z
Ewm
Demontratji aceasté inegalitate in cazul in care X este variabiléi aleatoare
discretfi cu un numéir ﬁnit de valori (adicii, ‘Val (X)| < 00).

***************Ending Page***************

***************Beginning Page***************
***************page number:228**************
228.
Solugie

Vom privi inegalitatea (30) ca pe 0 ipotezd inductivd, pe care 0 vom desemna
cu P(n), unde n € N, n Z 2.“ Vom demonstra c5 P(n) este adeviiratfi, folosind
principiul inductiez' matematz'ce:
P(2): Dacé $1,162 € R §i a1,a2 € [0,1] astfel incét a1 + a2 : 1, iar f este func§ie
convexé, atunci inegalitatea f(a1x1 +a2x2) g a1f(x1) +a2f(x2) se rescrie echiva-
lent ca f(a1x1 + (1 — a1)a:2) g a1f(331) + (1 — a1)f(ac2).
Aceasté ultimé inegalitate coincide practic cu relatgia (29) din deﬁnigia funcgiei
convexe, deci este adevéiratéi.
Presupunem 051 proprietatea P(n) este adevératéi §i vom demonstra P(n + 1),
adicé: dacé £81,...,513n,$n+1 € R §i a1, . . . ,an, an+1 E [0,1] astfel incét 22:11 ai I 1,
iar f 0 functgie convexii, atunci rezulté c5

n+1 n+1

f (Z aiwi) S Z aif(93i)- (32)
1:1 1:1
a Inegalitatea (30) se veriﬁcé §i pentru n I 1, ﬁindcé f(:c1) I f(231).

***************Ending Page***************


***************Beginning Page***************
***************page number:229**************
229.
Vom rescrie acum membrul sténg a1 acestei inegalitiilgi intr-o forméi conven-
abilé:
n+1 n
f (Z 611%‘) : f (261111‘ + an+1$n+1>
1:1 1:1
I f (2%) ‘Z +111 + an+1$n+1
1:1 1:1 21:1 ai
: f 1—a+1' —Z l"+a+193+1 33
(n);1_an+lz nn ()
a-
Este imediat c5 dacéi vom considera AZ- nét' 1—Z, pentru 2' I 1, . . . ,n, atunci
— an+1
TL
-_ a- 1 — a
rezultéi c5 AZ- 20 pentruz': 1,...,n $211 A1: h I —n+1 : 1.
2_1
1 — an+1 1 _ an+1

***************Ending Page***************

***************Beginning Page***************
***************page number:230**************
230.
Prin urmare,
n+1 (33) n
f( @1461) I f((1 — an+1) ' A1952‘ +an+1 $n+1>
W m2
$1
fcon'uexd n
S (1 — an+1) ' f<ZAi33i> + an+1'f(xn+1)
i=1
PM) n
g (1 _ an+1)'2 AJW) + an+1'f(xn+1)
i:1
n a1‘
: (1 — an+1)- (g 1_—an+1f($¢)) + an+1-f(:1;n+1)
n n+1
I Z aif(55i) + an+1f(55n+1) I Z aif(a3i)
¢:1 1:1
A§adar, f<2yz+11 aixi) g 22:11 all-ﬂag), deci proprietatea P(n —|— 1) este adevératzvi.
Sumarizénd, din faptul c5 P(2) este adevﬁraté, iar implicaija P(n) :> P(n + 1) este
adevératé, conform principiul inducgiei complete rezulté c5 proprietatea P(n) este
adevératé pentru orice n € N* \ {1}.

***************Ending Page***************


***************Beginning Page***************
***************page number:231**************
231.
b. Fie numerele 331,332,“me toate mai mari sau egale cu 0. Considerém
1
a1 : a2 : ...: an : — (ceea ce implicéi 227:1 a1‘ : 1).
n
Conform inegalitéitjii lui Jensen, in care vorn alege pe postul functiei f functia
1n (logaritmul avénd ca bazéi numérul e) care este concavii, putem scrie:
1n (Z aixi) Z Za21n(332) (:1n(5 221) Z E Zlnwi) <I>
2:1 2:1 2:1 2:1
1
2:1 2:1 2:1 2:1
1 n 27-1 $-
ln<— -)>1 n ®i>n .
n22, _ Il(\/ZU1ZU2 xn) 22 _ \/£61£E2 xn
2:1
Ultima echivalen§€1 are 10c intrucét functia 1n este strict crescéitoare.

***************Ending Page***************

***************Beginning Page***************
***************page number:232**************
232.
c. Fie f : R —> R 0 funcgie convexé §i X 0 variabilé aleatoare discreté avémd
urmiitorul tablou de repartitie:
P1 P2 2%
Conform inegalitéigii lui Jensen (pe care 0 putem aplica luénd in locul ,,p0n-
derilor“ ai probabilitétile pi, intrucélt pi Z 0 §i 2?:1191- I 1), rezultﬁ:
f(ZP¢ﬂ/"i) § Zpif(33i) <I> f(EP[X]) S EP[f(X)]-

1:1 1:1

in mod similar, dacé. f este functie concavéi, rezultéi céi f(Ep[X]) Z Ep[f(X)].

***************Ending Page***************


***************Beginning Page***************
***************page number:233**************
233.
Exemplifying
the gradient descent method
and Newton’s method:
ﬁnding the minimum of a real function of second degree
University of Utah, 2008 fall, Hal Daumé III, HW4, pr. 1

***************Ending Page***************

***************Beginning Page***************
***************page number:234**************
234.
a. Suppose we are trying to ﬁnd the minimum of the function
f(x) I 3552 — 2:1: + 1, for uni-variate (scalar) at.
First, verify that this function is convex (and therefore it has a
global minimum).
Second, ﬁnd the minimum of this function using calculus.
Finally, perform (by hand — show your work) three steps of gra-
dient descent with 17 I 0.1 and the initial point 250 I 1. How close
does it get to the true solution?

***************Ending Page***************


***************Beginning Page***************
***************page number:235**************
235.

Solution (in Romanian)

Pentru a studia convexitatea functiei f (:13) se cal-

culeazé derivata a doua:
Rezultzii 05 functia f este convexéi pe intreg dome-
niul ei de deﬁniigie, deci are un (singur) punct de 5
Pentru a calcula minimul func§iei f($) : 3x2 — 2x —|— 1

utilizéim derivata de ordinul intéi: 1

Punctul de minim este dat de solutia ecua§iei f’ (x) :
0, §i anume: x : — x 0.33.

3 x3x2 x1 x0

O. 3 1

***************Ending Page***************

***************Beginning Page***************
***************page number:236**************
236.

Observagii
Se constatéi u§0r (:5
1. apropierea de punctul de optim este [mai] rapidii atéta timp cét valoarea
primei derivate (i.e., panta tangentei la graﬁcul func§iei) este mare in valoare
absolutéi;
2. dacé rata de inv5§are 77 a fost ﬁxatéi la 0 valoare prea mare, atunci este posi-
bil ca la un moment dat sé depé§im punctul de optim §i apoi sé ,,pendulém“
in jurul lui. Acest punct slab a1 metodei gradientului poate ﬁ contracarat
reducénd in mod dinamic méirimea lui 77.
Altminteri metoda gradientului are avantajul de a ﬁ 0 tehnicii de optimizare
foarte simpléi din punct de vedere conceptual §i u§0r de implementat.
Alte douéi puncte slabe ale metodei gradientului descendent sunt:
imposibilitatea de a garanta gésirea optimul global §i
numérul mare de iteratii care trebuie executate pe unele seturi de date reale.

***************Ending Page***************


***************Beginning Page***************
***************page number:237**************
237.
Metoda 1111 Newton
In forma sa cea mai simpléi, metoda lui Newton1 —
cunoscuté in literatura de specialitate §i sub numele I
de metoda tangentez' — are ca obiectiv identiﬁcarea '
réidécinii (sau, a rédécinilor) unei functii f care este I
convexé §i derivabilé. (Rédiicinile unei functii sunt I
acele valori $* ale argumentului functiei pentru care I
se anuleazii functia respective, adicéi f (05*) : 0.) f(X) I
In acest scop, adicé pentru aﬂarea réidécinilor unei I
functii, metoda lui Newton, care este o metodé iter- x* Xj+1 X1,
ativé, folose§te urmﬁtoarea relatie de ,,actualizare“: '
f(1v ')
J
x ‘ : {r ’ _ —
3+1 j , 7
f ($1)
cu x0 ales in mod arbitrar.
Isaac Newton (1642-1727) a fost un maternatician, ﬁzician, astronom, alchirnist §i teolog englez.

***************Ending Page***************

***************Beginning Page***************
***************page number:238**************
238.
Metoda lui Newton poate ﬁ aplicata (modiﬁcata u§or) §i pentru gasirea opti-
mului unei functii. In acest caz, se cauta valorile argumentului :1; pentru care
se anuleaza prima derivata a functiei obiectiv f. Prin urmare, pentru a putea
aplica metoda lui Newton se cere ca functia f sa ﬁe dublu derivabila (adica sa
existe atat prima cat §i cea de-a doua derivata a lui f). Relatia de actualizare
acum devine:
JUW)
a:-+1:a:-——. (34)
J J f”(fvj)
Nota: La problema de fata, se va folosi metoda lui Newton in varianta aceasta,
adica pentru aﬂarea optimului (mai precis, a minimului) unei functii.
Observatie: Formula (34) se folose§te exact in aceea§i forma §i atunci cand
— in locul minimului — se cauta maximul unei functii, folosind metoda lui
Newton. (Pentru a Va convinge, este suﬁcient sa inlocuiti f cu — f in relatia
(34).) In schimb, relatia £6n+1 e xn —17f’(ac), care este folosita de metoda gradi-
entuluz' descendent (pentru cautarea unui punct de minim) trebuie inlocuita
cu xn+1 e ajn — 77f’(a:) in cazul folosirii metodei gradientului ascendent (pentru
cautarea unui punct de maxim).

***************Ending Page***************


***************Beginning Page***************
***************page number:239**************
239.
lVletoda Newton-Raphson
Generalizarea metodei lui Newton la cazul multidimensional, numita §i
metoda Newton-Raphson,“ lucreaza cu relatia de actualizare
_1
$j+1 I $1 — (HUGO) Vf<$j)-
Aici, V f (x) este, ca de obicei, vectorul gradient, format din derivatele partiale
ale lui f (x), calculate in punctul x, iar H (x) este matricea hessianab a lui f,
formata din derivatele partiale de ordinul al doilea, calculate in punctul :c:
32f<$>
H - ac : —.
Zk< ) (9x1 (911%
a Joseph Raphson (c.1668 - c.1715) a fost un matematician englez, contemporan cu Isaac Newton.
b Dupa numele lui Ludwig Otto Hesse (1811 — 1874), matematician german.

***************Ending Page***************

***************Beginning Page***************
***************page number:240**************
240.
Observatie

in mod obi§nuit, metoda lui Newton are o raté / vitezé de convergentfi
mai mare decét metoda gradientului (varianta “batch”) §i necesitéi mult mai
putine iteratii pentru a ajunge foarte aproape de punctul de optim.

Totu§i, trebuie retinut c5 o [singuré] iteratie a algoritmului lui Newton poate
ﬁ mai costisitoare deceit o iteratie a metodei gradientului, ﬁindcéi metoda lui
Newton necesité atét calcularea cét §i inversarea matricei hessiene.

Cu toate acestea, pentru valori ale lui d (numéirul de dimensiuni) nu prea
mari, metoda lui Newton este in mod obignuit mult mai rapidei decét metoda
gradientului.

***************Ending Page***************


***************Beginning Page***************
***************page number:241**************
241.
b. Céutati din nou optimul functiei f(:v) I 3x2 — 2x + 1 aplicénd de aceasté
datéi metoda lui Newton. Ce observati? (Céte iteratii sunt necesare pentru
ca algoritmul séi conveargia’. la solutia optimé?)
Solutie La prima iteratie vom proceda astfel:
f’ (x0) 6 - 1 — 2 2 1
l» :3; _—:1-—:1——:—-
1 0 f”(a:0) 6 3 3
Se observéi cfi f’(a31) I 0, iar daciei am mai continua s5 facem §i alte iteratii,
am obtine 502 I $3 I I x1. Agadar, cu metoda lui Newton, solutia optimii
se obtine (pentru aceasté functie) intr-o singuré iteratie.

***************Ending Page***************

***************Beginning Page***************
***************page number:242**************
242.
Observagle
Se poate constata u§0r (:51 solutia optimii pentru funcigia f se 0b§ine intr-o
singuréi iteragie, indiferent de valoarea atribuitéi lui $0:
f/(CUO) 6330 — 2 1
51:12a00——::c0——:—.
f” ($0) 6 3
Este interesant de retinut faptul 05 aceastii proprietate, care este valabiléi
de fapt pentru orice funcigie polinomialéi de gradul a1 doilea,“ se intélnegte §i
in cazul rezolvérii regresz'ez' liniare cu ajutorul metodei lui Newton (vedetji
Stanford, 2007 fall, Andrew Ng, HWl, pr. 1.b).
a Considerénd f(x) I a952+bx+c, rezulté f'(:c) I 2asc+b §i f”(x) I 2a. Agadar, 1a aplicarea regulii [de actualizare]
din metoda lui Newton, vom obgine:
f’(:cj) 2a50j —|— b b b
33. :$._ :w._ :ac-—:c'——:——,
‘7+1 j f”(a:j) j 2a j ‘7 2a 2a
adicéi exact abscisa punctului de optim a1 funcgiei de gradul a1 doilea.
Proprietatea aceasta — adicé faptul 05x metoda lui Newton converge intr-o singuré iteragie — se generalizeazd 1a
funcgii de gradul a1 doilea cu un numér oarecare de variabile (nu doar una singuré, cum a fost cazul mai sus).

***************Ending Page***************


***************Beginning Page***************
***************page number:243**************
243.
Problema de optimizare convexii cu restricigii
— 0 varianté u§0r simpliﬁcatﬁ —
demonstrarea proprietégii de dualitate ,,slabfi“
ClVIU7 2014 fall, E. Xing, B. Poczos, HWl, pr. 3.1

***************Ending Page***************

***************Beginning Page***************
***************page number:244**************
Introducere 244-
in cazul general, 0 problemii de optimizare convemd cu restrictii este deﬁnita
sub forma primald’ astfel:
a. i. 94m) g O pentru 2' I 1,...,m
hj(:c) I 0 pentru j I 1,...,l<:,
unde f (funcjia obiecti'v) §i gi sunt func§ii convexe, iar hj sunt funcjii aﬁne
(adica func§ii liniare augmentate cu termen liber), cu m Z O §i k Z 0. Conditiile
92(36) g O §i hj (as) I 0 se numesc conditii de fezabz'lz'tate primalii.
Pentru problema de optimizare convexa deﬁnitii mai sus, lag'rangeanul gen-
eralizat se deﬁnergte astfelza
def.
m, a, B) I f<w> + Z (Xi-gm) + Z @jhjm),
i j
unde 041- 2 O pentru i I 1,...,m §i [33- 6 R pentru j I 1,...,k:. Condiigiile
042- 2 0 se numesc condijii de fezab'ilitate duald. Unii autori numesc funcgia L
lagrangeanul primal §i 0 (re)noteaza cu L P.
a Dupa numele maternaticianlui gi astronomului Joseph-Louis Lagrange (in italiana, Giuseppe Lodovico Lagrangia),
1736-1813. E1 a lucrat mai intai in Italia (la Torino, oragul sau natal), apoi in Germania (1a Berlin) §i in ﬁnal in Franga
(1a Paris).

***************Ending Page***************


***************Beginning Page***************
***************page number:245**************
245.
Fie urmétoarea probleméi de optimizare convexﬁ cu restrictii in formﬁ, pri-
malﬁ, unde f, hl, hg : Rd —> R:
minm f (x)
a. i. h1(50) g O,
a. Scrie§i lagrangeanul generalizat L(x, A, u), unde A Z O §i u 6 R sunt "variabilele
duale care corespund inegalitéiﬁi §i respectiv egalitéitgii din cadrul problemei
de optimizare.
b. Vom nota cu P aga-numita regiune fezabild a problemei de optimizare in
forma primalé, adicé muligimea formaté din acele puncte a: € Rd pentru care
Demonstragi 05 pentru orice a: 6 P are 100 egalitatea
I L A .
f(w) 3;?ng (w, ,U)

***************Ending Page***************

***************Beginning Page***************
***************page number:246**************
246.

c. Datorita rezultatului de la punctul b, problema de optimizare data in enuni;
(in forma primala) se poate rescrie echivalent (in mod compact) astfel:

min max L(:1:,)\,u).

asEP AZOM
Mai departe, renungand la restrictgia 90 6 P — adica, lucrand cu £6 € Rd,
nerestricgionat — §i inversdnd cei doi operatori, min §i max, vom obtine prob-
lema de optimizare convexa in forma dualﬁ:

max min L($, Au).

AZOJL $€Rd
Putem spune ca func§ia obiectiv a problemei duale este (in maniera ,,c0ncep-
tuala“) g()\,u) ngt. minxeRd L(:c, Au).
Aratagi ca

min a: > ma A u .
mepﬂ )_ A2529“ )

Atfel spus, valoarea optima [a funcigiei obiectiv] din forma primala a problemei
de optimizare este mai mare sau egala cu valoarea optima [a func§iei obiectiv]
din problema duala. Ve§i putea constata u§0r ca acest rezultat se mengiune
§i atunci cand in problema de optimizare avem mai multe restrictii de tip
inegalitate sau egalitate. Proprietatea aceasta se nume§te dualitate slabd
(engl., weak duality).

***************Ending Page***************


***************Beginning Page***************
***************page number:247**************
247.
Rﬁspuns
a. Lagrangeanul generalizat se 0b§ine combinénd (intr-o singuré functie)
functia obiectiv §i functiile cu ajutorul céirora se scriu restricjiile din problema
de optimizare convexéz
def.
L(x, A, u) : f(x) + Ah1(x)+ uhﬂw).
b. Dacéi x € P, regiunea fezabiléi a problemei de optimizare in forma primaléi,
atunci urmeazé 05
f(ac) Z f(x) + Ah1(513) + uhﬂac) déf' L(:c, A, u) pentru orice A Z O §i orice u 6 1R.
Mai departe, se observé 05 inegalitatea f(a:) Z L(x,)\,u), pe care tocmai am
obiginut-o, este satisfficutéi cu egalitate dacéi se ia A : 0. Prin urmare, putem
scrie f(:13) I maXAZOM L(x, A, u).

***************Ending Page***************

***************Beginning Page***************
***************page number:248**************
248.
c. Vom relua acum inegalitatea
f(x) Z L($, Am), pentru orice x € P, A Z 0, u € R,
pe care am 0b§inut-0 la punctul b. Considerénd acum c5 A Z O §i u G R sunt
[alegi in mod arbitrar dar] ﬁxati, §i aplicénd apoi operatorul minwe P la ambii
membri ai acestei inegalitéiti, vom obtine
'>'L)\>'L)\n§t')\.
1%pr {£131 (w, ,u) _ gig; (w, ,u) 9( a“)
Ultima inegalitate se justiﬁcé prin faptul cé P Q Rd. Agadar, sumarizénd,
ceea ce am 0b§inut péné acum este inegalitatea
milr31f(x) Z g(>\, u) pentru orice A Z O §i orice u € R.
ac€
in particular, inegalitatea are 10c pentru acei A Z 0 §i u € R pentru care se
atinge maximul termenului drept a1 inegalitéﬁi. A§adar,
' > A .
gggﬂw) _ 3652M ,u)

***************Ending Page***************


***************Beginning Page***************
***************page number:249**************
249.
Comentariu
Folosind notatia din documentul Support Vector Machines de Andrew Ng,
putem rescrie intr-o manieréi foarte sugestivéi inegalitatea pe care tocmai am
0b§inut-0 (adicii, proprietatea de dualitate slabd):
19* 2 d*,
unde prin 10* §i d* am notat valom'le optime pentru problema primalii, respectiv
problema dualéi.
Este imediat c5 proprietatea care a fost demonstratéi in aceastéi probleméi are
10c pentru orice problemd’ de optimiza'r'e 0011,116an cu restricjiz'.

***************Ending Page***************

***************Beginning Page***************
***************page number:250**************
. Q. 250.
Condltule —|— teorema Karush-Kuhn- Tucker
Pentru 0 problemé de optimizare conuewﬁ cu restricgii, care este deﬁnite’! in
forma primalii (P), condipiz'le Karush-Kuhn- Tucker sunt urméitoarele:
primaléi g,($*) g 0 pentru i I 1, . . . ,m §i
0 fezabilitate hj(x*) : 0 pentru j : 1, . . . , k
dualéi oz:20pentrui:1,...,m
0 complementaritate [dualii]: ang-(afk) : O pentru 2' I 1,. ..,m
(9
0 optimalitate (sau, stagionaritate): 8—L(:13*,04*,[3*) I O,
m
. A v <9 . .
(Orlce punct 1n care se anuleaza 8—L se nume§te punct de statzonarztate.)
x
Teorema K arush-Kuhn- Tucker:
(1) Dacé este indeplinité condigia de dualitate tare pentru problemele de 0p-
timizare (P) §i duala sa (D) — adicé valorile lor optime satisfac relatia
p* I d* —, atunci soluigiile acestor doué probleme satisfac condijiz'le
K arush-K uhn- Tucker.
(2) Dacé 33*, 04* §i 5* satisfac condijiile Karush-Kuhn- Tucker, atunci 513* este
soluigie a problemei (P), iar oz*,6* constituie 0 solutie pentru problema
(D)-

***************Ending Page***************


***************Beginning Page***************
***************page number:251**************
251.
Notii istoricii
Teorema Karush-Kuhn-Tucker (abreviat, KKT) are 0 istorie interesanté.
Ea a fost descoperité in 1939 de cétre studentul american William Karush,
care a inclus-o in teza sa de master, dar n-a fost cunoscuté pénfi cénd a fost
redescoperitii in 1950 de céitre matematicienii Harold Kuhn (american) §i
Albert Tucker (canadian).
O variantéi a acestei teoreme fusese descoperité §i de céitre Fritz John (german)
in 1948, insé Duke Mathematical Journal i-a refuzat publicarea.
(Cf. Chuong B. D0, Convea: Optimization Overview (cont’d), 2009, pag. 7.)

***************Ending Page***************

***************Beginning Page***************
***************page number:252**************
252.
Conditii suﬁciente pentru ,,dualitatea tare“:
Conditia lui Slater
Pentru o problema de optimizare convexa cu restrictii (P), Conditia lui Slater
se enunta astfel:
Daca exista x astfel incat g,(a:) < O pentru orice i §i liq-(a?) I O pentru orice j,
atunci p* : cl*, adica are loc dualitatea tare.
(Cf. Morton Slater, Lagrange multipliers revisited, 1950.)
Mai general, pentru ca satisfacerea conditiilor Karush-Kuhn-Tucker de catre
solutiile optime pentru o problema de optimizare conveXa cu restrictii sa
devina necesara, este suﬁcient sa ﬁe indeplinita o anumita ,,conditie de regu-
laritate“ (engl., regularity constraint, sau constraint qualiﬁcation). Conditia
lui Slater reprezinta [doar] una dintre mai multe ,,conditii“ care pot juca un
astfel de rol.
Cf. Convea: Optimization Overview (c0nt’d), Chuong B. Do, pag. 6 (Lemma

***************Ending Page***************


***************Beginning Page***************
***************page number:253**************
253.
Two variants of the [Rosenblatt’s] Perceptron algorithm
for which the updating rule is obtained by
solving a convex optimization problem
using Lagrange’s method
University of Helsinki, 2014 spring, Jyrki Kivinen, HW5, pr. 2-3

***************Ending Page***************

***************Beginning Page***************
***************page number:254**************
254.

a. Consider a variant of the Perceptron algorithm, where the new
weight vector wt+1 is deﬁned as the w that solves the minimisation
problem

minimise ||w — wtl|2

subject to ytw - wt Z 1.
where mt € Rd and yt E {—1,+1}.
Write wt+1 as a function of wt, 51:7; and yt (in other words, solve the
optimisation problem).
Assuming that the initial weight vector wl is zero, show that wt is
a linear combination of 51:1, . . . ,xt_1.
Write the kernelised form of the algorithm.

***************Ending Page***************


***************Beginning Page***************
***************page number:255**************
255.
Let f : Rd —>R, f(w) I ||w—w,5||2 I (w—wt)-(w—wt) I w-w—2w-wt+wt-wt, and
let p(w) I 1 — ytw - mt. We prove [as a warm-up] formally that f is a convex
function.
A twice differentiable function is convex in a convex set if and only if its
Hessian matrix is positive semideﬁnite on the interior of the convex set (com-
pare with the non-negativeness of the second derivative of a function of type
R —> R). Because f(w) I 2221(101' — wt,¢)2, it is easy to see that
(9f(w) .
—:2 2-_ Z- e 1,2,...,d,
M <w wt, > <2 { }>
and for all 2',j G {1,2,...,d}
(Wm) 82f(w>
—:2if':' and —:0if' '.
81018103" Z J 81016103' 2 7é J
So the Hessian matrix of f is V2f(w) I 21d. It holds for all z 6 RdX1 that
zT(21d)z : 2||z||2 Z 0, which means that V2f(w) is positive semideﬁnite.

***************Ending Page***************

***************Beginning Page***************
***************page number:256**************
256.
The optimization problem is to minimize f(w) subject to p(w) Z O. The
Karush-Kuhn-Tucker conditions are
p(w) g O (primal feasability)
A Z O (dual feasability)
Ap(w) I 0 (complementarity)
V f (w) + AVp(w) : 0 (stationarity / optimality),
or equivalenty
l—ytw-xt§0 (2)
A Z 0 (22)
M1 — ytw 4131;) I O. (222)
2w — Zwt — Aytxt I 0 (22))
Equation (222) yields
From equality (222) we get two solutions.
In the ﬁrst case, A I 0, and inequality (22) is true. Also, w I wt, and 1—ytwt-zvt g
0 has to hold.

***************Ending Page***************


***************Beginning Page***************
***************page number:257**************
257.
In the second solution A > O, and we get from (iii)
1
1 — ytw ' $1: I 1 — yt(wt + ikyﬂt) ' $1:
1
where we used y? I 1. In this case (2') holds with equality. Solving A from (36)
and using (it) gives
1 — ytwt ' $t
A : 2 - —, (37)
llfliltH2

which implies 1 — ytwt - mt > O. Plugging A from (37) into (35) yields

1 1 — w -ac 1 — w ~35

2 llwtll llfIltll
Putting ﬁnally all things together, we have the update rule

wt+1 — wt + Utth$t> Where (7t —{ 1 if ytwt _ 513,5 < 1_

***************Ending Page***************

***************Beginning Page***************
***************page number:258**************
258.

We were asked to show, that if wl : 0, then wt+1 is a linear combination
of the vectors x1,x2,...,xt. In other words, wt : 041561 —|— agxg + + atxt,
where 041,042, . . .,ozt € R. If t I 0, we have an empty sum, and the statement
(wl I O I 2?:1 wi) is true. Let t € {1,2, . . .} and assume that wt I 27;; may.
Using our update rule,

t—1 1 _ y w ' a: t

wt+1 I Z 041ml‘ + Utyt%$t I Z aﬂi,

1-21 llwtll 2-:1
where we have written at : atytﬂ — ytwt cht)/szt||2 According to the induction
principle, the statement is thus true for all t G N.

***************Ending Page***************


***************Beginning Page***************
***************page number:259**************
259.

In the kernelized Perceptron algorithm we replace the x1- vectors with their
counterparts $(x1) in the feature space. The algorithm can be described as
follows:
Fort: 1,2,...,T do

1. Get the instance mt.

2. Let pt : wt-q5(xt) : (2:; digb(wZ-)) -¢(xt) : 2:1 aiK(xi,xt). Predict

:Qt I Selim)-

3. Get the correct answer yt.

4- If ytpt f 1,

set 041: e th — ytwt ' ¢($t))/ll¢($t)ll2 I (Qt — 2:1 @iKW/iﬁtD/Kmﬁﬁt),

otherwise set at e 0 (and act can be discarded).

1: 1: t t
5- Let l|wt+1||2 I ZZZ-:1 OWWU ' 23-21 Oli¢<$j> I 21-21 Ely-:1 @iaij-wj).
If Hwt+1|| > l, set 04¢ <— (Xi/Hilltll for all 2' € {1,2, . . .,t}.

***************Ending Page***************

***************Beginning Page***************
***************page number:260**************
260.
b. Similarly to the part a, consider an algorithm where wt+1 is obtained from
the minimisation problem
minimise dre (w, wt)
subject to ytw - wt Z 0 for i:1,. . . ,d
d
where w nit (w1,.. . ,wd), and dre(u,v) is the relative entropy
d u-
d : -l —Z.
re(u> U) Z U1 I1 'Ui
2:1
Show that if a solution exists, then there is a real number 5t Z 1 such that
the new weight vector satisﬁes
d
_ _ 1 . ytwtﬂl h Z _ _ ytwtg‘
w2€—|—1,2 — Zwt,26t W ere — 210150615 -
9:1
Do not try to ﬁnd out the actual value of 6t (there is no closed-form solution),
just show that the solution wt+1 has the required form.

***************Ending Page***************


***************Beginning Page***************
***************page number:261**************
261.
Let f : R1 —> R, f(w) I dre(w,wt). Similarly as in the part a, we start by
proving that f is convex in R1, which we do by examining the Hessian matrix
of f on the interior of R+.
Let w : (2111,1112, . . . ,wd) € (R+\{O})d. For all 2' € {1,2, . . .,d},
d
@fcw) I 82,2110 n<w wt, > 21W _1nw,, + 1’
(9102' 8w,- ’
and
32f(w) _ 8(lnw, — lnwm- —|— 1) _ i
Additionally, for all 2',j € {1,2, . . .,d}, 2'75 j,
82f<w> _ 0
310,810,- — '
Let z E RdX1. The Hessian of f is positively semideﬁnite because
d z2
T 1'
H I . . . : — > 0.
Z (f), (cl/@111), (22/102), ,<Zd/wd>>z 2:; w, _
We have now proved that f is convex in R+.

***************Ending Page***************

***************Beginning Page***************
***************page number:262**************
262.
Let
d
p(w) I —ytw - cut, and q(w) I Zwi — l.
1:1
The conditions wl- Z 0 are left out, because of the deﬁnition of f.
The optimization problem is to minimize f in the set R1 subject to
{ WU) § 0
(1(w) I 0-
The function p is linear, and therefore convex. Let a I (1,1,. . .,l). We see
that q(w) I a - w — 1 is affine. So our minimization problem is convex.
The KKT conditions for the optimization problem are
p(w) g O (primal feasability)
q(w) I O (primal feasability)
A Z O (dual feasability)
Ap(w) I O (complementarity)
Vf(w) —|— AVp(w) + 7Vq(w) I O (stationarity / optimality),
Where A Z 0 and 'y € 1R.

***************Ending Page***************


***************Beginning Page***************
***************page number:263**************
263.
For all 2' € {1,2, . . .,d} it holds that
8 8 8
M I lnwz- —lnth-—|— l, M I —yt:1:tZ-, and M I 1,
81121‘ 7 810$‘ 7 @"LUZ'
and the stationarity condition yields thus
(ln w,- — ln wm' + 1) — Aytxm- + 'y I O.
This implies
lIl w,- : lIl wt’,- — 1 + Aytwm — 'Y,
and further
1 a, ,
wz' I wt’,- 6Xp(—1 + Aytﬂﬁm — V) I 6Xp(—1 — 'Y)wt,i 6Xp(>\yt$t,¢) I 210mg? t’ 7
where l/Z I eXp(—l — 'y) and 6t : eXp()\). The condition q(w) : O yields now
d 1 d
25th 6, — 1 :0 a) Z: 211)th ,
1:1 2:1
and because of the condition A Z 0 we know that 8t I exp()\) Z 1.

***************Ending Page***************

***************Beginning Page***************
***************page number:264**************
264.
Remark
Although this particular algorithm is probably not very practical,
optimisation problems such as this lead to Winnow and other
multiplicative online learning algorithms.

***************Ending Page***************


***************Beginning Page***************
***************page number:265**************
265.
Representer Theorem
Liviu Ciortuz, 2020, following
Stanford, John Duchi, Supplemental lecture notes

***************Ending Page***************

***************Beginning Page***************
***************page number:266**************
266.

Fie L : R2 —> R. Vom spune céi L este functie de cost sau functie de pierde're
(engl., loss function) dacé ea este 2. nenegativéi §i 22. convexéi in raport cu
primul argument, pentru orice valoare ﬁxatii a celui de-al doilea argument.
Spre ewemplu,
— pentru regresia liniaré §i pentru perceptronul liniar se f010se§te — cu r01 de
functie de cost — péitratul erorii, adicéi L(2", y) : (y — 2")2,
— pentru regresia logisticéi se
folose§te functia de cost logis- 4
2;... my) :1n<1+@-W>, i i i f 35 exp”) ‘ ‘— 5
— pentru ma§1n1 cu vectorl- 11 11 1131 -|_og(sigma(z)) _
suport (SVM) se folosegte 1 1 1 1 181l_9ma(z) — 1
functia de cost hinge "X" 2'5 “196(2) 1 —
1/039) :max{0,1—yr}, iar 11 X 2 11
— pentru AdaBoost, se " " §"'\"\'§" \ ' "
folose§te functia de cost " " ‘5" '1'\' " i' 1
[negativ] exponentiala” L(2",y) : ~ ~ " ‘\  ~ a
eﬂﬂo, 1A‘ :

-4 -3 -2 -1 O 1 2 3 4
(in ﬁgura aléturatéi, am folosit Z
notatia z : yr.)

***************Ending Page***************


***************Beginning Page***************
***************page number:267**************
267.
Fie de asemenea instanQele xi E Rd (cu d E N*), precum §i yz- E R (desemnémd
ﬁe clase, ﬁe — ca in cazul regresiei liniare §i a1 perceptronului liniar — valori
reale oarecare pentru funcgia de invéQat), pentru 1L : 1, . . . ,n.
a. Demonstraij (:5 orice solutie 10* a urméitoarei probleme de optimizame (cu
termen de regularizare de normé L2)
_ 1 n A 2
mmwGRd(; 2m - why» + 5||wu ), <38)
1,:

unde A Z O, iar H - H desemneazfi norma euclidiané, poate ﬁ scrisii sub forma
urméitoare:

n

10* I Z aiml- pentru anumite valori 04¢ € R.

1:1
Sugestz'e: Puteti rezolva problema in cazul particular cénd /\ > 0, iar functia
de cost L : 1R2 —> R este derivabilé in raport cu primul argument, pentru orice
valoare ﬁxaté a celui de-al doilea argument.
Observajie: Acest rezultat se numegte teorema d6 reprezentare §i se datoreazii statisti-
cienilor Grace Wahba §i George Kimeldorf (1970).

***************Ending Page***************

***************Beginning Page***************
***************page number:268**************
268.

b. Se d5 0 func§ie gb : Rd —> Rm, unde m > d (de obicei m este chiar mult
mai mare decét d). In continuare vom considera ,,imaginile“ instan§elor $2-
prin funcigia (b, adicfi Mani), pentru 2' I 1, . . . ,n. Dintre toate operagiile care pot
ﬁ (sau, sunt deja) deﬁnite intre aceste ,,imagini“, singura care va ﬁ permisé
(pentru a ﬁ folosité) aici este produsul scalar: Mm) -q5(xj) ng' K(:vZ-, xj). Funcgia
K astfel deﬁnité — gi, de fapt, extinsii in mod natural la intreg spagiul Rd >< Rd
— se numegte functie-nucleu.
Aplicagi aga-numitul “kernel-trick” problemei de optimizare

~* ' (liw ¢< > >+*\|~||2) (39>

10:31" HllIl — w- x- - —w

gweRm n , 1 Z 7% 2
1:
_/—/
n0t.:J(1I1)

***************Ending Page***************


***************Beginning Page***************
***************page number:269**************
269.
Sugestii:
2. inlocuitgi in expresia J(2I1) din relagia (39) pe 2D cu expresia lui
TL
12* I 202M952) e Rm, (40>
3:1
existenta coeﬁciengilor 023- € R care satisfac aceast'éi egalitate ﬁind garantatﬁ
de rezultatul de la punctul a. Ve§i nota aceastii nouéi expresie cu J (02), unde
02 n25. (021,...,02n) E R".
22'. TransformaQi forma expresiei J (02) pe care a§i 0b§inut—0 la punctul b.2' intr-
una echivalentﬁ, in care nu mai apare functia (M) ci (doar) func§ia-nucleu
22'2'. Similar, scriegi ecuaigia separatorului dec'iz'ional 212* gb(a:) I 0 — unde a: este
0 instanté oarecare de test din Rd —, inlocuind 211* conform relaigiei (40) §i, in
ﬁnal, eliminagi ¢( ), punénd K(:cZ-,2c) in locul produselor scalare ¢(x,) - ¢($).

***************Ending Page***************

***************Beginning Page***************
***************page number:270**************
270.
c. Fie problema de optimizare
04 arg 015211125711 (04),
unde expresia J (a) a fost obtginuté la punctul b.2'z'. Presupunénd c5 se dore§te
rezolvarea acestei probleme de optimizare folosind metoda gradientului de-
scendent, deduce§i regula de actualiza're pentru vectorul de coeﬁcienti 04.
Sugestie: Pentru a v5 facilita deducerea gradientului functjiei J (a), v5 sugerém ca in
prealabil s51 rescrielgi expresia funcgiei J (04), pe care a§i 0b§inut-0 la punctul biz} sub
forméi matricealé. Concret, noténd cu K,- vectorul-coloané (K(a3i,a:1),...,K(a:n,331))T,
pentru i : 1, . . . ,n, §i apoi cu K matricea de dimensiune n >< n 0b§inut5 prin ,,a15turarea“
vectorilor Ki, adicéi K : (K1, . . . ,Kn), ve§i putea demonstra egalitatea
1 n T A T
J(a) : E Z;L(KZ 04,y¢) —|— 50¢ K04.
7,:
Apoi, pute§i, eventual, $5 folosigi urmétoarele formule de calcul cu derivate vectoriale (
Cf. Matrix Identities, Sam Roweis, 1999, http://www.cs.nyu.edu/Nroweis/notes/matrixid.pdf.)
8 8 8
5a — TX:—XT I 5b —XTAX: A ATX
()8Xa axaa ()8X (+)

***************Ending Page***************


***************Beginning Page***************
***************page number:271**************
Riispuns 271-
a. Conform Sugestz'ei din enun§, vom presupune c5 A > O §i, de asemenea, c5 funcjia de cost
L : R2 —> R este derivabilfi in raport cu primul argument, pentru orice valoare ﬁxatﬁ a celui
de-al doilea argument. Pentru convenienlgﬁi, vom nota costul (sau riscul empiric) mediu cu
regularizare L2 astfel:
1 n A 2
J<w> I 5 Zuw - H) + 5|le -
i=1
TLO . a ¢ c 0
De asemenea, in cele ce urmeazéi vom folosi notatia L/(z,y) It 8—L(z,y). Gradlentul func§1e1
z
J(w) in raport cu vectorul w este
1 n a A 6 2
(w) 228w (w 2 y>+ 2810an
_ 1 n , 8 A a 2 _ 1 n ,
intrucét in punctul de minim a1 funcgiei J gradientul ei se anuleazﬁ (adicii, VJ(w*) I 0 € Rd),
rezultéi
w,‘ _ _ii£2(w* .2. >2-
_ n)\_1(9w “yz Z‘
7,: _r—/
€1R
. A not. 1 (9 . . .
Cons1derand 042 : ——>\8—L(w - 2:2, yi), 0b§1nem concluzm d1n enun1;.
n w

***************Ending Page***************


***************Beginning Page***************
***************page number:272**************
272.
b. inlocuind in expresia funcigiei J(1D), care a fost deﬁnitii in relagia (39), argumentul
'LT) cu expresia lui 117* din rela§ia (40), vom obgine funcgia J exprimatii de asté datﬁ
in raport cu vectorul de coeﬁcienigi a:
1(a) I a ZL<<Zaj¢<wj>) wen), w) + 5(Zaj¢<wj>)
1:1 3:1 3:1
1 TL TL A n n
I 5 ZanJ-(Wj) - m»), yi) + diam») - (Zamm)
1:1 j:1 1:1 j:1
1 TL n A TL n
1:1 3:1 1:1 3:1
Am ‘ginut cont de faptul (:5 functjia K este simetricéi, intrucét produsul scalar a1
vectorilor este simetric: K(a:¢, xj) dgc' (Mm) - (15(xj) I (Mag) - (Maui) déf' K<$j7$i>-
Ecuaigia separatoruluz' decizional 15* Mas) I 0, unde a: este 0 instan§5 oarecare de test
din Rd, se scrie astfel:
11f‘ -¢(w) I 0 I (Ewan) -¢<w> I 0 I 20¢¢(¢($1)'¢($)) I 0 I
i:l i:1
Z ozz-K(asi, x) : 0. (42)
i:1

***************Ending Page***************


***************Beginning Page***************
***************page number:273**************
273.

Observagi ca expresia separatorulu'i decizional este linara in spatial de
trésdtu'm' Rm (si anume, 62* -gb(ac)), insa in general ea este neliniara in spagiul
ini§ial, Rd.

Pentru exempliﬁcare — si pentru a intelege mai bine aﬁrmatia aceasta —, Va
sugeram sa-l inlocuiij in relaigia (42) pe K (332-, :5) cu expresia nucleului gaussian,

2
a: — x-
H2—22||, unde 0 6 R+ este ﬁxat. (Aceasta functie-nucleu mai este cunoscuta
0
§i sub numele de “Radial Basis Function”, RBF.)

***************Ending Page***************


***************Beginning Page***************
***************page number:274**************
c. Folosind notatgiile matriceale introduse in Sugestz'a din enung, se veriﬁcﬁ imediat 274'
c5 rela'gia (41) se rescrie intr-adevéir sub forma
J(a) : l iL(K-Ta y) —|— ﬁozTKa
711:1 7“ 7 Z 2 '
Gradientul functiei J (a) se obigine u§0r folosind regulile de diferenlgiere vectorialé
indicate de asemenea in Sugestia, din enuntgz
1 n a T a A T
Va] I _ —L Ki 7 1; —— K
(a) 712604 ( 04y)+6a204 04
<56») 1 n / T a T ,\ T
: — L K - T — Ki — K K
n; (~1wa a>+2< + m
(510) 1 n / T
: — L K - i KT- /\K . 43
n g < Z my) + a < >
Am @inut cont de faptul 05 matricea K este simetricii (deci K : KT), intrucét
funcgia-nucleu K este simetricﬁ, dupii cum am justiﬁcat mai sus.
Acum putem scrie regula gradientului descendent, astfel:
1 n / T
<_ _ _ LKT ,TKT AK],
a 0417[n;( @y)+ a
uncle 77 > O este rata de invdja're.

***************Ending Page***************


***************Beginning Page***************
***************page number:275**************
275.
Observatie (1)2 Dacé ne propunem sii obtinem regula de actualizare core-
spunziitoare gradientului descendent stohastz'c, atunci este util (§i chiar necesar!)
de observat c5 produsl K a se poate scrie in mod echivalent astfel:
1:1
Iinénd cont de aceasté relatie, precum §i de relatia (43), rezulté c5 putem scrie
regula gradientului descendent stochastic astfel:
1 / T
a <— oz — n —L (K1- 04,y1-)K1- —|— AaiKi .
n
ObSGPVa§i€ (2)2 in documentul Supplemental lecture notes, John Duchi reco-
mandﬁ urmétoarea varianté a acestei reguli:
04 <— 04 — 17[L/(KZ-T04,yi)Ki+ nAozZ-Ki],
cu scopul de a evita ca estimatorul 04 calculat prin aplicarea metodei gradientului
s5 ﬁe deplasat (engl., biased). Se recomandéi de asemenea ca valoarea lui A s5 ﬁe
micé, iar 17 $5 se modiﬁce in raport cu iteratia. De exemplu, pentru iteratia t se
poate lua 17,; I 1/\/t sau un multiplu a1 acestei valori.

***************Ending Page***************

