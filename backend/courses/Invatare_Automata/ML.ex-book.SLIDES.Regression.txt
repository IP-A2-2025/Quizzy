***************Beginning Page***************
***************page number:1**************
1
Regressmn Methods
Contains:
Linear Regression: 0x. 1‘ a. 6, 1‘ s, 4. 5. 22‘
Logistic Regression: 0x. 1:1, 14. 35_ 11, 18‘ as, cw
from the 202m‘ version oflhc ML rxorcisc book by L. Ciurluz 01 a1.

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2
Linear Regression
with only one parameter, and without offset;
MLE and MAP estimation
CMU. 2012 fall, Tom Mitchell, Ziv Bar—.loseph, midterm, pr‘ s

***************Ending Page***************

***************Beginning Page***************
***************page number:3**************
3
Consider real—valued variables X and Y. The Y variable i5 generated.
conditional on Y, From the following proccss:
~ ;’V(O.<12)
Y : (LX i 5.
where every z” is an independent variable‘, called a noise term, which is
drawn from a Gaussian distribution with mean U, and standard deviation
a,
This i5 a one-feature linear 7897255110" model, where (l is the only weight
parameter.
The conditional probability of Y has the distribution pmx. u) ~ 1V(uX. a1),
50 it can bC Written as
. 1 1 a
)Y XJI : PX) i YillX ‘
1< i > m” (202i >)

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
4
MLE estimation
a. Assume we have a training dethset ofn pairs [Xt 14> for t :1. .u, uhd r1 is known.
Which ones of the following equations correctly represent the maximum likelihood
problem for estimating h? Say yes or no tu eadm one. More than one of them should
have the unswer yes.
2 ‘ 1 <Y X >1
t ‘u nmX” it.‘ if, r a ,
g ' v2“ P 2w
1 1
h. Jugumx“ 1'1, ﬁe“ (eﬁw, e (1:9)")
m mgmsm Z, up (eﬁo; , my)
l

m themes” new (epw. , mm’)

u. mgmax,‘ 20; e 11X‘)?

m. mgmtm DU? i uXJZ

***************Ending Page***************

***************Beginning Page***************
***************page number:5**************
a
Answer:
u“) '“J p0,. .meHXMJ
i ,v,x,_ i in fix/r x,
Eu \ w 11%” w( wt u >
Therefore
w " 1 1 , 2
an”; I mnmun) :élrgmaxnﬁvxp ’W(" iaXJ (u)
t. ,. M M _
i ar nmx< I )“ﬁrxw(i I (vi/moi?" max 1 0x filo/i001
g A J5” ,7, ' 20? ' ‘ g <4 (ﬂop p HQHQ ‘ ‘
i engmfxylloq\(iﬁ[v,*zLXJZ) (w)
n 1 / 1 H l / 1
:ngmjulnnvxp *ﬁU‘ , ax» “@me faint , “m
,Z, J H ‘Z. m
I ingllleX*$23‘O/, i ax)! : A“; “551,20; i um” (r!)

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
F»
b. Derive the maximum likelihood cstimatc of the parameter (z in terms
of the training example A75 and Y,’sl We recommend you start with the
simplest form of the problem you found above.
Answer:
em,‘ : (“gunning e.><,]1 : (ligllllu ((12 2x} 2112 X,Y, - Z l?)
H H H H
e fl 21L. KM , ZigXlY.
1 Z'Ll X3 21L, Xf

***************Ending Page***************

***************Beginning Page***************
***************page number:7**************
7

MAP estimation

Lots put a prior on a. Assume o ~ mu. A’), so
e 1 l 1
‘,(nm mum (eﬁn)
The pnsterior probability or a is
1)th ,Yttpﬁ,,..X,t.a)p[tl)t]
PM ‘ ‘ > t", my, ,mxt, ..Xn-a')p(u' A) (h'
We can ignore the denominator when doing MAP estimation.
t-. Assume o i l, and a ﬁxed prior parameter x. Solve for the MAP esthnete oft,
mgmtlxunplh. .mxt, v .X,,.u) + lttp[(t\,\)}

Your solution should he in terms of x,‘s_ YRS‘ and A.

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
a
Answer:
7,0’, mx‘ .Xﬂw) p[u A)
" 1 1 , 1 a‘!
I (Emuxp (iﬁu, "hm )) - WWJVW)
":1 " l 1 . 2) 1 ( “I >
i i~ ﬁ vhux, i if
(11 MW‘ 2( ) v5)‘ WP 2x1
Therefore the MAP optimization problem is
“my (I. mi i 520', i 1m)" +lllﬁ i in“)
, i “ - 1 i 1
i ("MW (120/, , um , Wu
I argmm (20/, i uxﬂ + I?) : mgmm ("Z (Z x3 + %> i z“ Z Ky, + Z v?)
.4 H H .i.
EL‘ xy,
i "M/U’ I i
2:’ Xf + V

***************Ending Page***************

***************Beginning Page***************
***************page number:9**************
9
d4 Under the following condniom haw do (he prim‘ and conditional likelihood curves
charge? D0 1| “W and “"1" become closer together, or funher apart?

mm prior 1I(!'\-_ _ MM} _.);n.1|) mp M,‘
pmbubimy: wndnwnnl llkcllhuud; \n n \
Wm“ “me wider. narrower, or increase Dr
0r 535,27 ’ same? decrease?
--—

As A a 0

More data:

“max

(ﬁxed A)

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
10
Answer:
pumpmr 1K ‘ .. \ 3 _ ) ‘ W? MP‘
probability. conditional llkcllhuud: u H
will“ narruwur Width narrower, or mere-m ur
0r samev >ume7 decrease?
dengue
A5 A A I» increase
More data:
a‘ " A X Same narrower derreagg
(ﬁxed x)

***************Ending Page***************

***************Beginning Page***************
***************page number:11**************
11
Linear Regression i the general case:
I MLE and the Least Squares Loss function
o Nonlinear Regression
o [Ll/Ridge] Regularization; MAP
CMU, 2015 spring, Alex Smola, HWl, pr‘ 2

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
12
Int'mductio'n: The objective of Lllis problem is to gain knowledge on lineal- regression,
Maximum Likellhnnd Estimation (MLE), Maxilllulll»a>Posteriori Estiinnilnn (MAP)
and the v-sri-snts nr regression prublen'ls with intrnnnetinn hr regnlniisntihn terms.
Part A: Linear Regression i MLE and Least Squares
Consider a lineal- model with Sullle Gnnssinn noise;

Y, : X, U‘ +1)“, where st ~1V(¥'J,02].1 I 1.. n. (l)
where K e 1R is a scalar, X, e 1R“ is a dediniensinnnl vector, b e lR is a eenstnnt, H! e 1R“
is d-dimensional weight m1 X“ and s, is n i.i.d. Gaussian nnise with variance (72. Given
the data X“, : l. n, our goal is tn estiinnte 0‘ nnd ll which Specify the rnnnel.

We will show that solving the lineal- lllodel (l) with the MLE methnd is the same as
solving the following Lem Squams pmblsnn

.ngnlinil e X'NQ/ e Xhi) (2‘;
where Y:(Y], m‘, X; : (I,X,)‘,X' : (x;_ ,x;,) and 3: (2th)‘.

***************Ending Page***************

***************Beginning Page***************
***************page number:13**************
13
a, nun. me mudel (1), derive me cunditiunal distributiun ul' mpg. u-JL Remember um
x‘ is u ﬁxed dam puinL
Answer:
Nnte chm YJY,‘ I.‘ r, N .’\'(X, u‘ +1, ML thus we can write me p.d.f. of wx, m1, in the
follnwing form:
1 ,ex, per l
my, :L/,‘4Y,IYA',1|): J?” mp (W 201' U ).

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
14
b. Aetuming mat between each I‘, 1 I 1, H", give an eeph'm ezpmssion for the
log'likelihood, HYM) of the data.
Nate: The notation for Y and a was given at (2). Given that the :"s are Lid“ it fulluws
that P(Y\,i) I 1'1, P(Y,\m,b). Remark that we are just omitting x, for ccmvenience: as the
problem explicitly tells that x‘ are ﬁxed points.
Answer:
Given t/ : m, . ,I/N, einee Y, are independent as {"5 are i_i.d. and X,"e are given, the
likelihood of YU is as fullows:
" " 1 (u, e Y, m e h):
frv : we) I 11mm 1,) e ,1] \mﬂ m, (, M
l H H , X, ' 7 V Z
\/2m7 2”
Now, taking Lhe I”, Lhe log-likelihood er y d is as follows:
1 'L .,
1(y , W) e Hum/27m)’ 27L“ ex, win)’ (a)
H

***************Ending Page***************

***************Beginning Page***************
***************page number:15**************
c. Now show that solving for a that maximizes the log-likelihood (La, MLE), is the
same as sulving the Least smmre problem of (2).
Answer:
To maximiu: the logrlikclihood 1(1/ : W). we want Lo focus on the second Lorin slum:
the ﬁrst term of the lugrlikelihuud (:i) is e eenetent. In ehert, to maximize the second
term uf (x), we went tn minimize ELM/r x, h» m Writing it in the mntrixeveeter
frnm, we. get:

rhino , W) n "3"2?“ e \’, w e h)“ , "EMU e X ,1) (i, e X ,1)
where again, Y : m. MYHJT, X,’ I R116)’ x' I (X,'. ,Xgﬂ end x1 : (Mfr

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
o. Derive 6 um nioxiiniseo use log-likelihuod. "l
Hmt: You may ﬁnd useful the following formulas?
r a T , 5 r , r 5 J , r
(on) Ho X I 67X oI a (011] WA AX I (A+A )X
Answer:
Setting the subjective functinn 1(5) as
VIM) I (1/ I XH] (v I X’Jil
implies (see rules (5a) and (5h) in the document menlioned in the Hint):
VMUJ I ZX'UX'J I v)
The logIlikelihuod maximizer la can be found by solving the following optimality cundiI
tion:
void) I u I x" (Xi? I ,w I u I X"X’Ci I X'U/ I 3 I 0;" x’) 'X" l/ (a)
New that (W X’)" is possible herallse it was assumed that X has full rank on me
column space.
,. Flum Mam: Main/104 5m, now», was». mtp /,/Www(5 Hyu edH/'~rvwel5,'rw(e§r'mahlxld pol

***************Ending Page***************

***************Beginning Page***************
***************page number:17**************
17
Part B: Nunlinear Regression

e. Considering the higher nrder tenn. tux‘) : (1,X,_X3 .xfﬂ, then we can lnodel Y
in the Ivm-order mode] as it follows:

i1: 0(X,) ii’ +5, with e‘ ~.V(0. r1‘).1 : 1. .u. <5)
where all the deﬁnitions are those from equation (i), except a e 1R“‘, and for eiinpiioity
let it : 1 [or Xi a FF‘.
As we did in Part A, show that
if we use MLE and assume that @(XJ ‘:" (¢(X|),0(X2), .. m(X.,))T has full rank in the
CDlumn space
then the optima] [value for] ,i in (a) is (4900’n(.‘(>)*‘o(X)’Y.
Hint: You are not expected to wriie the whole Steps again. Focus nn the ehnnge from
the log-likelihoud expressions of Part A_ and derive the optimization problem‘

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
m
Answer:
A: the mhttmh bctwcn t, and X, have changed, the cunditiunul Wu. hm! likelihood
function uhungcs us well‘ The expressions can be just modiﬁed by jut-t replacing x, to
WC) as following:
i (m i ("(th - fl}
h ,xt. v.1, i» i
n m t 1 MN“ W
my W) irllnb/‘Zmﬂipgujv*c'AQxJ ,1)
A: we did heme, the maximum likelihood method (‘an be expressed as
[.1 II: it H1: ,_ ’T it a’.
mi,‘ (Y m a) hamzth, WM) a) "Wm (/(YJ i) (h (120 )
and thus the maximum likelihuud estinlatur it 3' : (1»(X)T<>(x1)*‘m(X)U/. thmugh Sim’
ilar steps as m Part A.

***************Ending Page***************

***************Beginning Page***************
***************page number:19**************
19

Part C: [Lg/Ridge] Regularization; MAP
Comment: Many modem regression problems can have hun-
dreds or thousands of predictor Variables, If these variables are
correlated, standard techniques Will lead t0 overly Complex models
and poor generalization error-i Another issue is that problems may
have more predietor variables than examplesl When this happens,
standard regression models will fail.
0ne technique that addresses both these issues is called Ridge
1vgmssirml The idea is to modify the loss function by adding
a penalty term to the weights t0 encourage them to be small.
This penalty terrn is known as a myazoiizer and eontrols model
complewity hy putting large weight on only the moxt important
pmdictm‘ variables in the model.
The Ridge regression penalizes the squared length of the weight
vector .1. This is sornetirnes known as an L2 penalty heeause it is
the square of the L2 norrn (aka the Euelidenn norrn).

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
20
Important Note: The results from this Pm hold for both linear (Part A)
and nonlinear (Part B) regreeeienn For the Sake of simplicity, here we will
only prove them rnr the linear use. ln order Lu get the nenlineer form of
hheni, it is enough to replace x by M), where A’) was deﬁned in Part B.
f. ln ease XiX is not invertible.“ you can add the diagonal term AI to it so
that xTx + A! becomes invenihle, with 1 the idenmy matrix of size i!‘ and
A > u.”
Show that ,6” “:" (x ‘ x +)\I]’ lx ‘ v is the solution of the optimization problem
Many,“ (HY r X ﬁ'llé + MW é) (b)
~ 'rhh n the nee whelh hhirhhe the nnnher hi lemurs’, 4 n. PM: A hr k+ l in Pen h h larger hnn hie
numbcl ei iheieheee. h (i e , d > n) in thle em rehmxlm : rehnx) cl Main: ldevlhileﬁ by senn Rehen the
(2e) rennnh sh ienidxv) n leee cl equal to mll\\1l_li) e h which n enieuei than ii Theiehne the maLIlX \ Yx
which h l X (I n hn. h.“ nhh Mn vhm ehhnhe hr mvnvcd
'i New. [or e proper \mlu: er A. 1h: malllx xTx + A! n hill rank .md ehh he line-led To eee line not: Um. ir the
eehhnhe in xix were hheniy dependene chm A! adds zh: eehhe \nhie u] but tn two dlﬂ'elent Component: ei lheee
enlhhhh, eihh- vhry hrennn hhenh ih liqnhilnhl in xY v + AI

***************Ending Page***************

***************Beginning Page***************
***************page number:21**************
21
Answer:
The procedure is almost the same with [the one in] part (t. First, set the
objectivc l'uncLiun 9/’(9) as following:
1W) : (t , X'V'JTO/ <K'1">+ MW'Hé

m) t9», vim/"MW ‘2(A"A"I” , V] +2”,
Tm lug»likulihuud lnuxilnizcr 9” can bu fuund by solving the fulluwing upti»
mality condition:

vttt1”(3”) I u
< > (XWX'IV' , 1,)’ M” u<> (Y'T Y’ i m3" X'Tt/
a t9”: (X" X'+)J)"X’ y

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
22

g. Now ronsidar nee case where d” has n prior dimihnnnn x1” N Mu. WEI].
wme une posterior diemhnnnn nr my; given me nm ennnnle, end ,mx' given me whole
data, respectively, Assume independenee between d" end the noise 5, ~ ,VUJJYQ], for
1e 1... n.

P y a” P-d”
Hint 1: Use Baye>‘ rule: mum) I '( ‘l! (4,)“ J. Then follow similar steps with

1 n

Part A.
Hint 2; Make use enhe fart that by joining lwn or more independent Nmnnl univariate
variables we get a multivariate Normal distribution.

***************Ending Page***************

***************Beginning Page***************
***************page number:23**************
23
Answer:
We know thet Y,\X,.,i~ ~/t'(Xi $.02) and a” s mu, "11y Using Bayes‘ rule, it follows
that
u s , /// 'H 1 , n , 1 /// u
m1 mes/(m1)i/(iiJehxp( F0 de J’) KAP< 7,2,1 ‘a )
where 4p) is the density fiinetien for .f" and m ; is the density function for ,imq. The
normalizer f0!‘ the p-dJ- h is deﬁned as Z L1 fW, i1”)v(=i”)il,i”, hnii rewriting the p.d.f.
of I»[H”\Y,) leads to
H ,1‘ i; .i “kins,”
11(,iis/,)ezisp( w” Y, ti) 27,21 ,1)
Likewise. the p.d.f. of my is
, my] , is ii" ,Wtqyiww fifth,”
iii e Znutp 2111 . v y i J 2W,‘ t
l l . i,” 1 1 w
, Twﬁﬁu» ex i Hing/2i! HZ)-
where z,‘ is the neinieiizetien factor, deﬁned he 2,, : [i j(y\1/')i,(ii")iw”i

***************Ending Page***************


***************Beginning Page***************
***************page number:24**************
24
ll. 3". the MAP estlmete uf s", ls deﬁned es the value of a" that establishes tlre mode
of the a pusteriuri prubability lam" X. l')l Show that solving for the MAP estlmete
leads to the problem (n) if l can lse expressed lr. terms e and ,7, l.e., A e ole/.17]. Find
the explicit expression for w. .7).
Answer:
nom the previous solsprololem, lt ls clear thet
rrlrrx/llll"lla) : rem‘? y e 2(3le 7r" ll l2) <1)
"l
If ,,~‘ becomes T the“ the mlmmlmtlon problem hPcomes equal/elem to (0),
ererllllq \Y e X3”H§ + hum 5‘).

H1
In other worrls_ rearranging the tarms_ /\ e We” e 7 fnr (7) to heeome (o).

'7

***************Ending Page***************

***************Beginning Page***************
***************page number:25**************
2;
i. Describe unc puLaninl problem in llle abscncc er rcgulal-izaLion Lcrm in (e) and how
the rcgulurizutiun tel-In Ci“! alleviate me putcntiul problem.
Answer:
The reglllarizing term penalizes large components in ll‘ which leads m shrinking ll m
have :lllallol- nonn. Ae a consequence, the penalty lcnns encourage the model le avoid
oval-ﬁtting and thus prevent adjusting le uutliur data points wlllel. wuuld otherwise
inﬂuence 31 drastically.

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
2!»
Linear Regression with L2 Regularization (Ridge
Regression)
The derivation of the updating rules
fur the gradient descent algorithm:
the batch (steepest descent) and

sequential (online / incremental) variants
CMU, 2008 fall, Eric Xing, HWI, pr. 4.2.2

***************Ending Page***************

***************Beginning Page***************
***************page number:27**************
27

Let D : (it. 1:,}1 e be a eelleetien N training examples Let y,
be the respense variable fer example i. Let l/ e W“ be a column
vector of all response variables, Each training example alse has k
predieter variables. Let .r‘ e 1le‘ be a revv veeter of the predictor
variables for example e Let X e RM be a matrix of all predieter
variables where row i is in Let a e illth be a column veeter that
contains the weight panu'neters that we‘re trying tn learni
Note: At CMU, 2015 spring, Alex Smela, l-lWl, pr. 2 we have
shown (by using the veeter derivative method) that the analytic
snlntien tn

“gm?! HM i XT3H2 ~ MUN’
is given by A

am a (XTX + A1) ‘XE/-

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
28
u. Batch (Steepest Descent) Solution:
While the analytic solutieu is easy to iruplemeut it can become intractible
wheu there are very large huruhers of predletor variables. This ls heeuuse we
ueeri to eerupute XTX and its inverser One way to deal with this problem is
to use an eptiruizstieu teehrrique called steepest (a.k.a batch) deseeut. This
is uu ileranive procedure than updates the weights et the t-th iteration as
follows:
‘3’ Z 3"‘ e "Vs/(3).
where i, is a learning mte eoustuut and Vina) is the truuspose of the gradient
0f the loss function 2(a)‘
Compute the steepest descent update rule wher. m) is:
1 w
, i i, r‘ z it
Mr 22w. t. #1 +1 u
Hint: You may ﬁnd useful the following formulas (from Mntna: Identities,
by Sam R/uweis, 1999);
,0Tis'lqi ,0s’ ,—
(st) 67M X e 07A e e u (all) EX AX e (4+ A )X

***************Ending Page***************

***************Beginning Page***************
***************page number:29**************
29
Answer:
| N A | N A
1 8 : T (L/,*f,32+fﬁTj:T L/f’2!/,Jl?xﬁ+ 1,11)? +4“?
(I) 1; ,> _, 2? < >2,
[mﬂTupij
i limiwhuﬁﬁw~¢>+§ﬂa
And now take the gradient:
H%(d)<sm'w»71 A T } , v T ,
v‘f a; : 22( 2w, >21, 1,15?)4A;i:2[ um 41,,(Lsnma
‘ r1 r1
s” the ﬁnal update becomes:
N
a’ : W‘ i77(Z[*y,fr+:r,T(113"‘)l+ A3“).
‘:1

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
3n

b. Sequential (Online) Solution:
Another type of update is called stochastic (a_kla incremental) gradient dea
scent. This is a sequential method that updates that parameters after seeing
eaeh example. This is extremely useful for very large datasets and also when
data examples stream in over time This is sometimes called an onli'ne learn-
mg teehhique This update rule is given by:

a’ : 1"’ ai/VtWPM h)-
where i; is a learning rate eenstant and [sharpie] is the transpose of the
gradient of the less iunetien at a partieular example i.
Compute the stochastic deeeent update rule when 1,,(3. Thy) is:

[4.111, yr) : [yr r ifs)“ + 5K;
Answer:
ln similar fashion to the batch method we arrive at:

i’ 1H ei,(e l/,J + {(MP‘) +MH]

***************Ending Page***************

***************Beginning Page***************
***************page number:31**************
31
Linear Regression and Newton’s method
Stanford, 2007 fall, Andrew Ng, HWl, pr. 1

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
:12
Introduction to Newton’s method for MLE
“0m CMU, 2008 Spring, Tom Mild-ml], HWZ, pr. 1.2 and
Stnnrnrd, Andrew Ng, leetnre nntes #1 me //€522952an‘md edn/neree/eezzhetel pan eertinn 1
While MLE‘s can sometimes he l'uulld analyLically,
l'er complicated likclilloud lirnetiene it may need
te he eempnterl using numerical methods.
Om: rnethetl it the Newten nlgerithrn, which it» 1
eretively ﬁnds a sequence of HM’ .. that (under 1
irleel eenelitiene) converges te the MLE 0. 1
The idea here is thet we are trying te ﬁnd the PW] ;
value of a thet maximizes the likeliheerl function ;
I (whnse nnrenieter is H). ;
New1nn's rnetlnnl allows ne tn rln this by eﬁieiently r/ "i "n
ﬁnding a root for the likelihood function‘; ﬁrst
derivative by following successively closer tnn-
gentle ul' the ﬁrnt tit-rivetive rnnetienr
Nnte thnt nne may exnnntl the derivative nl the log-likelihood fnnetinn nrnnnri mt
u , mi) e l'w/l + (fl e 911/"(011

***************Ending Page***************

***************Beginning Page***************
***************page number:33**************
:13
Solving fnr A gives
s mm
H e w i'
WW]
Thie leads te an iterative scheme where
e mm
Wei i 1,, e
mun
The genemzmttt'n" of Newtnn-s method tn a mlllLl-dimensinnal setting (also called the
Newmn-Raphsnn method) is given hy
0m 0, ,H iW/mi]
llere vtuu) is. as nettelt the veeter er parLial derivatives el' 1(0) with respeet te the
145s; and u is en ii-liv-ir lnilh'ix (actually. n + l-by-H + 1, aesuming that we include the
inteieept term) called the Heseien, whese entries are given by
021(0)
H e i
'1 a”, 001

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
34
Important Remark

Newtonk methad typically enjoys faster convergence than
(batch) gradient descent and requires many fewer iteratinns m

get very close m the minimum

One iteration ol' Newlon’s can, however‘, be mom eepensiee than

one iteration of gradiunt dcscnnt, sinee it requires ﬁnding and in-
verting an "4;an Hessian; but so lung as n is not too large, it is
usually lnuch faster Overalli

***************Ending Page***************

***************Beginning Page***************
***************page number:35**************
a;

In this problem We will prove that if We use Newtonk method t0
solve the least squares problem, then we only need nne iteration
to converge m 3.
a, Find the Hesaian 0f the Cost/loss function

1W) : 1 iUTfm , 1H”)?

2 ‘
‘:1

I)‘ Show that, the ﬁrst iteration uf Newtnnk' lnethml gives us
13* (X ‘ xy'x ‘ y. the solution w our least squares problem‘

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
as
Solution
a,
Hm) , l i
031 iEUiTJllil/llhi/
lil
So,
DIV/(l3) 0/13) T i i
i 5, in , in n, i ln ll , xTX
admin g: 0le < ‘ *1 )‘1 Z‘: ‘1 ‘* < )1‘
Therefora the Hessian of 1(a) is x14 (This can also be derived by simply
applying the rules from the lecture notes on Linear Algebra.)
bl Given any W", Newton's method ﬁnds dill according to
W‘ : ,9l0/,H*‘Vilill<°\)
: Jim"(X‘X)"(Y‘X.f'mil\"l/l
I ﬁll», ‘54m l (XiXiniH
Z (X‘X)"X‘l/
Therefore. nu manni- WhuL W’ wl: pick. Nuwiunls mcthull always ﬁnds 3 after
one iteration.

***************Ending Page***************

***************Beginning Page***************
***************page number:37**************
n
Linear Regression: 2 variants / extensions
Locally-weighted linear regression for Gaussean noise;
[Lin-weighted] linear regression for Laplace noise
CMU, 2010 spring, Ev Xing7 T. Mitchell, Av Singh,
HW2, pr. 3.1-2

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
38
1n linear regression, we are given training data of the form 1v e (X. 1,) I
“my,” I : 1,2 n where .r,‘ e RM, i.e. .r‘ : (m “My, ‘1/, e 1R,
X e lRWL where mw t of X is t,‘ , end v : (In- tn) ‘. Assuming e peremet-
rie model of the form: u, : “,2 + e, where s, nre noise terms from s given
distribution, linear regression seeks to ﬁnd the parameter vector 8 that pro-
vidcs the best er ﬁt of the above regression moch One es-ttm'e te measure
ﬁtness is te ﬁnd a that minimizes a given loss jnnett'nn 1(a)‘ At problem
CMU, 2015 spring, A. Smnln, HWI, pr. 2 (pert l), we have shnwn thet if we
take the lnss funetinn tn he the sqnnreermr, i.e.=
m) e Em , ml e <th WW , t1-

then [assuming thet XTX is invertible]

3 : (XTXWXH/ ("l
Mnrenver, we hnve else shnwn thnt if we essume the: sl ere lid. and
sampled lrnm the same zero mean Gaussian thst is‘ 5, e Mal. e1), then the
least squnre estimate is ulee the MLE estimnte for PMX: u),
In this problem we will exnlnre twe variants /e1tensinns te this basic regree
sion model.

***************Ending Page***************

***************Beginning Page***************
***************page number:39**************
as)
Part I: Weighted Least-square
Assume that n, v . v . :1, are independent but each 5, WW). (1,1)‘
a. Calculate the IMLE of :3.
Hint: You may ﬁnd useful the following formulas (from Mutr'ia:
Identities, by Sam Rnweis, 1999);"
readeaT: raq/Z/J.
(1)1004le A OXX a a (1)0)0XA AX (.1+u1 )A
" hm) r'www c: nyu edu,'\m\~e\s, nule>,'md\v\xld w"

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
40
Answer:
1/, I PM x F, and 1mm: a) : )\‘>[F\T\¥.r7;l:¥. Thus the formula for the MLE of a
is:
,v, - : m’ nlen mph/1 :m "m l1\::,1,:,i
m a‘ 111m 1 g4 Z 1m >
, l [11» <1,’ 11"
(\1gf:m>\21|1(ﬁup(iT
1 rial ‘i Ta‘
< (1/, PM)!
_ 1 T, l
i [\rg|A|:|\Z(T;l(1/,i I, a) (01

***************Ending Page***************

***************Beginning Page***************
***************page number:41**************
'11
Now we write the expression (v) in matrix netntien. If we let W he a diagonal
l
matrix with diagonal entry it, I I, we get:
v,-
llms I arglnln (i, I X.§)T“'[)/ I X6)
ln order to get 3mg we just take the derivatives of (4, I x 2mm, I x3) as
follows:
u I 1(0 I Ylﬂml IXGJ)
ea J t V / >
g I I
I i (flwIl/Tuzw I a XTWy+B XTH'AB). (10)
Fol- any scalar e, J I z, therefore [(WxUWlN I yTWim I [wxu einee
WT I W ee w ie diagonal. New, putting thie buck in (lo) and taking the
derivatives, we get:
‘1 I in i I
0 I i (ﬂu l/IzliTx lw+ JUJH'X 8]‘ 2 M In Wy+ ZXTH'AH.
which [in case XTWX is invertible] meene that
in,“ I (XTWX)"XTW;/

***************Ending Page***************

***************Beginning Page***************
***************page number:42**************
42

b. sliew that the MLE you jiial calculated is tho minimizcr el' llie weightcll
least square lass function ‘71(1) I 2,",(11, a Jay. Express eaeli e, in terms er
thE variance of eaeli example.
Answer:
We have already shown iii (a) LhaL the MLE we just calculated is the minimize:
ul' llie weighted leeal square loss rilnelieii 111(1) e X, (m, <1, 1r’.

l
R'om llie same relationship (u) we have 4|, e 7

***************Ending Page***************

***************Beginning Page***************
***************page number:43**************
43
e. Explain why thie weighted least-square estimator ie [sometimes] preferred
te the non»weighted version. Hint: Cunsidel' the eeee when e; ie large and
when it is small.
Answer:
When the variance 0;‘ is high. then the data point in. it) might he en outlier,
ee the neiee turn: e, enn bu arbitrarily largo. ln thie eeee, we den't went an”,
te he biased te eeenlnmndete eneh entliere especially when neing the squared
errnr loss.
The weighLod loael square forlnulatiun in thie problcnl achieves that by
weighting the eenteihntien uf end. data peint te the ehjeetive rnnetien by
the inverse of the variance tenn.
Therefore points with large variance won’! eenti-ibute much Lo the loss lnne-
tien and can be safely ignored Ol' at least heing given less inipei-tnnee when
optinlizing fur la.

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
. 44
Part II: Laplace nelse-model
Assume that 2,. we" are independent and identieolly distributed according to a Laplace
dietnihntion. Thnt is eneh e, ~l.e|ilnee(u_1z) : % FKP (e?) with a > u.
dl Provide the lass function Md) whose minimizatiun is equivalent to ﬁnding the MLE
or ,1 under the above noise niooel,
Answer:
The fannulu fur the MLE of 1 is:
,1, H .ii ninnln ii, i, '1) Minn in i 1/, .,_ n
W ei 111m 1 Z 1t i
1 \i/i i MW‘
e iiihiyiMZln(%inp(eT
e 1 , . if ‘ , .71
w y i , i
e mining)”, , .il
Thus M4] : 2), Lu, , Jill,

***************Ending Page***************

***************Beginning Page***************
***************page number:45**************
45
e. What is the advantage of this model compared to the standard Gaussian
assumption? (Hint: Think ahout outliers.)
Answer:
Ifa point is an outlier, then the error in predinﬁng this point given the oorreet
it is much Inrger in the Gaussien assumption (us it is squered) than in this
model (as it is not squared). Therefore, outlier-s will aifeet the estimation of
1 in the Gaussian model more than in the Laplaee model.
nom a modeling point of view, sinre v. I 1,‘ ,i +s,, if v) is an outlier, then the
model ean explain that by making e, large to aeeomniorlate for the diirerenee,
This is possible in the Laplace model, sinee the Laolsee distribution has heav'
ier toils than the Genssien distrihution. To relete this to hart t, to erhieve the
seme eﬂeot, we ossumed that every exemple hes e different verinnee. How-
ever, these vai-iauees have to he estimated (using an EM-like algorithm) sinee
they alreet the optimization problem. while in the Laplace model, we do!“
have to do that. 0n the ether hand. optimizing the L, loss is harder than
optimizing L2 as the L, tunetion is not smooth.

***************Ending Page***************


***************Beginning Page***************
***************page number:46**************
U»
Linear Regression:
scaling of features does not change the regression predictions
MIT, 2001 fall, Tommy Jaakkola, HW2, pr. 1.6

***************Ending Page***************

***************Beginning Page***************
***************page number:47**************
17
Given the training feature vectors and output values (1,. 1/, )H ,,
and a test input vector 1W, and the scaling factors (I), },:\ 4, prove
that the prediction of the test output value would bc the same if
we trained a linear regression on (A. 1/’ he, where ,;,, s 0,1”,
and predictnd on is,“ where JIM X I m "Irw-

***************Ending Page***************

***************Beginning Page***************
***************page number:48**************
4x
Solution
We can represent the relatinnship between the new and old matrices ofillputs
u>ing a Lmﬂ8f01‘ﬂl0ﬁ0ﬂ matmil: willi unly the stilling coclllcicntl, in iLs diagonal
(A): ’
X I x4,
Plugging this hlLo the opmnnl weigh’. equation (where x‘ l, refer Lu the train-
ing set): we will get:
l» I lx l xwx l U
: ((xil) ‘ XA)"(XA) ‘ i/
: (,1‘ X ‘ X.1)"A‘X ‘ i,
: .1"(X‘X)" (A )“Jl x ‘ u I .r‘(X ‘ XY'X ‘ l/
7
Z A" in
And so, mp. predicted nutpl" is:
pivlllﬂml m“. XW a‘ xm Arm Am. u‘
i pmllclml mm

***************Ending Page***************

***************Beginning Page***************
***************page number:49**************
10
Linear Regression
The bias and the [culvariance 0f the estimator
CMU, (7) spring, 10-701 course, HWI, pr. 3.1-2, 4.1

***************Ending Page***************

***************Beginning Page***************
***************page number:50**************
30

The linenr regression model hss ihe fnrm

u I n1 I 5,
where s I (r,.....i,,)’. 1 I [3,, waif, y e 1R. and 2 is an additive noise wiih
5(s) I ll and “11(5) I n1.
We observe s sei of irsining dnin (nil) .<r,‘. W from which to esiinisie
the parameters j. Each n I (r,,.. ,lnllT is a vector of inputs for the lth
case.
Denote by x ihe n X n mnirix with each row sn inpni veeisr, and sirnilsrly
lei Y by the n-vecior of oriipnis in ihe irsining sei.
We have shown at CMU, zuls spring, A. Smula. HWl, pr. 2 that minimizing
the residunl Sum ofsquares HY I X3 l1 lends '0 ihe fOIIOWing estimate nf a:

{i I (XTX)*‘XTY
The lessi sqnnres predieiinn of i/ is given by
g I x3 I X[XTX) ‘XW

Noie ihni Y is s random variables because given any inpni verior i, the vslhe
0f y is up to the random noise. The lcast squares estimator a is a function 0r
Y and Lherefure nlsn a randuln quantity.

***************Ending Page***************

***************Beginning Page***************
***************page number:51**************
31
a. "Show that the least squares estimator is unbiased, that is,
503129,
Answer:
EU] I E[(XTX) 'XW]
I E[(YTX)"YT[X3+5)]
: F[3+(Y‘X)"Y‘5] (n)
i ﬁv(X‘X)"X EH
V
t
: ‘x.

***************Ending Page***************

***************Beginning Page***************
***************page number:52**************
32
b. Show that the covariance matrix of x? equals m 02(XTX)“.
Answer:
mm i (mm. mm“ .1)
“2 Em , mm i mm W .4)
I ELM] i FLHEMLM. m
I E if] i E[.%]E[3]T

***************Ending Page***************

***************Beginning Page***************
***************page number:53**************
53
Answer (cont’d):
0M3] ‘LU L'J[[>'3+(XTX)"XT2)(B+(XTX)"XT5)’]ids?
: E[[ﬁ+(XTX)".\’T2)(if+2TX[(XTX)"]T] fig
: E[[H+(XTX)"XT;)(if+5TX((XTX)T)"]:~qu
\an
: E[,ldT]iE[dsTX[XiX)" +(XTX)"XT@1T] +
,w
L'J[[XT.X)"ATSETX(XTX]"]*JdT
: aEkfmxTx)“ + (XTX)*‘x’ EkwT +
Y Y
E[[X'X)"X‘55‘X(X‘X]’1]
I E[[X'X)"X‘ss‘X(X‘X]"]
I (XTXYUFEEJ X(XTX)"
"Alu\n
I 01(XTX>*‘(X’X)(XTX)*‘
I #0:wa

***************Ending Page***************

***************Beginning Page***************
***************page number:54**************
H
n. Also nt CMU, 2015 spring, A. Sninhn Hw17 pr. 2 (Pan III) we have shown
thnt an extension of the least squares estimator can hn obtained by impusing
a penalty m1 ihe size nf regressinn coefficients
“XII i "HZ + Mil.
where the parameter A controls the contribution of the regularization Lei-m.
Minimizing this cost [uncLZun gives the [olluwing estimalur ul' the rcgrcssion
coeﬂicients A
a : (X ‘ X +AI)"X ,/
which in known ns the Ridge extivnutvv in statistics. The Ridge predictiun
of i, is than given by
J7 Z Xﬁ I X(X' X A AI)" Y’ 1/
shnw thnt thn Ridge sstinintnr is biased.

***************Ending Page***************

***************Beginning Page***************
***************page number:55**************
an
Answer:
FM fa i E[[XTX+/\l) ‘XTlﬂid
r:{(¥*x+m ‘XT(XU+5H*JT
: (XTY+AI> ‘Y’X3+(XTX+AI1 ‘XT FHﬂi
Y
I (XTX+/\I‘;"X’XH*3
I (XTA+/\l)"(XTX+Mi)JJX*H
: (XTX +/\l)"(XTX+)J)3*\A’X+/\l)"/\ll1*3
i jiA(XTX+A1]"H* x
: *A(XTX+A1] ‘d
/ u

***************Ending Page***************

***************Beginning Page***************
***************page number:56**************
f1!»
Linear Regression in 1R2
[without “intercept” term]
with either Gaussian
i including a weighted regression variant i
or Laplace noise
CMU7 2009 fall, Carlos Gucstrin, HW37 pr. 1.5.2
CMU, 2012 fall, Eric Xing, Aarti Singh7 HWl, pr. 2

***************Ending Page***************

***************Beginning Page***************
***************page number:57**************
37
noise
Input T
Ouiput

This ﬁgure shows a system s which takes two inputs l his and outputs a linear combi-
nation of those two inputs, 1,1,!‘ + to“, whore n, and (,1 are two unknown real numbers
The device you use to measure the output oi st Le“ l,“ +11J3s introduees an additive
error s, which is a random variable following some distribution. Thus, the output y
that you observe is given by equation (12):

J/:v>,.r,,+qil+s (12)
Assume that you have n > 2 instances (5,, r12 '11)]: ‘ or equivalently (swim/d‘
where t, ‘é’ [m ml~ In other words. having w meusurements in your hands is equivalent
to having n equations 0f the following form: 1/] : qr]! + (,zfjg + {1, l : 1, , ,n.
The goal is to estimate l\ and Q from those measurements using the maximum likeli-
hood.

***************Ending Page***************

***************Beginning Page***************
***************page number:58**************
58
a. Assume that the s, for I : l. ,a are. i.i.d. Gaussian ranrlom variahles with zero
mean and variance 02.
Compute the log-likelihocid function and use it to prove that the maximum likelihood
estimate l» : [r115] is the solution of a least squares approximation problem. Find the
solution of the least squares problem.
Answer:
L, I V, , tr, m + (‘5:33) Mum‘). Therefore it ~/\f(:,\1,i + 11.1,‘,0‘]. Since tho noise an.‘
i.i.d., the likelihood function is given by
, " l (l/rllmrlznef
L , . t I a.

Taking the logarithm, we get the lugelikelihocicl function:

,Hsmzi] ii t ,2

('!-'2) i *5 0M F” i W ‘('ll illyl‘ i'z'lQ)
Let i, e k" he the veetor containing the measurements_ x the. n X 2 matrix with XU I a]
and r lrllrpl t then we are trying to minimize Hl/ e Xrlg resulting in a solution r-
(XUi) ‘X y.

***************Ending Page***************

***************Beginning Page***************
***************page number:59**************
f9
bl [The weighted "egressim't variant] w
Assume that the for I = l. . .n are independent Gaussian random vari-
ables with zero mean and variance VLLTF‘) I aft
Compute the log-likelihood function and ﬁnd c" : [c;. c5] which maximizes
it, ire“ the MLEl
Answer:
‘CLill/l’<‘1‘il+IZI'Ill)N'/\f(0'0ll)‘
Similar as before,
n ” " r, e r‘ '1', e, .133)!
l(r',.r'1)* e5 103(21) e 21030,’ Z M+.
lei er ,
Now we are trying to minimize llml, ml l5‘, where u' is a diagonal matrix.
with in, e r7, resulting thc solution < e (x l1 ‘u XYUK u ‘u y. (Sec tho
correspondence with the formula obtained at CMU, 2015 spring, Alex
Smola, Hw1, pr, 2.d or directly the formula obtained at , CMU, 2010
Spring, El Xing, Tl Mitchell, A. smell, sz. pr. 3.1-2ia.)

***************Ending Page***************

***************Beginning Page***************
***************page number:60**************
60
1 'r
ol Assume that for l i 1,. ..11 has density Mi) i 1(1) i wirinleull,
with e > 0‘ 1n other words, our noise is Lid, following a Laplace distribu-
tion with location parameter ,1 : o and scale parameter e. Compute the
logelikelilioool function under this noise model and explain wliy this model
leads Lo inoi-e robust solutions,
Laplace no.6.
2 Vllﬂ"
Answer: ‘
mm) : nlugﬂﬂ] ‘*ZW xi-lllv E g
It is prepared to see higher values of "
residuals because it has a larger tail ;
[LC: than the, Gaussian]. Thus it is
more robust m noise and outliers‘ g

***************Ending Page***************

***************Beginning Page***************
***************page number:61**************
n1
Logistic Regression: introductory issues
Stanford, 2016 spring, Chris Piech7
Introduction to Probability for Computer Scientists course,
lecture notes #40

***************Ending Page***************

***************Beginning Page***************
***************page number:62**************
m
You are given H LLLL training datapointe (1,"). W). (WWW) v. (Jl"'.y‘">], where each
veetnn 1W hae d leatnree / attrihnteel Here we will assume that W e (0.1) ler i : l ,al
Logistic [Ingressnm is a classiﬁcation algorithm that works by trying te learn a func-
tian that upproxilnateh Pll'lx). It makee the central usslnnptio'n that P(Y\XJ can be
appruximuted a, a xiymuiii function (also culled logistic fnnatian) applieel ta a linear
eemhinatien nf input features.
IVIathen-iatically, for a single training datapeint (t H] Logistic Regression assumes:
PU’ I 1X e .i) e n[:] and, equivalently
P(Y:0X: i):len(z) where l
n(:) "2 ¥ : if’ with l
l + 1"" l + w’ g ,
ll i U)‘
2 “:" it, + Zuni, : H‘ r and ‘
,ei l
it "2 (no. in. . -H‘<I) e RM. assuming that e 1 1 a 2 t s
.i was extended with the eempenent ii, I l
Starting from the above formulas for the probability of HX, we can create an algorithm
that eeleete values of m that maximize thnt probability for all the training data.

***************Ending Page***************

***************Beginning Page***************
***************page number:63**************
a. Prove that the conditional lng-likelihcmd or all the training data (under the Logistic 53
Regression assumption) isz"
we) : Z (W In 0(a) - A“) + (1 , W) ll\(l e (ﬂu-1m») (1:5)
,ei

Solution:

To slarL here is n super slick way of writing the. conditional probability nlone datapoinh
PtY : MX : i) : out i)" [l a (1(u‘ i)]‘ t assuming l, e (0.1)

sinee eaeh datapoint is independent, the eonditional probability oi all the data is:
HPQ' I WM :11”): HHM' Jew" [1, (,[w Joiniemq (14)
Fl Fl

lr you apply ln te this luuetiou, yuu get the reported conditional log-likelihood for

Logistie Regression.

i (Mu n w WW, 11mm Benet. linen. ltlgavrnmmlhlim minutiae lent ,nnei thmpm n vgalluln (L!) an-
ions.n».m,teni.n,,.t uni.u,l.ﬂ,,wmlu entrant,” valet, my. 21m wing ruinremiuttuuu,
p, it \hlnnle yin it i , w mental we, amount“ tie ‘eliminate tunnle "in 1"’) a Y e "to 1"’)
tetanun ait», (tistntutu to prubtthlllmts [a tonne (em (Mullet; ,te mien luglouiit] (an terminates ennu
tlistntutie

***************Ending Page***************

***************Beginning Page***************
***************page number:64**************
m
Note
[from CMU, 2004 fall, Ti Lﬁhzhell, z. BnnJosepii, HWZT pr. 4]
Actually, the full [agililcelihaod function is:
I'M/VIXMJJHNNI I Inn Pl i ", W J
‘:i
I inﬂmnil/"WH'WPwH'Hv
iii
Z 111((H P\'\‘((l/m"m)) (H PAW!»
iii ‘7i
: lnHPy \'[!/l'y‘|’y'7]+1]\l—[ 13w")
‘7i iii
"1' 1m i h
Because 1, dues nut depend on the puranlctcr in, when doing MLE we cuulll
jinn cunsiLlcr maximizing mi).

***************Ending Page***************

***************Beginning Page***************
***************page number:65**************
as
Exempliﬁcation [by L. Ciortuz]
Let-,5 mnsmw the nearby Mining ——:—
dameL —|—-
(WM/“1) F, 1R‘ >< {0.1), the cunlli» j; § 3 Z i
“Dual logrukeuhwd Bums dame‘ -|_u
will be written as follows:
#(w1“:”2(m"1mw rm] > (1 ;/"))11‘[1 0m Wm
[:1 ,
: 1-1“an + w\1‘\"‘)+1 1110(w,,+ W'," + u'n'f') + 14mm ~11" d" + HQ“) ~
(lill)>l1\(liﬂ[u‘,+11w.|,:n)]+[1’(1) Inﬂiﬂun+mils’+u'gr42m+u‘;r.‘x3)])+
mo) ln(l*11[un+u‘\|(‘m in‘.1§“‘+ w,,"")>)+(14>) ln(lin(u\,+ WMI'WWYUH
(limvlnﬂirﬂun + My?»
i mm, + u,)+ 1mm, i In + WXJ+1IH7<WH+LU +u |)+ln[l*n(my+u‘1)]+
1n(l “(WW w‘ + M‘) +u’_;))+ln[\*r1(u'ui a" + w, + m) +1110 “(MM w‘ + “4); +
1n<1iﬂm+ m)

***************Ending Page***************

***************Beginning Page***************
***************page number:66**************
nr»
Comment
Starting from the expression (11X) for the eonditional log-likelihood funetion,
we simply need to elroose the values ol' ti that maximize it,
Unlike in other situations, here there is no eloaeli farm way to enleulnte in.
Instead we will ehoose it using optimization‘ so we will employ an algorithm
called gmdient eseeett That algorithm elaims that if you eontinuously take
small steps in the direetierr of your gradient, you will eventually make it to
a leeul nrelxilnu. In the can-l1: of Legistie Rugrcssiun it can bu provcn that the
result will always he a global maiimlz.”
The small step that we eontinually take given the training dataset ean he
calculated as: 0
to, u', + "OUILM/(w )-
wei-e i, is the magnitude of the step size (“learning rate") that we take.
~ \t‘t»\te\llltllt\ Qtlllrluii .lntlnn as H\\l pl is

***************Ending Page***************

***************Beginning Page***************
***************page number:67**************
67
h. Shnw that the pertiel derivetive of the conditional log-likelihood function
with respect to each parameter 1r, is:
0 uh) : ip/W e "(w rm) 1%" fur 1:“ 1., (I. <12;
Hm] E,‘ 1

P‘ 1'0 ewe, w)
Hint: You may use the following property for the derivative of e with respect
Lo iLs inpuLs: ,

gm) I mm e my‘ fnr v; e "2

***************Ending Page***************

***************Beginning Page***************
***************page number:68**************
Sulution 55$
The partial derivaﬁve or the conditional log-likelihood for only one datapoint
(i. ll) WAKL the oi, componan is computed as follows:
if
ﬁll-loll :)“[l*zr[ll‘ JilH/l
o u
i RI/llnﬂw I)’ dim/U fulhlll *ﬂ(u‘ I)‘
L , “iv L < i
’ "(o .i) lin(lr i) 0%” ‘J
.1/ 1 u
, iiin-vl’
[ﬁlo .i) lin(lr n] (“ Oi "U '1"
i/ i "(Mil
ow r)[l "(if if“ ‘)l "U ‘Hr’
, ill/fol“ will (In)
Because ilio derivative Qfsums is ilio sum orderivolivos, ilio partial doiiwiiivo
oi the conditional log-verosimility function w“. iii, is simply the sum o this
mm [or each training dalapoini. More exactly, after applying the lii runciioii
w the (H) equality, and than calculating iii- piiriiul derivative wow w, (fur
, é {ill 11)) we will get the (15) result, due to the (16) relation proven
above.

***************Ending Page***************


***************Beginning Page***************
***************page number:69**************
. r9
Another solutlon ’
[based on CMU, 2004 fall: T. Mitchell, zt Bnaneeph, sz, pn 4]
Starting rinni (13). the conditional log-likelihood function can he further written as:
mi} I Em" limpi- t1*')+(1I,/h>)|u(1I autumn»
,Ii
~ n . ,ui
I 2M" 1n mi) I ln(1 I m, Wm
H hm‘ w)
W H m I i i H u '. tr w ,_
2hr h ‘(Mm i‘ ')J+1n(l *ﬂ(u‘ I’ ‘m 2m wu- w '1 I1u(1+t 1:11,)
Ii Ii
And therefore the gradient vector for 11m eeu be written as:
vii/[u I WM ‘U v w I H" n u A" .w'
> 2(1 1W,” 2h t >1
0); Note that the sigmuidal function (r i=- hijeetive, and its inverse function is:
r7 ' (u 11 IHR deﬁned by (7 ‘(:1 AWL} shine people call nil the logit funnﬁnn.

***************Ending Page***************

***************Beginning Page***************
***************page number:70**************
. . 10
Exempllﬁcatlon conﬂd
The gradient vector for the conditional log-likelihood functiun of the given 01mm 1§=
1
Tum-1 2 2111"’ “(w ,0“ ,01
1:\
I [1-0101, + 0110.10.00)’ + [RUM + 10+ 0111101010)’ i
[P0100 + 011 + w,)](1.0.1.0.1)lq(w.,+ W110. 0 0.0.111’
am. + w. + 1'2 i 01>(1.1.1_1.o)la1wu + w, + M +11;.)(1.1.0.1.1)K
(110104.10, + 1011.10.11. DRUM, + 011)(1.0.1.0.0>T
14111, + 010+ 14(“0 + 1111+ “13) + 1*:1(11-<, +102 + 11'4)*(1(l1‘u i 111.1)
ﬂaw + 01+ w; + 030.1100 + 0.1, + 01 + mam-a + 11-, + u‘4)’47(|m + “11).
l’0[w1>+1n)+1*0(1lln+ w, + union“, + 1111+ 01 + 010010.. + 0, + 011+ 1-1)
Z #100,) + 1m + 11.1.1]. .
1-100., +11», +11'4)’p/[111U +11. +117 A “Hymn-u + 1112),,
I “[1110 + 0'. + ugkawo + 11>. + 11:2 *(1'1)*0(u70 +101 +1111 + 011).
Paw, + 0'2 + 11:4,’(10110 + 11'4)’a(11'0 + m, +101 +1174)*r7(l1'u +1111+1114J
and the update rule [or the gradient ascent method is:
11; k 11- + W010”)-

***************Ending Page***************

***************Beginning Page***************
***************page number:71**************
71
Exempliﬁcation conﬂd
Lug'veroaimilitatea condiﬁonali Evoluua valorilor ponderilor u,

W as
'5 ,_ "“ + U
v U
in’ v w;
7.,5 ‘ M
w 4 W0

K) w 40 nu an m no M n m m in m ‘mu nu m.
WW 4, Mm Mum-um a: WW
Gruﬁcu rculizutc Llc m1. Alina DUCA (2023)

***************Ending Page***************

***************Beginning Page***************
***************page number:72**************
12
Exempliﬁcation conﬁd
La convergeméz
mu 07529. w‘ 0mm. In iumv. u» in 7571 m *1 1100
Pmucgii:
U HZQMZHZHA
1
Zu,r, - w” : 22m <0: ﬁEdible
H
V 1|’|2’|"11;’U
1
Zu/Ijim, A 50840) > ‘Edible
/:|
u 1\:.|!:1,H:H:0
1
Z u , I, v um I 0 02*» > 0 Q Edible
H

***************Ending Page***************

***************Beginning Page***************
***************page number:73**************
13
Comment

Using (he gradient vector v‘, m1) and the hmhm hmmx H”, one can apply

um Newinn-Raphsnn method (see University nf mah, 2005 fall, Hal Daumé

m_ Hw4, m. 1) m mmpum chh. npnmhl value nf u‘, Le. um nne fnr which

w; reaches its maximum.

Tho update rule [or the Newton-Ruphsun mothud is:

h b u‘ i 11;’ V.»

***************Ending Page***************

***************Beginning Page***************
***************page number:74**************
14
Exempliﬁcation mnm
The hessizm lnutrix currespunding Lu the conditional log-verosimility function
m“) can be computed using the following formula (see StanfcirEL zuus fall,
Andrew Ng, HWI, pi. 1n):
H“ I fiafu'urmﬂl i mi > W» NWT (18)
ﬁ,—/
‘d all‘
an the given dataset, the hessian matrix of Am is:
H" I *(Uhﬂy+A1'1)(1’f7('“7u'71!y))(l.l r>.u,nﬂ(1.1.n.n n)+
"(in + u-i + n‘;)(lir1(uiu i u‘! + “mu.1,iv.i.u)’(1.1.0,i.u)+
(1m, + u‘; +11!4)(1iﬂ(1|m~11‘9 + Mimi 0.1,0. i)'(1.n 1,011+
HM“! +U74)(l*f1(l|‘n*KA‘4)](1.U u.u.i)*(i.u.0.u1y+
nLim + H'] +11‘: ~1A!;](l*r7(wu + H‘\ + NU + nw))(1,1. 1.1 (Nu 1,1.1.n)+
"(My + n-i + n-i i uvn<14<w + wi + in + mi))[l.l.0. l. 1W1. 1.0.i. 1)+
(l[il‘(]+u']+|174)(1*!7(1|111‘11“+VI‘4)](1 1,vi,u,i)'(1.i 0,011+
H[M‘(]+U72)(1*I7(U‘U*KWInU-u 1.0.u)*(i.u.i.u 0y)

***************Ending Page***************

***************Beginning Page***************
***************page number:75**************
Exempliﬁcation c0nt’d
Lug'vennimilitatea condigiunula Evolugiu valorilor punderilor u,
vaﬂgema W hkehhwd m NW 3mm WW mm“,
Craﬁce realizate =12 Sm. Alina DUCA (2023)

***************Ending Page***************

***************Beginning Page***************
***************page number:76**************
1n
Exempliﬁcation conﬁd
La convergeméz
w 019's. In foam. n in 7471 m 41 141-2. u] *15108
Pmuqm
U “ZQMZHZHA
1
Zu,r, - w” : 221m <0: ﬁEdible
H
V 1|’|2’|"11;’U
1
gull/i“, 4:99:10) > vrldible
p.
u (‘ZUZLHZHZO
1
ZN , I, v um:0011>0:' Edible
H

***************Ending Page***************

***************Beginning Page***************
***************page number:77**************
v i 4 . v 77
Logatura cu fimmﬂ de cost‘ lﬂgwﬁlca

MM...“ mum). (in Mm Duchi, .ic ia mum-“mi, 5mm
c. Arﬁtaﬁ cs data in 1°C de w g {U i} vol-n considera y'i'! s (fl i) pentru
i=1. .m" atunci se pirate demonstra <5 urmétoarea fimqie de cast media,

1M‘) "2 lilnﬂJrexpki/Wm WU) (IU)
n v’! ‘

este dual" "Am, unde a“) este ruiicgia de log-verosimllilale condigmnalé

n
din Magi; (1:1). Ca o mnsecinjﬁ, a maximiza funcﬂia d2 log'verosimilitate
[W este echiwalent cu a minimize funcﬁa (‘1E cost marlin 1(0).
“mm my“: r) ll’ mu WWW/w 1)) sail, mai simplu, 19(2) "é' mu +ri)
este numité funcpa a! cast (53“, pierdere) laguatxcﬁ.
Atenﬁc! in acest context, Z ‘:' v'u: J. Mai inainte (Cami g e (0.1)) am iii/iii
: “2' w-L

~ mini“. .,' 'v : l .1)“;va : u ,i WI : l (iii/i i) 'v :1

***************Ending Page***************

***************Beginning Page***************
***************page number:78**************
vs
Observagie
Legétura dintre m1) mu + (4 '> 4
p0 11¢ o part0 51 (mangle!) junclm 3.5 Qggﬁfz'faun j
1011mm (sau funcﬁu sigmoidald) 3
H 1
"(1) " f + 4 i pe de alté part6 esm 25
mmawarea: 2
15
0(1) : mu fr’) I imm) ‘ ._.
Din accasth Baum. l'uncﬁa 11¢ cost 10- ",,95 "'
giatici ESLE numica unzori ‘i funngia M” "1 "'
log'sig'mmdalri. 'A '3 '2 4 v * 2 a 4
1

***************Ending Page***************

***************Beginning Page***************
***************page number:79**************
19
Solugic
Demonstrnﬁn 5e hazeazﬁ in esems p2 prnprietatea 1 I "(:1 I MI), (‘are Em
imediam
Cazul y“) I1 . HY I 1\,\ IN; Imp M") Immm-H”).
Cuzul W" I I1 = my’ I qu I 1"‘) I 1 I I’[Y I nx I 1W)I1I m: N")
I”( u 1W) IOU/"w w")
Pm. urn-lure,
mp) I I\\HP(Y I WM’ I M) I 21“ my I J/wx I A")
,I, I‘
I 21“ my’ I "Wm I M) I 21"”me M")
I, .I.
in‘; Iimu WWII/“w WU)
Vs’ l+vxp[*1/'("u w») p‘
1
A§adar, TH“) I 1m‘

***************Ending Page***************

***************Beginning Page***************
***************page number:80**************
80
Exempliﬁcation conﬁd

For the given damsel‘ the logistic cost is

1m‘? limwwwww W1)
H ril

: gum prpm, + m) Hm HAWM +1“ + u‘;]]+ln(1+\'xp[*(u‘u+ My + my, ,

1.1(1 + wpum + m! + ln(l +vwhm + In + u‘? +11‘;;Ah\(1 +\'X)»[H‘u+u’! +u~LAw|)+
1.,(1 +Pxp(u'u + w‘ + w,)+m<1+m,(m,+ m]

***************Ending Page***************

***************Beginning Page***************
***************page number:81**************
Pil
Logistic Regression:
the log-verosimility function is concave
Stanford, 2008 fall. Andrew Ag, HWl, pr. 1.3

***************Ending Page***************

***************Beginning Page***************
***************page number:82**************
82
cliniillen the lug»likulilluud function fur logistic regression:
an) : Z (y\"1liil.ir“>)+rl i l,'~'>)ln[l 441W»).

,el
when: i I (in. MT and/(1')‘?! "(n-n) I é

’ Hamill» i)’
Find me Hessian H of Lhi> function. and show that for any vcutur e I
<1» . zl)‘, it holds true that

z’ H : g u

Him; You migln want In start hy showing me fact min ZIEV:,l,LJ:, i
(Fl)? 5 0.
Remark: This is um: Ur the >Lundurll Wuya of showing unn me nlutrix H ii
negative semi'deﬁnite, written "H < u". This implies that r is concave, and
has no local rnaxilna lather than the global nne.

***************Ending Page***************

***************Beginning Page***************
***************page number:83**************
m
Note: we do things in a shorter WRY here: whis solmion does not use the Hint.
REE-411 um we have ml) : "mu "(11). and thus fur hm I m- 1) we have
% : mm , hm)”. This lam“ fact is very useful to make the following
11‘
deri‘vations.
Now
MW i a m W m m
im i (m g‘, 1mm ) (1 ‘1/ )hm Mr ))
H 11'} n FY’ < >
i g 11/ htr'“) (1 u )1 110%) M1 )(1 m ))
: Z [If [W ilﬂnvwﬂ’ WW; Ayﬂllvhfm'ﬂ
.4
i ZIM"‘*’Y(P“‘J)I1"
‘A
“d ,72“ ) m 1H “1) m
I i: ,;m: i m —i ,m mm
m iiuvdw Z‘ (m u Z. m, )(1 1m 1).r, u
'7 ‘i Ft!‘

***************Ending Page***************

***************Beginning Page***************
***************page number:84**************
m
sD we have for me Hessian matrix H (using that for x I i J if and only if
xv I 1,”);

U : *ZM'IWJU*/'("“‘J]1("'1“'T
H
To prove mm H is negative semi'deﬁnite, we show um 1TH; g u for all z:
J11: e iJ (anwu e mimnn'“ ("Y );
,ei
: Zn(i>'")(1 11(i"'))sl"/[1“'JT:
):i Hg
I meu li(..,<"))(s A")? <0.
,ei
with the last inequality holding. since u < MN) < 1, which implies Ivg:'~'>)(l i
mum) >0, and (2‘ W11 3 u.

***************Ending Page***************

***************Beginning Page***************
***************page number:85**************
a;
Logistic Regression is not alfacted by variable duplication
CMU, 2011 spring, Tom Mitchell, midterm, pr. 5.3

***************Ending Page***************

***************Beginning Page***************
***************page number:86**************
2m
Consider a binary cluasiﬁcutiun problem with variable X, r, (0.1) and label
Y 'v; (0,1).
We have a training set u, made ul' a examples: u, I (my), "(rial/"n.
Snppeae we generate anether training set D1 ul' a cxmnplcs, 01 I
((t;,r},v‘),,.. (quell/'9): where in each example r, and l/ are the sarne as
in nl and then r; is a rlnnlieate of ll.
Now we learn a logistic regression from v,‘ which should eentain two param-
ctcrs: an and a‘: we alse lcurn anetlrer legistie regreasien frunl 02, which
should have three parameters= m», “a and “7.
Flrat, write down the training rule (maximum eemlilienal likelihood estima-
tien) we use Lu estimate the parameters (ma-i) antl (1|!l!-ll'l,L/"J) l'rern data.
Then, given the training rule: what is the relationship between (m, m) and
(“Mm”) we estimated from Dl and 02? Use this fact ta argue whether
or nnt the lngisﬁc regression will suffer from having an additional rlnplirate
variable X1‘

***************Ending Page***************

***************Beginning Page***************
***************page number:87**************
m
Answer
The trnining rule for (mm) aims tn maximize the following Iog-Iikehnnd
function:
IHHPU" Xeru-i) ‘I! ZY'UFU 4 riixJ) ln(l l (txp[l4‘“ l ll.x{)).
l:l rel

Similarly the training rnle em (m5. in; mg) aims tn maximize

i. 17‘ i,‘
lllH1)(Yt‘4Y1,M‘L ti}. mg) ‘:’ Llﬂln', + ll ',.\'( + ichgi e lil(1 + “pm, + ll ',.\'( + mgxp)

rt ml

:Zl"<r»[, l in‘; l ll-g)X,'] lll(l l “pm, l (ll; l ilig)X{l)
,ei

which ie lineic-nlly thc eemc ne [thc lugrlikclihuud inncticn for deriving] the
training rnle ier (H1'.H“]y with the enhetitntinne lm : “>1, and ‘c, : m; + ‘cg.
Thehu enlntitnticne cxprcsle thc rcl-ntienehip hctwccn tlrc eete uf pncnmctcre
(my. ‘in ) and (mg. mg, mg) that we estimate frum the training eete Di enrl reepee
tively 17,.
Thererere, logistic regreeeien will eimply eplit the weight in inte ll’, l in; I in,
when facing the rlnplieeted variable X2 I x,.

***************Ending Page***************

***************Beginning Page***************
***************page number:88**************
PM
Kernelized Logistic Regression
using the gradient method
CMU, 2005 fall, Tom Mitchell, HW3, pr. 2.ab

***************Ending Page***************

***************Beginning Page***************
***************page number:89**************
89

Comment
The idm. behind the kPrnPl trirlc (nr, simnly, kemellzlltinn) nsed in classi-
ﬁeaiian (primarily iniradneeri for SVM) is m map e feeinre VeCtDr in low
dimensional spsee X inm a higher dimensional space 24 This can yield a
more ﬂexible elassiller while retaining computational simplicily.
In general, kemelizah'on invnlves ﬁnding a mapping n X e 2 such that
i. z has a higher dimension ihnn x;
ii. the computation in z only nses inner pruduct:
in; there is a functinn K called kernel such that ihe inner product of m(1,)
end 6K”) is li'l-l. U).
(K has to he positive deﬁniie, eg. eanssian kernel is one of sneh kernel; you
doni have w worry ii for ihis issue.)
NOLO llial a linear elnssilier in a higher dimensional space will [usually] cor-
respenrl to a nonslinear classiﬁer in the original spacer

***************Ending Page***************

***************Beginning Page***************
***************page number:90**************
90
The standard logistic regression has the following form:
my : HX) : 0m, +2 hex‘)
H
my I nix) :1 my : llxl.
(i1
where "(In l/(l +1 u),
Consider a l'llncLioll a) which maps [an instance] X from a 10w dinlensional
space x (dimensionality:li) into h high dimensiunal space 2 (dimensonality
is m, 7n > a). The lngistic regression, when "sing ihe mapping h, heeemes
(l
P(Y:l\o>(1\'1):ﬂ(uh l Zuwlxm <1“)
lei
Note that X is a zlsdimensional vector; our) is the currespondlng m’
dimensional femure veﬂnl': MA’), is the l-lh elemehi hf MA’).

***************Ending Page***************


***************Beginning Page***************
***************page number:91**************
01

a. Assulnc the weight “~er u: is the linear combinatiun of all input feature
vectur ow»); more formally,

(“*1 Harm)‘ : 20mm”), <21)

H

and “1, I m, where n is the number nr dam points and XW is um mq dam
point. Use the kernelization trick m compute 1-0 mun) (in order m avoid
explicitly computing in 2).
n. Write duwn LI“: gradient descent “pam- rulc for the kernel lugiatic regrow-
Sam.

***************Ending Page***************

***************Beginning Page***************
***************page number:92**************
92
Answer
a. Starting from the relationship (1U), we can write
PLY I HM» I arm” < Z Mm I "w" \ u »~(X>)
H
“1' ﬂu“! + [jwww Mi) , "(w + iH/wwm» mm)
1:‘ ,:\
I 0(0‘,+i‘<k,1\1,\" ,20]
F‘

***************Ending Page***************

***************Beginning Page***************
***************page number:93**************
93
Answer
b. We will use the well»known Lrick
P[Y:vl/‘Q(4‘(]] I 0(3)”[1 o(:))‘*‘/ for all”; (0,1).
where
:"2 in, l anqum X) (22)
lei
we will alsn use the following properties gr the logistic functinn:
l ll! ii * l
n(_) W W > l in“) T T >1|l(l “(ell *1ll(l+l4)
for any z e 1&4
The log'likelihoucl of a training instance (0(X),Y I .,) can be written as:
a I
lnP(Y I new» : vii-0(1) l (1 ,i/lhlu "(ma/hi1’ ’ ( (1 1/) lnl ‘ Z
- he n
Z U; egg»??? ln(l w‘) +W'ﬁ
: i/e lll[1 - ii). (2:1)

***************Ending Page***************

***************Beginning Page***************
***************page number:94**************
, 94
The log-likelihood of the training set {(xi“ WI)“ ..(x"‘>.yi"1>} as a function of 1i "i
(m, .. m.) i!
mi) I XYWM + XAVATXKHHXM»*1!|(1—l'xp[ﬂu+ Xu,1<(.\"1' x\'>)))
iii F1 n
The partial derivative uf mi) with respect tu m for i 2 1 i>
0111i) , Z v“! , “XP["II + 21L. In “1X”: {mn KW". X4“)

0n, H 1 prw, + £7,‘ (ijmwuxl'w)

and, similarly
amt) 2‘: y H, i "XPV'u +23; iijqui/h x'“);
am, m 1 + “mm, + 2;,‘ (nmxmxm)
: Z (W' i my” A anmx'ﬂ. XV»)
H H
The update rule for 1 z u is
my“, Z "in + "WW
um

***************Ending Page***************

***************Beginning Page***************
***************page number:95**************
(In Romanian)
c‘ Scrieﬁ regulu dz decin'z pentru o animus Mfume de test X din spagiul
a’, presupllnﬁnd c5 parametrii 1,, (pentru , : 1, ..m) nu fast dejn invsgagi.
Answer:
Conform relaﬁei [2“): model"! imam de regresia logisticé asociazi instanﬁei
X e'icheta Y i 1 darﬁ 5i “"in dami mm, + 2:51pm“) 3 1/2, (‘ens m ESIP,
echivalent cu m, + 2L, “won, 2 0, adirﬁ um + w 0m) 2 o.
Tinénd cont up, ralaﬁa [21y 5i de fantul c5 m, : "n, mm“; ‘mama inagalilam
ravine la 1m +2)", ,erW) 0(X] 5 u, mica
m, + 201 mxllhxu 3 u
M

***************Ending Page***************

***************Beginning Page***************
***************page number:96**************
m;
Linear Regression and Logistic Regression:
deﬁnitions [revisited],
conditional log-verosimility, gradients
and a common property
CMU, 2004 fall, T. Mitchel]. Z. Bar-Joseph, HWZ, pr. 4

***************Ending Page***************

***************Beginning Page***************
***************page number:97**************
Linear Regression and Logistic Regression: 97
Deﬁnitions [revisited]
Given an input vertor X‘ linenr regression [wixh gnuisian neiee] models n realrvalued output
i’ es
l'lx ~1VonnnlLiL[X).dz).
where “(Al : 9U : an + Qixi + ~ Hm.
Given an inpne vectur X, lngixtir rPgl-esxiun men-leis e binary nutput v liy
le ~ Bernoullqull
where Lhe Bernoulli parameter is related to a x by the logutvc / ugmindﬂl inneeien
VMX) : q(if.\')
where , L
‘if’ sis. iie iin
m) 1%; 1+er i tlelogsl fuxclo
or, equivalently, by the legit Lransfnrmation (which is the inverse of lhe legim'e / algvnmdnl
function):
ii, n MX) i i
inq:r[hg[X)] el FMX) en x

***************Ending Page***************

***************Beginning Page***************
***************page number:98**************
98
[hr che follnwing qnescaone consider the data ((cr. 5/1) (11.112). "(rho/"n.
o. For eoch omre two regression models deﬁned nbove, write che log-likelihood rnncuon
and its gradient wach respect to ure parameter vector 11 : (00.0,. .04)‘
Answer:
For imam‘ regression, we can write ure [conditional] log-likelihood rnncnnn he:
m) I 1“ H 1 mp rho/1m)!“
M vﬁo 201
e " I v wow)»:
i .21“ (KW GT
in lll(\/'ZTHT] r £ 20/, r N r ,>I
.0 r:\
e in Inuﬂo) r % 20h *17TI‘)T(l/I *17T1‘)
‘ﬂ w\
Therefore, its gradient is:
i l H T
mm) 7 gm ,,, 1,)‘,

***************Ending Page***************

***************Beginning Page***************
***************page number:99**************
99
m logistic Wg'mssian:
Imk'X) T H X 7n/(X) a‘ \ 0W
I ‘ : a X : » I 1 X 1 ‘
“1 mm c" 1 MX) a‘ M X H )
“Mame,
, Mn 1 |
MM : W : W and 1 imv) : W.
None um y \x ~ ulr1-~w(/m(l\')) moans chat
my , m) iwx) and my , om , 1 imxu.
which can be equivalently Mm". u>
my Z MK) :11H[X)"(1 Impnw' a“- .\|1 v < (0.1)

***************Ending Page***************

***************Beginning Page***************
***************page number:100**************
100,
So, in this case the [conditional] log'likelihood function is:
1(0) i In(HUMAN/‘(LiI:u(l,])‘7v‘)>
.7,
I Zimmmh»)+ll’v/,)ln(iihm»);
H
: 20W J, -m(1 n,,(r,1)-(1 M11110 lmll‘)])
,Z,
I {1w wilnuiﬂ'“ n
H
And therefore,
“MU/1:201, Q71) I 20/, WHOM

***************Ending Page***************

***************Beginning Page***************
***************page number:101**************
101.
b. Show that for each of the two regressirm models above, 9 ‘I’ 9mg has the following
property:
Z M! I Z me I 1,,” I i1“
I‘ I‘
Answer:
For linear mgrze'sio'n: Fur Iog'iﬂic rcg'resmun:
mun I I» I’ Z WM I 2w m, v 1w‘ I n I H , I u 1
H H H ) gun EMA)‘
Since WX ~ NWMIMX] "1)‘ Since HX I Bmmulmlh,/(X)),
E[Y\XIr,.#IA]I,,(1,)I§‘r,. (w,
E nx I “.14 I e] I My“) I i,”
50 gill/HIZIQHHXI 1,.5I911,‘ 1+‘ "
So XL, v”, 51;, umx .,.o All‘.

***************Ending Page***************

***************Beginning Page***************
***************page number:102**************
102,
Multi-class Logistic Regression
with L-Z regularization (softmaz regression)7
using the gradient ascent method
CIVIU, 2012 fall. Toll! IVIitchell, Ziv Bar-Joseph, HWZ, pr, 2
IVIIT. 2016 fall, R. Barzilay, S, Sra, Weekly Exorciscs, chk 4, pr. 5.1161

***************Ending Page***************

***************Beginning Page***************
***************page number:103**************
1m.
We esn easily extend the binary Legistie Regression model to handle nnrltie
elass classiﬁcation. Let's assume that we have Ii' different rlasses, and the
posterior probability for eless k is given by:
PWIHX: r] I Wi‘o rer k:l. ,K 1
1 > 2H weir, e)
1
Pry : IGX : r) I ,H
1+ 2,7‘ fXPUIU ' m)
where .i and rl for l e l.‘ .li are d-dimensiollal vectors‘ Notiee that we
ignored the eonrponents r,“ in order to simplify the expression.
oirr goal is to estimate the weights ur, nsing the gradient nsnent nptimizatinn
method‘ We will also deﬁne Friars on the parameters to avnid overlitting
and very large weights‘

***************Ending Page***************

***************Beginning Page***************
***************page number:104**************
1m,
a. Assume that you are given a n >< 1L training rnsirun where n is the number
of training uxunlplczs. und a is nu.- nulnbcr 0f umibuies / ilirnensiunr. Pleas‘:
explicitly write Lluwn nu.- lug-cunditiunul likelihood functiun, LUV‘. ,LLA')
with L; regularization on the weights. shew yuur sieps.
Hint: You (‘an simplify the multi-class logistic regression expression above by
introducing a ﬁxed parameter vecmr u-K 0 (the d-dimensional vector made
eniireiy of 0's).
bi Note that like fur the binary classiﬁcation cuse there is not a clmred form
solution w maximize the lug'conditional likelihood, L(iri., Hun‘), with re
sneer to wi. However, we enn still ﬁnd the snluiinn wiih the gmnh'rnc luaPnl
meihed, by using partial derivuiives, Derive the expressiun for the i-eh eern-
ponem in Mic vacuu- gradient 14w,“ -U‘I\] with respect to u-h which is the
partial derivative of L(wi. ..w|r J with respect Lu WA.
1.‘. Beginning with the initial weigms of u, write duwn the update rule fur AL",
using 1/ for the step size. Wiii ihe solution eunverge Lu a global maximum?

***************Ending Page***************

***************Beginning Page***************
***************page number:105**************
105,
Answer
a. Let lnzll be an indicator flinﬂion‘ where ‘UZH l if l/' k, otherwise
low I ll. Then we onn wrZLc the conditional likelihood as:
., n ,l n WMIPAW') in,“
L11‘,,,.ll‘]: Pl'IIA-lx':.l,.ln‘< l»: < ' ) .
< ‘ " EH11 < J HE Zoxplln, r’)
Taking ln=
.l n
1m. WA) 221M, (ml ‘LliiZoxlloi, A)
l:\ lZi ,
Adding the LI regularization term:
H n A n
_v"‘ M,’ a ,H‘vz
ml, ,IAK)’%%1H’H (ill l IULQqJQl, ll) 2% l-llll

***************Ending Page***************

***************Beginning Page***************
***************page number:106**************
106,
u. Taking derivative with respect to W
(a ” , lwpllll, 1' l r’
[ v , v‘ : 1 i, , A -,
all‘, (“' "') ;( "4 r gmpw. ll) "
Emlzq in)" M5) l’ i/Ww
H
r. Than the update rule with gradienm ascent for ll, is:
ll, H MMEUUZU i mv' i llY'J) l’ *t/All’,
H
Tm; will uunvurgl: m u global Inaxinnun since it is u cuncavl.‘ functiun.

***************Ending Page***************

***************Beginning Page***************
***************page number:107**************
lUT,
Softmax (multinomial logistic) Regression:
The relationship with mixtures of multivariate Gaussians
MIT, 2001 fall, Tnnnny Jaakkola, HWz, pr. 3.7
MITi, 2016 fall. Rt Barzilay, s‘ Sra. Weekly Exercises, week 4. pr, 5e

***************Ending Page***************

***************Beginning Page***************
***************page number:108**************
102s,
Dcmonstraﬁi (:5, pcntru oricc 1\' e N, 1; 2 2, modclul probabilist
invégat de regresia softmax este echivalent cu un anumit tip de
mixmrs de distribugii gallssiene lnulti-variate.
Vc§i speciﬁca / dctcrmina in mod riguros cc tip anumc dc mixmra
de distribuﬁi gaussiene multi-variate este echivalem. cu modelul
invﬁgat dc rcgrcsia softmax.

***************Ending Page***************

***************Beginning Page***************
***************page number:109**************
Solution 1w
Cnnxirh-r a softmax mnrlnl with K rla>snx and wniglﬂx w,‘ and dcnnh- 'H‘, ‘hr vl-
element vector with sompunsuts mi), : M for l g j g l1. Then the softmax
pmtnrinr psi-inability ix glvnrl by:
Hymn,
I'M»: i2 i
We would like like to ﬁnd Gaussian mixture parameters whit, mm A that would
yiuld um sun-in pustnriur. Tln- nustui-iur cluss pruhabilitics ur sun). u Guussiun mix-
Lure is given by:
mm ' "XP(’E(T’/lv) u, (Hm)
Pr(g\l,[7ri,p,:wl all i‘ it): f
m ,"' \xxl ,7 I,’ l, YE’ I’ lk
Biz i(2111‘(»>)
s in; 1 l s l - v l s l l s l
"ﬂit <‘XD( 5(r Lu I J Ll, w, uvLi t - All/Lu M)
‘I ii 7i s s, s s,
gmltih'wunﬂ3pm; iii»; Minx, 1+”, 2, ,i,))
Taking allvanlage uf covariance "lattice: being syl'nlnetriu:
, \
mm ‘WM 5o 23-» M2,‘: Maw)
Ple <n it :ilsi it) If
Erminls'Mi-XPQ Emit; i,’ 1,1,‘ 2f ‘My 2f m)

***************Ending Page***************

***************Beginning Page***************
***************page number:110**************
110.
Tue expunenl: ere quadratic runeliuue uf r. But in the eumnex pueleriur lire
exponents nre linear functions ol'r. in order tn get linear functions in the exponents‘
we knuw we ruuel enuuee identical covariance nuilrieee, unueiug lire quadratic lerrue
in cancel out‘ Setting ell cnvariance matrices Lu >: leads to:
rilylulnuu, :lu. El :21) i i)
, l 7 7 e
Meir 7 7 l *u 7‘ l *‘i l *‘i
7 mix p( 2( z 2m: +m: w)
x um "'"cXD( l(r‘ll ‘r 1M‘L"14M‘L"llr])
1 T 7i T 7|
1m 7772, i , ii,
i ,, i( 2< in my H1)
’ 1 r r r
2mm‘ 2( 2M,>*\1>~,>iwi>)
l
pr H“. ‘r77lw ‘l, lur
7 p () i 2A i l ,+ i)
7 l 7 f
Z, l-xp (/Jkr‘! 7 5,1, Lil“, + him)
This posterior looks very much like the softmax pusterior, with a linear functicm of
r in U11: ﬁxponcnte To get nir cncfﬁrirnts of): (n hr as in urn Kortmax pnitrrior, we
rnuel eel Llle mean: and uuvnrinueee euuli leL:
ur, r I: ‘,u fnr alli (21)

***************Ending Page***************

***************Beginning Page***************
***************page number:111**************
111,
We nun du this ior any invertible 2. nut it is enough cor us tu shuw thut there is
one set uf Gaussian mixture perameters that yields the desired posterior‘ and so
We will arbitrarily set > : l. Tn satisfy (zit We get that ll, must be equal to uh,
(for this sueeilie choice oitlre covariance rnatrixy This gives us a Gaussian mixture
posterior nf:
r l r
("ql(|lli,1 e Elli‘, my + luv")
I'l-(vllam 111:1lw >11 : 111:, It) : f
Llcxp(ll" r 5d», w, - 111W‘)
We would like to set the nriore r, such that the aﬂ'ine terms wuuld agree with u it
l
lt is tempting to set the Friars sueli that 1.. r, e Elli, ll‘, : um, whirh would yield the
softmax posterior. However‘ this might result in negative priurs, nr priors thet do
nut sum up tn one. Instead, wr ﬁrst mulﬁply hath the nnmr-rutnr and drnnminntar
by some eoustsnt z (to be determined). which will effectively normsllse the prim-s:
i e
Zw,,t“(p (“1,7, e Eur” is”)
mill (m.ll.:u>,.>:r:l).el A): 7 y Y
glam, (u, l e Em‘ n)

Now we eau set erm 5"‘ e r by setting m e A“ """l'l/L and in urdn to
nnrmalizu the priurs we get thut Z , Z‘ sill‘ l M.

***************Ending Page***************

***************Beginning Page***************
***************page number:112**************
112,
To summarize, we sew lluw i'ur eny sul'trnux weights, we L'im ulwuyi, ehuuse
the following parameters:
l
> i tn, (El-Tu, + um)
e i i
Z, twp (5% no e 0-10)

u, e w,

u, l
Note:
1n our derivation, not: thrit we nrhitrerily eheese z Z Ir hut in fuet we could
have setisﬁerl (21) using any invertible [LC: symmetrie hurl pesitive deﬁnite]
matrix z (regardless nf the weights of the snftrnax). [Lei However, in this
case the formula for 1, wnuld be different]
Chnnsing a rliirerent covariance rnetrix, we would get n diﬂerent Gaussian
mixture model‘ with diirereut ioiut distribution l’l(X.Y]_ hut with the same
posterior Plum. Note that the softmax model only speciﬁes a posteriou
and not e joint dietributiun.

***************Ending Page***************

