[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What is the certain event in a sample space Q?
--InteriorSeparator--
Q
--InteriorSeparator--
easy
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following statements are true regarding independent events A and B?
--InteriorSeparator--
(right) P(A ∩ B) = P(A)P(B)
(wrong) P(A ∪ B) = P(A)P(B)
(right) When P(B) ≠ 0, then P(A|B) = P(A)
(wrong) P(A|B) = P(B|A)
--InteriorSeparator--
medium
--InteriorSeparator--
7
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the formula for the cumulative distribution function F(x) of a random variable X?
--InteriorSeparator--
F(x) = P(X ≤ x)
--InteriorSeparator--
easy
--InteriorSeparator--
8
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How is the variance of a random variable X defined?
--InteriorSeparator--
Var(X) = E((X - E(X))^2)
--InteriorSeparator--
easy
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which parameters define the Bernoulli distribution?
--InteriorSeparator--
(right) p
(wrong) n
(wrong) sigma
(wrong) mu
--InteriorSeparator--
easy
--InteriorSeparator--
10
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
If X is a discrete random variable, what is the formula for the expectation (mean) of X?
--InteriorSeparator--
E(X) = Σx p(x)
--InteriorSeparator--
medium
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are properties of random variables?
--InteriorSeparator--
(right) E(aX) = aE(X)
(right) E(X + Y) = E(X) + E(Y)
(wrong) Var(X+a) = aVar(X)
(wrong) E(g(X)) = g(E(X))
--InteriorSeparator--
medium
--InteriorSeparator--
14
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does i.i.d. stand for in the context of random variables?
--InteriorSeparator--
Independent and identically distributed
--InteriorSeparator--
easy
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the main idea behind the Central Limit Theorem?
--InteriorSeparator--
The sum of a large number of random variables has a probability distribution that is approximately normal.
--InteriorSeparator--
medium
--InteriorSeparator--
18
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What does the weak law of large numbers state?
--InteriorSeparator--
(right) The average of a sequence of random variables converges to the expected average
(wrong) The sum of a sequence of random variables converges to the expected sum
(wrong) The product of a sequence of random variables converges to the expected product
(wrong)  The variance of a sequence of random variables converges to zero
--InteriorSeparator--
medium
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is Markov's inequality useful for?
--InteriorSeparator--
Providing an upper bound on the probability that a non-negative random variable exceeds a certain value
--InteriorSeparator--
medium
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does MLE stand for in the context of parameter estimation?
--InteriorSeparator--
Maximum Likelihood Estimation
--InteriorSeparator--
easy
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a serious drawback of maximum likelihood estimation?
--InteriorSeparator--
It gives poor results when data is scarce.
--InteriorSeparator--
medium
--InteriorSeparator--
30
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the main difference between MLE and MAP estimation?
--InteriorSeparator--
MAP estimation incorporates prior knowledge while MLE does not.
--InteriorSeparator--
medium
--InteriorSeparator--
30, 31
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the advantages of using posterior mean estimation?
--InteriorSeparator--
(wrong) It always provides the most accurate estimate
(right) It incorporates prior knowledge
(right) It can be more robust than maximum likelihood when data is scarce
(wrong) It avoids using Bayesian statistics
--InteriorSeparator--
hard
--InteriorSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define Entropy H(X) for a discrete random variable X.
--InteriorSeparator--
H(X) = - Σ p(x) log2 p(x)
--InteriorSeparator--
medium
--InteriorSeparator--
32
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is Mutual Information (or Information Gain) IG(X;Y) between two random variables X and Y?
--InteriorSeparator--
IG(X;Y) = H(X) - H(X|Y)
--InteriorSeparator--
medium
--InteriorSeparator--
32
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is Kullback-Leibler (KL) divergence?
--InteriorSeparator--
KL(p||q) = Σ p(x) log(p(x)/q(x))
--InteriorSeparator--
hard
--InteriorSeparator--
36
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are true for information gain IG(X;Y)?
--InteriorSeparator--
(wrong) IG(X; Y) ≤ 0
(right) IG(X; Y) ≥ 0
(right) IG(X; Y) = 0 if X and Y are independent
(wrong) IG(X; Y) = H(X) + H(Y)
--InteriorSeparator--
medium
--InteriorSeparator--
34
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Is relative entropy (Kullback-Leibler divergence) symmetric?
--InteriorSeparator--
No
--InteriorSeparator--
hard
--InteriorSeparator--
38
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the properties of the variance of information (VI)?
--InteriorSeparator--
(right) Non-negative
(right) Symmetric
(wrong) Always zero
(wrong)  Not applicable to continuous variables
--InteriorSeparator--
hard
--InteriorSeparator--
39
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following statements are true regarding cross-entropy CH(X,q)?
--InteriorSeparator--
(right) CH(X, q) ≥ 0
(right)  CH(X, q) ≥ H(X)
(wrong) CH(X,q) is always less than H(X)
(wrong)  CH(X,q) is symmetric in its arguments
--InteriorSeparator--
hard
--InteriorSeparator--
38
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Provide the formula for the joint probability P(A1 ∩ A2 ∩ ... ∩ An).
--InteriorSeparator--
P(A1)P(A2 | A1)P(A3 | A1,A2) ... P(An | A1,A2, ...,An-1)
--InteriorSeparator--
medium
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the total probability formula for P(A)?
--InteriorSeparator--
P(A) = P(A | B)P(B) + P(A | ¬B)P(¬B)
--InteriorSeparator--
medium
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
State Bayes' Theorem for P(B|A).
--InteriorSeparator--
P(B | A) = [P(A | B) P(B)] / P(A)
--InteriorSeparator--
medium
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define conditional probability P(A|B).
--InteriorSeparator--
P(A | B) = P(A ∩ B) / P(B)
--InteriorSeparator--
easy
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What conditions must be met for X to be considered a continuous random variable?
--InteriorSeparator--
(right) Val(X) is an unfinite non-countable set
(right) The cumulative distribution function of X is continuous
(wrong) Val(X) is finite
(wrong) X must take on integer values only
--InteriorSeparator--
medium
--InteriorSeparator--
11
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Give the probability density function for a standard normal distribution.
--InteriorSeparator--
N(x; 0, 1) = 1/(sqrt(2π)) * e^(-x^2/2)
--InteriorSeparator--
hard
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the parameters of the Normal (Gaussian) distribution?
--InteriorSeparator--
(right) μ (mean)
(right) σ (standard deviation)
(wrong) λ (rate)
(wrong) k (shape)
--InteriorSeparator--
easy
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
For a multi-variate Gaussian distribution, what does it mean if the covariance matrix is diagonal?
--InteriorSeparator--
(right) The marginal distributions are independent.
(wrong) The means of the marginal distributions are zero.
(wrong) The variances of the marginal distributions are equal.
(wrong)  The random variables are perfectly correlated
--InteriorSeparator--
hard
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
State Chebyshev's inequality.
--InteriorSeparator--
(right) P(|X - μ| ≥ a) ≤ σ^2 / a^2
(wrong) P(X ≥ a) ≤ E[X]/a
(wrong) P(X > μ + a) ≤ σ^2 / (σ^2 + a^2)
(wrong) P(X < -a) <= e^(-t a) M(t)
--InteriorSeparator--
hard
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
When can Binomial distributions be approximated by Normal distributions?
--InteriorSeparator--
For np such that np(1 - p) > 5
--InteriorSeparator--
medium
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the context of the categorical distribution, what is the formula for the maximum likelihood estimate (MLE) of the probability θi?
--InteriorSeparator--
θi-ML = ni / N
--InteriorSeparator--
medium
--InteriorSeparator--
28
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is the posterior mean estimation (PME) of parameters in the case of scarce data in parameter estimation?
--InteriorSeparator--
(right) θPME = (ni + αi) / (N + Σ αj)
(wrong) θPME = ni / N
(wrong) θPME = αi / Σ αj
(wrong)  θPME = ni/ Σ αj
--InteriorSeparator--
hard
--InteriorSeparator--
29
--FlashCardSeparator--
}], role=model}, finishReason=STOP, avgLogprobs=-0.15849252132320138}]