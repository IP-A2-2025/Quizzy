[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What is the neuron switching time in humans, according to the provided information?
--InteriorSeparator--
0.001 sec
--InteriorSeparator--
easy
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are design issues for neural network examples?
--InteriorSeparator--
(right) Input encoding
(right) Learning parameters
(wrong) Algorithm selection
(wrong) Data normalization technique
--InteriorSeparator--
medium
--InteriorSeparator--
7
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
According to the information provided, what kind of input is suitable for neural networks?
--InteriorSeparator--
High-dimensional discrete or real-valued data
--InteriorSeparator--
easy
--InteriorSeparator--
8
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the output of the perceptron if w ‚ãÖ x ‚â• 0?
--InteriorSeparator--
1
--InteriorSeparator--
easy
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to the information provided, what condition(s) are required for the Perceptron Training Rule to converge?
--InteriorSeparator--
(right) The training data is linearly separable
(right) Œ∑ is sufficiently small.
(wrong) The training data is noisy
(wrong) The training data is high-dimensional
--InteriorSeparator--
medium
--InteriorSeparator--
11
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What error is minimized by learning the weights (w) using a linear unit?
--InteriorSeparator--
Squared error
--InteriorSeparator--
easy
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the gradient descent rule, what does Œ∑ (eta) represent?
--InteriorSeparator--
Learning rate
--InteriorSeparator--
easy
--InteriorSeparator--
11
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a potential risk if the learning rate (Œ∑) is too large in gradient descent?
--InteriorSeparator--
Overstepping the minimum in the error surface
--InteriorSeparator--
medium
--InteriorSeparator--
16
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are practical difficulties in applying the gradient method?
--InteriorSeparator--
(right) Multiple local optima in the error surface
(right) Slow convergence to a local optimum
(wrong) Difficulty in differentiating the error
(wrong) Requirement for discrete hypothesis space
--InteriorSeparator--
hard
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What condition needs to be met for Incremental Gradient Descent to closely approximate Batch Gradient Descent?
--InteriorSeparator--
Œ∑ is small enough
--InteriorSeparator--
medium
--InteriorSeparator--
18
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What function is used in the Sigmoid Unit?
--InteriorSeparator--
Sigmoid function (œÉ(x) = 1 / (1 + e^(-x)))
--InteriorSeparator--
easy
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Why is the sigmoid function used in neural networks?
--InteriorSeparator--
(right) It is differentiable, allowing gradient descent.
(wrong) It always produces binary outputs (0 or 1).
(wrong) It is computationally inexpensive.
(wrong) It is always linearly separable
--InteriorSeparator--
hard
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is an alternative to gradient descent for finding hypothesis consistent with separable data?
--InteriorSeparator--
Linear programming
--InteriorSeparator--
medium
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the typical structure of the neural network for which the Backpropagation algorithm is formulated?
--InteriorSeparator--
Feed-forward 2-layer network of sigmoid units
--InteriorSeparator--
easy
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
In the Backpropagation algorithm, what does Œ¥k represent for an output unit k?
--InteriorSeparator--
(right) Error term for output unit k
(wrong) Input to unit k
(wrong) Weight from hidden layer to output unit k
(wrong) Target value for output unit k
--InteriorSeparator--
hard
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
During Backpropagation, what is used to calculate the update to the network weights?
--InteriorSeparator--
(right) Learning rate (Œ∑)
(right) Error signal (Œ¥)
(wrong) Activation function
(wrong) Momentum
--InteriorSeparator--
hard
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a characteristic of the initial decision surfaces in Backpropagation, due to the weights being initialized near zero?
--InteriorSeparator--
Near-linear
--InteriorSeparator--
medium
--InteriorSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What effect does weight momentum have during neural network training?
--InteriorSeparator--
Speed up convergence and help escape local minima.
--InteriorSeparator--
medium
--InteriorSeparator--
30
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a strategy for determining when to stop training an ANN to prevent overfitting?
--InteriorSeparator--
Monitor error on a validation set
--InteriorSeparator--
medium
--InteriorSeparator--
31
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What function was the network in the example on page 32 trained to learn?
--InteriorSeparator--
Identity function f(x) = x
--InteriorSeparator--
easy
--InteriorSeparator--
32
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following techniques can be used as alternative error functions when training ANNs?
--InteriorSeparator--
(right) Penalize large weights
(right) Train on target slopes
(wrong) Maximize data entropy
(wrong) Increase the learning rate
--InteriorSeparator--
hard
--InteriorSeparator--
37
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does ùëî(ùë•) represent when predicting a probability function using a NN?
--InteriorSeparator--
P(f(x) = 1)
--InteriorSeparator--
hard
--InteriorSeparator--
38
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the core components of recurrent networks?
--InteriorSeparator--
(right) Applied to time series data
(right) Trained using a version of the Backpropagation algorithm
(wrong) Only uses feedforward connections
(wrong) Typically applied for image data
--InteriorSeparator--
medium
--InteriorSeparator--
41
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the core idea behind dynamically modifying network structure?
--InteriorSeparator--
Begin with a simple or complex network and either grow or prune it based on performance.
--InteriorSeparator--
medium
--InteriorSeparator--
42
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Name an alternative optimization method for training ANNs.
--InteriorSeparator--
(right) Linear search
(right) Conjugate gradient
(wrong) Stochastic descent
(wrong) Incremental programming
--InteriorSeparator--
hard
--InteriorSeparator--
43
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is discussed in Chapter 6 regarding ANNs?
--InteriorSeparator--
A Bayesian justification for minimizing the sum of square errors.
--InteriorSeparator--
hard
--InteriorSeparator--
44
--FlashCardSeparator--

}], role=model}, finishReason=STOP, avgLogprobs=-0.19688810785132718}]