[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What is the primary purpose of Decision Tree Learning?
--InteriorSeparator--
To learn a decision tree from data that predicts the value of a target variable based on input attributes.
--InteriorSeparator--
easy
--InteriorSeparator--
1
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are considerations when using decision trees?
--InteriorSeparator--
(right) Instances are described by attribute-value pairs
(right) Target function is discrete valued
(wrong) Target function is continuous valued
(wrong) Instances are described by mathematical equations
--InteriorSeparator--
medium
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In a decision tree, what does each internal node represent?
--InteriorSeparator--
A test on an attribute.
--InteriorSeparator--
easy
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What does the ID3 algorithm do in its main loop?
--InteriorSeparator--
(right) Selects the "best" decision attribute for the next node
(right) Sorts training examples to leaf nodes
(wrong) Prunes the tree
(wrong) Randomly selects an attribute
--InteriorSeparator--
medium
--InteriorSeparator--
8
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
According to the basic version of the ID3 algorithm, if the attribute list is empty, what label is given to the single-node tree?
--InteriorSeparator--
The most common value of the target attribute in the examples.
--InteriorSeparator--
medium
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is the inductive bias in ID3?
--InteriorSeparator--
(right) Preference for shorter trees
(right) Preference for trees with high information gain attributes near the root
(wrong) Preference for longer trees
(wrong) No bias
--InteriorSeparator--
medium
--InteriorSeparator--
10
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is information gain?
--InteriorSeparator--
The expected reduction in entropy due to sorting on an attribute.
--InteriorSeparator--
medium
--InteriorSeparator--
11
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How is entropy defined in the context of decision tree learning?
--InteriorSeparator--
A measure of the impurity of a set of training examples.
--InteriorSeparator--
easy
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which attribute would ID3 select for the root node?
--InteriorSeparator--
(right) The one with the highest information gain
(wrong) The first attribute in the list
(wrong) The one with the lowest entropy
(wrong) A randomly selected attribute
--InteriorSeparator--
medium
--InteriorSeparator--
14
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is Occam's Razor in the context of decision trees?
--InteriorSeparator--
Prefer the shortest hypothesis that fits the data.
--InteriorSeparator--
medium
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the time complexity of decision tree induction, assuming depth is O(log m)?
--InteriorSeparator--
O(d * m * log m), where d is the number of attributes and m is the number of training instances.
--InteriorSeparator--
hard
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are other impurity measures besides entropy?
--InteriorSeparator--
(right) Gini Impurity
(right) Misclassification Impurity
(wrong) Variance
(wrong) Standard Deviation
--InteriorSeparator--
medium
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How can continuous-valued attributes be handled in decision tree learning?
--InteriorSeparator--
By creating one or more discrete attributes to test the continuous attribute, using threshold values.
--InteriorSeparator--
medium
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What problem arises when an attribute has many values, and how is it addressed?
--InteriorSeparator--
Gain will select it, so use GainRatio instead.
--InteriorSeparator--
hard
--FlashCardSeparator--
22
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How are attributes with different costs handled in decision tree learning?
--InteriorSeparator--
Replace gain by a cost-sensitive measure, like Gain/Cost or (Gain - Cost).
--InteriorSeparator--
hard
--FlashCardSeparator--
23
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
How are training examples with unknown attribute values handled?
--InteriorSeparator--
(right) Assign the most common value of the attribute.
(right) Assign probability to each possible value and split the example accordingly
(wrong) Ignore the example
(wrong) Replace the missing value with zero
--InteriorSeparator--
medium
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define overfitting in the context of decision tree learning.
--InteriorSeparator--
When a hypothesis performs better on the training data than on the entire distribution of data, and there exists another hypothesis that performs better on the entire distribution.
--InteriorSeparator--
hard
--FlashCardSeparator--
26
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
How can overfitting be avoided in decision trees?
--InteriorSeparator--
(right) Stop growing when the data split is not statistically significant
(right) Grow a full tree and then post-prune
(wrong) Always using a shallow tree
(wrong) By never using attributes more than once
--InteriorSeparator--
medium
--InteriorSeparator--
28
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Describe the process of reduced-error pruning.
--InteriorSeparator--
Split data, evaluate the impact of pruning nodes on the validation set, and greedily remove the node that most improves validation set accuracy.
--InteriorSeparator--
hard
--FlashCardSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Describe the process of rule post-pruning.
--InteriorSeparator--
Convert the tree to rules, prune each rule independently, and then sort the rules based on estimated accuracy.
--InteriorSeparator--
hard
--FlashCardSeparator--
31
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are ensemble learning methods using decision trees?
--InteriorSeparator--
(right) Boosting
(right) Bagging
(wrong) Stacking
(wrong) Regression
--InteriorSeparator--
medium
--InteriorSeparator--
32
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the AdaBoost algorithm, how are the weights of incorrectly classified instances adjusted?
--InteriorSeparator--
Their weights are increased, forcing the learner to concentrate on more difficult cases.
--InteriorSeparator--
hard
--FlashCardSeparator--
32
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How does the Bagging algorithm construct new trees?
--InteriorSeparator--
Each tree is independently constructed using a bootstrap sample of the data set.
--InteriorSeparator--
medium
--FlashCardSeparator--
32
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the final classification in Bagging based on?
--InteriorSeparator--
Simple majority voting.
--InteriorSeparator--
easy
--FlashCardSeparator--
32
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the main difference between Random Forests and Bagging?
--InteriorSeparator--
Random Forests adds an additional layer of randomness through random feature selection at each node.
--InteriorSeparator--
hard
--FlashCardSeparator--
35
}], role=model}, finishReason=STOP, avgLogprobs=-0.13123239717720303}]