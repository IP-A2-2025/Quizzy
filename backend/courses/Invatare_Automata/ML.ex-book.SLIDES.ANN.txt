***************Beginning Page***************
***************page number:1**************
1.
Artlﬁolal Neural Networks
(abbrev. ANNs, or simply NNs)
Contains:
ex. 1, 5, 31, 7, 9, 17, 18, 36, 14, 20, 21, 22, 15, 26, 19
from the 2023f version of the ML exercise book by L. Ciortuz et a1.

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2.
Calculation of the output of a neural network
made of threshold perceptrons
CMU, 2010 fall, Aarti Singh, HW5, pr. 4.1.1

***************Ending Page***************


***************Beginning Page***************
***************page number:3**************
3.
Consider-51m reteaua neuronalé din ﬁgura
alfituratéi. Toate unitiigile acestei regele neu- xo=1 -1 a
ronale sunt de tip prag, adicé folosesc pentru v
activare funcgia sign deﬁnitéi prin sign(z) : 1 1
dacé z Z 0 §i —1 in rest. Pentru unitatea de pe _1 1
nivelul de iegire (lfisaté nenumerotaté), pon- 1
derea corespunzéitoare termenului liber (11:0 : x10‘ a
1) este 0. \, _1
a. Scriegi funcgia matematicé calculatéi de _1'—>o
ﬁecare dintre unitéiii'ile regelei, in raport cu ‘
intrérile x1 §i x2. Vetgi nota cu 01- (unde 2' I x2. a
1, . . . ,4) ie§irile unitiitilor de pe nivelul ascuns v
§i cu 0 iegirea retelei, adicii valoarea produséi _1 1
de ciitre unitatea de pe nivelul de ie§ire. A
b. Calculati output-ul reigelei atunci cénd xo=1 _1 ‘a
intriirile $1 §i x2 iau valori in multimea {-1, 1}.

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
4.
Riispuns
a.
01(371, $2) I sign(x1 + 51:2 — 1),
02(331, $2) I 529M951 — $2 — 1),
03(:131,;E2) I sign(—x1 + $2 — 1),
04(:131,x2) I sign(—:v1 — x2 — 1),
0(1131, $2) I 5Z977J<01<$17 332) — 02(55171132) — 03(951, $2) + 0433175132»-
b.
“III-II!
-----I
------I
-----I
------|

***************Ending Page***************


***************Beginning Page***************
***************page number:5**************
5.
c. Indicagi func§iile booleene reprezentate de céitre uitii§ile de pe
nivelul ascuns (1, 2, 3 §i 4) atunci cénd $1,332 € {—1, 1}. Proceda§i
similar pentru ie§irea 0.
Riispuns:
Din tabelul 0b§inut 1a punctul precedent este imediat c5, atunci
cénd 51:1, x2 6 {—1,1}, ie§irile calculate de ciitre unitiigile 1, 2, 3 §i 4
corespund conceptelor/funcgiilor 51:1 Aacg, $1/\—|ZE2, _IZ171/\ZC2 §i respectiv
—IZIZ1 /\ 11:2. Ie§irea re§elei, 0, corespunde conceptului —\(:131 XOR x2).
Observatie: Se poate remarca faptul c5 in aceasté retea un neu-
ron (§i dear unul!) de pe nivelul ascuns este activat la ﬁecare
combinatie de valori (din cele 4 posibile) pentru 3:1, x2 G {—1,1}.

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
6.
d. Speciﬁca§i cum anume ar putea ﬁ modiﬁcatii regeaua datéi
astfel incét noua varianté sii calculeze func§ia de variabile reale
(nu booleene ca mai inainte!) x1 §i x2, a ciirei relagie de deﬁnigie
este: f(x1,x2) I 1 dacé $1,332 Z O sau $1,552 < O, §i —1 in caz contrar.
Réspuns:
Este imediat (:5 funcgia indicatﬁ in enuni; se poate scrie sub forma
1 dacéi sign(:131) - sign(a:2) Z O
f(ZU1, 5132) I v . .

—1 daca szgn(:131) - szgn(a:2) < O.
Se observé imediat c5 aceasta coincide cu funcgia
ﬁ(sz'gn(x1) XOR sign(x2)). Prin urmare, este suﬁcient sii adéugiim
la regeaua datéi in enun§ un nivel ascuns suplimentar, format din
douﬁi unitéiti cu func§ie de activare de tip prag, care sé transforme
intriirile x1 §i 11:2 in sign(x1) §i respectiv sign(352). Vorn 0b§ine ca
rezultat relgeaua din slide-ul urméitor.

***************Ending Page***************


***************Beginning Page***************
***************page number:7**************
7.
xo=1 _1
1 1
-1 1
01
x1 1 1
0 -1 —1
—>O
0 1 -1
x2
1 02
—1
1
_1 1
x0=1 _1

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
8.
Expressivity 0f neural networks:
boolean functions
CMU, 2010 fall, Aarti Singh, HW5, pr. 4.3

***************Ending Page***************


***************Beginning Page***************
***************page number:9**************
9.
Consider the set of binary functions over d Boolean (binary) vari-
ables: F : {f : {0,1}d e {0, 1}}. Show that any function in F can
be represented by a neural network with a single hidden layer.
Note: You can work with values other than {0, 1}, such as {1, —1},
as long as the function is still binary.

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
10.
Solution
Let S denote the set of all possible d-bit binary vectors. For any function
f € F, deﬁne Tf I {b € {O,1}d\f(b) I 1}. We construct a neural network with
one hidden layer to represent f as follows:
The neural network has |Tf| hidden units, each of which corresponds to a
distinct binary vector in Tf. The weights from the input layer to the i-th
hidden unit are determined by the values of the corresponding binary vector
bi: the j-th weight is 1 if bij is 1 and —1 otherwise. The activation functions
in the hidden units are threshold functions:
1 ifv > 24 b--
h. U I — 3:1 Z]?
Z( ) { 0 otherwise.
It is easy to see that h2- outputs 1 if and only if the input is bi. The weights
from all hidden units to the output layer are 1, and the activation function in
the output unit is the identity function.
To show that the above neural network represents f, we ﬁrst note that for
every b in Tf exactly one hidden unit, the one corresponding to b, outputs
1, and therefore the neural net outputs 1. For any b Q Tf, all hidden units
output 0, and the neural network outputs 0. This completes of the proof.

***************Ending Page***************


***************Beginning Page***************
***************page number:11**************
11.
Exempliﬁcation:
CMU, 2011 spring, Roni Rosenfeld, HW4, pr. 1.c
A neural network with only one hidden layer that
implements (A \/ ﬁB) XOR (ﬁC \/ D).

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
12.
Solution (in Romanian)
Expliciténd deﬁnigia funcigiei logice XOR §i apoi aplicénd regulile lui DeMorgan,
expresia booleanéi din enun§ devine:
(A v ﬁB) XOR (ﬁC \/ D) I [(A v ﬁB) /\ ﬁbc \/ D)] \/ HA \/ ﬁB) /\ (ﬁC v D)]

I [(A \/ ﬁB) /\ (C /\ ﬁD)] \/ [(ﬁA /\ B) /\ (ﬁC \/ D)]
intrucét operatorii logici \/ §i /\ sunt distributivi unul faté de celéilalt, rezulté
05:

(AvﬁB) XOR (ﬁCVD) : (AACAﬁD)\/(ﬁBACAﬁDM/(—IAABAﬁCM/(ﬁAABAD)
Fiecare din cele patru paranteze din partea dreapté a egalitétii de mai sus
poate ﬁ reprezentatéi cu ajutorul unui perceptron-prag cu patru intréiri, §i
anume céte o intrare pentru ﬁecare din cele trei variabile booleene din paran-
tezé, plus o intrare pentru termenul liber.

De ememplu, conjunctiei A/\ C /\ ﬁD ii putem asocia inegalitatea x-l- z — t > 5/2,
deci ponderile intrérilor perceptronului corespunzétor vor ﬁ —5 / 2, 1, 1 §i —1.)
Cei patru perceptroni vor ﬁ plasa§i pe primul nivel ascuns a1 regelei pe care
o construim.

***************Ending Page***************


***************Beginning Page***************
***************page number:13**************
13.

1 -5/2
Apoi, ie§irile acestor patru G 1
perceptroni vor consti-
tui intriiri pentru un alt -1
perceptron-prag, care s5 1 w 1
reprezinte disjuncigia a patru 61"’ 7/2
variabile booleene. (Acestui 1 \ )’
perceptron, situat pe nivelul k9‘
de ie§ire, ii putem asocia, e ‘a
de exemplu, inegalitatea ‘ 1
at+y+z+t>—7/2.)
Reteaua neuronalé rezultatéi G
este cea din ﬁgura aliituratii. 1

-5/2
1

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
14.
A negative expressivity result
concerning networks of threshold perceptrons
CMU, 2010 fall, Aarti Singh, HW5, pr. 4.1.2

***************Ending Page***************


***************Beginning Page***************
***************page number:15**************
15.

Consider the function f : R2 e R deﬁned as f(a:1,x2) I 1 for $1,562 Z 0 or
5131,1132 < 0, and —1 otherwise. It was shwown (see CMU, 2010 fall, Aarti Singh,
HW5, pr. 4.1.1) that it can be represented using a network of threshold
perceptrons with two hidden layers.
Show that no neural network made of threshold units, with only one hidden
layer, can represent f. For simplicity, you shall assume that the number of
hidden units in a hidden layer is ﬁnite, but it can be arbitrarily large.

Hint
When there is only one hidden layer, the separating border corresponding to
each hidden unit is a line in the 2-d plane such that the hidden unit outputs
+1 on one side of the line and —1 on the other.
Consider a circular neighborhood (V) of the origin such that if a separating
line determined by a hidden unit that crosses the neighborhood V, then it
passes through the origin. (Since the number of hidden units is ﬁnite, we can
always ﬁnd such a neighborhood.)
Can any neural network made only of threshold units, with one hidden layer,
represent f in the neighborhood V?

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
16.
Solution
Given any neural network with one hidden layer of ﬁnite hidden units, we
will consider a circular neighborhood of the origin
N8 :{(x1,:c2)\\/a3%+$g g a},
with a being [less then] the minimum of the distances from the origin to the
separating lines (hidden units) which do not pass through the origin.
We will consider two cases:
x 2
Case 1: No line crosses the neighborhood. In
this case, the neural network must output the
same value in this neighborhood, but f outputs x1
1 or —1. Therefore, neural networks in this case
cannot represent f. GD

***************Ending Page***************


***************Beginning Page***************
***************page number:17**************
Case 2 17.
Some lines cross the neighborhood. By construction, x2
these lines must all pass through the origin. Denote
the set of the hidden units corresponding to these lines GD
as O. Let us consider a point P I (191,192) situated in
this neighborhood and in the ﬁrst quadrant, that does
not fall on any line (see the ﬁgure). We can always ﬁnd x
such a point because the number of the hidden units, V 1
hence the number of lines, is ﬁnite. By symmetry, the
point P’ : (-p1,—p2), also in the neighborhood, does
not fall on any line either, and f(P) I f(P’) I 1. Let GD
yh(P) € {1, —1} denote the output of the hidden unit h
on P.
It is easy to see the following:
yh(P) I —yh(P’) Vh E O, and yh(P) I yh(P’) Vh g2 O.
Let wh denote the weight from the hidden unit h to the output unit. We thus have
thinP) I Z wwMP) + Z Whyﬂp) I — Z whyhUD') + Z whyhUD')
h hEO thO hEO thO
I — thyh(P’) + 2 Z whyh(P/)- (1)
h hQO

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
18.
Similarly, we can pick Q and Q’ I —Q in the fourth and the second quadrantal
parts of the neighborhood, and have that
Z whyh(Q) I — thleQ’) + 2 Z whyh(Q/)- (2)
h h h§ZO
Since f(P) I f(P’) I 1 and f(Q) I f(Q’) I —1, in order to represent f, the
free weight wO (corresponding to x0 I 1) for the output unit of the neural net
must satisfy the inequalities
1
thyh(P) 2 —w0 7 thyhUD’) Z —w0 (IQ Z whyh(P') Z —wo,
h h hgo
2
thyh(Q) < —w0 7 thyh(Q/) < —w0 (IQ Z whyh(Ql) < —w0,
h h hQO
which cannot happen because 2W0 whyh(P’) : 2W0 whyh(Q’), since yh(P’) :
yh(Q’) for every h g2 O. This completes the proof.

***************Ending Page***************


***************Beginning Page***************
***************page number:19**************
19.
Expressivity of neural networks:
Any Lipschitz continuous function
deﬁned on a bounded interval from R
can be approximated by a neural network
with a single hidden layer
CMU, 2011 fall, Torn Mitchell, Aarti Singh, HW5, pr. 2.3

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
20.
Let f (x) be any function whose domain is [(1, D), for real values
C < D. Suppose that the function is Lipschitz continuous, that is,
for some constant L Z 0.
Construct a neural network with one hidden layer that approxi-
mates this function within e > O, that is, V55 G [Q D), |f(x)—out(:v)\ g
e, where 0ut($) is the output of your neural network given the in-
put x.
Hint: You may use as building blocks the network(s) designed at
CMU, 2007 fall, Carlos Guestrin, HWZ, pr. 4.
Note: Your network should use only the activation functions g;
and gs (the identity and respectively the step function) mentioned
at above referred problem.

***************Ending Page***************


***************Beginning Page***************
***************page number:21**************
21.

You need to specify the number K of hidden units, the activation
function for each unit, and a formula for calculating each weight
w0,wk,wék), and wilt), for each k E {1,2, . . .,K}. These weights may

be speciﬁed in terms of C, D, L and e, as well as the values of f(x)
evaluated at a ﬁnite number of a: values of your choosing (you need

to explicitly specify which a: values you use). You do not need to
explicitly write the 0ut(a:) function.

Why does your network attain the given accuracy?

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
22.
Rﬁspuns
Este imediat 05 din ipoteza \f(a:) —f(x’)| g L|x—a:’\, in cazul in care |x—a:’\ g é,
rezulté
|f($) — f(113')| S 6 (3)
A§adar, este de dorit s5 ,,ac0perim“ intervalul [C, D) cu intervale de lungime
2
—€ (sau mai micé):
L
25 25 25 25
CC — C —C 2—...C K—1—D

§i s5 luéim punctele :13’ de forma @- : (04¢ —|— ﬂ¢)/2, pentru cu i I 1, . . . , K, unde

K :t D — 0 —

[< >261,
25 25

041:0, 61:042IC+f,...,6K_1:CYKIC+(K—1)Z, BKID.

A§adar, prin 51' am notat mijlocul intervalului 2' de mai sus, [oz/i, 51-).

***************Ending Page***************


***************Beginning Page***************
***************page number:23**************
23.
in continuare, vom aréita c5 putem construi 0 retea neuronaléi cu un singur
nivel ascuns §i care folosegte doar unitiiti de tip liniar sau de tip prag, astfel
incét

v . d .

pentru Va: € [C,D), daca x € [04,360, atunc1 0ut(:c) if f(§Z-). (4)
in consecinté, dacéi in relatia (3) vom inlocui x’ cu £1, va rezulta imediat c5
|f(1v) — out(w)| é 8-
Revenind acum la proprietatea (4), vom aréita 05 existii 0 retea avénd K
unitéti pe nivelul ascuns (§i 0 singuréi unitate de ie§ire), astfel incét, dacé
x € [cm-,5?)7 atunci unitatea 2' de pe nivelul ascuns se activeazéi (producénd
valoarea f (5i), iar toate celelalte unitéiti de pe nivelul ascuns réimén neactivate
(adicéi produc output O). Prin urmare, rezultatul va ﬁ exact cel dorit.

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
24.
Conform problemei CMU, 2007 fall, Carlos Guestrin, HW2, pr. 4.b, existé o
retea neuronaléi care produce valoarea c I f (£1) pentru orice input x € [0%, 57;)
§i O in rest, §i care folosegte funcgii de activare g5 pe nivelul ascuns §i g; pe
nivelul de ie§irez
Q y E a
x = 1 : .
0 f(§ i) , c 17%,) ,
a @ @
x a I '
1 0 (Xi I31‘ x

***************Ending Page***************


***************Beginning Page***************
***************page number:25**************
25.
_a1
1 ‘Q
x 9
"Q
_[31
,,Ansambl€1nd“ K astfel de re§ele, vom n 1
obtine reigeaua din ﬁgura aléturatéi. -0¢2 a
p
Output-ul acestei regele este exact cel x1 a 1
dorit (deci satisface condigia din GHUHL'): 1 ig
x0 = 1 —[32 a o
f(§1) Pt- w Q [041 I C751) 5 E
f(€K) Pt- 9L‘ € [04K I 5K_1,5K I D) 5 1
:> |f($) _ Guam g 5, v1,- e [0, D). “if ,Q
x 9
"Q
_[3K
level 1 level 2 level 3

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
26.

g f( $1)

a 1732)
Neajunsul este c5 reteaua aceasta are doué niveluri as- " -
cunse, in 10c de unul singur, a§a cum se cere in enung. '
Totugi, e1 poate ﬁ ,,corectat“ imediat, comasénd a
nivelurile 2 §i 3 — formate doar din unitégi liniare — f
intr-o singuréi unitate liniaré (care va constitui nivelul I g
de ie§ire a1 noii reQeIe), ca in ﬁgura aléituratéi. __ _ ﬂ K")

a —f(EK_1)

Q ﬁg”

a 4%,‘)

***************Ending Page***************


***************Beginning Page***************
***************page number:27**************
27.
—(x1 a f( g 1)
De asemenea, datoritéi faptului 05 31 : —a2 f( E 2)-f( E 1 )
042,52 I 043, . . . ,5K_1 : 04K, nivelul ascuns 4
(a1 acestei noi retele), format din cele 2K 1 I
unitéiti de tip prag, poate ﬁ echivalat cu xo=1 x4251“ f a 0
un altul, compus din doar K + 1 unitéti 1 ' f(§ ﬁg )
de tip prag. La ﬁnal, obtinem reteaua _O‘K\ ~. a K) K4
din ﬁgura aléturatﬁ.
a -f(€K)
_[5K
levelI level 2

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
28.
The sigmoidal / logistic perceptron:
Exempliﬁcation: setting up its weights so as to represent
a given boolean expression
CMU, 2003 fall, T. Mitchell, A. Moore, midterm, pr. 6.21

***************Ending Page***************


***************Beginning Page***************
***************page number:29**************
29.
XI . w] x0=1
x2 . w2 W0
: 9 n Q9 <>
. W” "etzgowixi 0=o(net)=ﬁ
xn .
Consider a single sigmoid threshold unit with three inputs, x1, x2,
and x3.
1
y I 0(w0 + w1$1 + 102952 + w3x3) where 0(2) I T.
e—Z
We input values of either 0 or 1 for each of these inputs. Assign
values to weights wo, wl, w2 and wg so that the output of the
sigmoid unit is greater than 0.5 if an only if (x1 AND x2) OR x3 is
true.

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
30.
Solution (in Romanian)
Vorn scrie mai intéi tabela de valori a funcigiei / expresiei booleene date. Vom
adéuga in aceasté tabelii o coloané care va conigine forma [particularii a] sumei
ponderate wO +w1x1 +w2x2 +w3333 — adicé, argumentul pe care-l va lua funcgia
sigmoidalé (0) — pentru ﬁecare combinatie de valori ale variabilelor 3:1, 3:2, x3.
in plus, la ﬁecare linie vom preciza §i condijia care trebuie séi ﬁe satisféicuté
de céitre suma ponderatéi de pe linia respectivéi, in aga fel incét perceptronul
$5 realizeze codiﬁcarea ceruté in enunig.
A§adar, rezuménd,
O O O O wo < O tatea sigmoidalé $5
O O 1 1 wo _|_ w?) > 0 codiﬁce expresia
O 1 O O wo _|_ w2 < 0 booleané datﬁ, tre-
(5) buie ca ponderile wi
1 0 O 0 wo _|_ wl < 0 s5 satisfacéi sistemul
1 0 1 1 wO + wl + w?) > O de inecuafgii scrise
1 1 0 1 we + wl _|_ w2 > 0 in ultima coloané a
1 1 1 1 wo _|_ U11 + w2 _|_ wg > O tabelului aléiturat.

***************Ending Page***************


***************Beginning Page***************
***************page number:31**************
31.
in vederea gasirii unei solulgii pentru acest sistem,
putem observa ca in expresia (x1 A3132) ng variabilele wO < 0
logice x1 §i $2 joaca r01 simetric in raport cu opera- wO + “)3 > 0
torul /\. Deci este natural sa consideram ca ponder- wO + wl < 0 (6)
ile alocate lui 331 §i 51:2 [ca input-uri] in perceptronul wO + wl + w3 > O
sigmoidal pot ﬁ egale. Introducand deci restrictia wO + 2w1 > O
wl : wg, sistemul (5) va deveni cel scris in dreapta. wO + 2w1 + w3 > 0
Mai departe, analizand din nou expresia (x1 Ax2)\/x3,
este natural sa gandim ca, in raport cu operatorul wO < 0
\/, ponderea variabilei :133 ar trebui sa ﬁe aceeagi cu wO + 2101 > 0
ponderile ,,cumulate“ ale variabilelor x1 §i x2. Prin wO + wl < 0 (7)
urmare, ,,injectand“ §i restrictia U23 : wl + U22 : wO +3w1 > 0
2101, sistemul de inecua§ii (6) va deveni cel scris in wO + 42,01 > 0
dreapta.

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
32.
Din forma sistemului (7), apare natural s51 explorém ce anume se intémplé
dacé restrictionéim spagiul de solutii inpunénd condigia wl > 0. In aceasté
ipotezé, sistemul (7) va avea aceea§i solugie ca §i inecuatia dublé
w0+w1 <O<w0+2w1,

care este echivalenté cu

w1 < —w0 < 2101- (8)
Pentru aceasté ultimé inecuagie dubléi este foarte u§0r sé indicéim solutii. Iaté

1 3 1

una dintre ele: wl : 5 §i wO : —1. Prin urmare, 102 I 5 §i 'U)3 : 1.

***************Ending Page***************


***************Beginning Page***************
***************page number:33**************
33.
Observaigie (1)
De fapt, dacé lucrém intr-un reper
de coordonate wOOwl, orice punct a W1
(100,101) din cadranul a1 doilea [gi] /\\\
care este situat intre dreptele de )6
ecuagii wl : —w0 §i respectiv wl : = 1/2
—§w0 este 0 solugle a 1necua§1el duble
(8), deci §i a sistemului de inecuagii —1—3/4—1/2 0 W0
(5), la care, aga cum am vézut, au h/7%\
fost adiiugate restricigiile wl I wg I 0-5;”
1 0
—w .
2 3
(Invers, adicé a preciza care este intreg spagiul de solutgii a1 sis-
temului (5) este mult in afara obiectivului acestui exercigiu.)

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
34.

Observatie (2)
Rostul acestui [tip de] exercitiu este sa arate ca pentru a gasi valori
corespunzatoare ponderilor unui perceptron in a§a fel incat e1 sa reprez-
inte/codiﬁce 0 anurnita functie, putem apela la [rezolvarea de] sisteme de
restrictii / inecuatii.
Mai general, a§a cum precizeaza §i Tom Mitchell in Machine Learning la pag.
95, putem folosi metode de programare liniara sau ne-liniara.
Pe de alta parte, acest exercitiu pune in evidenta in mod indirect faptul ca
ar ﬁ de dorit ca (alternativ) sa dispunem de 0 procedura de calcul generala,
cat mai simpla, prin care sa atingem obiectivul mentionat anterior.
O astfel de procedura — bazata pe metoda gradientului descendent — va
face obiectul problemelor urmatoare. In raport cu programarea lininiara
sau cea ne-liniara, aceasta procedura are avantajul ca se scaleaza in mod
elegant / convenabil la nivel de retea neuronala.

***************Ending Page***************


***************Beginning Page***************
***************page number:35**************
35.
R0senblatt’s Percept'ron Algorithm:
some simple properties
CMU, 2015 spring, Torn Mitchell, Nina Balcan, HW6, pr 3.ab
CMU, 2017 spring, Barnabas Poczos, HW3, pr. 1.bc

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
36.
Consider running the Percept'r'on algorithm :cgitlzalizzelwes do
(see the nearby pseudo-code) on some se- if yl- (117.531.) §Othen
quence of examples S. (Remember that an v? <- IBM/iii
example is a data point and its label.) endegjr "c
Let S’ be the same set of examples as S, but presented in a different
order.
a. Does the Perceptron algorithm necessarily make the same num-
ber of mistakes on S as it does on S’?
If so, why? If not, show such an S and S’ Where the Perceptron
algorithm makes a different number of mistakes on S’ than it does
on S.

***************Ending Page***************


***************Beginning Page***************
***************page number:37**************
37.
Solution (in Romanian)
a. Consideram urmatorul set de exemple (S), in ordinea indicata:
—---—u
Eticheta y —1 +1 +1 —1 —1 +1
Sintetizam execu§ia algoritmului Perceptron pe aceste date, fara a folosi ter-
menul “bias” (wO), alcatuind tabelul urmator:
Itera§1a(i) EZ- I yi yﬂIJ-ZTZZ' z?) gregeala ec. separatorului
inipiahzare —- <01» ——
1 <—1,2> <1,—2> m x2 I 0-5961
2 <17<>> <1,—2> $2 I 0-5961
3 <171> <2,—1> m x2 I 2x1
4 <-1,0> <27-1> $2 I 2m
5 <—1,—2> cw u 9:2 I —3w1
6 <1,—1> <371> $2 I ml

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
38.
Agadar, algoritmul a comis trei gre§eli. Cei trei separatori 0b§inu§i in cursul
aplicéirii algoritmului Perceptron pe §irul de exemple S sunt prezentati in
ﬁgura de mai jos.
x2 x2 x2 x2
3 3 3 3
X =2X
o 2 o 2 o 2 2 1 0 2
1 O 1 O 1 O 1 O
X =- X
x2 =1/2 x1 2 3 1
x x x x
-1 1 1 -1 1 1 -1 1 1 -1 1 1
_1 0 _1 0 _1 0 1 0
O —2 O -2 —2 O —2
—3 —3 —3 —3

***************Ending Page***************


***************Beginning Page***************
***************page number:39**************
39.

Este imediat ca modul in care se construieste separatorul — a carui expresie
analitica este 0 combinatie liniara de instante gresit catalogate — este depen-
dent de exemplele prezentate. intrebarea care a fost pusa in enunt este daca
pentru doua succesiuni/ordini diferite, rezultatele ﬁnale (i.e., expresiile sepa-
ratorilor obtinuti) coincid (intotdeuna) sau nu. in continuare vorn arata ca in
mod obisnuit rezultatele ﬁnale (i.e., pozitiile ﬁnale ale separatorilor obtinuti)
nu coincid.

Consideram S’ secventa de exemple obtinuta din S prin inversarea [ordinii]
exemplelor ail si 552. Este imediat ca la prima iteratie perceptronul greseste si
apoi calculeaza 1T) I (1, 0), ceea ce conduce la ecuatia separatorului x1 I O, adica
axa 0x1. Evident, aceasta dreapta este un separator perfect pentru intregul
set de exemple, deci algoritmul nu va mai comite pana la ﬁnal nicio [alta]
greseala. Concluzia este ca, pe un acelasi set de exemple de antrenament,
algoritmul Percept'ron poate comite un numar diferit de greseli (si, de aseme-
nea, un alt separator) daca exemplele ii sunt prezentate in doua seccesiuni
diferite.

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
40.
Observagie importantéi
De§i in ambele cazuri de mai sus (S §i S’) algoritmul Perceptron
gﬁse§te un separator liniar pentru datele de intrare, acest fapt nu
este garantat in gazul general, chiar dacﬁ datele sunt liniar sepa-
rabile. Vezi de exemplu problema CMU, 2015 spring, Alex Smola,
midterm, pr. 4. (Motivele sunt: rata de inv5§are (implicitﬁ) prea
mare gi/sau ne-parcurgerea setului de date deceit 0 singuré date.)

***************Ending Page***************


***************Beginning Page***************
***************page number:41**************
41.

b. The Perceptron algorithm classiﬁes data points by their position relative
to a hyper-plane. The weight vector, 1D, learned by Perceptron will be normal
to this hyperplane? (True or False?)
Raspuns:
Cunoastern din geometria analitica faptul ca orice doi vectori 1i) si 5: pentru
care are loc relatia U) - E I 0 sunt ortogonali (adica, perpendiculari unul pe
celalalt).
in cazul perceptronilor (de tip liniar, prag sau sigmoidal), ecuatia separatoru-
lui este tocmai de forma ’U—)-£l_3 I 0, unde E este un punct arbitrar de pe dreapta
[sau, in general, hiper-planul] separator. Considerand E1 si 5:2 doua astfel de
puncte, diferenta lor, 57:1 —:1_c2, va ﬁ un vector care are directia dreptei-separator.
Din relatiile 15-121 I O §i w-Eg I 0 rezulta 1T)- (531 —i2) I 0. Prin urrnare, vectorul
de ponderi 1T) este perpendicular pe directia dreptei-separator.
Proprietatea aceasta se poate veriﬁca §i in mod direct / particular pe datele de
la punctele precedente. De exemplu, la punctul a, pe sirul de exemple S, la
iteratia 1 avem 11) I (1, —2) si se poate observa direct pe graﬁc ca acest vector

1
este perpendicular pe directia dreptei y I 5x.
Asadar, raspunsul este Adevérat.

***************Ending Page***************

***************Beginning Page***************
***************page number:42**************
42.
Rosenblatt’ Perceptron Algorithm:
Convergence / Mistake bounds
MIT, 2009 fall, Tommy Jaakkola, lecture notes 2

***************Ending Page***************


***************Beginning Page***************
***************page number:43**************
43.
Presupunem ca folosim perceptronul de tip prag in varianta Rosenblatt §i vrem sa
inanam un concept, folosind instantele de antrenament x1, . . . , :13”, . .. € Rd impreuna cu
etichetele corespunzatoare yl, . . . ,yn, . . . € {—1, 1}.
Demonstralgi ca in cazul in care sunt indeplinite conditgiile i-z'v de mai jos, algoritmul de
actualizare a ponderilor perceptronului termina intr-un numar ﬁnit de pagi. Formal,
exprimam acest fapt astfel: Elm € N astfel incat ytwm) 03,; Z O pentru orice t G {1, . . . , n, . . .},
unde 10W“) este vectorul de ponderi obtjinut de perceptron la iteratgia m.
Iata acum condijiile menis'ionate mai sus:
71. Instanigele x1, . . . ,xn, . .. sunt separabile liniar prin originea sistemului de coordonate,
cu 0 margine ﬁnita y > 0. Din punct de vedere formal, aceasta inseamna ca exista
10* G Rd astfel incat ytufk -:1:t Z 7 pentru t : 1, . . . ,n, . . ..
ii. Toate instantele :61, . . . ,atn, . . . sunt continute intr-o sfera din Rd cu centrul in origine,
U V A A d - .

adlca EIR > O astfel meat ||:13t\| if wwrit -a:t g R pentru orlce t.
iii. invaigarea se face in maniera incrementalci, folosind urmatoarea regula de actualizare
a ponderilor:

w<k+1> : 111(k) + ytkwtk pentru un tk 6 {1, . . . ,n, . . .} a.i. ytkww) -mtk g O, (9)
ceea (:e inseamna ca instaniga 3% este clasiﬁcata eronat de catre perceptron la iteraigia
k.
iv. Startarea procesului de invatare se face cu 111(0) I O € Rd.

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
44.

Indicajie: Aréitagi c5 la ﬁecare iteraii'ie (k) a algoritmului, sunt satisfécute
urmfitoarele proprietégi:

a. 10* -w(k) Z kw;

b. ||w(k)|\2 g kRZ;

!|w*|| 2
c. k g —R .
V

Ultima inegalitate indicéi [0 margine superioaré pentru] numérul maxim de
iteratii executate de céitre perceptron.

***************Ending Page***************


***************Beginning Page***************
***************page number:45**************
45.
Observajia 1: Noténd cu 6t unghiul format de vectorii mt §i 10* in Rd §i @inénd
cont de faptul c5
w -w*
cosHt I cos (xt,w*) I t—*,
llwtll \lw \l
deci
ytw* cut I gt ||w*|| |\a:t|| coth,
ceea ce, coroborat cu condigia 2', implicii yt HattH cos Qt \|w*\| Z 7, adicé
yt HastH cos 6t Z H'Y—H. Din punct de vedere geometric, aceasta inseamné c5
w>|<
in spagiul Rd distantga de la orice instantgé azt la vectorul 10* (care trece prin
originea sistemului de coordonate) este mai mare sau egalii cu ny—*‘|.
w
Observatia 2: Separatorul "mu/J) este 0 combinalgie liniaréi de instangele xi,
intrucét regula de actualizare este w<k+1> I 100“) +yZ-xi. Observatia aceasta este
valabiléi §i la antrenarea perceptronului-prag clasic, bazat pe regula delta.

***************Ending Page***************

***************Beginning Page***************
***************page number:46**************
46.
Solution (in Romanian)
Ideea de baza
Algoritmul de actualizare a ponderilor perceptronului determina schimbarea
pozitgiei separatorului la ﬁecare iteragie (k7) la care avem de a face cu un ex-
emplu clasiﬁcat gre§it.
Intuitiv, ne agteptam ca pozi§ia separatorului la iteratia k + 1 (adica hiper-
planul de ecuaii'ie w<k+1> m I 0) sa se apropie de pozitia separatorului 211* care
deﬁne§te conceptul de invagat.
in consecin§a, cosinusul unghiului dintre ww §i w* ar trebui sa creasca de la
0 iteragie la alta.
Pentru a dovedi/veriﬁca in mod riguros aceasta, vom tine cont ca, prin
deﬁnigie,
< (k) *) ww) -w*
00s w ,w I —*.
\|w("’)l\ llw ll

***************Ending Page***************


***************Beginning Page***************
***************page number:47**************
47.

Mai intai vom compara valorile produsului scalar de la numaratorul fractiei

de mai sus, la doua iteragii succesive. Folosind regula (9), putem scrie:

intrucat ytk xtk -w* Z 'y (vezi conditia 2' din enuntg), rezulta ca valoarea produsu-

lui scalar 111(k) -w* cregte la ﬁecare iteraii'ie cu 0 cantitate cel pu§in egala cu y.

Cum 10(0) I 0 conform restricli'iei i'v, este imediat ca 111(k) -w* Z kw la iteragia kt.

A§adar, numaratorul frac§iei care deﬁne§te cos(w(k),w*) cre§te cel puigin liniar

in raport cu k. Cu aceasta, tocrnai am demonstrat relagia a.

***************Ending Page***************

***************Beginning Page***************
***************page number:48**************
48.

A analiza evolutia numitorului fracgiei care deﬁne§te cos(w(k),w*) revine la a
compara ||w<k+1)|| cu ||w(k)\|, intrucét \|w*\| este constant in raport cu k.

d . 9

Hw(k+1)||2 if w(k+1) ' {HUM-1) (I) (10%) + ytkmtk) ' (100“) + ytkmtk)

I |\w<k>|y2 + Zytkww) 'wtk + \lwtk ||2

s |\w<k>|!2 +R2-
Inegalitatea de mai sus derivéi din faptul c5 ytkwu“) -wtk g 0 (i.e., exemplul
tk este clasiﬁcat gregit la iteratia k, conform condigiei iii din enuniJ') §i din
ipoteza c5 orice instanii'é de antrenament este continuté. in sfera de razéi R
§i avénd centrul in originea spagiului Rd, conform condi§iei z'z'. Cum 10(0) I O,
este imediat c5 \|w(k)\|2 g kR2, deci \|w(k)]| g @R la iteragia k. Cu aceasta,
am demonstrat relatia b.

***************Ending Page***************


***************Beginning Page***************
***************page number:49**************
49.
Acum, combinénd rezultatele a §i b, 0b§inem urméitoarea inegalitate:
(k). * k
w w
cos(w(k),w*) I W Z —’y I @Lak.
Hw \l llw || ﬁRllurkH RHw \l
Stim (:5 valoarea funcigiei cos este intotdeauna cel mult egalé cu 1. in
consecinlgé,
* 2
w
1 Z cos(w(k),w*) Z @Lyﬁ :> k g (Ru) ,
RHw \l v

deci am demonstrat relagia c.
Aceasta inseamné c5 in conditgiile speciﬁcate in enung, antrenarea

* 2

w
perceptronului-prag se va face in cel mult {(RM) j iteralgii, uncle

V
perechea de simboluri H desemneazéi funcigia parte intreagﬁ inferioaréi.
Observatia 3: Marginea superioaré calculaté mai sus este remarcabilii,
intrucét ea nu depinde nici de instanigele wz- §i nici de dimensiunea (d) a
spagiului din care sunt selectate aceste instante.

***************Ending Page***************

***************Beginning Page***************
***************page number:50**************
50.

Observajia 4
Restricgia referitoare la separabilitatea prin origine a instan§elor de antre-
nament (vezi condi§ia 2', prima parte) — §i anume, componenta ,,libera“
100 asociata separatorului w* trebuie $5 ﬁe 0 — nu este de fapt limitativa.
Cazul general al separabilitagii liniare in Rd, adica yt(wf§ + 10* -a?t) Z O pen-
tru orice t é {1, 2, . . . , n. . .}, poate ﬁ redus la cazul particular a1 separabilitaigii
prin origine in Rd+1 daca pe de 0 parte se considera w’ : (100,101, . . . ,wd), cu
10* I (w1,.. .,wd), iar pe de alta parte se mapeaza toate instanli'ele de antre-
nament ZEt I (33%,...,a:§l) din Rd in Rd+1 astfel: x2 I (x?,x%,...,a3f), cu at? I 1
pentru orice t. Cu aceasta mapare, rezulta ytw’ - w; Z 0 pentru orice t (re-
spectiv ytw’ - as; Z 'y cand se lucreaza cu margine ﬁnita, ca in enun§ul acestei
probleme).
Nici restrictia 2'1) (10(0) I O) nu este cu adevarat limitativa; in general algorit-
mii de antrenare a unitagilor / re§elelor neuronale ini§ializeaza ponderile w la
valori mici (in modul).
Similar, este imediat ca restricigia potrivit careia sfera in care sunt conginute
instangele x1, . . . ,ajn, . . . trebuie sa aiba centrul in originea lui Rd (vezi condiigia
z'z', partea a doua) poate ﬁ eliminata fara ca rezultatul de c0nvergen§a sa ﬁe
afectat.

***************Ending Page***************


***************Beginning Page***************
***************page number:51**************
51.
Observatia 5

Deducerea marginii superioare de la punctul c de mai sus in cazul in care se
folose§te 0 ratii de inviigare oarecare 77 > 0 se face extinzénd in mod natural
demonstraQia de mai sus (vezi CMU, 2013 spring, A. Smola, B. Poczos, HW2,
pr. 2.a).

in concluzie, mai rémén de examinat douéi condi§iiz separabilitatea liniaré
cu margine 7 > 0 §i conginerea instantelor de antrenament intr-o sferéi de
razfi ﬁnitii. Se poate demonstra c5 ambele condigii sunt esengiale pentru
convergenga perceptronului Rosenblatt in regim de invétare online (vezi prob-
lema CMU, 2008 fall, Eric Xing, midterm exam, pr. 3.2).

***************Ending Page***************

***************Beginning Page***************
***************page number:52**************
52.
Theoretical foundations for the
perceptron training algorithm
Liviu Ciortuz, 2017
following Tom Mitchell, Machine Learning book, 1997, p. 89-93

***************Ending Page***************


***************Beginning Page***************
***************page number:53**************
53.
in acest exercigiu vom elab-
ora/reda [mai intéi] partea X =1
de fundamentare teoreticﬁ X1 w 0
pentru antrenarea percep- 1 wo
tronului liniar, date ﬁind X2
instan§ele (i1, t1), . . . , (in, tn), W2 O
cu @- € Rd §i t1- G R pentru wd 0:22;) Wm
2' z 1, . . . ,n.
A§adar, vom considera Xd
perceptronul liniar avénd
intrérile 31:0 I 1&1, . . . ,xd §i
ponderile 100,101, . . . , wd.

***************Ending Page***************

***************Beginning Page***************
***************page number:54**************
A 0 o a 0 Q 54-

a. In 11n1e cu metoda gradzentuluz descendent, deduce§1 care este forma reg-
ul'i'i de actualiza'r'e a vectorului de ponderi 1T2 6 Rd+1 (sau, echivalent, forma
regulilor de actualizare a ponderilor wi, cu 2' : O, . . .,d) pentru géisirea acelei
valori care minimizeazé semi-suma pétratelor erorilor comise de perception,“
adicé

d f 1 n

_ e . 2
1:1
In aceastii ultimé expresie, 01- este output-ul perceptronului liniar pentru in-
_ renot. . o d

trarea 332' I (CEO I 1,1131'71, . . . 7332',d)7 adlca OZ‘ I ZUO + ijl wjxig"
Vii reamintim (:5 la aplicarea metodei gradientului descendent pentru gésirea
unui punct de minim a1 unei func§ii derivabile f : Rd+1 —> R, regula de ac-

. O — O O O _ _ _ _ d6 o
tuallzare a parametrllor w a1 func§1e1 f are forma w <— w + Aw, cu Aw :f
—nV@f(w), unde 77 > O este un numieir real mic, numit rata de inviitare, iar

. . (9 5
Vmﬂw) GSte vectorul gradzent, adlca —f(112), —f(112), . .. .
8100 8101
a Conform problemei CMU, 2001 fall, T. Mitchell, A. Moore, ﬁnal exam, p1". 10.2, am putea scrie — in ipoteza c5
datele au fost generate probabilist, cu o componenté ,,zgomot“ urménd o distribugie gaussiané uni-variaté de medie
O, aga cum s-a precizat acolo i,
1 TL
@ML I arg m_inE(u_1) I arg m_in — 2(151- — 1T1 - ji)2.
w w 2 . 1
Z:

***************Ending Page***************


***************Beginning Page***************
***************page number:55**************
55.
Answer
8E a 1 n 1 n a
@—w@- I 6,7125 2%‘ — Oj)2 I 5 Z 67%(751 _ Oj)2
j:1 j:1
1 n 59 n a _ _
I 5 22W — 019%(7511 01') I 2051' — 00%(1511 w ' 5'51‘)
1:1 Z 1:1 Z
Z 2051' — Oj)(—5@j,v;)
d
Therefore,
sz' = 77 2% — 03-):129-1-
j:1

***************Ending Page***************

***************Beginning Page***************
***************page number:56**************
56.
b. Dacé in locul perceptronului liniar vom considera perceptronul-prag, respectiv per-
ceptronul sigmoidal, prin ce va diferi forma regulii de actualizare a ponderilor faté de
rezultatul de la punctul a? (Pentru perceptronul-prag, se va presupune c5 t2- € {—1,+1},
in vreme ce pentru perceptronul sigmoidal vom considera t1 6 {0, 1}, pentru 71 : 1, . . . ,n.)
Answer:
8E (9 1 n 1 n 6 1 n a
_ : __ t._0.2:_ _t._0.2:_ 2t._0._t._0.
810i 810122“ 1) 22(9wa 1) 22: (1 1)8wi(1 1)
9:1 3:1 3:1
TL TL
8 _ _ (9 _ _
I 321011 — O1)3—m(751 — “111-110): 121051 — 01)3—wi0(w ' $1)
_ ‘r _
n 8 n (9
I — 2051' — 011001’ '531)(1 — 0(5) 'f1))%’51'51 I — 2(751 — 01)01(1 — 01)%1D ' f1
1:1 Z 1:1 Z
TL
I — 2% — 01)01(1 — 01)$1,1
1:1
Note: Here above we used the fact that
, _ 1 ’__(1+e—Z)/_ (fz _1+e—Z—1_ 1 _ 1 _ _

U (Z) _ (HT) — (1+e_z)2 _ (He-1P _ (1+e-Z)2 _ 1+e-Z (He-Z)2 _U(Z)(1 UWNZ QR

***************Ending Page***************


***************Beginning Page***************
***************page number:57**************
57.
c. Elaborati algoritmul de antrenare a perceptronului liniar (gi apoi, dacé
dorili'i, §i a celui sigmoidal, dar ve§i preciza doar diferen§ele!). La initializa'r'e,
ponderile wz- vor primi ca valori numere reale mici. In ce prive§te condijia
de op'r'ire a algoritmului, se poate opta pentru una din urméitoarele variante
(eventual combinate):
— toate instangele de antrenament sunt corect clasiﬁcate (in ipoteza c5 datele
ii, cu 2' I 1, . . . ,n, sunt liniar separabile);
— efectuarea unui numér prestabilit de iteratjii;
— veriﬁcarea condi§iei \E(w<t+1)) — EMF”)! < a, unde pragul numeric 6 > O este
stabilit inigial, iar E(w<t>) este valoarea functiei de eroare E (i.e., ceea ce mai
sus am numit semi-suma péitratelor erorilor) pe setul de date de antrenament,
la ﬁnalul iteragiei t.

***************Ending Page***************

***************Beginning Page***************
***************page number:58**************
58.
The gradient descent algorithm for the linear unit
GRADIENT-DESCENT(training_eztamples, n)
where each training example is a pair of the form (it), with
a? — the vector of input values
t — the target output value.
77 is the learning rate (e.g., .05).
o initialize each wi to some small random value
o until the termination condition is met
initialize each Awi to zero;
for each <E,t> in training_e:camples,
input the instance 53 to the unit and compute the output 0;
for each linear unit weight wi,
Awi <— sz- + 17(t — 0)xi;
for each linear unit weight wi,

***************Ending Page***************


***************Beginning Page***************
***************page number:59**************
59.

d. Ce cunoasteti (de la curs) despre convergenta algoritmului de la punctul
c? Dar despre convergenta perceptronului-prag?
Answer:
[Hertz et al., 1991] has proven that the gradient descent-based training algo-
rithm [centered on the weight updating rule] for the linear unit is guaranteed
to converge to a hypothesis that minimizes the sum of squared errors

o given a sufficiently small learning rate 77,

o even when the training data contains noise,

0 even when the training data is not separable by H.
Prior to that, it was proven by [Minsky & Papert, 1969] that [the same
algorithm, applied to] the threshold perceptron converges

o if the training data is linearly separable,

o and 77 is sufficiently small.

***************Ending Page***************

***************Beginning Page***************
***************page number:60**************
60.
Using another loss-function
in conjunction with the linear perceptron
and the gradient descent method:
The log-sigmoidal function;
deduction of the update rules
University of Utah, 2008 fall, Hal Daumé III, HW4, pr. 2-4

***************Ending Page***************


***************Beginning Page***************
***************page number:61**************
61.
The 0/1 loss function is hard to optimize using gradient descent
because it is poorly behaved (discontinuous, non-differentiable,
etc.). A common “smooth” version of 0/1 loss is the sigmoid
loss, which makes use of our favorite function, the sigmoid: 0(2) I
1/(1 + eXp(—z)).

' ' " ' ' ' ' "4 V-Io (si mag»: ' —

1 , A or ,1 , , i, 3,5 Vsig%,a?z)

iii ihie exeieiee we will week Z i y ii iii jg: i i3 i iii iii ii;
with another loss function, the 1 V f. W f.’ V in :2 V V j, V f. V '1' W j
log-sigmoid: V or 1,5’ V V V or
£(y,f(a:)):—10g(0(yf(a:))). ‘a e - or _;“_;_;____Q,5_

-4 '_3'““"- -2 -1 O 1 2 3 4

***************Ending Page***************

***************Beginning Page***************
***************page number:62**************
62.

a. First, verify that this is a reasonable loss function: when f (:13)
is correct, the loss is low, and when f(:v) is incorrect, the loss
is high. You may assume that f (x) produces real values, where
positive values mean class +1 and negative values mean class —1.
b. Consider optimizing a linear function under log-sigmoid loss,
so that we have f(’LTJ) I 2n—log(0(yn(w - in + b))). Compute the
gradient of this function with respect to w and with respect to b
so that we might construct a gradient descent algorithm.

c. Show that the loss function from the previous two questions is
convex in both 2D and b.

d. Finally, state the update rules for both w,- and b which serve
for training a linear perceptron, based on the gradient descent
method in conjunction with the log-sigmoid loss function.

***************Ending Page***************


***************Beginning Page***************
***************page number:63**************
63.
Solution (in Romanian)
a. Funcigia l se poate scrie sub forma:
[(y h(:c)) I _ 111m hm) I _ 1n ; I 111(1 + 6-yh<w>)
7 1 _|- e—yh(£ll)
Din proprietégile logaritmului §tim c5 1n 1 I 0, 1na < 0 dacfi O < a < 1, §i Ina > O
dacé a > 1.
Dacéi ipoteza Mm) este corectéi, adicé are acela§i semn cu y, atunci e_yh($)
este o valoare micé (§i cu atét mai micé cu cét |h(x)| este mai mare), deci §i
valoarea func§iei [(y, h(m)) este micéi.
Dacé ipoteza h(a:) este incorectii, adicii are semn contrar semnului lui y, atunci
e_y Mi”) este o valoare mare (§i cu atét mai mare cu cét |h($)\ este mai mare),
deci §i valoarea functiei [(y, h(a:)) este mare.
De ewemplu, dacé y I 1 §i Mm) I —1 (sau y I —1 §i 11(33) I 1) atunci
1
pierderea/costul este —lnF z 1.31, iar dacé jg I Mm) I 1 (sau, ambele,
e
1
—1) atunci costul este —ln — I —1n L % 0.31.
1 + 6-1 1 + e

***************Ending Page***************

***************Beginning Page***************
***************page number:64**************
64.
b. Calculiim derivatele pargiale ale funcgiei f(b, u?) I Zn —1n0(yn(w - in + 19)) in
raport cu wz- §i respectiv b. Vom folosi proprietatea 0’(x) I 0(x)(1 — 0($)).
(9f _ (9 _ _ _ (9 _ _
1 8 _ _
1
: — — n_'_n 1- n_'_n TLTLi
; mm _ in + m)” (w w + b>>< 0w (w w + b>>>y w ,
I — 2(1 — 0(yn(w - :73” + b)))yn33nz.
. . . 5f _ _
Slmllar, vom obtlne % I — 2A1 — 0(yn(w - sen + b)))yn.

***************Ending Page***************


***************Beginning Page***************
***************page number:65**************
65.
. _ v A . . v (92f . .
c. Functla f (b, w) este convexa 1n raport cu varlablla wi daca W Z O. Slmllar,
wz'
_ v A v 32f . v v
f (b, w) este convexa 1n raport cu b daca w Z 0. A§adar, trebule sa calculam
aceste derivate pargiale de ordinul 2. Pentru aceasta, vom folosi derivatele
par§iale de ordinul intéi care au fost calculate la punctul precedent.
(92f (9 (9f (9
— I — — I — _ 1 — n _ ' _n b n nz'
8w? 8M (8%) 8w‘ Q aw (w x + >>> y @- ,>
_ _ _ _ 3 _ _
I Z ynx'rm' 0(yn(w ' $11 + 5)) (1 — 0(yn(w ' 3771 + b)» 67(3/1101) ' $71 + b»)
I 2ny gem-91"” + b» <1 — U<yn<w - in + b)» M
I Z OWWIJ - in + 5)) (1 — dynw - in + b))) 1117215631-
Cum 0(x) E (0,1) pentru orice :c E R, rezultéi c5 §i 1 — 0(56) E (0,1)Vzc. A§adar,
82
0(x) > 0, 1 — 0(w) > O §i, evident, ya Z O, deci Yb; Z 0, adicé f este convexé in
wi, pentru orice 2'.

***************Ending Page***************

***************Beginning Page***************
***************page number:66**************
66.
Analog, calculém derivata parigialé de ordin secund a lui f in raport cu b:
(92f (9 (9f (9
— : — — : — — 1 _ n _ ° _n b n
W 85 (8b) @b( Q (my <w w + >>> y)
I Zynéﬂynw - in + 11))
n (9b
_ _ _ _ (9 _ _
I Zyn U<yn<mn+b>> <1—0<yn<w-wn+b>>> an-Wbm
: Zyn U<yn<mn +b>> <1 —U<yn<mn+b>>> yn
I 2%an - in + 5)) (1 — 0(yn('w '1?” + 10)) 3132 0
Agadar, f este convexé §i in raport cu variabila b.
Observatie: La curs, b a fost asimilat cu wO, iar 337170 I 1 prin deﬁnitgie. Dacé
s-ar ﬁ procedat la fel §i aici, atunci nu am ﬁ avut de calculat decét 0 derivatéi
la punctul a §i una la punctul b.

***************Ending Page***************


***************Beginning Page***************
***************page number:67**************
67.
d. in concluzie, valoarea optimé a func§iei f este un minim, iar algoritmul
de invégare automatéi bazat pe metoda gradientului descendent va géisi (la
limité, eventual) acest minim. Antrenarea acestui perceptron se face similar
cu antreanarea unitélgii liniare care a fost prezentaté la curs, folosind reguli
de actualizare de forma:
in cazul de fagéi, Awi §i Ab sunt deﬁnite de expresiile
3 f _ _ _
Awi I —77% I 772(1 — 0(yn(w ' a3” + b)))yn$n,i
Z
TL
§i respectiv
5f _ _
Ab I —17% I n 2(1 — 0(yn(w - a?” + b)))yn
TL

***************Ending Page***************

***************Beginning Page***************
***************page number:68**************
68.
Algoritmul de retro-propagare:
deducerea regulilor de actualizare a ponderilor
pentru 0 regea feed-forward cu 2 niveluri,
folosind 0 functgie de activare oarecare (derivabilﬁ) f
CMU, 2008 spring, Eric Xing, HWZ, pr. 2.2

***************Ending Page***************


***************Beginning Page***************
***************page number:69**************
69.

La acest exercigiu vegi deriva regula de actualizare pentru algoritmul de retro-
propagare (varianta stochasticé) pentru 0 re§ea neuronaléi artiﬁcialé de tip
“feed-forward”, cu douéi niveluri de unitéiti sigmoidale. Se consideré D unitéiti
de intrare, H unitéigi ascunse §i K unitiiﬁ de iegire.
Functia de eroare cu care se lucreazé este E('u_)) :
1 1 . o
5 Zk<tk3 — Ok)2, unde 0k I f(n€tk) I W’ lar
c este 0 constanté.
a. Fie wkj ponderea conexiunii de la unitatea as-
cunsé j ciitre unitatea de ie§ire k. Arétaigi c5 reg-
ula de actualizare pentru wkj este de forma wkj +
wkj + n6kyj, unde yj este ie§irea unitétii ascunse
j, iar 5k I f’(netk)(tk — 0k), cu net;€ I 23-, wkj/ yjl. °

w .
b. Arétagi c5 regulile de actualizare pentru 0 wjl. a k1
ponderile care corespund conexiunilpr input-t0-
hidden sunt de form? wji e wji + 0753a?“ uncle xi
este intrarea 2', iar 53- I f’(netj)[2£{:1wkj6k], cu
TL€tj I 27;, ij-l $21.

***************Ending Page***************

***************Beginning Page***************
***************page number:70**************
70.
v
Raspuns
Observatie: Vom face rezolvarea problemei in raport cu 0 funcgie oarecare
f derivabilﬁ, lésaté nespeciﬁcatii. Forma particularé datii in enuni; pentru
func§ia f nu are nicio conseciniﬁi particular-51 asupra rationamentului dezvoltat
in continuare.

***************Ending Page***************


***************Beginning Page***************
***************page number:71**************
71.
training algorithm
_ ——

B 5.‘!
E as’ 0 -
2 1
_ O i
cu I >
h :
E xi.
Q . : _ :
m s y]: f(neti) 5 5 -

input hidden output

layer § layer g layer 5

***************Ending Page***************

***************Beginning Page***************
***************page number:72**************
a. Conform metodei gradientului descendent, H
8E
w - <— w '— — uncle este 0 constantia’. ozitivéi
(rata de invégare). 0yj OK
k
Intuitiv, ponderea wkj inﬂuen§eaz5 valoarea func§iei de eroare E (doar) prin intermediul
lui netk. (S-a notat cu netk suma 23* wkj/yj/.) A§adar, este necesarii aplicarea formulei
pentru derivarea unei compuneri de funcigii derivabile:
8E 8E 3726i a 1K a “k a 1

k 2 2
—:——: —— t/—0/ — w-/-/ I ——t— net -
(9wa ﬁnetk 8w,”- <3netk 2 k;( k k ) > 810M a; k9 y] (@netk3 2< k f< k» ) y]

1 (9 ,
:yj —2(tk — f(n@tk))—(tk — f(n@tk)) I —yj(tk — f(n@il<:))f (716%)
2 ﬁnetk
unde nk este numiirul de intréri ale unitéiigii k. in particular, nk I H dacéi se consideré
toate conexiunile posibile (hidden-t0-0utput).
8E
Dacé notém 6k : (tk — f(netk))f’(netk), atunci avem ﬁ : —5kyj, §i
J
<— 8E + 6
w . w ._ — :w . .

***************Ending Page***************


***************Beginning Page***************
***************page number:73**************
73.
. 6E Q v
b. Ca g1 la punctul precedent, wji e wji — 1787. Ponderea wji 1nﬂuen§eaza
jZ
valoarea funcigiei de eroare E (doar) prin intermediul lui netj. Folosim din nou
formula pentru derivarea unei compuneri de funcgii derivabile §i tinem cont
cii netj I 21" ij-I 331v:
K D
(9E (9E (971675 (9 1 (9
0— I a—a—j: a—§Z(t’“_O’“)2 YEW”
wji netj 'UJjZ' netj kIl wji 27:0
K K
1 8 80k:
I — 2t—0—t—0 x'I—x' t—0—
(2 Z (k k>8netj( k H) Z 12h‘; k>6netj
k:1 k:1
Pe de alté parte, valoarea netj inﬁuenteazé valoarea 0k; (doar) prin intermediul
lui netk. A§adar,
(90k (90k 8netk f,( t > (9 i
— I —— I n6 — w -/ -/
ﬁnetj @netk Gnetj k ﬁnetj _ k] y]
3'20
8 m”
I I net — w -, net'l I I net 'net- w '
f( 0591161532 kyf( y) f( k)f( a) k3

***************Ending Page***************

***************Beginning Page***************
***************page number:74**************
74.
inlocuind acest rezultat in egalitatea precedentéi, vom avea:
K
(9E
— I - - t — ’ t ’ t- ~
8w”, 951;; k Oldf ("6 10f (n6 3)wa
K K
I —a:Z-f’(netj) Z (15k — 0k)f’(netk)wkj I —:1;Z-f'(netj) 26km”-
kzl kzl
~ K
Folosind nota§ia 63- I f’(netj) Z5kwkj, egalitatea de mai sus devine
k:1
(9E ~ . . .. v A
8— : —ocZ-5j, 1ar regula de actuallzare a ponderu se transforma 1n:
wjz-
wji k wji + 7753332'

***************Ending Page***************


***************Beginning Page***************
***************page number:75**************
75.
Observatgie

in mod similar cu demonstratia din cartea Machine Learning de Tom
Mitchell, pag. 101-103, demonstratia de aici poate ﬁ extinséi in mod facil

la retele neuronale feed-forward cu un numéir oarecare de niveluri ascunse.

De asemenea, mai este posibiléi incci 0 generaliza're: funcgia de activare f

poate ﬁ diferité. de la 0 unitate la alta (sigmoidalii, generalizat-sigmoidaléi (cu

0 anumitii constanté c ﬁxatéi), liniaré etc). In demonstralgie va ﬁ suﬁcient sii

ata§5m simbolului f indicele unitéigii neuronale respective.

***************Ending Page***************

***************Beginning Page***************
***************page number:76**************
76.
Aplicarea algoritmului de retro-propagare
pe 0 reigea feed-forward de unitégi sigmoidale
cu 2 niveluri
prelucrare de Liviu Ciortuz, dupé un exemplu din
,,Apprentissage artiﬁciel“, A. Cornuéjols, L. Miclet, 2010, pag. 332, 341

***************Ending Page***************


***************Beginning Page***************
***************page number:77**************
77.
x0 :1
\02
Reteaua neuronala d1n ﬁgura alaturata este 1 0.5 \OA
formatéi din unitégi sigmoidale. -°-2
§ —
a. Care este rezultatul produs de aceastéi Q3. 0' O5
O t" —
retea pentru 1ntrarea :13 n; (m1, x2) : (1,1)? 0'4
x20 0.4

b. Executatji manual prima iteratie a algoritmului de retro-propagare pe
regeaua daté. Presupunem c5 in cazul intrérii x : (1,1) ie§irea produsii de
retea ar trebui s5 ﬁe t I O. Luénd rata de invitare 77 I 1, precizaigi care vor ﬁ

— valorile ponderilor 1030, 1031,1032, 1040, 1041, 1042, 1050, 1051, 1052, dupéi aplicarea

acestei prime iteratii in antrenarea re§elei;“
— noul output produs de regea (dupéi actualizarea ponderilor) pe aceea§i
intrare x I (1,1).
Sugestie: Pentru a vedea detaliile algoritmului de retro-propagare a erorii,
consultagi cartea Machine Learning de Tom Mitchell, pag. 98.
c. Comparalgi rezultatele de la punctele a §i b. Ce constataigi?
a Semniﬁcagia pentru wji este cea din cartea Machine Learninga lui Tom Mitchell, 1a pag. 98: wji este ponderea
de pe arcul/legétura de la unitatea (sau intrarea) i cétre unitatea j .

***************Ending Page***************

***************Beginning Page***************
***************page number:78**************
78.
Solution (in Romanian)
a.
2' netz- : 29- wijxj 02- : 0(neti)
3 | 0.2 + 0.1 + 0.3 I 0.6 1/(1 + 6-0-6) z 0.646
4 | -0.3 - 0.2 + 0.4 I -0.1 1/(1 + 60-1) 2 0.475
5 | 0.4 + 0.5 - 0.646 _ 0.4 - 0.475 Z 0.533 1/(1 + e_0'533) Z 0.630
A§adar, output-ul produs de re§ea este 0.63.

***************Ending Page***************


***************Beginning Page***************
***************page number:79**************
79.
b.
Precizare:
Ca sa facilitam / sintetizam
ingelegerea modului in care se I, " 1 :11;\
aplica acest algoritm, in schema x°_w30 *11T°"”a’°'
alaturata am aratat cum anume vor (x1 O W31 e x0=1
. . . . w W
ﬁ calculate n011e valor-1 ale ponderilor : W4 53 5° X
w, precum §i cantitatjile 5 implicate in i —3 _ o
O O O .O O O : W32 5-
execuii'la unel 1tera§11 a algorltmulul. . ‘ w54 \5 /( t )(t >
v : 5 I 0' ne 5 — O5

Ilustrarea se rezuma la parcurgerea Ixz. W42 “\B/ _w54 +05%

A o a a w 54 —
(1n ordmea output-to-mput) unula x_1 4° ,

- . . v 0- 54 I (T (net4)55w54
dmtre drumurlle dm aceasta retea, , _

. _ W42 i U142 + 7754362
§1 anume drumul 5, 4, 2. Exten51a la backward
celelalte cai este facila.
In aceasta imagine (dar numai aici) am notat cu w’ noile valori pentru ponderi (calculate
in ultimul pas al iteragiei), pentru a nu ﬁ confundate cu vechile valori, care intervin in
calculul cantitaigilor 6.

***************Ending Page***************

***************Beginning Page***************
***************page number:80**************
80.
Conform algoritmului de retro-propagare,
e . (9E
65 d-f $7775 I 07661501 - 0) I 6(6615)(1 - 0(667500 - 0)
I 0(1 — 0)(t — 0) I 0.63(1 — 0.63)(0 — 0.63) I —0.147
putput-ul neuronului 3 constituie unul din input-urile neuronului 5.
In notaigia folositii de Tom Mitchell, vom scrie D0wnstream(3) I {5}.
A§adar,
e . (9E
63 dzf -— I 03 - (1 - 03) 65-1053 I 0.646 - (1 - 0.646) - (-0.147) - 0.6 z -0.017
@netg
in mod similar, Downstreamﬂ) I {5}, §i
e . 8E
64 dzf -6—t I 04 - (1 - 04) 65-1054 I 0.475 - (1 - 0.475) - (-0.147) - (-0.4) '1 0.015
77,6 4

***************Ending Page***************


***************Beginning Page***************
***************page number:81**************
81.
intrucét am terminat de calculat toate cantitfiigile de tip 63-, vom trece acum
la actualizarea ponderilor retelei. Mai intéi vom calcula noile valori pentru
ponderile de pe conexiunile hidden-to-output, deci vom avea:

1053' I 105]‘ + A1057 C11 A1053‘ I 776503' I 6503' pentru j E {0,3,4}.
A§adar,
A1050 I —0.147- 1 I —0.147 1050 <— 0.4 — 0.147 2 0.253
A1053 I —0.147- 0.646 f: —0.095 I> @053 <— 0.5 — 0.1 I 0.405
A1054 I —0.147- 0.475 2 —0.070 1054 <— —0.4 — 0.070 I —0.470
Apoi vom actualiza ponderile care corespund conexiunilor de tip input-t0-
hidden.
Stlm ca A103, .2 053%‘ pentru Z G Similar, ﬁindcéi A104, I 7764510, pen-
{0,1,2}, agadar 1nput-ul 51:0 I 901 I . 0 0
x _ 1 va implica Aw _ Aw _ tru 2 € {0,1,2}, rezulta ca A1040 I
A1032 I 1. (_0.017)_1 I _0.017. A1041 _ A1042 _ 1 0.015 1 0.015 §1,
A . 0 prln urmare:
In consecm§a,
1030 <_ 0.2 _ 0.017 I 0.183 1040 + —0.3 —|— 0.015 I —0.285
1041 + —0.2 —l- 0.015 I —0.185
1031 <—0.1—0.017I0.083 w +04+0015—0415
@032 <— 0.3 - 0.017 z 0.283 42 ' ' _ '

***************Ending Page***************

***************Beginning Page***************
***************page number:82**************
82.

Acum putem calcula noua valoare produsé de regeaua neuronalé:

2' I neti I 23- wijmj 0i I 0(neti)

3 I 0.183 + 0.083 + 0.283 I 0.549 1/(1 + e_0'549) z 0.634

4 I —0.285 — 0.185 + 0.415 I —0.055 1/(1 —|— 60'055) 2 0.486

5 I 0.253 —l- 0.405 - 0.634 — 0.47 - 0.486 I 0.281 1/(1 + e_0'281) z 0.569
c. Valoarea produséi de re§ea dupéi ce a fost aplicaté 0 iteratie din algoritmul
de retro-propagare (0.569) este mai aproape de valoarea-target (0) deceit fusese
output-ul produs de retea inainte de aplicarea acestei prime iteraigii (0.63).
Este de a§teptat ca rezultatul s5 se imbun5t5§easc5 la execu§ia urmétoarelor
iteratii ale algoritmului.

***************Ending Page***************


***************Beginning Page***************
***************page number:83**************
83.
Regularizarea perceptronilor / regelelor
pentru prevenirea overﬁtting-ului,
prin extinderea cu incé un termen
a funcgiei de cost
Tom Mitchell, Machine Learning book, 1997, pr. 4.10

***************Ending Page***************

***************Beginning Page***************
***************page number:84**************
84.
Consider the following error function (which is an alternative to the sum of
squares function):
- 1 2 2
dED [<36 Outputs iJ
Derive the gradient descent update rule for this deﬁnition of E.
Show that it can be implemented by multiplying each weight by some constant
before performing the standard gradient descent update (w P w + Aw).
Conventie de notajie (in Romanian):
Am folosit mai sus obisnuita [de acum] scriere a variabilelor w, si anume wji
este ponderea care corespunde coneXiunii dinspre unitatea 2' catre unitatea j.
Notatiile tkd si okd corespund valorii-target si respectiv output-ului unitatii
liniare k de pe nivelul de iesire al retelei, corespunzator input-ului 56d.

***************Ending Page***************


***************Beginning Page***************
***************page number:85**************
85.
Remark (in Romanian)
Suma 22w? se poate scrie ca ||w\|2, unde ||w\| noteaza norma vectorului w.
Intuitiv, minimizarea funcgiei obiectiv E('u_)) va implica meniginerea lui ||w\|2 la
0 valoare destul de redusa.
Efectul practic, in cazul folosirii de
unitaigi sigmoidale este urmatorul: graniiga
(suprafaiga) de decizie calculata de catre Q
retea pentru ponderi w mici (in modul) g - 0.5 -
este aproape liniara — a se vedea graﬁ- "7°
cul funcgiei 0 in jurul originii —, ceea ce
0 impiedica sa se ,,muleze“ in jurul nereg-
ularitaigilor din datele de antrenament. _6 _4 _2 o 2 4 6
X

***************Ending Page***************

***************Beginning Page***************
***************page number:86**************
86.
Exempliﬁcation: The Banana Dataset
I I.
'1. ,‘
I‘ Ih
rI
Cf. CMU, 2014 fall, W. Cohen, Z. Bar-Joseph, HW3, pr. 2.

***************Ending Page***************


***************Beginning Page***************
***************page number:87**************
Exempliﬁcation (cont’d) 87-
LO
Lo
‘r
q— (‘O
N
-2 —1 0 1 2
X
Cf. CMU, 2014 fall, W. Cohen, Z. Bar-Joseph, HW3, pr. 2,

***************Ending Page***************

***************Beginning Page***************
***************page number:88**************
88.
Remark (in Romanian)
Pentru simplitate, vom considera ca re§ea noastra este de tip feed-forward §i
ca are [doar] doua niveluri, iar unitaigile sunt liniare.
Din acela§i motiv, vorn lucra cu varianta stochastica/incrementala a algorit-
rnului de retro-propagare, adica vom considera 0 singura instaniga de antre-
nament pe ciclu/,,epoca“ de antrenare.
Demonstralgia urrnatoare se poate extinde imediat la cazul unitalgilor de di-
verse tipuri (de exemplu sigmoidal etc.), §i la varianta “batch” a algoritmului
de retro-propagare.

***************Ending Page***************


***************Beginning Page***************
***************page number:89**************
89.
Solution (in Romanian)

in conditgiile stabilite mai inainte, putem scrie:

E011): 1 Z (tk — 0m +7|W|2

2 7
kr€ Outputs

unde tk §i 0k corespund valorii-target §i respectiv output-ului unitéiigii liniare
k de pe nivelul de ie§ire a1 reigelei, pentru instanga de intrare x consideraté.
in cele ce urmeazéi, wkj va corespunde unei conexiuni hidden-to-output (unde
j indicii 0 unitate de pe nivelul ascuns, iar k 0 unitate de pe nivelul de ie§ire),
iar wji va corespunde unei conexiuni input-to-hidden (2' — input, j — hidden).

***************Ending Page***************

***************Beginning Page***************
***************page number:90**************
90.
Prin urmare, pentru ponderile de pe conexiunile hidden-to-output, vom avea:
— : — —(tk—0k) +71%‘ + — —(151<;/—01<:/) +’ka;/-
1 3
: ——2 t — — 2 '
2 ( k: Ok)8wkj 0k + kag
(9
I —(tk — 01$)le Zwkj’yj/ + 21/wa I —(tl<: — 0k)yj + vakj
J j/
A§adar,
(9E
wkj P wkj — 77W I wkj + Wm — 0km — 2710M] I (1 — 217%wa + Wk — 0101/1
J

***************Ending Page***************


***************Beginning Page***************
***************page number:91**************
91.
Tratém acum cazul ponderilor de pe conexiunile input-to-hidden.
Notém Downstreamﬁ) mul§imea tuturor indicilor k cu proprietatea c5 existéi
conexiune de la unitatea j céitre unitatea k.
8E 8 1 2 2 8
— : — _ t _ .. :2 .1. _ t _ —
k: E DownstreamQ) k: E Downstream“)
8
: 2'7wji — Z (tk — 010% Zwkj/yj/
szDownstream(j) JZ j,
: 21/1034‘ — Z (tk: — 0k)i 210k ‘/ ZION/$11
. 8wj¢ . J . 9
kEDownstreamQ) j/ 7/
: 21/11)]‘1' — Z (tk — 0k)l Z Z wk -/ w -/Z-/ ZUi/
. 8wj¢ . . ‘7 j
kGDownstream(_7) j’ z’
: Z'y'wji — 13¢ Z (tk: — Ok)wkj
kEDownstreamU)

***************Ending Page***************

***************Beginning Page***************
***************page number:92**************
92.
A§adar,
(9E
wji <- wji — 77—8’wji I wji + n [zm < Z . wkj(tl<: — 01-6)) — ZWwJ-i]
k: E DownstreamQ)
I (1 — 2n'y)wj¢ + 173%‘ Z wkj (15k: — 0k)
kEDownstTeamQ)
Concluzie
Analizénd regulile de actualizare a ponderilor deduse mai sus, se observé c5 in
ambele cazuri (input-to-hidden §i hidden-to-output) ele sunt de forma w <— (1 —
2777)w + Aw.
Deosebirea fa§5 de cazul clasic (w <— w —|— Aw), este evidentﬁ. intrucét algoritmul de
retro-propagare inigializeazé ponderile w cu valori apropiate de O — iar in practicii
se folosesc valori mici pentru constantele 17 §i 7 —, lucrénd cu varianta funcigiei de
eroare propusﬁ in aceastfi problemﬁ rezultﬁ valori ale ponderilor w mai aproape de
0 (decét in varianta clasicéi).

***************Ending Page***************


***************Beginning Page***************
***************page number:93**************
93.
[Networks of] sigmoidal units:
The cross-entropy error function
CMU, 2011 fall, Eric Xing, HWl, pr. 3.3

***************Ending Page***************

***************Beginning Page***************
***************page number:94**************
94.
Fie datele de antrenament D I (X,t) I {(a:¢,t¢)},z' I 1, 2, . . . ,N. De exemplu, a31-
(element dintr-un spatiu Rd) poate ﬁ imaginea unei fete, in vreme ce t2- este
0 eticheta binara care are valoarea 0 daca fata respectiva este a unui barbat,
respectiv 1 in cazul unei femei.
Vom considera 0 unitate neuronala de tip sigmoidal.a Notand cu y valoarea
reala produsa de aceasta unitate, rezulta ca y E (O, 1). Este natural sa inter-
pretam acest output ca ﬁind probabilitatea ca eticheta booleana t sa ﬁe 1.
. de .

Formal, putem scr1e y :f P(t I 1 | x;w).
In consecinta, este natural sa cautam valori convenabile pentru ponderile w
folosind principiul verosirnilitatii maxime (MLE),b sub forma urmatoare:

N

wMLE I argmaXHPUi | £15510).
U} .
2:1
a Mai general, se poate considera 0 retea formata din unitati sigmoidale. (De fapt, aga a fost formulata problema
originals} de 1a CMU.) Intr-un astfel de caz este insa suﬁcient sa presupunem ca tipul unitatii / unitatilor de pe nivelul
de iegire din retea este cel sigmoidal.
b De fapt, in cazul de fata vom lucra cu verosimilitate conditionala.

***************Ending Page***************


***************Beginning Page***************
***************page number:95**************
95.

a. Aratati ca a maximiza expresia Hill P(ti | ail-nu) revine la a minimiza
expresia urmatoare, despre care putem spune, dupa o observare atenta, ca
este o cross-entropie:

N

E011): — thz' 111%‘ + (1 — ti)1n(1 — yd],

i:1
unde yl- este output-ul produs de reteaua neuronala pentru exemplul xi.
b. ssi presupunem ca este posibil ca etichtele datelor de antrenament sa ﬁ fost
puse eronat, §i anume cu probabilitatea e. Considerand ca datele de antre-
nament sunt distribuite in mod identic §i independent unele de altele, scrieti
expresia functiei de eroare / cost E*(w,e) care corespunde log-verosimilitatii
cu semn schimbat (engl., the negative log likelihood).
c. Veriﬁcati ca functia de eroare E(w) se obtine din E*(w,e) pentru cazul
particular a I O.
d. Explicati de (:e anume functia de eroare E* (w, a) va face ca modelul obtinut
sa ﬁe [mai] robust in raport cu date incorect etichetate, spre deosebire de
functia uzuala E(w).

***************Ending Page***************

***************Beginning Page***************
***************page number:96**************
96.
a. Conform enunigului, yl- déf' P(t¢ I 1|wi; w). A§adar, putem scrie
Se veriﬁcéi imediat 05., din punct de vedere matematic, relatia de mai sus se poate
exprima in mod echivalent sub forma
P(ti\$i;w):(yi)ti(1 — (001%-
Aceasté expresie, in ciuda faptului céi este mai putin intuitivéi, este mult mai con-
venabilé pentru calculele pe care trebuie s5 le facem in vederea aplicéirii principiului
verosimilitiitgii maxime:
wMLE if argmaxP<t1, . . . ,tn|ac1, . . . ,xn; w) MI'd' argmaXH P(t2- | x510)
w w 1:1
N N
: argmalenPﬁl- | x1910) I argmaleMyZ-yiﬂ — yi)1_ti
w 1:1 w 1:1
N
: argmax 2(151 1n yi —|— (1 — ti) 111(1 — yﬂ) I argmax(—E(w)) I argminE(w).
w i:1 w w

***************Ending Page***************


***************Beginning Page***************
***************page number:97**************
97.
b. in cazul in care etichetarea datelor de antrenament s-a fécut in mod eronat, cu
probabilitatea 5, rezulté imediat (:5
1 cu probabilitatea 1 — 5, dacé t2- : 1,
. 0 cu probabilitatea 5, dacéi t1- : 1,
3:2 a fOSt etlchetat cu 0 cu probabilitatea 1 — a, dacé t2- : 0,
1 cu probabilitatea a, dacéi 151' : 0.
Prin urmare, putem scrie in manieré condensaté
. J _ yZ-(1—6)+(1—y¢)5 dacé 151:1,
P(tz|x2, w) —{ (1 — yi)(1 — a) + yie dacii t2- : O.
Ba chiar §i mai mult, aceastii expresie este echivalenté cu urmétoarea:
PWIIM; w) I Ml — 6) + (1 — 1108]“ [(1 — y@)(1 — 6) + 211611”-
Fécénd un calcul similar cu cel de la punctul a, va rezultaza
N
E*(w, 6) I — Z{t¢1n[(y¢(1 — 6) + (1 — Mg] + (1 — ti)1n[(1 — y¢)(1 — 6) + Ml}-
i:1
a Se veriﬁcé imediat c5 [yi(1 — a) + (1 — yﬂa] + [(1 — yi)(1 — a) + yie] : 1.

***************Ending Page***************

***************Beginning Page***************
***************page number:98**************
98.

c. inlocuind parametrul a cu valoarea 0 in expresia pe care tocmai am obginut-
0 mai sus, vom avea:

N

E*(’w70) I —Z{[t¢ 111%‘ + (1 — yi) >< 01+(1 —'5@')1I1[(1 — Z91) + yl- >< 0]}
1:1
N
I — Em- 1n 1% + <1 — ti>1n<1 — gm I E<w>

1:1
d. Vom considera ca ewemplu cazul extrem (inséi instructiv) in care toate
etichetele au fost puse incorect. Dacﬁ am folosi funcigia de eroare E(w), atunci
modelul produs de algoritmul de retro-propagare ar ﬁ complet eronat. Dacéi
in schimb vom folosi funcgia de eroare E*(w,5) cu a I 1, atunci algoritmul
de retro-propagare va incerca s51 invege un model care clasiﬁcé datele exact
invers faiﬁi de etichetele originale, ceea ce corespunde obiectivului nostru.

***************Ending Page***************


***************Beginning Page***************
***************page number:99**************
99.

Important Remarks (in Romanian)
1.
Analizand cu atentie demonstratia pentru deducerea regulilor de actualizarea
ponderilor w pentru algoritmul de retro-propagare atunci cand functia obiec-
tiv este prezenta cross-entropie (vezi CMU, 2008 fall, E. Xing, HW2, pr. 2.2),
g1
facand comparatia cu demonstratia din cartea lui Tom Mitchell (alternativ,
vezi CMU, 2008(?) spring, HW2, pr. 2.2-4) — cazul utilizarii semi-surnei
patratelor erorilor ca functie obiectiv —,
se contata ca singura diferenta [intre cele doua seturi de reguli de actualizare]
apare in expresia lui 69- pentru j indice de unitate de pe nivelul ascuns:
in acel caz, in componenta lui 5j este inclus factorul f’(netj), insa aici (folosind
0, functia sigmoidala) acest factor lipse§te.
in rest, totul este la fel!
2.
Ar ﬁ interesant de vazut care este forma regulilor de actualizare atunci cnd
in locul prezentului criteriu de optimizat se foloseste E (10,5) deﬁnit [in acest
set de slide-uri] la punctele b §i c.

***************Ending Page***************

***************Beginning Page***************
***************page number:100**************
100.
Deep neural networks:
gradient vanishing / explosion
CMU, 2015 fall, Eric Xing, Ziv Bar-Joseph, HW3, pr. 1.3

***************Ending Page***************


***************Beginning Page***************
***************page number:101**************
101.
In this problem we will study the difficulty of back-propagation in training
deep neural networks. For simplicity, we consider the simplest deep neural
network: one with just a single neuron in each layer, where the output of
the neuron in the jth layer is zj n2. f(netj) I f(wjzj_1 + bj). Here f is some
activation function whose derivative on a: is f’(x). Let m be the number of
layers in the neural network, L the training loss function.
a. Derive the derivative of L w.r.t. b1 (the bias of the neuron in the ﬁrst
layer).
b. Assume the activation function is the usual sigmoid function 0(a3) I l/(l +
exp(—w)). The weights 2T1 are initialized to be \wj| < l (j I 1, . . . ,m).
Explain why the above gradient (@L/dbl) tends to vanish (—> O) when m is
large.
Even if \w| is large, the above gradient would also tend to vanish, rather than
explode (—> oo). Explain why.
c. One of the approaches to (partially) address the gradient vanish-
ing/explosion problem is to use the rectiﬁed linear (ReL) activation func-
tion instead of the sigmoid. The ReL activation function is 0(00) I max{0,m}.
Explain why ReL can alleviate the gradient vanishing problem as faced by
sigmoid.

***************Ending Page***************

***************Beginning Page***************
***************page number:102**************
102.
d. A second approach to (partially) address
the gradient vanishing/explosion problem is
layer-wise pre-training. Restricted Boltze- 0 a a
mann machine (RBM) ‘is one of the widely- )\E::_:A
used models for layer-w15e pre-tralnlng. ,4" 1P’ 4v “*-
The nearby ﬁgure shows an exa_mple of RBM
which includes K hidden units h, and J input
units 17.
Let us deﬁne the joint distribution over i1 and B as having the following general
form:
P<— 1;) 1 29¢ <— R)
U I — eX z‘ i U, 7
7 Z p i
where Z I 26, 71/ exp (El 91gb¢(17’,h’)) is the normalization term; gbZ-(i), h) are some
features; 91- are the parameters corresponding to the weights in the RBM.
Consider the simplest learning algorithm, gradient descent.
Show that
(910 P 17 _ - _ _ _ _ _ _
§—@i<) I ;¢,,;(v, h)P(h|v) - Eh gm’, mm)’, h’).
'U’, /

***************Ending Page***************


***************Beginning Page***************
***************page number:103**************
103.
Rﬁspuns

a. Ilustrém regeaua neuronalé profundii deﬁnitii in enunt;

x0 =1 x0 =1 x0 =1 x0 =1

x1 1 2 m-I

W2 W3 Wm-1 Wm
I Z1 Z2 zm-2 Zm-1 zI71
xd
:> Z1 I f(net1) I f(b1 —|— wllwl + . . . —|— wldzcd)
22 I f(n6t2) I f<b2 + 10221)
0 I zm I f(netm) I f(bm + wmzm_1)

***************Ending Page***************

***************Beginning Page***************
***************page number:104**************
104.
Functia de pierdere (engl., loss function) L se va exprima astfel:
L(w11,. . .,w1d,w2,. ..,’U)m,b1,.. .,bm)
not.
I L tm
(f( n6 ))
bm+wmzm—1
HF}
bm_1+wm—lzm—2
I L bm+ m bm_ m_ bm_ t
(f( w f( 1+’w 1f( 2+ +w3f(n62) D)
b2+w2z1
I L(f(bm + wmf(bm—1 + wm—1f(bm—2 + - - - + 1013wa + 1U2f( 77,6251 ). . .)))
bl+w11$1+---+w1d33d

***************Ending Page***************


***************Beginning Page***************
***************page number:105**************
105.
Pentru a deriva functia L in raport cu b1, ne vom aminti mai intéi regula de derivare a
unei compuneri multiple de funciji f1(f2(. . . fn(:1:) . . .)). Pe léngii forma clasicé pe care 0
§tim din liceu,
fi<f2(- --fn($) - - -)) -f£(- - - fn(f11) - - -) ' fMﬁ),
ea se poate scrie sub forma aga-numitei reguli de inldntu'i're:
5V2 (91% 5h (90
Folosind aceastéi regulé pentru a calcula derivata par§ialé ceruté in enunig, vom avea:
(9L _ 8L ﬁnetm (9716132 (9716151
(9191 — 8netm 8netm_1 8net1 8191
I L/(f(netm)) ' f/(netm)-
wm - f’(netm_1)-
wm—1 ' f/(netm—2)'
102 - f/(net1)-
1
I L’(f(netm>) -f’(net1)- H(f’(netk) wk)
k::2

***************Ending Page***************

***************Beginning Page***************
***************page number:106**************
106.
b. Stim 05 0(56) E (O, 1) §i 0’(:c) I 0(5c)(1 — 0(5c)) pentru orice :c E R, iar max-
imul funcgiei z(1 — z) este 1/4 §i se atinge in punctul z I 1/2. Prin urmare,
1 m—1
In}; 01mm s (4) -
intrucét |wk| < 1 pentru orice k (conform ipotezei din enung), rezultéi imediat
5L
(:5 ‘%‘ I \L’(0(netm))-a’(net1)|1-1212\0’(netk)-wk\ tinde la 0 pentru k —> +00 §i
1
pentru orice input i‘ ﬁxat.
Pentru a contracara acest fenomen de ,,dispari§ie“ a gradientului, ar trebui sii
impunem condigii de forma Iwk0’(netk)| > 1. Trebuie inséi s51 remarcéim faptul
c5 0(netk) depinde de wk: 0(netk) : 0(1);€ + wkzk_1).
in consecingé, dacé ii vom da lui wk posibilitatea $5 ia valori mari in modul,
atunci zk nit bk + wkzk_1 va lua de asemenea valori mari in modul, iar 0(zk) va
tinde ﬁe la 0 ﬁe la 1, deci 0’(2k) : 0(zk)(1 — 0(zk)) va tinde la 0.
in sfér§it, se poate veriﬁca in mod riguros c5 hmz_>j:oo zo’ (z) : O (lésém aceasté
demonstraQie ca exerci§iu).

***************Ending Page***************


***************Beginning Page***************
***************page number:107**************
107.
c. Dacé f este funcigia de activare ReL, atunci f’(a:) I 0 pentru w < O §i
f’ (an) I 1 pentru x > O. Prin urmare, folosind aceasté functie putem impiedica
,,dispari§ia“ gradientului.

***************Ending Page***************

***************Beginning Page***************
***************page number:108**************
108.
d. Pornind de la expresia distributiei probabiliste P(17,B) care a fost datéi in
enunig, expliciténd mai intéi constanta de normalizare Z, vom putea scrie apoi
expresia distributiei marginale P(17):
P<- 11> —1 2e ¢ <- R)
'0, : _ — €Xp k: k 'U,
217,71’ eXp<Ek¢ @WW’, h’)) k
1 _
i P017) I —_— eXp QWWZh)
26/,R/6XP(21€ Qkéka, W) 2h: (2k:
Prin urmare,
1mm?) I 1n Zexp (Z 9kq5k(17,h)) _ 1n Z eXpQ: mm’, 71>).
B k 17,5’ k

***************Ending Page***************


***************Beginning Page***************
***************page number:109**************
109.
in consecintéi, derivata partialéi a functgiei 1n Phi) in raport cu parametrul 91'
se poate calcula astfel:
@lnPw) _
(96¢ —
EB eXp (2k: 9k¢l<=<17=h>> Z eXP<Z 9k¢k (17/7 El»
17,5’ k
Z
1 _ _
Z— — eX1O (Z 9k¢k<ﬁ>h)) '¢i(17»h) _ _
I thk—_ — Z) §exp<26k¢kww>> - @(Qh)
EH Zexp (2k: 6k¢k(’l_],h)) @ljb/ k:
_ ZR P(ﬁvﬁ)'¢i(ﬁaﬁ) _ -/ _/ . -/ _/
_ —P(1_1) 2/ P(v ,h )qbﬂv ,h)
_ P0773) _—__ _/_/ .—/_/
— 253W '¢@(’Uah) agglpw WWW” 7h)
I ZPGLW) '¢¢(17»}_l) — Z P(Y7/vill)¢i(17”fll)
R 17,5’

***************Ending Page***************

***************Beginning Page***************
***************page number:110**************
110.
The kernelized Percept'mn
Liviu Ciortuz, following
CMU, 2013 spring, A. Smola, B. Poczos, HWZ, pr. 2.d
Stanford, 2016 fall, A. Ng, J. Duchi, HWZ, pr. 2
MIT, 2006 fall, Tommy Jaakkola, HWZ, pr. 3

***************Ending Page***************


***************Beginning Page***************
***************page number:111**************
111.

Majoritatea clasiﬁcatorilor liniari pot ﬁ kernel-izaij, _ _ _ _ _ _
in vederea clasiﬁcarii de date care sunt nesepara- ""“éhze w <— 0
bile liniar. Aici vom elabora varianta kernel-izata for Z I 1’_' ‘ ‘in do

v . . If yz- (w - x1) g O then
[duala] a alg0r1tmulu1 Perceptron. w <_ w + a?
Pseudo-codul variantei simple, ne-kernel-izate a al- end if y, Z
goritmului Percept'ro'n, este prezentat alaturat. (Op- end for
eratorul - reprezinta produsul scalar.)
Ca input, Perceptron-ul kernel-izat va lua secven§a de instan§e etichetate
{mi,yZ-}?:1 cu mi € Rd si yl- 6 {—1,1}, precum §i functia-nucleu K : Rd >< Rd —> Rm,
cu proprietatea ca exista (in general, m > d) si 0 funcgie (,,mapare“) (b : Rd —>
Rm astfel incat K(:1:7;,asj) I #5131) -gb(xj) pentru orice 33¢, xj. (Din punct de vedere
practic, este necesar ca funcigia K sa poata ﬁ calculata in mod eﬁcient.)
Percept'ron-ul kernel-izat va lucra nu cu instangele 5132-, ci cu imaginile lor, Mag),
si va incerca sa gaseasca pentru acestea un separator liniar in spagiul Rm.
Arataigi cum anume se scrie regula de actualizare a coeﬁcienigilor 041, . . . , an la
procesarea unui nou exemplu (331-, yi) de catre algoritmul Perceptron kernelizat,
si cum se face predic§ia (y I 1 sau — 1) pentru 0 instan§a noua x.

***************Ending Page***************


***************Beginning Page***************
***************page number:112**************
112.
Raspuns
in spagiul Rm (care se mai nume§te, in contextul kernel-izarii, spatiul de
trdsdtum'), regula de actualizare a ponderilor w se scrie astfel:
w“) <— w(i_1) —|— yZ-w(i_1) -¢(xz) daca y¢w(i_1) ~q5(x,) g 0,
unde operatorul - desemneaza produsul scalar.
in consecinta, intrucat facem initializarea 10(0) : (_), vectorul w E Rm va ﬁ
intotdeauna 0 combinagie liniara de vectori de trasaturi, #5131). Aceasta
inseamna ca exista coeﬁciengii 041- € R astfel incat ww I 22:10“wa dupa
procesarea primelor 2' instante de antrenament. A§adar, vectorul wm) poate
ﬁ reprexentat in mod compact prin c0eﬁcien§ii 04; (cu l I 1, . . . ,2') din aceasta
combinatgie liniara.
in particular, valoarea inigiala 10(0) corespunde cazului cand suma nu contine
niciun termen (adica avem 0 lista Vida de coeﬁcienigi 04;).

***************Ending Page***************


***************Beginning Page***************
***************page number:113**************
113.

Aratarn acum ca putem actualiza in mod eﬁcient coeﬁcientii 04¢.
La iteratia 2' trebuie sa calculam produsul scalar w(i_1) ~q5(a:i). intrucat produsul
scalar in spagiul de trasaturi Rm este 0 operatic costisitoare atunci cand m
este mare, vom gine cont ca

i-l i-l i—1

10W” $1: (Z al¢<$l>> '¢($1): ZOQWW) ' W560) I ZQZK(33Z,$Z')7

1:1 1:1 1:1
ceea ce inseamna ca acesti coeﬁcientj se pot calcula intr-adevar in mod eﬁ-
cient.
in mod similar, se poate face in mod eﬁcient predictia clasei/etichetei pentru
0 instanta noua (de test) :1: € Rd:

2' i
111(1) '¢($) I Zﬁzﬂm) ' Ml") I 2511“??le
1:1 l=1

***************Ending Page***************


***************Beginning Page***************
***************page number:114**************
114.
Sumarizand, algoritmul pentru antrenarea Percept'ron-ului kernel-izat se
poate scrie in pseudo-cod astfel:
initialize 04¢ : 0 for 11 : 1,. . . ,n;
for i : 1,._..,n do
if yz- 22;; 041K(:l:l,:1:Z-) g 0 then
041' <— yi
end if
end for
ObSQTUajie impOTtantdI Se poate arata relativ u§0r ca rationamentele (§i
rezultatele) din acest exercitiu se pot extinde §i la cazul perceptronului care
folosegte functie de activare de tip prag (in particular, functia sign), rata de
invatare oarecare 17 > O — dar face initializarea vectorului de ponderi w € Rd
cu 6, ceea ce este echivalent cu 041- : O, pentru 2' : 1, . . . ,n — §i care eventual
parcurge de mai multe ori setul de date de antrenament. In acest caz, regula
de actualizare a perceptronului kernel-izat este de forma
041- + 04¢ + 77(y1- — sign(w(i_1) '¢(ZL‘@)) daca yisign(w(i_1) -¢(x1)) g O
unde, exact ca §i mai sus, w(i_1) gb(xZ-) I (22;; alqb(a:l)) $(mz) I 2;:1 041K(5vl,xl).

***************Ending Page***************


***************Beginning Page***************
***************page number:115**************
kernel I ‘exp(-transpose(xi — wj)(w7; — xj)/(252))’; s I 3; 115-
function aITRAlN_KERNEL_PERCEPTRON(X, y, kernel)
n, dIsize(X); E, “u U a Wag-33
Kzlli a nu la “El mil: “300900
fOl’ i : 1171 Suuﬁl$fllunlgluuo no n n o
for j I 1 : n JED: 2-1151)‘ ll:“r.-;I'il.-"-l-.:- .0 m
- / 2-‘! '.,':, l1
$9‘ .2 _X(], :) ; 1 O my i;_I-;|-= ':,'_._-,__"'l-‘il£5;' l: g
K(2,j) I eval(kernel); o D o l‘ Iii-17'" DU :Puo
:"-I'"'_'_'1'."- -|.
dend " ‘9m Ll D Ia *1? a
en '5' nu '53-:

-1 a in goal
or I zeros(n, 1); D DD“ “:5 D BOG? g D “0%; gains‘?
mistakesIl; s a“ r10 ‘3.30% Dance ‘be
while mistakes > O '3 ‘a '1 ‘1 1 2

mistakes I O;
for 2' z 1 : n function fIDlSCRlMINANT_FUNCTION(a, X, kernel, Xtest)
if 0/K(:,i)y(2') g O n, d I Sl2e(X);
an) I an) + w); K I, [1;
mistakes I mistakes + 1; for 7“ I 1 3 n /
end ati I X(z, z) ;
end 5139' I Xtest;
end K(i) I eval(kernel);
end
Source: MIT, 2OOSrt'aél, Tommy Jaakkola, HW2, f : O/K;

***************Ending Page***************

