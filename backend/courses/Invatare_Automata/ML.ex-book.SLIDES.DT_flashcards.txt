[{content={parts=[{text=```text
--FlashCardSeparator--
Single
--InteriorSeparator--
What algorithm does the page mention as containing examples 34, 36, 2, 4, 12, 10, 47, 15, 19, 20?
--InteriorSeparator--
ID3
--InteriorSeparator--
easy
--InteriorSeparator--
1
--FlashCardSeparator--
Single
--InteriorSeparator--
What is the CMU year associated with the information gain example?
--InteriorSeparator--
2013
--InteriorSeparator--
easy
--InteriorSeparator--
2
--FlashCardSeparator--
Single
--InteriorSeparator--
What two features does Timmy collect data on for his ML exam model?
--InteriorSeparator--
“whether stay up late before exam” (S) and “whether attending all the classes” (A)
--InteriorSeparator--
medium
--InteriorSeparator--
3
--FlashCardSeparator--
Single
--InteriorSeparator--
According to the provided information, which feature (S or A) should Timmy choose for the first split, and what is the information gain?
--InteriorSeparator--
Choose feature A, IG(all, A) = 0.557728
--InteriorSeparator--
hard
--InteriorSeparator--
4
--FlashCardSeparator--
Single
--InteriorSeparator--
Who are the authors mentioned regarding convenient formulas for entropy in 2017?
--InteriorSeparator--
Sebastian Ciobanu, Liviu Ciortuz
--InteriorSeparator--
easy
--InteriorSeparator--
5
--FlashCardSeparator--
Single
--InteriorSeparator--
In the decision stump example with counts a, b, c, d, e, and f, what does the variable 'Y' represent?
--InteriorSeparator--
Binary output variable (label)
--InteriorSeparator--
medium
--InteriorSeparator--
6
--FlashCardSeparator--
Multiple
--InteriorSeparator--
What condition(s) must be true to use the simplified information gain formulas on a pocket calculator?
--InteriorSeparator--
(right) All counts (c, d, e, f) are strictly positive
(wrong) Any of the counts can be zero
(wrong) All a,b,c,d,e, and f must be equal
--InteriorSeparator--
hard
--InteriorSeparator--
7
--FlashCardSeparator--
Single
--InteriorSeparator--
What is a serious problem to be aware of when using the provided formulas on a pocket calculator?
--InteriorSeparator--
The internal capacity of representation for intermediate results can be overflown.
--InteriorSeparator--
medium
--InteriorSeparator--
8
--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to the text, if a count of 'c' in the decision stump is zero, what changes in the mean conditional entropy formula?
--InteriorSeparator--
(right) The term involving 'c' is removed
(wrong) The formula remains the same
(wrong) You must recalculate all the counts
--InteriorSeparator--
hard
--InteriorSeparator--
7
--FlashCardSeparator--
Single
--InteriorSeparator--
In Romanian, what should you replace log2 with in the formulas if you are using a pocket calculator?
--InteriorSeparator--
ln or lg
--InteriorSeparator--
medium
--InteriorSeparator--
11
--FlashCardSeparator--
Single
--InteriorSeparator--
What university and year is associated with the mushroom dataset example?
--InteriorSeparator--
CMU, 2002(?)
--InteriorSeparator--
easy
--InteriorSeparator--
12
--FlashCardSeparator--
Single
--InteriorSeparator--
In the mushroom dataset example, what differentiates mushrooms A through H from U through W?
--InteriorSeparator--
Whether or not they are known to be poisonous.
--InteriorSeparator--
medium
--InteriorSeparator--
13
--FlashCardSeparator--
Multiple
--InteriorSeparator--
In the mushroom classification example, what attributes are considered?
--InteriorSeparator--
(right) Smooth
(right) Smelly
(wrong) Color
(wrong) Size
--InteriorSeparator--
medium
--InteriorSeparator--
14
--FlashCardSeparator--
Single
--InteriorSeparator--
What is the entropy of Edible in the mushroom example dataset (mushrooms A through H)?
--InteriorSeparator--
0.9544
--InteriorSeparator--
medium
--InteriorSeparator--
15
--FlashCardSeparator--
Single
--InteriorSeparator--
According to the text, what is a sufficient action in determining the best attribute to select, instead of calculating actual information gains?
--InteriorSeparator--
To compare values of average conditional entropies
--InteriorSeparator--
hard
--InteriorSeparator--
19
--FlashCardSeparator--
Single
--InteriorSeparator--
What is the classification outcome if Smooth is 0 AND Smelly is 0, OR Smooth is 1 AND Smelly is 1?
--InteriorSeparator--
Edible
--InteriorSeparator--
medium
--InteriorSeparator--
22
--FlashCardSeparator--
Single
--InteriorSeparator--
What is the university and year for the ID3 greedy character example?
--InteriorSeparator--
CMU, 2003
--InteriorSeparator--
easy
--InteriorSeparator--
23
--FlashCardSeparator--
Multiple
--InteriorSeparator--
In the greedy character example, what is the output attribute Y actually representing?
--InteriorSeparator--
(right) A XOR B
(wrong) A AND B
(wrong) A OR B
--InteriorSeparator--
hard
--InteriorSeparator--
28
--FlashCardSeparator--
Single
--InteriorSeparator--
What characteristic of the ID3 algorithm causes it to sometimes not produce the optimal tree?
--InteriorSeparator--
The "greedy" character
--InteriorSeparator--
medium
--InteriorSeparator--
29
--FlashCardSeparator--
Single
--InteriorSeparator--
What type of attributes are present in the dataset used in the ID3 example from CMU, 2012?
--InteriorSeparator--
Categorical and continuous
--InteriorSeparator--
medium
--InteriorSeparator--
30
--FlashCardSeparator--
Single
--InteriorSeparator--
In the extrasolar planets example, which split occurs at the root level of the final decision tree?
--InteriorSeparator--
Size
--InteriorSeparator--
medium
--InteriorSeparator--
33
--FlashCardSeparator--
Single
--InteriorSeparator--
In the habitable planets example, according to the final decision tree, would a planet with the features (Big, Near, 280) be predicted to be habitable or not habitable?
--InteriorSeparator--
Habitable
--InteriorSeparator--
medium
--InteriorSeparator--
38
--FlashCardSeparator--
Single
--InteriorSeparator--
Which university and year is associated with the decision zones example?
--InteriorSeparator--
CMU, 2002
--InteriorSeparator--
easy
--InteriorSeparator--
39
--FlashCardSeparator--
Single
--InteriorSeparator--
What does LOOCV stand for?
--InteriorSeparator--
Leave-one-out cross-validation
--InteriorSeparator--
easy
--InteriorSeparator--
40
--FlashCardSeparator--
Single
--InteriorSeparator--
What is the key feature of Algorithm DT2 in the context of decision trees?
--InteriorSeparator--
It learns a decision tree with only two leaf nodes (i.e., only one split).
--InteriorSeparator--
medium
--InteriorSeparator--
40
--FlashCardSeparator--
Single
--InteriorSeparator--
What are the two alternative impurity measures to entropy mentioned for splitting criteria in decision trees?
--InteriorSeparator--
Gini Impurity and Misclassification Impurity
--InteriorSeparator--
medium
--InteriorSeparator--
53
--FlashCardSeparator--
Single
--InteriorSeparator--
In terms of impurity measures, what is Drop-of-Impurity defined as for binary category splits?
--InteriorSeparator--
The difference between the impurity of the current node and the weighted sum of the impurities of children
--InteriorSeparator--
medium
--InteriorSeparator--
55
--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to the text, what is a problem with the Drop-of-Impurity based on Misclassification?
--InteriorSeparator--
(right) With some conditions, Ai(n) can be 0 for all attributes
(wrong) Too computationally intensive
(wrong) Never produces accurate results
--InteriorSeparator--
hard
--InteriorSeparator--
58
--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is a top-down approach to pruning decision trees?
--InteriorSeparator--
(right) Starts at the root and prunes splits with information gain below a threshold
(wrong) Starts at the leaves and prunes upwards
(wrong) Doesn't use information gain
--InteriorSeparator--
medium
--InteriorSeparator--
60
--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is a key advantage of top-down pruning compared to bottom-up pruning?
--InteriorSeparator--
(right) Top-down is computationally cheaper
(wrong) Top-down pruning is more accurate
(wrong) Bottom-up is computationally cheaper
--InteriorSeparator--
medium
--InteriorSeparator--
63
--FlashCardSeparator--
Single
--InteriorSeparator--
What statistical test is employed in the described decision tree pruning algorithm?
--InteriorSeparator--
Pearson’s chi-square test
--InteriorSeparator--
medium
--InteriorSeparator--
65
--FlashCardSeparator--
Single
--InteriorSeparator--
What is the null hypothesis in the chi-square test for pruning decision trees?
--InteriorSeparator--
The attribute is independent of the class labels
--InteriorSeparator--
medium
--InteriorSeparator--
65
--FlashCardSeparator--
Single
--InteriorSeparator--
If the p-value from the chi-square test is less than 0.05, what decision do we make regarding the null hypothesis?
--InteriorSeparator--
We reject the null hypothesis.
--InteriorSeparator--
medium
--InteriorSeparator--
67
```}], role=model}, finishReason=STOP, avgLogprobs=-0.25090692434484774}]