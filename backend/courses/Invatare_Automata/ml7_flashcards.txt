[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What is the primary goal of Computational Learning Theory?
--InteriorSeparator--
To characterize the number of training examples necessary/sufficient for successful learning and identify inherently difficult/easy to learn concept classes.
--InteriorSeparator--
easy
--InteriorSeparator--
2
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following factors are considered when seeking answers in Computational Learning Theory?
--InteriorSeparator--
(right) Sample complexity
(right) Computational complexity
(wrong) Algorithm name
(wrong) Hardware specifications
--InteriorSeparator--
medium
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What type of learning is primarily focused on in the provided material?
--InteriorSeparator--
Inductive learning.
--InteriorSeparator--
easy
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does PAC stand for in the context of the PAC learning model?
--InteriorSeparator--
Probably Approximately Correct.
--InteriorSeparator--
easy
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the context of the presented material, what type of functions are being learned?
--InteriorSeparator--
Boolean functions.
--InteriorSeparator--
easy
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define the "true error" of a hypothesis, errorD(h).
--InteriorSeparator--
The probability that h will misclassify a single instance drawn at random according to the distribution D.
--InteriorSeparator--
medium
--InteriorSeparator--
7
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Why is it often necessary to approximate the true error of a hypothesis?
--InteriorSeparator--
Because learners may be misled by random training examples and may not be able to find a hypothesis with zero true error.
--InteriorSeparator--
medium
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define PAC-learnability.
--InteriorSeparator--
A class C is PAC-learnable by L using H if for all concepts in C, distributions, and error/confidence parameters, L outputs a hypothesis in H with error less than the error parameter with at least the confidence parameter, in polynomial time.
--InteriorSeparator--
medium
--InteriorSeparator--
10
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to the text, what needs to be shown to prove class C is PAC learnable?
--InteriorSeparator--
(right) Each concept in C can be learned from a polynomial number of examples.
(right) The processing time for each example is polynomially bounded.
(wrong)  The hypothesis space must equal 2^X
(wrong) A zero training error hypothesis can always be found.
--InteriorSeparator--
medium
--InteriorSeparator--
11
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Why is sample complexity important in machine learning?
--InteriorSeparator--
Because limited success in practical settings is often due to limited available training data.
--InteriorSeparator--
medium
--InteriorSeparator--
13
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a "consistent learner"?
--InteriorSeparator--
A learner that perfectly fits the training data.
--InteriorSeparator--
easy
--InteriorSeparator--
14
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define when a version space VSH,D is ε-exhausted.
--InteriorSeparator--
VSH,D is ε-exhausted with respect to the target concept c and the training set D if errorD(h) < ε, for all h ∈ VSH,D.
--InteriorSeparator--
medium
--InteriorSeparator--
15
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
According to Haussler (1988), what is the probability that VSH,D is not ε-exhausted?
--InteriorSeparator--
Less than |H|e^(-εm)
--InteriorSeparator--
hard
--InteriorSeparator--
16
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the sufficient number of training examples (m) needed to ensure a consistent hypothesis will be probably approximately correct (PAC)?
--InteriorSeparator--
m >= (1/ε) * (ln|H| + ln(1/δ))
--InteriorSeparator--
hard
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the size of the hypothesis space (|H|) for the EnjoySport example, as given in Chapter 2?
--InteriorSeparator--
973
--InteriorSeparator--
easy
--InteriorSeparator--
18
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the size of the hypothesis space (|H|) defined by conjunctions of literals based on n boolean attributes possibly with negation?
--InteriorSeparator--
3^n
--InteriorSeparator--
medium
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Are k-term DNF expressions PAC-learnable?
--InteriorSeparator--
No.
--InteriorSeparator--
medium
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Are k-CNF expressions PAC-learnable?
--InteriorSeparator--
Yes.
--InteriorSeparator--
medium
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the size of the hypothesis space for unbiased learners with instances described by n boolean features?
--InteriorSeparator--
2^(2^n)
--InteriorSeparator--
medium
--InteriorSeparator--
22
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does Agnostic Learning assume?
--InteriorSeparator--
Agnostic learning doesn’t assume c ∈ H, therefore c may or may not be perfectly learned in H.
--InteriorSeparator--
easy
--InteriorSeparator--
23
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define a "dichotomy" of a set S.
--InteriorSeparator--
A partition of S into two disjoint subsets.
--InteriorSeparator--
medium
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
When is a set of instances S said to be "shattered" by a hypothesis space H?
--InteriorSeparator--
If and only if for every dichotomy of S there exists some hypothesis in H consistent with this dichotomy.
--InteriorSeparator--
hard
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define the Vapnik-Chervonenkis dimension, VC(H).
--InteriorSeparator--
The size of the largest finite subset of X shattered by H.
--InteriorSeparator--
medium
--InteriorSeparator--
27
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
If X = R and H is the set of all intervals on the real number line, what is VC(H)?
--InteriorSeparator--
2.
--InteriorSeparator--
medium
--InteriorSeparator--
27
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
If X = R^n and H is the set of all linear decision surfaces in R^n, what is VC(H)?
--InteriorSeparator--
n+1
--InteriorSeparator--
hard
--InteriorSeparator--
28
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the VC dimension for H consisting of conjunctions of up to n boolean literals?
--InteriorSeparator--
n.
--InteriorSeparator--
hard
--InteriorSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
According to Blumer et al. (1989), what is the number of randomly drawn examples that suffice to ε-exhaust VSH,D with probability at least (1 - δ)?
--InteriorSeparator--
m >= O(VC(H) log2(13/ε) + log2(2/δ))
--InteriorSeparator--
hard
--InteriorSeparator--
30
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How does the mistake bound model of learning differ from PAC learning?
--InteriorSeparator--
The mistake bound model focuses on bounding the number of mistakes a learner makes before converging, rather than the number of examples needed for learning.
--InteriorSeparator--
medium
--InteriorSeparator--
32
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the significance of mistake bounds in practical settings?
--InteriorSeparator--
Mistake bounds are especially important in settings where the system learns while in use, such as detecting fraudulent credit card transactions.
--InteriorSeparator--
medium
--InteriorSeparator--
33
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How many mistakes does FIND-S make before converging if H consists of conjunctions of up to n boolean literals?
--InteriorSeparator--
n+1
--InteriorSeparator--
hard
--InteriorSeparator--
34
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How does the HALVING algorithm classify new instances?
--InteriorSeparator--
By taking the majority vote of version space members.
--InteriorSeparator--
medium
--InteriorSeparator--
36
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How many mistakes does the HALVING algorithm make before converging?
--InteriorSeparator--
At most [log2 |H|].
--InteriorSeparator--
hard
--InteriorSeparator--
36
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the key characteristic of the WEIGHTED-MAJORITY algorithm?
--InteriorSeparator--
It takes a weighted vote among a pool of prediction algorithms and learns by altering the weight associated with each prediction algorithm.
--InteriorSeparator--
medium
--InteriorSeparator--
38
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the WEIGHTED-MAJORITY algorithm's relative mistake bound?
--InteriorSeparator--
For δ = 1/2, the number of mistakes made is at most 2.41(k + log2 n), where k is the minimum number of mistakes made by any algorithm in A when training on D.
--InteriorSeparator--
hard
--InteriorSeparator--
40
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define Opt(C).
--InteriorSeparator--
The optimal mistake bound for C, representing the number of mistakes for learning the hardest concept in C with the hardest training sequence, using the best algorithm.
--InteriorSeparator--
hard
--InteriorSeparator--
42
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the relationship between VC(C), Opt(C), MHalving(C), and log2(|C|)?
--InteriorSeparator--
VC(C) <= Opt(C) <= MHalving(C) <= log2(|C|)
--InteriorSeparator--
hard
--InteriorSeparator--
42
--FlashCardSeparator--
}], role=model}, finishReason=STOP, citationMetadata={citationSources=[{startIndex=9494, endIndex=9616, uri=https://www.scribd.com/document/754374753/ML-Chapter-7-CLT-Notes}]}, avgLogprobs=-0.1066993896118895}]