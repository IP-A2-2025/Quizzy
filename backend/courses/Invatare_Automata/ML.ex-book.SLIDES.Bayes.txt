***************Beginning Page***************
***************page number:1**************
1.
Q O O
Bayes1an Class1ﬁoatlon
Contains:
MAP Hypotheses: eX. PS-42 from Foundations ch. (LDP), 1, 2, 3, 4,

Naive Bayes and Joint Bayes: eX. 7, s, 6, 33, 11, 35, 13, 14, 38,

Gaussean Bayes classiﬁers ex. 15, 21, 45, 17, 18, 19
from the 2023f version of the ML exercise book by L. Ciortuz et a1.

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2.
A simple application 0f
Bayes’ formula
CMU, 2006 fall, Tom Mitchell, Eric Xing, midterm, pr. 1.3

***************Ending Page***************


***************Beginning Page***************
***************page number:3**************
3.
Suppose that in answering a question in a multiple choice test, an ewaminee
either knows the answer, with probability p, or he guesses with probability
l — p.
Assume that the probability of answering a question correctly is 1 for an
examinee who knows the answer and 1 /m for the examinee who guesses,
where m is the number of multiple choice alternatives.
What is the probability that an examinee knew the answer to [such] a ques-
tion, given that he has correctly answered it?
Answer:
P t k - P k
P<kn6w,cwect) I w
P(co’rrect)
_ P(correct|l<:new) - P(knew)
— P(correct|knew) - P(l<7neu)) + P(correct\guessed) - P(guessed)'

Notice that in the denominator we had to consider the two ways (s)he can
get a question correct: by knowing, or by guessing. Plugging in, we get:
m _ i

1 _ m + l — '
p + — - (1 — p) p p

m

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
Maximum A posteriory Probability
(MAP) Hypotheses

***************Ending Page***************


***************Beginning Page***************
***************page number:5**************
5.
Exemplifying

o the application of Bayes’ theorem
o the notion of MAP (Maximum A posteriori Probability) hypotheses
o the computation of expected values for discrete random variables

and
o the [use of] sensitivity and speciﬁcity of a test

in a real-world application

CMU, 2009 fall, Geoff Gordon, HWl, pr. 2

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
6.
There is a disease which affects 1 in 500 h
people. A 100.00 dollar blood test can C
help reveal whether a person has the
disease. A positive outcome indicates
that the person may have the disease.
The test has perfect sensitivity (true
positive rate), i.e., a person Who has tn
the disease tests positive 100% of the
t1me: ‘However, the. test has. 99% sensitivity (or: recall).- t tp
speclﬁclty (true negat'we rate), 1.e., a P + f”
healthy person tests positive 1% of the _ _ tn
time. speczﬁczty: W?

***************Ending Page***************


***************Beginning Page***************
***************page number:7**************
7.
a. A randomly selected individual is tested and the result is pos-
itive.
What is the probability of the individual having the disease?
b. There is a second more expensive test which costs 10, 000.00
dollars but is exact with 100% sensitivity and speciﬁcity.
If we require all people who test positive with the less expensive
test to be tested with the more expensive test, what is the expected
cost to check whether an individual has the disease‘?
c. A pharmaceutical company is attempting to decrease the cost
of the second (perfect) test.
How much would it have to make the second test cost, so that the
ﬁrst test is no longer needed? That is, at what cost is it cheaper
simply to use the perfect test alone, instead of screening with the
cheaper test as described in part b?

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
8.
Answer:
Let’s deﬁne the following random variables:
B’ 1 / true for persons affected by that disease,
' 0 / false otherwise;
T1: the result of the ﬁrst test: + (in case of disease) or — (otherwise);
T2: the result of the second test: again + or —.
P< B) I ﬁ tn h
Known facts: P(T1 I + \ B) : 1, P(T1 : + | B) : ﬁ, €
P(T2:+yB):1,P(T2:+|B):0 \
c a
a.
a es P T I B ~P B
P(B ‘ T1 I +> TBIy BM
P(T1 : + | B) -P(B) +P(T1 I + \ B) -P(B)
1 1
_ ' ﬁ _ @ N
_ 1 i+i %_599~0.1669:>
_ 500 100 500 _
P(B \ T1 : +) I 0.8331 > P(B | T1 : +). Therefore, B is the MAP hypothesis.

***************Ending Page***************


***************Beginning Page***************
***************page number:9**************
9.
b.
Let’s consider a new random variable:
C 01 if the person only takes the ﬁrst test
_ 01 + 02 if the person takes the two tests
i P(C I cl) I P(T1 I —) and P(C I 01 —|— 02) I P(T1 I +)

I (:1 _Cl -P(T1 :+)+c1-P(T1 :+)+cQ-P(T1 :+)

I 61+(32-P(T1 2+)

— 100 + 10000 g — 219 8 ~ 220$

_ 50000 _ ' N
Note: Here above we used

P<T1 : +) total probgz'lity form. P<T1 : + ‘ B) . P(B) + P(T1 : + I B) 'P(B)
1 1 499 599
: 1-— —-—:—:.11
500 + 100 500 50000 0 0 98

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
10.
c.
cn n25. the new price for the second test (T 2’)
Cn g E[C'] I (:1 - P(C : cl) + (01 + an) - P(C I (:1 + an)
599
I n-PT: :100 n-—
cl+c (1 +) +0 50000
Cn I 100 —|— Cn - 0.01198 I> Cn z 101.2125.

***************Ending Page***************


***************Beginning Page***************
***************page number:11**************
11.
The “Monty’s haunted house” problem
0 Exemplifying the application of Bayes’ theorem
o the notion of MAP (Maximum A posteriori Probability) hypotheses
CMU, 2009 fall, Geoff Gordon, HWl, pr. 1

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
12.
You are in a haunted house and you are stuck in front of three doors. A ghost
appears and tells you: “Your hope is behind one of these doors. There is
only one door that opens to the outside and the two other doors have deadly
monsters behind them. You must choose one door!” You choose the ﬁrst
door. The ghost tells you: “Wait! I will give you some more information.”
The ghost opens the second door and shows you that there was a horrible
monster behind it, then asks you: “Would you like to change your mind and
take the third door instead?”
What’s better: to stick with the ﬁrst door, or to change to the third door?
For each of the following strategies used by the ghost, determine probabilities
that the exit is behind the ﬁrst and the third door, given that the ghost
opened the second door.
a. The ghost always opens a door you have not picked with a monster behind
it. If both of the unopened doors hide monsters, he picks each of them with
equal probability.
b. The ghost has a slightly different strategy. If both of the unopened doors
hide monsters, he always picks the second door.
c. Finally, suppose that if both of the unopened doors hide monsters, the
ghost always picks the third door.

***************Ending Page***************


***************Beginning Page***************
***************page number:13**************
1 .
Answer 3
What we know:
IE P<G I 0>
II———
———-—-
1--——-
POIl IPOI2 IPOI3 I— 1
<><><>3<> ———-“—-
III-II.-
What we must compute:
BayesF. P(G:2‘O:1)P(O:1)
P I 1 I 2 : —
(O ‘G ) P(GI2|OI1)-P(OI1)—|—P(GI2\OI3)-P(OI3)
BayesF. P(G:2‘O:3)P(O:3)
P O I 3 G I 2 : —
( ‘ > P(GI2|OI3)-P(OI3)+P(GI2\OI1)-P(OI1)
Notice that we should have added P(G I 2 | O I 2) - P(O I 2) at the denominator, but
we know that it is O since P(G I 2 \ O I 2) I O.
Therefore, P(OI3|GI2)I1—P(OI1|GI2).

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
14.
Variant a:
1 1
_ ' _ 1 1 2
P<OI11GIZ>IA> P<0:3|G:2>:1__:_
1 1 1 3 3 3
_ . _ + 1. _
2 3 3
Therefore, we should choose door 3.
Variant b:
1
1' g 1 1 1
P(O:1\G:2):i:— P(O:3|G:2):1——:—
1_ _ + 1_ _ 2 2 2
3 3
Therefore, we can choose either door 1 or door 3.
Variant c:
P(O:1|G:2):O P(O:3\G:2):1—O:1
Therefore, we should choose door 1.

***************Ending Page***************


***************Beginning Page***************
***************page number:15**************
O Q 15-
Alternatlve Solutlon (1)
(in Romanian)
Echivalent, in cazul variantei a, pentru a determina maximul dintre P(O : 1 |
G : 2) si P(O I 3 | G : 2) ar ﬁ fost suﬁcient, conform formulei lui Bayes, sa
comparam
P(G:2|O:1)-P(O:1) siP(G:2\O:3)-P(O:3).

Mai mult, tinand cont de relatia (1), aceasta revine la a compara

P(G:2]O:1) siP(G:2\O:3).
Raspunsul poate ﬁ citit imediat din tabelul de mai sus (vezi prima linie si
penultima linie): O I 3 este varianta pentru care se obtine probabilitatea [a
posteriori] maxima.
Altfel spus, O I 3 este ipoteza de probabilitate maxima a posteriori (engl.,
maximum a posteriori probability (MAP) hypothesis).
Absolut similar se poate proceda si pentru variantele b si c.
Observatie: in cazuri precum cel de mai sus (P(O : 1) : P(O : 2) : P(O :
3) I é), ipoteza MAP coincide cu ipoteza de verosimilitate maxima (engl.,
maximul likelihood (ML) hypothesis).

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
16.
Alternative Solution (2)
(in Romanian)
1 2 3

Echivalent, pentru variantele b §i c putem raspunde la intrebare facand
urmatorul ra§ionament, fara a folosi formula lui Bayes:
Ie§irea cea buna se poate gasi in spatele uneia dintre cele trei u§i
(vezi ﬁgura alaturata). Cum fantoma a deschis deja u§a cu numarul 2, m
una dintre aceste situaigii (§i anume, a doua din ﬁgura) este eliminata,
ﬁindca in spatele ei este un monstru. in continuare, putem ra§iona in
felul urmator:
Varianta b: intrucat fantoma alege u§a 2 cu probabilitate 1, putem afirma, (in lipsa
unor alte 1nforma§11) ca ambele varlante — 1 §1 3 — au probabllltagl egale, §1 anume 5.
intr-adevar,

— ﬁe u§a 1, cea aleasa de mine, d5 inspre afara, iar atunci fantoma trebuie, conform principiului P2, sa

aleaga u§a 2;

— ﬁe u§a 3 da inspre afara, iar atunci, din nou conform principiului P2, fantoma trebuie s5 aleaga u§a 2;

— conform principiului P1, nu exista 0 a treia posibilitate;

— nu dispun de alte informatii pentru a decide intre cele doua situatii de mai sus.
Varianta c: Stiind ca fantoma nu a deschis u§a 3 (care ar ﬁ op§iunea corespunzatoare
principiului P2), ci a ales u§a 2, inseamna ca nu a putut face altfel, deci u§a 3 reprezinta
ie§irea.

***************Ending Page***************


***************Beginning Page***************
***************page number:17**************
17.
Exemplifying
0 the application 0f Bayes’ theorem
0 the notion 0f MAP (Maximum A posteriori Probability) hypotheses
CMU, 2012 spring, Ziv Bar-Joseph, HWl, pr. 1.5

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
18.
Mickey tosses a die multiple times, hoping for a 6.
The sequence of his 10 tosses is 1,3,4, 2,3,3, 2,5, 1,6.
Mickey is suspicious whether the die is biased towards 3 (or fair).
Conduct a simple analysis based on the Bayes theorem to inform
on Mickey — to what degree is the die biased?
Explain your reasoning.
Assume in general every 100 dice contain 5 unfair dice that are
biased towards 3 with the probability distribution of the siX faces
(1,2,3,4,5,6) as P I [0.1, 0.1, 0.5, 0.1, 0.1, 0.1].

***************Ending Page***************


***************Beginning Page***************
***************page number:19**************
19.
Solution
Deﬁnition for the notion of Maximum A posteriori Probability:
e . P D h - P h
hMAP dzf argmaXP(h|D) Fg' argmax M I argmaXP(D\h) - P(h),
hGH heH 13(1)) heH

Let us denote:

o D : {1,3,4,2,3,3,2,5,1,6}: {x1,x2,...,:c10}

o H I {FD, LD}, Where FD is the fair die and LD is the loaded die.

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
20.
10 10
2.2‘.d. 1 95
P(D|FD) - P(FD) : P(x1,x2,...,:1:10|FD) -P(FD) : (H P(wZ\FD)) - P(FD) : 6 -m
1:1
1 19
I 210 , 310 ' E
. ' 10
P(D|LD) -P(LD) I P(x1,x2,.. .,a:10|LD) - P(LD) “z'd' (H P(xZ-|LD)) - P(LD)
1:1
_11111111115_11_11
_ 10 2 10 10 2 2 10 10 10 10 100 _ 107-23 20 _ 210-57 20'
In order to compare P(D|FD) -P(FD) and P(D|LD) -P(LD), it is easier to ﬁrstly apply the
ln:
1n P(D|FD) - P(FD) > 1n P(D|LD) -P(LD) (I)
1 1
ln 3T?) > ln F @11119 — 101103 > 71115 ¢> —8.0417 > —ll.2661
Note: We could have directly computed the so-called log-odds ratio:
P(LD\D) P(D\FD) -P(FD)
l—:l —:...:—3.2244<0 h t h FD.
nP(FD|D) nP(D|LD)-P(LD) sowe ave oc oose

***************Ending Page***************


***************Beginning Page***************
***************page number:21**************
21.
Exemplifying
ML hypotheses and MAP hypotheses
using decision trees
CMU, 2009 spring, Tom Mitchell, midterm, pr. 2.3-4

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
22.
—|—9—9—|—<P—<P—'—|—>
11.5 22.5 33.5 4X
Let’s consider the 1-dirnensional data set shown above, based on the single
real-valued attribute X. Notice there are two classes (values of Y), and ﬁve
data points.
Consider a special type of decision trees where leaves have probabilistic la-
bels. Each leaf node gives the probability of each possible label, where the
probability is the fraction of points at that leaf node with that label.
For example, a decision tree learned from the data set above with zero splits
would say P(Y : 1) : 3/5 and P(Y : 0) : 2/5. A decision tree with one split
(at X I 2.5) would say P(Y I 1): 1 if X < 2.5, and P(Y I 1): 1/3 if X Z 2.5.
Solution:
a. For the above data set, draw a tree that maxi-
mizes the likelihood of the data. @
TML : argmaXT PT(D), where Nu Da
de . i.i.d.
PM?) If P<D|T> I HLPO/ I yilX I m). REE] w
where y,- is the label/classs of the instance x7; Nu [)3
($1 I 1.5, 5:2 I 2, $3 I 3,154 I 3.5, :65 I 3.75.) w @I

***************Ending Page***************


***************Beginning Page***************
***************page number:23**************
23.

b. Consider a prior probability distribution P (T ) over trees that penalizes the number

of splits in the tree.

1 spli1fs(T)2
P T —
< >O< (4)

Where T is a tree, splits(T) is the number of splits in T, and o< means “is proportional

to”.

For the same data set, give the MAP tree When using this prior, P(T), over trees.

Solution:
0 nodes: 3 2 0
PT D — -— -— :—:—:0.0336
< 0| )0‘ <5) <5) (4) 55 3125
1 node:
2 2 1 1 1 1 @
PT D 12- — -—- — :—:0.037
(1| )O< (3) 3 <4) 27 Nu la
2 nodes:
1 4 1 4 1
P(T2) o< (1) :> P(T2 | D) oc 1- (1) I ﬁ I 0.0039 :> the MAP tree is T1.

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
The Naive Bayes and Joint Bayes
Algorithms

***************Ending Page***************


***************Beginning Page***************
***************page number:25**************
25.
Exemplyifying the application of
Naive Bayes and Joint Bayes algorithms;
the minimum number of parameters
to be estimated for NB and respectively J B
CMU, 2008 fall, Eric Xing, HWl pr. 2

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
26.
Cons1der a classiﬁcatlon problem: the ta- ——|
ble of observatlons for wh1ch lS glven
. 0 0 O 2
nearby. X1 and X2 are two b1nary random
. . . 0 0 1 18
var1ables wh1ch are the observed var1- 1 0 0 4
ables. Y is the class label which is ob-
. . . 1 O 1 1
served for the tralnlng data glven below. 0 1 O 4
We will use the Naive Bayes classiﬁer and
. . . 0 1 1 1
the J 01nt Bayes class1ﬁer to class1fy a new
. . . 1 1 O 2
1nstance after tra1n1ng on the data below,
1 1 1 18
and compare the results.
a. Construct the Naive Bayes classiﬁer given the data above. Use
it to classify the instance X1 I O, X2 I O.
b. Construct the Joint Bayes classiﬁer given the data above. Use
it to classify the instance X1 = 0, X2 : 0.

***************Ending Page***************


***************Beginning Page***************
***************page number:27**************
27.

c. Compute the probabilities P(Y I lle I O, X2 I O) for the Naive
Bayes classiﬁer (let’s denote it PNB(Y I 1|X1 I 0,X2 I 0)) and for
the Joint Bayes classiﬁer (PJB(Y I lle I 0,X2 I 0)).
Why is PNB(Y I 1|X1 I 0,X2 I 0) different from PJB(Y I lle I
0,X2 I 0)? (Hint: Compute P(X1,X2|Y).)

——|
d. What happens to the difference be- 0 () () 3
tween PNB(Y I lle I O,X2 I O) and 0 () 1 9
PJB<Y I 1|X1 I O,X2 I O) if the table en- 1 () () 3
tries are changed to the nearby table? 1 () 1 9
(Hint: Will the Naive Bayes assumption 0 1 0 3
be more violated or less violated compared 0 1 1 9
to the previous situation?) 1 1 0 3

l l l 9

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
28.
e. Compare the number of independent parameters in the two
classiﬁers. Instead of just two observed data variables, if there
were n random binary observed variables {X1, . . . , Xn}, what would
be the number of parameters required for both classiﬁers?
From this, what can you comment about the rate of growth of the
number of parameters for both models as n e oo?

***************Ending Page***************


***************Beginning Page***************
***************page number:29**************
29.
Answer
a.
A de.
yNB(X1 I 0,X2 I 0) If argmaXP(X1 I 0|Y I y) -P(X2 I 0|Y I y) -P(Y I y)
1161071}
p0 "It P(X1 I 0)Y I 0) - P(X2 I 0|Y I 0) - P(Y I 0)
MAE E E E _ i _ i
_ 12 12 50_50_100
p1 "It P(X1 I 0)Y I 1) - P(X2 I 0|Y I 1) - P(Y I 1)
M_LE 19 19 ﬁ _ 2
_ 38 38 50 _ 100
P0 < P1 :> @NB(X1 I 07X2 I 0) : 1.

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
30.
b. d
gJB(X1 I 0,X2 I 0) If‘ argmaXP(X1 I 0,X2 I 0yY I y) - P(Y I y)
y€{071}
, not. MLE 2 12 2
/ not. MLE18 38 18
I PXI XI Y:1-PY:1 I_-_I_
p6 <p’1 I» @JB(X1 I 0,X2 I 0) I 1.

***************Ending Page***************


***************Beginning Page***************
***************page number:31**************
31.
C.
PNB "2' P(Y: 1|X1 :0,X2:0)
F.B_ayes P(X1:O,X2:0|Y:1)-P(Y:1)
_ W
indegcdt. P(X1:0]Y:1)-P(X2:0|Y:1)-P(Y:1)
P(X1 I 0|Y I 1) - P(X2 I 011/ I 1) - P(Y I 1)
19
_ P1 _ ﬁ _E
— P0+P1—i+£—25
100 100
PJB "ét'P(1/:1|X1:0,X2:0)
F.B_ayes P(X1:0,X2:0|Y:1)-P(Y:1)
— W
18
I _P’1 I E IE
p6+pll Z+E 20
50 50

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
32.
PNB 75 PJB because [in this case] the conditional independence assumption
doesn’t hold.
Indeed,
2
P(X1 :0,X2:0|Y:0)M:LE—
12 6 6 1 i
MLE
PX: Y: -PX: Y: :—-—:—
<10| 0) <2 0|0>1212 4
:>P(X1:0,X2:0\Y:0)75P(X1:0]Y:0)-P(X2:0\Y:0):>
Therefore X1 and X2 are not conditionally independent W.r.t. Y.

***************Ending Page***************


***************Beginning Page***************
***************page number:33**************
33.
d.
PNBIP(YI1)X1I0,X2I0)
_ P(X1:O,X2:0|Y:1)-P(Y:1)
_P(X1I0,X2I0|YI1)P(YI1)+P(X1I0,X2I0)YI0)P(YI0)
_ P(X1 I O|Y I 1) - P(X2 I 011/ I 1) - P(Y I 1)
_ P(X1 I OIY I 0) -P(X2 I 0|Y I 0) - P(Y I 0)+
P(X1 I 011/ I 1) - P(X2 I O|Y I 1) - P(Y I 1)
6 5 3 3
I 36 36 43 I 43 22:5
6 6 12+13E§ 3+3 12 4
12 12 43 36 36 43 43 43
PJB:P(Y:1\X1:0,X2:0)
_ P(X1:0,X2:0|Y:1)-P(Y:1)
_P(X1I0,X2I0|YI1)P(YI1)+P(X1I0,X2I0)YI0)P(YI0)
9 36 9
_ £5 _ 4_3 _2_§ - - _
_ 3 E+g %_3+2_12_4. Therefore, 1nth1scase PNB—PJB-
12 43 36 43 43 43

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
34.
In fact, it can be easily shown that for the newly given distribution the con-
ditional independence assumption (for X1 and X2 w.r.t. Y) holds. Therefore,
the predictions made by Naive Bayes and Joint Bayes Will coincide.

***************Ending Page***************


***************Beginning Page***************
***************page number:35**************
35.

e. For our dataset, Naive Bayes needs to compute the following probabilities:

P(Y:0) :> P(Y:1):1—P(Y:O)

P(X1:0|Y:0) :> P(X1:1|Y:O):1—P(X1:O|Y:O)

P(X1:0|Y:1) :> P(X1:1|Y:1):1—P(X1:O|Y:1)

P(X2:O|Y:O) i P(X2:1|Y:O):1—P(X2:0|Y:0)

P(X2:0|Y:1) :> P(X2:1|Y:1):1—P(X2:O|Y:1)
Therefore, we will need only 5 values in order to fully construct the Naive
Bayes classiﬁer.
In the general case, when n input attributes / variables are given, we need to
estimate P(Y), P(XZ- l Y) and P(XZ- l ﬁY) for 2' I 1,—n, therefore 2n + 1 values /
parameters.

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
36.

For the Joint Bayes, we need to estimate

P(Y:0) :>P(Y:1):1—P(Y:O)

P(X1 :0,X2:1|Y:0) 1—(P(X1 :0,X2:0|Y:O)+P(X1:O,X2:1\Y:0)+

P(X1:1,X2:0|Y:0) P(X1:1,X2:O|Y:0)).

P(X1:0,X2:O|Y:1) :>P(X1:1,X2:1|Y:1):

P(X1 :0,X2:1|Y:1) 1—(P(X1 :0,X2:0|Y:1)+P(X1:0,X2:1\Y:1)+

P(X1:1,X2:0|Y:1) P(X1:1,X2:O|Y:1)).
ForNthe general case, when n imput variables are given, we will need to estimate P(Y),
P(X1, - -- ,Xn l Y) and P(X1, - -- ,Xn | ﬁY), where
and ~ ~

(X1,"' 7)(n) 75 (—.X17... ,“an

Therefore, 2(2'” — 1) —|— 1 I 2n+1 — 1 values / parameters.
It can be seen that Naive Bayes uses a linear number of parameters (w.r.t. n, the number
of input attributes), while Joint Bayes uses an exponential number of parameters (w.r.t.
the same n).

***************Ending Page***************


***************Beginning Page***************
***************page number:37**************
37.
Exemplifying
spam ﬁltering using the Naive Bayes algorithm
CMU, 2009 spring, Ziv Bar-Joseph, midterm, pr. 2

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
38.
About 2/3 of your email is spam, so you downloaded an open
source spam ﬁlter based on word occurrences that uses the Naive
Bayes classiﬁer.
Assume you collected the following regular and spam mails to
train the classiﬁer, and only three words are informative for this
classiﬁcation, i.e., each email is represented as a 3-dimensional
binary vector Whose components indicate Whether the respective
word is contained in the email.
————|-
“II-—I-
———qu
———ul-
“——U-I
“———|-

***************Ending Page***************


***************Beginning Page***************
***************page number:39**************
39.

a. You ﬁnd that the spam ﬁlter uses a prior P (spam) I 0.1. Explain
(in one sentence) why this might be sensible.
b. Compute the Naive Bayes parameters, using Maximum Likeli-
hood Estimation (lVlLE) and applying Laplace’s rule (“add-one”).
c. Based on the prior and conditional probabilities above, give the
model probability P(spam| s) that the sentence

s = “money for psychology study”
is spam.

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
40.
Answer:
a. It is worse for regular emails to be classiﬁed as spam than it is for spam email to be
classiﬁed as regular email.
b. When estimating the Naive Bayes parameters from training data only using the
MLE (maximum likelihood estimation) method we would have:
0 P f 1 — 1
P(study]spam): g I O ( ree|regu ar)_ 1
3 4 1
P(study]regular): 1 P(money\spam): é : 5
8
_ _ _ 1
P(free\spam)- 8 _ 1 P(money\regular): 1
By applying Laplace’s rule (“add-one”) we get:
3 + 1 2
0 + 1 1 P(study|regular): — I —
P t d I — I —
(su ylspam) 8+2 10 4+2 3
8 + 1 9 1 + 1 1
( reelspam) 8 + 2 10 ( ree|regu ar) 4 + 2 3
4 1 1
P(money|spam): L : — _ 1 + 1 _ 1
g + 2 2 P(money\regular)_ m _ §
Notice that the occurrence of 2’ s at denominators corresponds to the number of values
for each of the attributes used to describe the training instances.

***************Ending Page***************


***************Beginning Page***************
***************page number:41**************
. . 41.
c. Class1ﬁcatlon of the message
s I “money for psychology study”,
using the a priori probability P(spam) I 0.1:
P(spam | s) I P(spam | study, ﬁfree, money)
F. B_ayes P(study, —|free, money I spam) - P(spam)
— P(study,—|free,money | spam)P(spam) + P(study,—|free,money | reg)P(reg)
P(study, —|free, money|spam) indeg Cdt' P(studyIspam)-P(—|free]spam)-P(money|spam)
_ 1 1 1 _ 1
_ 10 10 2 — 200
P(study, ﬁfree, money|reg) male}; Cdt' P(study|reg)-P(ﬁfree\reg)-P(money|reg)
_ 2 2 1 _ 4
_ 3 3 3 _ 27
Therefore, Notice that this is a small probability.
i 1 However, without using Laplace’s rule,
200 . E it would be 0 due to the fact that the
(spamI S) i . i + i _ g word study did not appear in any of the
200 10 27 10 spam emails in the training data.

***************Ending Page***************

***************Beginning Page***************
***************page number:42**************
42.
Naive Bayes and Joint Bayes:
application when a joint probabilistic distribution
(on input + output variables) is provided
CMU, 2010 spring, T. Mitchell, E. Xing, A. Singh, midterm pr. 2.1

***************Ending Page***************


***************Beginning Page***************
***************page number:43**************
43.

$1 £132 y P($1,5132,Z/)

0 0 0 0.15

0 0 1 0.25
Consider the joint probability distribution 0 1 0 0.05
over 3 boolean random variables x1, x2, and 0 1 1 0.08
y given in the nearby ﬁgure. 1 0 0 0.10

1 0 1 0.02

1 1 0 0.20

1 1 1 0.15
a. Express P(y I 0 1 £131,552) in terms of P(x1,x2,y I 0) and
P(x1,$2,y I 1).

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
44.
b. Complute the marginal probabilities which will be used by a
Naive Bayes classiﬁer. Fill in the following tables.
y P(y) P(x1|y) 331:0 :131I1 P(a:2|y) xQIO ngl

yIO y:0 yIO

yzl yzl yzl
c. Write out an expression for the value of P(y I 1,551 I 1,552 I O)
predicted by the Naive Bayes classiﬁer.
d. Write out an expression for the value of P(y I llxl I Lxg I O)
predicted by the Joint Bayes classiﬁer.
e. The expressions you wrote down for parts (c) and (d) should
be unequal. Explain why.

***************Ending Page***************


***************Beginning Page***************
***************page number:45**************
45.
Answer
a. Using the deﬁnition of conditional probability and then the total probability formula,
we get:
P(a:1,x2,y:0) P(a:1,:1:2,y:0)
P I 0 :1; ,x : — I —
(y I 1 2) P($1,952) P($1>$2,y:0)+P($17$2,3J:1)
b.
y P(y) P(a:1\y) $1:0 561:1 P(w2|y) 362:0 ZEQIl
y I 0 0.5 y I 0 0.40 0.60 y I 0 0.50 0.50
y : 1 0.5 y : 1 0.66 0.34 y : l 0.54 0.46
Explanations:
2'. P(y) was computed as a marginal probability, stating from the joint probability,
P(.§C1,332,y):
P(y I 0) I P(:c1 I 0,3132 I 0,y I 0) +P(:c1 I 0,332 I Ly I 0) +
+P(£l?1 I l,£E2 I 0,y: O)+P(331 I 1,162 I Ly: 0)
I 0.15 + 0.05 + 0.1 + 0.2 : 0.5
P(y:1) I 1—P(y:0):0.5

***************Ending Page***************

***************Beginning Page***************
***************page number:46**************
46.
2'2'. P($1 | y) was computed using again the deﬁnition of conditional probability and then
the total probability formula:
P I O I 0 P I 0 I 0
P(y:0) P(:01I0,yIO)—|-P(a:1:1,y:0)
P001 I O,y I 0) I Pm I 0,22 I 0,3; I 0) +P(a:1 I 0,3:2 I 1,y I 0) I 0.15+0.05 I 0.2
P(a:1 I Ly I 0) I P(a:1 I 1,502 I (Ly I 0) +P(x1 I 1,502 I 1,y I 0) I 0.1+0.2 I 0.3
Therefore 0 2
P I 0 I 0 I é I 0.4
($1 lg > 0.2 + 0.3 ’
and
P(x1 :1|yI0)I1—P(231IO|yIO)IO.6
Similarly,
Pm I 0, y I 1) 0.25 + 0.08
P IO I1 I —I—IO.66I>
P(x1:1\y:1) I 1—P(231I0|yI1)I0.34
P(a:2 l y) was calculated in a similar way.

***************Ending Page***************


***************Beginning Page***************
***************page number:47**************
47.
c. The Naive Bayes classiﬁer uses the conditional independence assumption. Therefore:
_% cdt- We...
P(x1 I 1,5U2 :0 I y: 1)P(y: 1)+P(:131 I 1,332 :0 I y: 0)P(y:0)
I 1306121|y:1)'P($2:0\y:1)'P(y:1)
P($1 I liy I 1)P($2 I my I 1)P(y I 1) +P($1 I lly I 0)P($2 I Oly I 0)P(211 I 0)
0.34 - 0.54 - 0.5
:— : 0.38
0.34 - 0.54 - 0.5 —|— 0.6 - 0.5 - 0.5
d. The Joint Bayes classiﬁer d0esn’t use the conditional independence assumption.
Therefore:
P<$1 I 1,382 I an I 1)
(y |£U1 ,562 > P(a:1:1,w2:0,y:1)+P($1:1,$2:0»Z/:0)
0.02
: — : .1
0.02+0.1 0 6

***************Ending Page***************

***************Beginning Page***************
***************page number:48**************
48.
e. The values calculated by the Naiv Bayes and respectively the Joint Bayes
classiﬁers for P(y I 1 | x1 I l,£E2 I O) are different because the conditional
independence assumption dos not hold.
Indeed,
P(:c1 I 0,x2 I O,y I O) 0.15
P :07 :0 I0I—I—IO.3
While
P(:clI0|yIO)-P(:c2I0|yI0)IO.4-O.5IO.2750.3
Therefore, 331 are x2 are not conditionally independent W.r.t. y.

***************Ending Page***************


***************Beginning Page***************
***************page number:49**************
49.
Exemplifying
The computation of the 6TTO'T‘ rate for the Naive Bayes algorithm
CMU, 2010 fall, Aarti Singh, HWl, pr. 4.2

***************Ending Page***************

***************Beginning Page***************
***************page number:50**************
50.
Consider a simple learning problem of determining whether Alice and Bob
from CA will go to hiking or not Y : Hike 6 {T, F} given the weather conditions
X1 : Sunny € {T, F} and X2 : Windy € {T, F} by a Naive Bayes classiﬁer.
Using training data, we estimated the parameters
P(Hike) : 0.5
P(Snnny| Hike) I 0.8, P(Sunny| —|Hike) I 0.7
P(Windy | Hike) I 0.4, P(Windy | ﬁHike) I 0.5
Assume that the true distribution of X1,X2, and Y satisﬁes the Naive Bayes
assumption of conditional independence with the above parameters.
a. What is the joint probability that Alice and Bob go to hiking and the weather is
sunny and windy, that is P(Sunny, Windy, Hike)?
Solution:
P(Sunny, Windy, Hike) 06515.2(1610. P(Sunny]Hike) - P( Windy|Hike) - P(Hike) : 0.8 - 0.4 - 0.5 I 0.16.

***************Ending Page***************


***************Beginning Page***************
***************page number:51**************
b. What is the expected error rate of the Naive Bayes classiﬁer? 51'
(Informally, the expected error rate is the probability that an “observation” / instance
randomly generated according to the true probabilistic distribution of data is incorrectly
classiﬁed by the Naive Bayes algorithm.)
Solution:
II‘.—\__
X1 X2 Y P(X1|Y) -P(X2\Y) - P(Y) YNB(X1,X2) PNB(Y\X1,X2) Note'
F F T 0.2 - 0.6 - 0.5 I 0.060 F 0.444444 corresponding to
F T T 0.2 - 0.4 - 0.5 I 0.040 F 0.347826 tions are shown in
T F F 0.7 - 0.5 - 0.5 I 0.175 T 0.421686 bold.
T F T 0.8 - 0.6 - 0.5 I 0.240 T 0.578314
T T F 0.7 - 0.5 - 0.5 I 0.175 F 0.522388
III—--ll
Note:
error déf. EP[1{YNB(X1,X2)75Y}l 1{ , } is the indicator function;
1ts value 1s 1 Whenever the as-
: Z 1{YNB(X1,X2);£Y} ' P(X17X2,Y) sociated condition (in our case,
X1,X2,Y YNB(X1,X2) 75 Y) is true, and 0
I 0.060 + 0.040 + 0.175 + 0.160 I 0.435 OthePWise-

***************Ending Page***************

***************Beginning Page***************
***************page number:52**************
52.

Next, suppose that we gather more information about weather conditions and

introduce a new feature denoting whether the weather is X3 : Rainy or not.

Assume that each day the weather in CA can be either Rainy or Sunny. That

is, it can not be both Sunny and Rainy. (Similarly, it can not be n Sunny and

nRainy).
c. In the above new case, are any of the Naive Bayes assumptions violated? Why (not)?
What is the joint probability that Alice and Bob go to hiking and the weather is sunny,
windy and not rainy, that is P(Sunny, Windy, ﬁRdiny, Hike)?
Solution:
The conditional independence of variables given the class label assumption of Naive
Bayes is violated. Indeed, knowing if the weather is Sunny completely determines
whether it is Rainy or not. Therefore, Sunny and Rainy are clearly NOT condition-
ally independent given Hike.

P (Sunny, Windy, nRainy, Hike)
: PbRainylHike, Sunny, Windy) -P(Sunny, WindyIHike) - P(Hi/<:e)
m/—/
1
cond'zmdep' P(Sunny|Hil<:e) - P( Windy|Hil<§e) - P(Hil<;e)
I 0.8 - 0.4 - 0.5 I 0.16.

***************Ending Page***************


***************Beginning Page***************
***************page number:53**************
d. What is the expected error rate when the Naive Bayes classiﬁer uses all three at- 53'
tributes? Does the performance of Naive Bayes improve by observing the new attribute
Rainy? Explain Why.

Solution:
IIIlI-\—--
P (X3150-

X1 X2 X3 Y P(X1,X2,X3, Y) P(X1|Y) - P(X2\Y) - P(Y) YNB(X1,X2,X3) PNB(Y|X1,X2, X3)
F F F F 0.075 - 0.7 I 0.0525 F 0.522388
\IIIU——1—
F F T F 0.075 0.075 - 0.3 I 0.0225 F 0.652174
IIII————\\
F T F F 0.075 - 0.7 I 0.0525 F 0.621302
“III——l-
F T T F 0.075 0.075 - 0.3 I 0.0225 F 0.737705
IIII————\\
T F F F 0.175 0.175 - 0.7 I 0.1225 T 0.389507
IIII————\\
T F T F 0.175 - 0.3 I 0.0525 F 0.522388
“III——l-
T T F F 0.175 0.175 ~ 0.7 I 0.1225 T 0.489022
IIII————\\
T T T F 0.175 - 0.3 I 0.0525 F 0.621302
IIII———\—\

***************Ending Page***************

***************Beginning Page***************
***************page number:54**************
54.
The new error rate is:
Ep[1{yNB(X1,X27X3)¢y}] I 0.060 + 0.040 + 0.175 + 0.175 : 0.45 > 0.435 (see question
Important Remark:
Please notice that we always compute the error rate with respect to P, the
true distribution, and not PNB, which is the distribution computed by Naive
Bayes by using the conditional independence assumption.
Here above, the Naive Bayes classiﬁer performance drops because the condi-
tional independence assumptions do not hold for the correlated features.

***************Ending Page***************


***************Beginning Page***************
***************page number:55**************
55.
How bad / naive is Naive Bayes?
CMU, 2010 spring, E. Xing, T. Mitchell, A. Singh, midterm, pr. 1.2

***************Ending Page***************

***************Beginning Page***************
***************page number:56**************
56.
Clearly Naive Bayes makes what, in many cases, are overly strong
assumptions. But even if those assumptions aren’t true, is it pos-
sible that Naive Bayes is still pretty good‘? He'r'e we will use a
simple example to explore the limitations of Nai've Bayes.
Let X1 and X2 be i.i.d. Bernoulli(0.5) random variables, and let
Y € {1, 2} be some deterministic function of the values of X1 and
X2.
a. Find a function Y for which the Naive Bayes classiﬁer has a 50%
error rate. Given the value of Y, how are X1 and X2 correlated?
b. Show that for every function Y, the Naive Bayes classiﬁer will
perform no worse than the one above. Hint: there are many Y
functions, but because of symmetries in the problem you only need
to analyze a few of them.

***************Ending Page***************


***************Beginning Page***************
***************page number:57**************
57.
Riispuns

X1 X2 Y

O 0 1
a. Considerém Y deﬁnit conform tabelului aléturat. 0 1 2

1 0 2

1 1 1
Observatie: Dacéi se consideré valoarea lui Y ﬁxatéi (ﬁe 1, ﬁe 2), atunci putem
s5 stabilim 0 regulé astfel incét dacii i1 cun0a§tem pe X1 sii-l determiniim pe
X2 (§i invers).a Altfel spus, X1 este unic determinat de X2 (gi invers) datéi
ﬁind 0 valoare ﬁxaté a lui Y. Deci conditia de independeniﬁi condigionaléi
este incéilcatéi. Mai mult, in acest caz avem maximul posibil de ,,dependen§é“
intre cele doué variabile (in raport cu Y).
Pe slide-ul urmétor vom calcula rata erorii inregistrate de algoritmul Bayes
Naiv pe datele din tabelul de mai sus.

a Pentru Y I 1, regula este: X2 are aceeagi valoare ca §i X1. Pentru Y I 2, regula este: X1 g1 X2 au valori
complementare.

***************Ending Page***************

***************Beginning Page***************
***************page number:58**************
58.
Bayes Naiv estimeazfi valoarea lui Y astfel:
.19 I argmaXP(X1\Y I y)P(X2 l Y I y)P(Y I y)
y€{1,2}
Pentru X1 I 0, X2 : 0, algoritmul comparé urméitoarele douﬁ valori:
1 1 1 1
p1 : P(X1:0|Y:1)P(X2:0|Y:1)P(Y:1):—-—-—:—
2 2 2 8
1 1 1 1
p2 : P(X1:0|Y:2)P(X2:0|Y:2)P(Y:2):—-—-—:—
2 2 2 8
Cum p1 : p2, algoritmul va alege una dintre ele cu 0 probabilitate de 0.5.
Deoarece valoarea lui Y din tabel este 1, inseamnéi c5 algoritmul va alege
gregit cu 0 probabilitate de 0.5.
Pentru celelalte 3 cazuri, (X1 I 0,X2 : 1), (X1 : 1,X2 : 0) §i (X1 : 1,X2 : 1),
se observé u§0r c5 se obtin de asemenea valori egale, iar algoritmul va alege
pentru Y una dintre valorile 1 sau 2 cu 0 probabilitate de 0.5.
Deci pentru aceastéi deﬁnitie a lui Y rata erorii este de 50%.

***************Ending Page***************


***************Beginning Page***************
***************page number:59**************
59.
b. Vom calcula rata erorii pentru ﬁecare dintre cele 3 moduri de deﬁnire a lui Y care
nu a fost studiat.
Cazul 1: X1 X2 Y Este similar cu cazul: Y
O 0 1 2
0 1 1 2
1 0 1 2
1 1 1 2
0 Pentru X1 : 0, X2 : O, algoritmul compariiz
2 2 1
p1 : P(X1:0|Y:1)P(X2:O|Y:1)P(Y:1):1-1-1IZ
p2 : P(X1:0|Y:2)P(X2:0|Y:2)P(Y:2):0-0~0:0
Cum p1 > p2 algoritmul alege pentru Y valoarea 1, ceea ce este corect.
0 Pentru celelalte 3 cazuri, (X1 I O,X2 I 1), (X1 I 1,X2 I O) §i (X1 I 1,X2 I 1), se
observé c5 se 0b§in aceleagi valori pentru p1 §i p2 ca mai sus, deci algoritmul alege
(in mod corect) pentru Y valoarea 1.
Agadar, am 0b§inut c5 rata erorii este in acest caz 0.

***************Ending Page***************

***************Beginning Page***************
***************page number:60**************
60.
CazulQ: X1 X2 Y Cazuri Y Y Y Y Y Y Y
O 0 1 similare: 1 1 2 2 2 2 1
O 1 1 1 2 1 2 2 1 2
1 0 1 2 1 1 2 1 2 2
1 1 2 1 1 1 1 2 2 2
QPentru X1:0,X2:0:
2 2 3 1
p1:P(X1:0\Y:1)P(X2:0|Y:1)P(Y:1):_._._:_
3 3 4 3 :>@_1
1 _
0 Pentru X1 I O,X2 : 1:
2 1 3 1
p1:P(X1:0\Y:1)P(X2:1|Y:1)P(Y:1):_._._:_
3 3 4 6 :>A_1
1 y—

***************Ending Page***************


***************Beginning Page***************
***************page number:61**************
61.
0 Pentru X1 : 1,X2 : O:
1 2 3 1
P1:P(X1:1IY:1)P(X2:O|Y:1)P(Y:1):_-_._:_
3 3 4 6 :>A_1
1 y—
p2:P(X1:1|Y:2)P(X2:0|Y:2)P(Y:2):1.0.1:0
o Pentru X1 I 1,X2 I 1:
1 1 3 1
p1:P(X1:1|Y:1)P(X2:1|Y:1)P(Y:1):_._._:_
3 3 4 12 A_
1 1 ig-Z
192:P(X1:1\Y:2)P(X2:1|Y:2)P<y:2):1_1_1:1
Deci rata erorii este O pentru acesté deﬁnitie a lui Y.

***************Ending Page***************

***************Beginning Page***************
***************page number:62**************
62.
Cazul 3: X1 X2 Y Cazuri similare: Y Y Y
0 0 1 2 1 2
0 1 2 1 1 2
1 O 1 2 2 1
1 1 2 1 2 1
o Pentru X1 I O,X2 I 0:
1 1 1 1 1 O 1 0 :> > :> A 1 ( t)
:_. ._:_ :_. ._: : corec
P1 2 2 4, P2 2 2 P1 P2 y
0 Pentru X1 : 0,X2 I 1:
1 1 1 1 1
29125.0.520’p2:§.1.§:1:>p1<p2:>§:2 (corect)
0 Pentru X1 I 1,X2 : O:
1 1 1 1 1 A
p1:§-1-§:1,p2:§-0-§:0:p1 >p2:>y:1(c0rect)
0 Pentru X1 : 1,X2 : 1:
1 0 1 0 1 1 1 1 :> < :> A 2 ( t)
:__ ,_: :_. ._:_ : corec
P1 2 2 7 P2 2 2 4 P1 P2 y
Prin urmare, rata erorii este O §i in acest caz.

***************Ending Page***************


***************Beginning Page***************
***************page number:63**************
63.

Cazul 4: (cel de la punctul a) X1 X2 Y Este similar cu cazul: Y

0 O 1 2

0 1 2 1

1 O 2 1

1 1 1 2
Concluzie: Doar pentru 2 moduri (cazul 4) de deﬁnire a lui Y rata erorii este
de 50%; pentru celelalte 14 moduri (cazurile 1, 2, 3) rata erorii este 0.

***************Ending Page***************

***************Beginning Page***************
***************page number:64**************
64.
Unlike Naive Bayes,
the Joint Bayes classiﬁer has 0 training error rate for
all boolean functions
(and even all mathematically deﬁned functions on categorical attributes);
“in-between Naive and Joint” Bayesian classiﬁers
CMU, 2004 fall, T. Mitchell, Z. Bar-Joseph, HW3, pr. 1.2

***************Ending Page***************


***************Beginning Page***************
***************page number:65**************
65.
Suppose we have a function Y I (AA B) \/ ﬁ(B \/ C), where A, B, C are indepen-
dent binary random variables, each having a 50% chance of being O.
a. How many parameters a Naive Bayes classiﬁer needs to estimate (Without
counting Phx) as a parameter if P(a:) is already counted as an estimated
parameter)?
What will be the error rate of the Naive Bayes classiﬁer (assuming inﬁnite
training data)?
b. How many parameters the Joint Bayes classiﬁer needs to estimate? What
Will be the error rate of the Joint Bayes classiﬁer (assuming inﬁnite training
data)?
Conventie: in cazul in care, pentru o setare oarecare a variabilelor A, B si C,
cele doua probabilitati calculate de catre algoritmul Bayes Naiv in vederea
determinarii valorii yNB sunt egale, convenim ca algoritmul va lua decizia
yNB I 1-

***************Ending Page***************

***************Beginning Page***************
***************page number:66**************
Answer 66'
: PY: A: ,B:b,C:
yNB argyergglfy) ( yl a C)
P(A:a,B:b,C:c|Y:y)-P(Y:y)
: ar max —
gyGVal(Y) P(A I a, B I b, C I c)
: arg max P(A:a,B:b,C:c|Y:y)-P(Y:y)
y€Val(Y)
: arg max P(A:a|Y:y)-P(B:bley)-P(C:c\Y:y)-P(Y:y)
y€Val(Y)
Naive Bayes Will need t0 estimate:

P(y:0)—>P(y:1):1-P(Y:O)
P(A:O\Y:O)—>P(A:1|Y:0):1—P(A:O|Y:O)
P(A:0\Y:1)—>P(A:1|Y:1):1—P(A:O|Y:1)
P(B:0\Y:O)—>P(B:1|Y:O):1—P(B:0\Y:0)
P(B:O\Y:1)—>P(B:1|Y:1):1—P(B:0\Y:1)
P(C:O\Y:O)—>P(C:1|Y:O):1—P(C:O|Y:O)
P(C:0\Y:1)—>P(C:1|Y:1):1—P(C:O|Y:1)

Altogether it is 7 parameters.
In general, for n input binary variables it would have been 271+ 1 parameters.

***************Ending Page***************


***************Beginning Page***************
***************page number:67**************
67.
"-“I ﬁ<B V C) l-
0 0 O O O 1 1
To compute the error rate we can con- 0 0 1 0 1 O O
struct a Boolean table of the func- 0 1 0 0 1 0 O
tion and use it to estimate probabilities 0 1 1 O 1 0 0
(since we assume inﬁnite training data). 1 0 0 0 O 1 1
1 0 1 0 1 O O
1 1 0 1 1 0 1
1 1 1 1 1 0 1
The estimated parameters are:
1 1 1
P(Y:O):§—>P(Y:1):1_P(Y:0):1_§:§
P<A|Y> P<B|Y> m- PW)
3 1 2 2 1 3
Y : — — Y : — — Y I — —
1 2 2 1
Y : 1 — é Y : 1 — — Y I 1 é —
4 4 4 4 4 4

***************Ending Page***************

***************Beginning Page***************
***************page number:68**************
68.

The predictions of the Naive Bayes classiﬁer are then as follows (assuming
that in case of a tie it always predicts 1):

"E|__--I

O O O 3/4-1/2-1/4-1/2 1/4-1/2-3/4-1/2 1 no

O O 1 3/4-...-3/4-... 1/4-...-1/4-... O no

0 1 0 3/4-...-1/4-... 1/4-...-3/4-... 1 yes

0 1 1 3/4-...-3/4-... 1/4-...-1/4-... 0 no

1 0 0 1/4-...-1/4-... 3/4-...-3/4-... 1 no

1 O 1 1/4-...-3/4-... 3/4-...-1/4-... 1 yes

1 1 0 1/4-...-1/4-... 3/4-...-3/4-... 1 no

1 1 1 1/4-...-3/4-... 3/4-...-1/4-... 1 no
From this, we can compute the error rate as the number of mistakes over the
number of possible inputs (since each input is equally likely): error rate =
2/8: 0.25.

***************Ending Page***************


***************Beginning Page***************
***************page number:69**************
69.
yJB I arg max P(Y : y|A:a,B I 19,0: c)
yGVal(Y)
P(A:a,B:b,C:c|Y:y)-P(Y:y)
I arg max —
yEVal(Y) P(A : a, B : b, C I c)
I PA: ,B:b,C: Y: -PY:
argyerélgfy) ( a Cl y) ( y)
Joint Bayes will need to estimate:
P(y:0)—>P(y:1):1—P(Y:O)
P(A: 0,B 20,0: OlyIO)
P(A:0,B:0,C:11y:0) —>P(A:1,B:1,C:1|y:0):1—...
P(A: 1,B : LC: OlyIO)
P(A:O,B :0,C:0\y: 1)
P(A:0’B:0’Czlly:1) —>P(A:1,B:1,C:1|y:1):1—...
P(A: 1,B : 1,0:0|y: 1)
Thus, it is 1 + 2- (23 — 1) : 15. In general, for n input binary random variables
it would have been 1 + 2- (2” — 1) : 2n+1 — 1.
The error rate of the Joint Bayes classiﬁer is zero (assuming inﬁnite training
data) since it can model an arbitrary complex Boolean function.

***************Ending Page***************

***************Beginning Page***************
***************page number:70**************
70.
c. Consider a Bayes classiﬁer that assumes that A is independent of C when
conditioned on B and on Y (unlike a Naive Bayes classiﬁer that assumes that
A, B, C are all independent of each other when conditioned on Y).
Show that this Bayes classiﬁer will need to estimate fewer parameters than
the Joint Bayes classiﬁer, but will still have the same error rate (assuming
inﬁnite training data). Compute the error rate of this classiﬁer.
Answer
Starting from Y I (A /\ B) \/ ﬁ(B \/ C) I (A /\ B) \/ ((ﬁB) /\ (ﬁC)), one can infer
that A is independent of C when conditioned on B and on Y.
LC: Just to convince yourself...
By using the truth table which we have already written for Y I (A/\B) \/—|(B\/
C), you can easily check the equalities
P(A:a,C:c|B:b,Y:y):P(A:a|B:b,Y:y)-P(C:c|B:b,Y:y)
if you analyse — for each of the four combination of values for b and y €
{0,1}, separately — the probabilities for P(A I a,C I c|...), P(A I a|...),

***************Ending Page***************


***************Beginning Page***************
***************page number:71**************
71.
Therefore P(A, B, C|Y) I P(A, C|B,Y) - P(B\Y) I P(A\B,Y) - P(C|B,Y) - P(B|Y).
For any input A I a, B I b, C I c, our new Bayes classiﬁer will predict
ynewBayes I arg m[aX} P(A I (ZIB I b,Y I y)-P(C I clB I b,Y I y)-P(B I b|Y I y)-P(Y I y),
yG 0,1
which, due to the above equality, will be exactly the same as the output of
Joint Bayes.
Therefore, the error rate for new Bayes classiﬁer will be zero.
The parameters it needs to estimate are:
P(Y I 0),
P(B I oyY I 0), P(B I 0W I 1),
P(A I OlB I 0,YI0), P(A I 0|B I 0,YI1), P(A I 0|B I 1,YI0), P(A I 0|B I 1,YI1),
P(CI0|B I O,YIO), P(CIO|B I 0,YI1), P(CI0|B I 1,YI0), P(CIO\B I 1,YI1).
Thus, there are 11 parameters to estimate, which is more than what’s required
by Naive Bayes (7), but less than Joint Bayes (15).

***************Ending Page***************

***************Beginning Page***************
***************page number:72**************
72.
LC: Just to convince yourself...
P(Y:O):1/2,
Using the Boolean table of P(B I OIY I 0) I 1/2, P(B I OIY I 1) I 1/2,
y:(AAB)\/ﬁ(B\/C), P(A:0|B:0,Y:0):1/2,P(A:0|B:0,Y:1):1/2,
the estimations of these pa- P(AI0|BI1,YI0)I1,P(AI0|BI1,YI1)I0,
rameters are: P(C I OlB I 073/ I O) I 0, P(C I O|B I 07y I 1) I 1,
P(C:0\B:1,Y:O):1/2,P(C:0|B:1,Y:1):1/2.
The predictions of the classiﬁer are then as follows (there are no ties!):
P(A\B,Y : 0) -P(C\B,Y : 0)- P(A|B,Y : 1) - P(C\B,Y : 1)-

A C P(B|Y : O) - P(Y : O) P(B|Y I 1) - P(Y : 1) YnB err.

0 0 0 1/2-0-1/2-1/2 1/2-1-1/2-1/2 1 no

0 0 1 1/2-1-...-... 1/2-0-...-... 0 no

0 1 0 1-1/2-...-... 0-1/2-...-... 0 no

0 1 1 1-1/2-...-... 0~1/2....-... 0 no

1 0 0 1/2-0-...-... 1/2-1-...-... 1 no

1 0 1 1/2-1-...-... 1/2-0-...-... 0 no

1 1 0 0-1/2-...-... 1-1/2-...-... 1 no

1 1 1 0-1/2-...-... 1-1/2-...-... 1 no

This corresponds to zero error rate.

***************Ending Page***************


***************Beginning Page***************
***************page number:73**************
73.
Computing
The sample compleamlty 0f the Naive Bayes and Joint Bayes Clssiﬁers
CMU, 2010 spring, Eric Xing, Tom Mitchell, Aarti Singh, HWZ, pr. 1.1

***************Ending Page***************

***************Beginning Page***************
***************page number:74**************
74.

A big reason we use the Naive Bayes classiﬁer is that it requires less training
data than the Joint Bayes Classiﬁer. This exercise should give you a “feeling”
for how great the disparity really is.
Imagine that each instance is an independent “observation” of the multi-
variate random variable X I (X1, ..., Xd), where the X1- are i.i.d. and Bernoulli
of parameter p I 0.5.
To train the Joint Bayes classiﬁer, we need to see every value of X “enough”
times; training the Naive Bayes classiﬁer only requires seeing both values of
Xi “enough” times.
Main Question: How many “observations”/ instances are needed until, with
probability 1 — e, we have seen every variable we need to see at least once?
Note: To train the classiﬁers well would require more than this, but for this
problem we only require one observation.
Hint: You may want to use the following inequalities:

0 For any k Z 1, (1 — 1/k)k g e_1

0 For any events E1, ...,Ek, P1"(E1 U . . . U Ek) g 2:le P7“(EZ').

(This is called the “union bounds” property.)

***************Ending Page***************


***************Beginning Page***************
***************page number:75**************
75.
Consider the Naive Bayes classiﬁer.
a. Show that if N observations have been made, the probability that a given value of
1
Xi (either 0 or 1) has not been seen is 2N—_1.
. d .
b. Show that 1f more than NNB I 1 +log2 (—) observatlons have been made, then the
a
probability that any X2- has not been observed in both states is g a.
Solution:
1 N 1 N 2 1
a. P(component Xi not seen in both states) I (5) + (5) I 2—N I W
b. P(any component not seen in both states)
g 2;; P(component X2- not seen in both states)
_ d 1 _ 1 _ 1 _ 1 _ 1 _ a _
—Zizlw—d'w—d'm—d'21<.-,T2g—dﬁ—d'3—€
a

***************Ending Page***************

***************Beginning Page***************
***************page number:76**************
76.
Consider the Joint Bayes classiﬁer.

c. Let i be a particular value of X. Show that after N observations, the probability

that we have never seen f is g e_N/2d.

2d
d. Using the “union bounds” property, show that if more than NJB : 2d 1n <—)
a
observations have been made, then the probability that any value of X has not
been seen is g a.
Solution:
c. P(573 not seen in N observations)
d N/Zd d
1 1 N 1 1 2 < 1 N/2 —N/2d
I — — I — — — I 6
2d 2d _ e

d. P(any 5E not seen in NJB observations)

g 2% PU: not seen in NJB observations)

d 1 2d
S Egg-NJB/2d :2d'€—NJB/2d :2d_6—ln2? : 2d. elnﬁ : E :8
E a

***************Ending Page***************


***************Beginning Page***************
***************page number:77**************
77.
e. Let d I 2 and 5 I 0.1. What are the values of NNB and NJB?
What about d I 5?
And d I 10'?
Solution:
2
NNBI 1+10g2— I 1+log220% 5.32
0.1
a I 0.17 d I 2 I> 22
5
NNBI 1+1og2—I 1+log250% 6.64
0.1
a I 0.17 d I 5 I> 25
NJB: 25-1n0—1I 32-1n320w 184.58
10
NNB I 1 +10g2 — I 1 + log2 100 z 7.64
0.1
a:0.1,dI10 I> 210
NJB I 210-1nﬁ I 1024 - 1n 10240 z 9455.67

***************Ending Page***************

***************Beginning Page***************
***************page number:78**************
78.
The relationship between [the decision rules of]
Naive Bayes and Logistic Regression;

the case of Boolean input variables
CMU, 2005 fall, Torn Mitchell, Andrew Moore, HW2, pr. 2

CMU, 2009 fall, Carlos Guestrin, HWl, pr. 4.1.2

CMU, 2009 fall, Geoff Gordon, HW4, pr. 1.2-3

CMU, 2012 fall, Tom Mitchell, Ziv Bar-Joseph, HW2, pr. 3.a

***************Ending Page***************


***************Beginning Page***************
***************page number:79**************
79.
a. [NB and LR: the relationship between the decision rules]
In Tom’s draft chapter (Generative and discriminative classiﬁers: Naive
Bayes and logistic regression) it has been proved that when Y follows a
Bernoulli distribution and X I (X1, . . . ,Xd) is a vector of Gaussian variables,
then under certain assumptions the Gaussian Naive Bayes classiﬁer implies
that P(Y|X) is given by the logistic function with appropriate parameters w.
So,
1
P(Y : 1|X) I —d.
1 + eXp(w0 + 2,:1 szz')
and therefore,
d
P(Y I 0‘ X) I —6Xp(w0 + 21:51” )
1 + eXp(w0 + 2,:1 win)

Consider instead the case where Y is Boolean (more generally, Bernoulli) and
X : (X1, . . . ,Xd) is a vector of Boolean variables. Prove for this case also that
P(Y\X) follows this same form and hence that Logistic Regression is also the
discriminative counterpart to a Naive Bayes generative classiﬁer over Boolean
features.
Note: Discriminative classiﬁers learn the parameters of P(Y|X) directly,
whereas generative classiﬁers instead learn the parameters of P(X|Y) and

***************Ending Page***************

***************Beginning Page***************
***************page number:80**************
80.

Hints:
1. Simple notation will help. Since the X,’s are Boolean variables,
you need only one parameter to deﬁne, P(X,\Y I yk), for each
2' I 1, . . . , d.
Deﬁne ﬁn I P(X, I llY I1), in which case P(X,; I O|Y I 1) I l —€,~1.
Similarly, use (9,0 to denote P(X,- I llY I O).
2. Notice that with the above notation you can represent P(X,-\Y I
1) as follows:

P(XZ-\Y :1) I @iim — MM”,
except the cases when 9,1 I O and X,- I O, respectively 6,1 I 1 and
X,- I l. Note that when X, I l the second term is equal to 1
because its exponent is zero. Similarly, when X,- I 0 the ﬁrst term
is equal to 1 because its exponent is zero.

***************Ending Page***************


***************Beginning Page***************
***************page number:81**************
Solution 81-
P X I Y : 1 P Y I 1
P<Y: 1|sz) B-ZF- X
Zy/E{O,1}P(X I 37|Y I y)P(Y I y)
_ 1
_ 1 + P(X : 90|Y I 0)P(Y I O)
P(X I $|Y I 1)P(Y I 1) Conditions:
1 1. P(X::C|Y:1)P(Y:1)¢O;
I P(X:$]Y:0)P(y:9) 2. P(X::c|Y:O)P(Y:O);/$O;
1+6XP 1mm 3. P(X I wilY I 0) ¢ 0 and
P(X = my = 1) 72 0.
_ 1
_ P X : X : Y : P Y :
P(X1 : :51, . . . ,Xd : xd|Y : 1)P(Y I 1)
cond._indep. 1
_ P Y:0 d P Xizazi Y:0
1 + exp (ln FEE/:1; + 21:1 1n PEXZ-zwiinlg)
Prior probabilities are: P(Y I 1) n2” 7r and P(Y I O) I 1 — 7T.
Also, each X1- follows a Bernoulli distribution:

***************Ending Page***************

***************Beginning Page***************
***************page number:82**************
So, 82.
1
P(Y:1\X:$) I W
1 — 7T d 81' 1 — 9m _ i
1 1 — . 1 10—
+eXp (n 7r +2121 n effm _ eillem)
_ 1
_ 1 —7T d 9m 1 —‘9i0
_ 1
_ 1-71‘ d 1-910 d 81'0 1_62'0
1 1 — . 1 — . X1- 1 _ _1 —
+eXp<n 7T +Zzzln1_6i1+22:1 (nail n1_92_1))
Therefore, in order to reach
d
P<Y I 1|X I a» I 1/(1 + GXPWO + Eur-Xi»,
i=1
we can set
d
1— 1—9Z- 61- 1—62' ,

***************Ending Page***************


***************Beginning Page***************
***************page number:83**************
83.
Note
Although here we derived for P(Y|X) a form which is speciﬁc to
Logistic Regression starting from the decision rule (better said,
from the expression of P(Y I 1\X)) used by Naive Bayes, this
does not mean that Logistic Regression itself uses the conditional
independence assumption.

***************Ending Page***************

***************Beginning Page***************
***************page number:84**************
84.

b. [Relaxing the conditional independence assumption]
To capture interactions between features, the Logistic Regression model can
be supplemented with extra terms. For example, a term can be added to
capture a dependency between X1 and X2:

P(Y — llX) — 1
Similarly, the conditional independence assumptions made by Naive Bayes
can be relaxed so that X1 and X2 are not assumed to be conditionally inde-
pendent. In this case, we can write:

P Y P X X Y TL P X ' Y
P (X )

Prove that for this case, that P(Y|X) follows the same form as the logistic
regression model supplemented with the extra term that captures the de-
pendency between X1 and X2 (and hence that the supplemented Logistic Re-
gression model is the discriminative counterpart to this generative classiﬁer).

***************Ending Page***************


***************Beginning Page***************
***************page number:85**************
85.
Hints:
1. Using simple notation will help here as well. You need more
parameters than before to deﬁne P(X1, X2, Y). So let’s deﬁne ﬁijk I
P(X1 = 2',X2 : j,Y = k), for each 2',j and k.
2. The above notation can be used to represent P(X1, X2|Y : k) as
follows:
P(X1> X2lY I k) I (511k)X1X2 (510k)X1(1_X2) (501k)(1—X1)X2 (500k)(1—X1)(1—X2)
for k E {0,1}, except for the cases when 611k I 0 and X1X2 I 0, or
31% I O and X1(1—X2) I O, or [301k I O and (1—X1)X2 I 0, or 500k I 0
and (1 — X1)(1 — X2) I O.

***************Ending Page***************

***************Beginning Page***************
***************page number:86**************
86.
Solution
P(Y I 1|X) B.:F. P(X|Y I 1)P(Y : 1)
P(X|Y I 1)P(Y I 1) + P(X\Y I 0)P(Y I 0)
1
I W
P(X|Y I 1)P(Y I 1)
_ 1
— 1+eXp (In iP(X|Y:O)P(Y:O)) . .
P(X|Y I 1)P(Y I 1) Condztzons:
011112161810. 1 1. P(X I sc|Y I 1) P(Y I 1) I 0;
1 + exp (1n P(X1,X2|Y I 0) H?:3 P(X1-IY I 0)P(Y I 0)) 2. P(X I a:|Y I 0) P(Y I 0) I 0;
P(X1,X2|Y I 1) Uglzg, P(XZ-|Y I 1)P(Y I 1) 3_ P(X1X2|Y I 0) 75 0 and
I 1 P(X1X2|Y I 1) I 0; P(XZ-|Y I
1+eXp(1n:+Z¢_31nw+lnw) 0)¢0and P(XZ-|Y:1);é0.
11 Z— P(X2-|Y I 1) P(X1,X2|Y I 1)
_ 1
1 + exp (1n 1_—7T + 2923111 9531(1 _ 9i0)(1—X7;) -|-111(3110)X1X2(5100)X1(1_X2)(B01O)(1—X1)X2 (5000)(1—X1)(1—X2)>
7r Z— 6ii(1 _ 911)(1—X75) (5111)X1X2 (5101)X1(1—X2) (5011)(1—X1)X2 (5001)(1—X1)(1—X2)
_ 1
1 + exp (In 1-; + 221:3 (X1- (ln 62 + 1n i) +1n i) + 1n E + w1X1 + '11]ng + w172X1X2>
7T 911 1 — 910 1 — 910 5001

***************Ending Page***************


***************Beginning Page***************
***************page number:87**************
87.
with

1—7T d 1_6i1 5000
wo I lnT+i22331nm+1nE
wl I ln@+1n@

6101 6000
1,02 I 1n%+ln%
@0172 I 1n%+1n%+m%+m%
wl' I 1ng%+1n% forz':3,...,d.

***************Ending Page***************

***************Beginning Page***************
***************page number:88**************
88.
Maximum Likelihood Estimation (MLE)
for the parameters of Naive Bayes
and the linearity of the model
— the case of Boolean variables —
Stanford, 2007 fall, Andrew Ng, HWl, pr. 4

***************Ending Page***************


***************Beginning Page***************
***************page number:89**************
89.
In this problem, we look at maximum likelihood parameter estimation using
the Naive Bayes assumption. Here, the input features 33,-, j I 1,...,n to
our model are discrete, binary-valued variables, so :Bj € {0,1}. We call :1: I
(9:1, x2, . . . ,xn)T to be the input vector. For each training example, our output
targets are a single binary-value y € {0,1}. Our model is then parameterized
by 930 I P(acj I 1|y I 0), 631 I P(a:j I 11y I 1), and 9y I P(y I 1). We model the
joint distribution of (agy) according to
P(y) I (6y)y(1 — 9y)1_y (the exponentiation trick)
P(~’IJI?J I 0) I H PWIIQ I 0) I H((9j0)$j(1 _ 310)1_$j)
jIl jIl
PM?! I 1) I H Pley I 1) I H((9jl)$j(1 _ 6101:”).
j:1 9:1

***************Ending Page***************

***************Beginning Page***************
***************page number:90**************
Exempllﬁcatlon [by L1v1u Clortuz]
Let’s consider the —I NotHea/vy Smelly Spotted Smooth I Edible
nearby training dataset. —l 1 0 0 0 l-
By ﬁrstly renaming —z-|—|—
the input attributes as ——|—|
X1, Xe, X3 and respec- “—|—-
tively X4, and secondly —l 1 1 1 0 I“
the output variable as —I 1 0 1 1 I“
Y, the eet et eeremetee —|—l“
(e) will eeeeet et —|—lu
610 : P(X1 : 1|Y : 0), 611 : P(X1 : llY : 1), 620 : P(X2 : 1|Y : 0), 621 : P(X2 : 1|Y : 1),
630 : P(X3 : llY : 0), 631 : P(X;e> : llY : 1), 640 : P(X4 : llY : 0), 641 : P(X4 : llY : 1),
6y : P(Y : 1).
By denoting D : {A, B, . . . , H}, and assuming that all the above training instances are proba-
bilistically independent of each other, the verosimility function of D will be
L(6) : P(A|6) - P(Bl6) - . . . - P(Hl6), where

P(A|(9) : 911(1 — 821)(1 — 931)(1 — 941)9y, P(Bl9) I Q11(1 — 621)631(1 — 941)9y,

P(Cl6) I (1 — 611)621(1 — 631)6416y, P(D|6) I (1 — 611)(1 — 621)(1 — 931)941(1 — 9y),

P(El9) : 910620630(1 — 940)(1 — 6y), P(F|8) : 910(1 — 920)630940(1 — 99),

P(Gl6) : 910(1 — 920)(1 — 630)940(1 — 6y), P(H|9) : (1 — 910)620(1 — 630)(1 — 640)(1 — 9y).

***************Ending Page***************


***************Beginning Page***************
***************page number:91**************
91.
Exempliﬁcation cont’d
I 911(1 — 921)(1 — 931)(1 — 941)9y ' 911(1 — 921)931(1 — 941)9y'
(1 — 911)921(1 — 931)9419y ' (1 — 911)(1 — 921)(1 — 931)941(1 — 9y)-
910920930(1 — 940)(1 — 9y) ' 910(1 — 620)930940(1 — 9y)-
610(1 — 920)(1 — 630)640(1 — 9y) - (1 — 610)620(1 — 930)(1 — 940)(1 — Hy)
I 551(1 _ 911) - 9%(1 _ 910? - 621(1 _ 921V - 930(1 _ e20)?’-
931(1 _ 931)2-e§0(1 _ 930)?’ - 941(1 _ e41? - 9510(1 _ e40? - a; - (1 _ 9y)?
2(a) "2' blue)
I 21n911 —|— ln(l — 611) + 31n 610 + 21n(1 — (910)-1-
ln 321 —|— 21n(l — 621) + 21H 620 + 3ln(l — (920)-1-
ln 331 + 2111(1 — 631) + 21H 630 + 3111(1 — 630) +
111641 —|— 21n(l — 641) + 31n 640 + 21n(1 — (940)-1-
31mg + 5ln(l _ 6y).
(9 3 5 A 3
a—§y£(6):0@€—HIOI>6yI§.
Similarly, one would get 911 I g, élO I g, égl I é, égo I g, @31 I é, é30 I g, é41 I é, é40 I g.

***************Ending Page***************

***************Beginning Page***************
***************page number:92**************
Exempliﬁcation cont’d 92'
For the following test datset
——|—|
——|—|
——|—|
——|—|
Naive Bayes will produce the following results:
1 3
P(Edible: 1|U) I —37'g— z i <1: yNB(U) I 0
1 3 23-3 5 53+23-34 2
22 3 22
P(Edz'ble: 1\V)<1: —37Jg— I 3——3 I ; < l :>yNB(V) :0
2 22 3 2-33 5 22 2-33 1 36 2
3712+ng ¥+5—3 1+5?
23 3 23
P(Edible: 1|W)> 1:»: —34$— : 3—3 : ; >1? yNBWV) I 1
2 23 3 22-32 5 23 22-32 35 2
e'Wng 53+? “T53

***************Ending Page***************


***************Beginning Page***************
***************page number:93**************
93.
a. Find the joint likelihood function 6(a) :1nﬂfi1 P(o<i>,y<i>; e) in terms of the
model parameters given above. Here 6 represents the entire set of parameters
{9w 930, 871,]- I 1, . . . ,n}.
Solution
W) I 1n H Paw-“W”; 6)
1:1
I 1n H P($(i)|y(i);9)P(1/(i);9)
1:1
I 1n H (H P(x§-Z) |y(i); 9))P(y(i); 9)
1:1 j:1
I Z (1n P(y(i); Q) + ZlnPQg-i) \yw; (9))
1:1 W . 9:1
<61>y<Z><1—@y>1—y‘”
I Z [yw 1n (9y —|— (1 — y(i))1n(1 — 9y) —|— 2:5?) 1n @j yo) —|— (1 — $52))1n(1 — 9jy(i))].
1:1 3:1

***************Ending Page***************

***************Beginning Page***************
***************page number:94**************
94.
b. Show that the parameters which maximize the likelihood function are the
same ast hose given in the lecture notes; i.e.,
6 21:1 1{m§.i>:1/\y<i):0} 6 21:1 1{a:§.i):1/\y(i):1} 6 27:1 1{y<1):1}
'0 : —m 7 -1 I —m 7 I —.
J 21:1 1{y<1'>:0} J 21:1 1{y<1'>:1} y m
Solution
The only terms in 6(6) which are non-constant with respect to (93-0 are those
which include (99-11(1). Therefore,
we) a m @- i
(‘iT I (97 2(385 ) lnHjW) + (1 _ xi )) 111(1 — ‘99' 2N)»
90 90 1:1
5 m (1') (1')
: 870 2(133- ln6j01{y<1)20}+(1 — ZUj )lIl(l — 6j0)1{,y(i):0})
J 1:1
m <1) 1 (1') 1
: m —1 (1-): —(1—$- )—1 <1): >
2222(1 67.0 {y 0} J (1_gj0) {y 0}

***************Ending Page***************


***************Beginning Page***************
***************page number:95**************
95.
Solution (cont’d)
. 3 ,
Settlng —£(<9) I 0 glves
593-0
O I f: (23(-i)i1 (11) — (1 — $(-i))—1 1 (i) ) ¢>
1.:1 9 630 {y =0} J (1 _ 93.0) {y :0}
1:1
1:1 1:1 1:1
I Z 1{$§?>:1/\y<i>:0} _ ‘910 Z 1{y<i>:0}-
i=1 i=1
We then arrive at our desired result
'0 I m—.
] 21:1 1{y<i>:0}
The solution for (93-1 proceeds in the identical manner.

***************Ending Page***************

***************Beginning Page***************
***************page number:96**************
96.
Solution (cont’d)
To solve for 9y,
lﬂw) I l f: (ya) 1113 + (1 — y(i)) 111(1 — (9 ))
86y 86y 1,:1 y y
m . 1 . 1
_ (I) (I)
- y — — <1 — y >— -
. 3 .
Then settmg —£(@) : O glves us
89y
m . 1 . 1
I (Z)_ _ (I) —
0 20’ ey+(1 y )1-ey)©
2:1
0 z Z (y<”(1—%> - <1-M81) : 21W 261 : 2w Wy-
1:1 1:1 1:1 1:1
Therefore,
y — —.
m

***************Ending Page***************


***************Beginning Page***************
***************page number:97**************
97.
Note: It can be easily shown that the Hessian matrix of the log-verosimility
function £(9) is negative deﬁnite, therefore @(9) is a concave function, and the
above solutions 9y, (93-0, (93-1 for j I 1,. .. ,n correspond to its maximum.
c. Consider making a prediction on some new data point a: using the most
likely class estimate generated by the Naive Bayes algorithm. Show that the
hypothesis returned by Naive Bayes is a linear classiﬁer — i.e., if P(y I 0|:v)
and P(y I llx) are the class probabilities returned by Naive Bayes, show that
there exists some w E Rn+1 such that
. . l
P(y I lIx) Z P(y I OISE) 1f and only 1f wT l x I Z O (Vx).
(Assume wO is an intercept term.)

***************Ending Page***************

***************Beginning Page***************
***************page number:98**************
98.
Solution
P(y I 1|w) PWy I 1)P(y I 1) M
Pyzlx ZPyIOx (I)—21<I>—-—21
< H < ') P<y:0|w> M P<w\y:0>P<y:0>
n _ _ n ZCj _ . 1—£Cj
(i) H£:1P(33j|3/—1)P(y—1)21® nHj:1((9j1‘)(1 911)’ >621 21
Ply-:1 Pley I 0) P(y I 0) 111:1 ((99.0% (1 — 610) “1)(1 — 9y)
TL 9- ‘E1 1—€~ lira‘ 9
szl ((910)361 (1 _ ‘990) ‘3%)(1 _ 9y)
n 6-1 1_e'1 6
.1 L 1_ .1 —3 1 —y
‘i’ gcvjnejoﬂ $3)n1_ej0)+n1_ey
n 6‘1(1_e'0) n 1_9'1 e 1
<I> wing-k 1n—3+1n—y20<:>9T[ 120,
j; ‘7 (9j0(1—9j1) jzzl 1—(9j0 1—(9y x
wj V
Where
n 1-9-1 9 01(1-9-0) ,
wO: 1n—3+1n—y, w':1n%,]:1,n.,n.
j; 1_ej0 1—ey J 0j0(1—9j1)

***************Ending Page***************


***************Beginning Page***************
***************page number:99**************
. . 99.
Exempllﬁcatlon cont’d
For the previously given dataset we Will have:
8
1—9- <9 1—9 1—6 1—<9 1—<9 €
_ 1_6j0 1—(9y 1—610 1—620 1—630 1—(940 1_6y
3:1
1<1-2/3 1-1/3 1—1/3 1—1/3 3/8)
1—3/5 1—2/5 1—2/5 1—3/5 1—3/8
1 2 2 2 2- 3 2
I1n(ﬁ.ﬁ.M3_/8):1n—5:1n2:0_028
2/5 3/5 3/5 2/5 5/8 35 243
611(1—610) 2/3 2/5 4
:1—:1(—-—):1-:0.288
wl n610(1 -@11) n 3/5 1/3 n3
1/3 3/5 3
Z :1(—-—):1_:- :—0.288
1112 w3 n 2/5 2/3 n4 wl
1/3 2/5 1
:1(——):1_:_13:—1.099,
M n 3/5 2/3 n9 n
therefore the equation of the linear separator corresponding to the Naive Bayes classiﬁer
trained on the given dataset is:
‘(1)1331 + 1025132 + 103563 + 104564 + 'LUO I 0 <I> 0.2885131 — 0.288(332 —|— £63) — 1.099564 + 0.028 I 0.

***************Ending Page***************

***************Beginning Page***************
***************page number:100**************
100.
Exempliﬁcation c0nt’d
Checking the equivalence of the Naive Bayes decision rule and the classiﬁcation made
through linear discrimination:
yNB(U) : O <I>
4
ijatj + wO < 0 for x1 :0,w2::v3:a"4:1 (I) —0.288- (1 —|— 1) — 1.099 + 0.028 < 0 (True!)
9:1
yNB(V) : 0 42>
4
ijwj + wO + wO < 0 for $1 :x2:x4:1,x3:0 e —0.2ss- (1—1)—1.099 + 0.02s < 0 (True!)
j:1
yNB(W) I 1 (1*
4
ijxj + wO < 0 for x1 :x2:1,$3:x4:0 (I) —0.288- (/1/—I) + 0.028 > 0 (True!)
9:1

***************Ending Page***************


***************Beginning Page***************
***************page number:101**************
101.
Gaussian Baysian Classiﬁcation

***************Ending Page***************

***************Beginning Page***************
***************page number:102**************
102.
Exemplifying the Gaussian [Naive] Bayes algorithm on data from R
CMU, 2001 fall, Andrew Moore, midterm, pr. 3.a

***************Ending Page***************


***************Beginning Page***************
***************page number:103**************
103.
O A
2 A
Suppose you have the nearby training set with one real-valued 3 B
input X and a categorial output Y that has two values. 4 B
5 B
6 B
7 B
a. You must learn from this data the parameters of the Gaussian Bayes
classifer. Write your answer in the following table.

***************Ending Page***************

***************Beginning Page***************
***************page number:104**************
104.
Solution (in Romanian)
. .. . . 277121 372'
Pentru a estlma medule ,LLA §1 ,uB, vom f01051 formula ,LLMLE I Z—, unde n este
n
2
. X1- 2
numarul instanigelor de antrenament. A§adar, ,LLA I ZZITl I 0% I 1, iar ,uB I
217:3)(1- _ 3+4+5+6+7 _5
5 _ 5 _ '
Similar, pentru calculul varian§elor 0A §i O'B, vom folosi formula (Ti/[LE :
1"” Z- — 2 1 1
—ZZ:1($ HMLE) . A§adar, 031 I 5K0 — 1)2 —|— (2 — U2] I 1, iar 0129 I 5K3 _ 5)2 ‘l’ (4 _
n
1
5)2+02+(6—5)2+(7—5)2] I 5-2-[4+1] :2.
Pentru calculul probabilitaigilor P(Y I A) §i P(Y I B) se Qine cont de faptul ca Y este
variabila de tip Bernoulli. A§adar, P(Y : A) : 2/7 §i P(Y : B) : 5/7.
Centralizand aceste estimari, obtinem:

***************Ending Page***************


***************Beginning Page***************
***************page number:105**************
105.
b. Using the notation 04 : p(X : 2|Y : A) and 6 : p(X I 2|Y I B),
— What is p(X : 2,Y : A)? (Answer in terms of 04.)
— What is p(X : 2,Y : B)? (Answer in terms of 6.)
— What is p(X : 2)? (Answer in terms of oz and 6.)
— What is p(Y : AIX : 2)? (Answer in terms of o: and 6.)
— How would the point X I 2 be classiﬁed by the Gaussian Bayes algorithm?
(Answer in terms of 04 and 6.)
Solution (in Romanian)
2
p(X:2,Y:A) : p(X:2\Y:A)-P(Y:A):70‘
5
p(X:2,Y:B) I p(X:2\Y:B)-P(Y:B):7ﬁ
1
p(X:2) I p(X:2\Y:A) -P(Y:A)+p(X :2|Y:B)-P(Y:B) I 5(2@+55)
p(Y I A, X I 2) 204
Y I A X : 2 : — I —_
p< | > p(X I 2) 20. + 56

***************Ending Page***************

***************Beginning Page***************
***************page number:106**************
106.
Algoritmul Bayes [Naiv] gaussian va asocia punctului X I 2 eticheta Y I A
5
dac5p(Y:A|X:2)Zp(Y:B|X:2)<:>2aZ5ﬂ<:>0¢Z 53.
Folosind valorile estimate pentru parametrii MA, ,LLB, 0A §i O'B la punctul a,
vom putea scrie:
2 _ 1 2 1 2 — 5 2 9
1 _Q 1 __ . 1 _Q 1 __
\/ 2w \/ 271' \/ 27v - \/§ 2\/7_T
Deci,
5 1 l 5 2 Z 5 7 5
a>— <:>—e_2 >—e—4<:>e4 >—<:>—>1n—
—26 ‘ET —4ﬁ —2\/§ 4— 2\/§
3
(I) 1.75 Z 1n5 — 51n2 (I) 1.75 Z 0.5697 (adev.).
Prin urmare, algoritmul Bayes [Naiv] gaussian va asocia punctului X I 2
eticheta Y I A.

***************Ending Page***************


***************Beginning Page***************
***************page number:107**************
107.
Graphical representations
[made by Sebastian Ciobanu]
Without multiplying after multiplying the p.d.f.’s by

the p.d.f.’s by the selection probabilities the selection probabilities
g i Y = A g‘ i Y = A
i Y = B i Y = B

-2 O 2 4 6 8 —2 O 2 4 6 8

***************Ending Page***************

***************Beginning Page***************
***************page number:108**************
108.
Observatie
Se poate constata relativ u§0r céi existii douéi puncte de intersectie (5131 I —8.451
2 5
§i x2 : 2.451) pentru graﬁcele functiilor ?N(,LLA,O'124) §i ?N(,uB,0129).
Toate instantele de test x situate intre aceste puncte de intersectie ($1 <
:13 < $2) vor apartine clasei A (acolo curba r0§ie este situatéi deasupra celei
albastre).
Instantele situate fie la sténga lui $1 ﬁe la dreapta lui x2 v01‘ apartine clasei
B (acolo curba albastréi este situatéi deasupra celei r0§ii).
Separatorul decizional este de tip péitratic, ﬁind constituit din punctele $1 §i
$2.
LC: Multumesc studentului MSc Dinu Sergiu pentru aceasté observatie.

***************Ending Page***************


***************Beginning Page***************
***************page number:109**************
109.
Exemplifying the Gaussian [Naive] Bayes algorithm on data from R2
CMU, 2014 fall, W. Cohen, Z. Bar-Joseph, HWZ, pr. 5.0

***************Ending Page***************

***************Beginning Page***************
***************page number:110**************
110.
In a two dimensional case, we can visualize how Gaussian Naive Bayes be-
haves when input features are correlated. A data set is shown in Figure (A),
where red points are in Class 0, blue points are in Class 1. The conditional
distributions are two-dimensional Gaussians. In (B), (C) and (D), the ellipses
represent conditional distributions for each class. The centers of ellipses show
the means, and the contours show the boundary of two standard deviations.
a. Which of them is most likely to be the true conditional distribution?
b. Which of them is most likely to be estimates by a Gaussian Naive Bayes
model?
c. If we assume the prior probabilities for both classes are equal, which model
will achieve a higher accuracy on the training data?

***************Ending Page***************


***************Beginning Page***************
***************page number:111**************
3 + +
+ ++++¢ + + 4
++ + ‘H- + i 1H + +
+ _ r =‘-- ﬁ+++ +
2 + + M";5g.i.=?.ilﬁ-;,- *+ ++ +
0 #:é; 'ﬁﬁL; '7- v .4 1 0
+1¢+++++ ‘5-’ " 2;")? :‘Y'
w“ my?" 1'25 1
"-1 ‘ _ "'1‘ I ' .hfv
+ ‘ -* .‘ . '. ‘v
‘i -.“"- : 3 “ ‘ _2
m ‘ "'3; ‘ - v ' ‘k
2 ~ ‘g -. _ ‘w ‘r Y ‘A ‘
‘ Mi‘ *
i A 2 _
_3 2 a‘ 4
‘A
“3 --2 --1 0 1 2 3 4 —4 —2 O 2 4
(A) Data (B)
4 4
2 2
O 0
—2 —2
—4 —4
*4 *2 0 2 4 *4 *2 0 2 4

***************Ending Page***************

***************Beginning Page***************
***************page number:112**************
112.

Solution:

a. (C) is the truth.

b. (B) corresponds to the Gaussian Naive Bayes estimates. [LCz Here follows

the explanationz]

Because the Gaussian Naive Bayes model assume independence of the two

features conditioned on the class label, the estimated model should be aligned

with the axies. Both (B) and (D) satisfy this, but only in (B) the width and

height of the oval, which are proportional to the standard deviation of each

axis, matched the data.

c. (C) gives the lowest training error.

***************Ending Page***************


***************Beginning Page***************
***************page number:113**************
113.
Estimating the parameters for
Gaussian Naive Bayes and Full/ Joint Gaussian Naive Bayes algorithms
CMU, 2014 fall, W. Cohen, Z. Bar-Joseph, HW2, pr. 5.ab

Let Y € {0, 1} be class labels, and let X € Rd denote a d-dimensional feature.
We Will ﬁrst consider a Gaussian Naive Bayes model, Where the condi-
tional distribution of each feature is a one-dimensional Gaussian, X(j)|Y ~
a. Given n independent training data points, {(X(1),Y(1)),--- ,(X("),Y(n))},
give maximum-likelihood estimates (MLE) for the parametres of the proba-
bilistic distribution of XUHY, for j : 1, . . . ,d.

***************Ending Page***************

***************Beginning Page***************
***************page number:114**************
Solution: 114'
The likelihood of the samples in Class 0 is
- - 1 "O 1 <X@—MQP
L(XZ-(J)|M(J), (0;1,(‘7))2) I . eXp _ 1, _
’0 0 0 g @059) was”?
1 O "O (Xi? — My)?
I /— <1) eXp _Z (j)
27T00 i=1 2(50 )2
and the log-likelihood is
. 1 "0 . .
lnL : —n01n0(()3) — T 2(X7SJO) — pg)? + constant
2((703 )2 1:1 7
Taking the partial derivatives of the log-likelihood, we have
alnL "0 - ' - 1 "0 -
8/10 2':1 0 1:1
51M "0 1 no <'> <12 (‘>2 1 no <'> 412
—-:0 4:) _—-+—-Z(Xijo _H03) :0@(0#09) :—Z(Xz]0 _H03)
806]) (76]) (063))?’ i:1 no i:1
Similarly, one can derive the MLE for the parameters in Class 1.

***************Ending Page***************


***************Beginning Page***************
***************page number:115**************
115.
b. Suppose the prior of Y is already given. How many parameters do you
need to estimate in Gaussian Naive Bayes model?
Solution:
For each class, there are 2 parameters (the mean and variance) for each feature, there-
fore there are 2 - 2d : 4d parameters for all features in the two classes.
c. In a full/Joint Gaussian Bayes model, we assume that the conditional
distribution Pr(X|Y) is a multidimensional Gaussian, XlY ~ N(,uy, Ey), where
,u € Rd is the mean vector and E 6 RdXd is the covariance matrix.
Again, suppose the prior of Y is already given. How many parameters do you
need to estimate in a full/Joint Gaussian Bayes model?
Solution:
For each class, there are d parameters for the mean, d(d + 1)/2 parameters for the
covariance matrix, because the covariance matrix is symmetric. Therefore, the number
of parameters is 2 - (d + d(d + 1)/2) : d(d + 3) in total for the two classes.

***************Ending Page***************

***************Beginning Page***************
***************page number:116**************
116.
Proving
the relationship between the decision rules for
Gaussian Naive Bayes and the Logistic Regression algorithm
when the covariance matrices are diagonal and identical
i.e., 01-20 :02-21 for i: 1,...,d
CMU, 2009 spring, Ziv Bar-Joseph, HWZ, pr. 2

***************Ending Page***************


***************Beginning Page***************
***************page number:117**************
117.

Assume a two-class (Y € {0,1}) Naive Bayes model over the d-
dimensional real-valued input space Rd, Where the input variables
X |Y : 0 € Rd are distributed as

Gaussianwo :< H10, . . . ,Mdo >7 0 :< 01, . . . , 0d >)
and XIY:1 €Rd as

Gaussianwl :< #11,. ..,,ud1 >, 0 :< 01, . . . , 0d >)
i.e., the inputs given the class have different means but identical
variance for both classes.

***************Ending Page***************

***************Beginning Page***************
***************page number:118**************
118.
Prove that, given the conditions stated above, the conditional
probability P(Y z 1|X : x), Where X : (X1,...,Xd) and x I
(x1, . . . , and) can be written in a sirniar form to Logistic Regression:
1
1 + exp(w0 + w - x)

with the parameters wO 6 R and w I (201,. . .,wd) E Rd chosen in a
suitable way.

As a consequence, the decision rule for the Gaussean Bayes clas-
siﬁer supported by this model the desion rule has a linear form.

***************Ending Page***************


***************Beginning Page***************
***************page number:119**************
119.
Solution
P(Y — 1|X — x) BIF' —P(X I my I 1) PW I 1)
Zy/€{0,1}P(X I $le I 2/’) P(Y I y’)
_ 1
_ 1+ P(X :m/ I O)P(Y:0)
P(X I my :1)P(Y I 1)
_ 1
P(X I my :1)P(Y I 1)
_ 1
— P X I X I Y I P Y I
P(X1 I $1, . . . ,Xd I my I 1)P(Y :1)
_/—Jeatponent

***************Ending Page***************

***************Beginning Page***************
***************page number:120**************
d 120.
00nd. indep. P(Y I O) P(XZ I ZCZIY I O)
exponen nP(Y:1)+; nP(X1;:$¢|Y:1)
1 2
_(95¢—H10) >
P(Y I 1) , 1 (xi-Mm
1:1 — exp (——2>
ﬁU/L' 207;
P(Y I 0) d (w)- — #102 (r121 — #10?
Z 1 — — _ —
n P(Y I 1) + ;( 205 20,?
P(Y I 0) d 211-0110 — #11) + (#21 — #20)
n P(Y I 1) + 2:; 20?
P(Y I 0) d ilh'Wio — M11) (#21 _ #20)
I 1 — #
nP(Y:1)+;< a? + 202-2
P(Y I O) d (#21 — #20) d H10 — M11
1“Pa/21¢; 20? +2 a? a:
2:1 1:1 \_ —/
wO wi

***************Ending Page***************


***************Beginning Page***************
***************page number:121**************
121.
In conclusion,
1
P(Y I 11X I m) I —1 + 6mm)
with
d
P(Y I O) (/1121 _ N120) Hi0 — M1 .
Note that
€(w-a:+w0)
P<Y : OIX I I’) I —1 + 6(w.$+w0)
and
P(Y:1|X:x) >P(Y:OIX:x)<:>w'x+wO<0

***************Ending Page***************

***************Beginning Page***************
***************page number:122**************
122.
Since the coefficients wz- for 2' I 1, . . . ,d do not depend on 221-, it
follows that this decison rule of Gaussian Naive Bayes [in the
conditions stated in the beginning of this problem] is a linear rule,
like in Logistic Regression.
However, this relationship does not mean that there is a one-to-
one correspondence between the parameters wz- of Gaussian Naive
Bayes (GNB) and the parameters wz- of logistic regression (LR)
because LR is discriminative and therefore doesn’t model P(X),
while GNB does model P(X).
To be more speciﬁc, note that the coefficients wl- in the GNB decision rules
should be devided by P(a;1, . . . ,xd) in order to correspond to P(Y : llX : x),
which means that then they will not anymore be independent of xi, like the
LR coefficients.

***************Ending Page***************


***************Beginning Page***************
***************page number:123**************
123.
Proving
the relationship between
The full Gaussian Bayes algorithm and Logistic Regression
when EU I El
CMU, 2011 spring, Tom Mitchell, HW2, pr. 2.2

***************Ending Page***************

***************Beginning Page***************
***************page number:124**************
124.

Let’s make the following assumptions:
1. Y is a boolean variable following a Bernoulli distribution, with parameter
7T:P(Y:1) and thus P(Y:0):1—7T.
2. X I (X1,X2, . . . ,Xd)T is a vector of random variables not conditionally in-
dependent given Y, and P(X ‘Y : k) follows a multivariate normal distribution
Note that ,ak is the d >< 1 mean vector depending on the value of Y, and E is
the d >< d covariance matrix, which does not depend on Y. We will write/ use
the density of the multivariate normal distribution in vector / matrix notation.

N< a 1 1< FEW >

at‘ I — ex —— x — st —

Is the form of P(Y|X) implied by such this [not-so-naive] Gaussian Bayes
classiﬁer [LC: similar to] the form used by logistic regression?
Derive the form of P(Y\X) to prove your answer.

***************Ending Page***************


***************Beginning Page***************
***************page number:125**************
125.
We start with:
: 1 z 1
P(X\Y I 1)P(Y I 1) —|— P(X|Y I 0)P(Y I O)
1 1
I PY:0PXY:O I PY:0PXY:0
1+< ><| >1+6Xp1n< ><|>
P(Y : 1) P(X|Y I 1) P(Y I 1) P(X|Y : 1)
1
I P(Y I 0) P(X|Y I 0)
1 l — l —
+eXp (nP(Y: 1) + nP(X|Y: 1)
P X Y I O
Next we will focus on the term ln M:
1
P X y I 0 d/2 1/2
1n PEXh/ I 1) :1“ (2W) 1L‘ +1neXp[(*)l I 1neXp[(*)l I (*)
(2W)d/2|E\1/2
Where (*) is the formulation obtained as the difference between the exponential parts
of two multivariate Gaussian densities P(X|Y I 0) and P(X\Y : 1).

***************Ending Page***************

***************Beginning Page***************
***************page number:126**************
126.
1 _ _
(*) I 51(X ﬂu)TE 1(X — H1) — (X —uo)TZ 1(X —#o)l
_ 1 _ 1 _
I (ml —#1T)E 1X+ 51$ng 1m — 5mm 1H0
As a result, we have:
1
— 7T
1 + exp (1n T + §M1T2F1p1 — 51152-1110 + (,ug — ,ulT)E—1X)
_ 1
— 1 —|— eXp(wO + wTX)
1 — 1 1
Where wO I ln—7r —|— ialTZFl/il — §MJZ_1/.LO is a scalar,
7r
and w : E_1(,uo — p1) is a d >< 1 a parameter vector.
NOte that ((MOT—M1T)E_1)T I ((M0—H1)TZ_1)T I (2-1)T((M0—M1)T)T I E_1(M0—H1)because
E_1 is symmetric.
(E is symmetric because it is a covariance matrix, and therefore E_1 is also symmetric.)
In conclusion, P(Y\X) has the form of the logistic regression (in vector and matrix
notation).

***************Ending Page***************


***************Beginning Page***************
***************page number:127**************
127.
The quadratic nature of the decision boundary for
the Gaussian Joint Bayes classiﬁer
WhGH 20 7; 21
Stanford, 2014 fall, Andrew Ng, midterm, pr. 2.b

***************Ending Page***************

***************Beginning Page***************
***************page number:128**************
128.
The probabilistic distributions learned by the Gaussian Bayes Joint (GJB)
classiﬁer can be written as:
My) I Ml — W”, where (b I 19(2/ I 1)
— 0 — 1 1 1Y1
PWZ'J — ) — W6“) < _ 5(1‘ —H0) 0 (1‘ I160»
_ _ 1 1 T _1
P<93|y — 1) — WHP < — 5(515 — H1) Z1 (1‘ — #1))
The decision rule of GJB predicts
y I 1 is p(y I 1|50) Z p(y I Ola?) and y I O otherwise.
Show that if EU 5i El, then the separating boundary is quadratic in x. That
is, simplify the decision rule p(y : 1|:U) Z p(y I 013:) to the form
xTA$+BTx+C Z O,
for some A € RdXd, B € Rd, C € R, and A 73 0. Please clearly state your values
for A, B and C.

***************Ending Page***************


***************Beginning Page***************
***************page number:129**************
129.
Examining the log-probabilities yields:
: 1
1np(y I 1|$)21np(y I 0|$><I>1I1p(y I 1|$)—111P(ZJ I 01$) 2 0 <I> 1mm 2 0
My I Ola?)
a es I 1 I 1
F. 5Q 1n Wily )p(y ) 2 0
Wily I 0)p(y I 0)
Q5 |El11/2 1 T -1 T -1
I mm “W — 5((w—#1)21(IBIM1)—(95—H0)30($—M0)>20
1 T _1 -1 T -1 T -1 T -1 T -1 ¢ |El|1/2
1
e J (5(251 - 251))93 +(M1TZT1 - Mgzglﬁ
¢ |EO|1/2 1 T _1 T —1
+1Ilm+hlw + 5(ILLO EO ILLO—1LL1 E1 #1) ZO-
1

From the above, we see that A I 5(261 — Bil), BT I ,ulTZfl — #5261, C I ln% +

IEOI1/2 1 T -1 T -1 . . .
mm + 5010 ED ,uO — ,ul El ,ul . Furthermore, A 75 0, s1nce 20 75 El 1mp11es that

1

261 71$ 21-1. Therefore, the decision boundary is quadratic.

***************Ending Page***************

