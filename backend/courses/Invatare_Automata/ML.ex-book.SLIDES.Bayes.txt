***************Beginning Page***************
***************page number:1**************
1
Bayesmn Class1ﬁcat10n
Contains:
MAP Hypotheses: ox, PS-42 from Foundations cl» (LDP). 1. 2. 3‘ 4,
Naive Bayes and Joint Bayes: cx. 7. s. 6‘ as. 11, as. 1:1, 14‘ as.
Gaussean Bayes classiﬁers ex. 15. 21‘ 45_ 17‘ 18‘ 19
from the 20231 version ofthe ML exorcise book by 1.. (Jim-Luz m m.

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2
A simplc application of
Bayes’ formula
CMU, 2006 fall, Tom Mitchell. Eric Xingﬁ midterm. pl" 1;;

***************Ending Page***************

***************Beginning Page***************
***************page number:3**************
3
Suppose that in arnwering a questiun in a multiple chuiue test, an examiner:
either knows the anewer, with probability ll, ur he guesses with probability
l e1).
Assume that thc probability of urnwcring a queotmn correctly is 1 for on
exiuninee who knows the answer and l/m fur the examines whu guesses.
where W is the number of multiple choice olternotioee.
What is the prubability that an examinee knew the answer to [such] a ques-
tion, given that he ha> correctly answered it?
Answer:
PWWMHWU I PM, wt tn, it‘) - P(klir'ti')
ricer/m)

i P[(,(IY‘H‘1f‘/VH(‘U’) P(km:tir)

i 1’(r'mn'r'/ Itm ti) PUMH'U‘) + 1mmwilymml) I)[1/iit»\r'4l)'
Notice that in the denominator we had to eonsider the two ways (snie ron
get a question correct: by kuuWing, or by guessing. Plngging in, we get:

,l in]!
1 i n l e l‘
I/+iv(l*p) "+1 I
I!‘

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
i
Maximum A posteriory Probability
(MAP) Hypotheses

***************Ending Page***************

***************Beginning Page***************
***************page number:5**************
Exelllplifying

0 ihe application of Bayes’ theorem
e ihe haiiba bf MAP (Maximum A posteriuri Probability) hypotheses
e the eaihpiiiaiieh 0f ewpe'LMd values for discrete random variables

and
0 the [use 0f] sensitivity and speciﬁcity of a test

in a real-world application

CMU, 2009 fall, Geoff Gordon, HWl, pr. 2

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
I;
There is a disease whieh affects 1 in 500
peoples A IOUtOO dollar blood CESL can t.‘ h
help reveal whether a person has the
disease. A pesitive euteeme indieetes
that. the pei-seh nlay have the disease
The test has perfect sensitivity (trite
positive rate), i.e., a person who has tn
the disease tests positive 100% of the
time. However, the test has 99% . . . H . t
speeiﬁeity (‘THC negative rate). i.e., h “"“nm'” W‘ "mm ‘11+ 7"
healthy person tests pesitive 1% of the _ m
time. specificity: m i [p

***************Ending Page***************

***************Beginning Page***************
***************page number:7**************
7
a. A randmnly seleeted individual is tested and the result is pos-
itivet
what is the probability of the individual having the disease?
he There is a seeond more expensive test whieh eests 10, 00000
dollars but is exaet with 100% sensitivity and speciﬁcity,
If we require all people who test positive with the less expensive
test t0 be tested with the rnore expensive test, what is the empeetetl
cost to chock whether an individual has the disease?
e. A pharrnaeeutieal company is attempting to decrease the eost
of the seeond (perfect) testl
How much would it have to make the second test eost, so that the
ﬁrst test is no longer needed? That is, at what enst is it ehe-aper
simply to use the perfeet test alone. instead of screening with the
eheaper test as described in part I)?

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
a
Answer:
L205 deﬁne the following random variables:
1/tnis for persDns affected by that disease.
a; .
D/false ntharwlsﬁ:
n: the rssiiit nfihe ﬁrst test: + (in sash, nfdisaase) or s (otherwise);
r2: the result ofthe setnnd test: again + 0r ,_
P(B] I 5;“, m "
Known facts: Pm I i \B):1. P(Ti : - B): “ﬂu. ‘i
P(T2:>\B):1.P(T2:- B):u \
c
a,
" in,” ' ' ' i‘ '
P("\Ti:+l “I —,. m‘ My“) r r
I'm i+ \ B) Fwwrui s + i 1i) rim
1
s l mm i H10 N u um A
’ l i+i E ’ EN ’ *
500 mo sou
m? i Ti : t) : (183.11 > PU? i T. : H Thsrsmhs. 12 is the MAP hypothesis.

***************Ending Page***************

***************Beginning Page***************
***************page number:9**************
9
b4
Let's consider a new random variable:
C U if the person only Lakes the ﬁrst test
U + <1, 1r ﬂu: pol-sun mm ﬂu: two tum
a mt: ,1) I m-r. : ,1 and mcﬂ, +12] a Pm I +>
~> EC} a 1‘ (141(1',’ +))+(r‘1 +41] 1’(1‘\ a +1
a 1, an Pm a+)+1, l’(1'1*+)*:¢'1‘(T1’+)
11 +<1 Pm +)
5w
Z :- a a .
1W] { 1900!] BUUUU 21H‘ 221$
Note: Here above we used
Pm :H wwmimmm Pm :+ ‘ a) I’[B)+P(T1 I H1?) Pu?)
l \ M19 5X19
a 1 i i i a i , H‘
suu J’ um sou suuuu U U J8

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
10
c,
1-,, ‘:" the new price for the second test (Tg)
1-,, g E[C'] : 1l - P(C : 1-1) + (1-, + 1,,,) - m0 : 1, + 1,")
: . I : : _ 1 v 509
1.>1,1 1(1l 1) 100 1,, 50000
1,5100‘ 1,,4101108 ﬁ1-,,~10121251

***************Ending Page***************

***************Beginning Page***************
***************page number:11**************
11
The “Monty’s haunted house” problem
0 Exemplifying the application of Bayes’ theorem
o the iioiioii or MAP (hlaximum A posteriori Probability) hypotheses
CMU, 2009 fall, Geoﬂ' Gordon, HWl, pri 1

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
12
You are in a haunted houeo and you are stuck in l'ront or thrce duurs, A ghoet
nppcare end tolls you: “Your hupc is behind one or there doore. Thcrc ie
only one door that opens to the uutside and the two other doors have deadly
monsters behind them. You must rhnnse one door!" Ynu choose the ﬁrst
door. 'rhe ghost tells you: ~~Waitl l will give you some more information.“
The ghost opens the second door and shows you that there was a horrible
monster bchind it, then asks you: ~~Would you like to change your mind and
take the third door instead?“
What‘s betier: to stick with the ﬁrst door, or to change to the third dour’!
For each of the following atroteyiea used by the ghost, determine probabilities
that the exit is hohind the llrst and the third door. given that the ghoet
opened the second door.
a. The ghost always opens a door you have not pieked with a monster hehind
it. If both or the unopened doors hide mousters, he picks each of them with
equal probability.
bi The ghost hes a elightly diirerent strategy. If both of the unopened doors
hide monsters, he always picks the seoond door.
c. Finally, suppose that ii both or the unopened doors hide monsters, the
ghost always picks the third doort

***************Ending Page***************

***************Beginning Page***************
***************page number:13**************
Answer 13
What we know:
P<G \ m
———-
2 1 1/2 1 n
, i 1 :1 1 1,2 u 1
P”:11IP“’:21:P“’:'”*§ m
———-———-
———-—-
I; 5 l! 1] 11
WhaL we musk compute:
i it “My, maizwin mow)
mailmiz) i P[(¥:2\1):1)Pgr):1]+P(C:2\O:3)P((l:.$]
i it “my, P[G 2 \ 0 :s; r-(o J)
p[o”‘m’2) ’ Prn:2\0::s) P(():X]+P(G:2\():1)-P(O:l]
Nun“- um we =huuld have added Pm I 2 \ 0 Z 2‘; Pro : 2‘; at n“.- dunuxninutur. but
wu know um it a! n since Pm I 2 \ 0 I 21:n.
Therel'urc. Pm I :1 \ r; I 2) I 1 , P(() I 1 \ z; I 2).

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
14
Variant m
l l
, i T l , I 2
PLOIHUIZ): 2 l I 1>kr):.x\(;:g):1 >:>
‘ i 5+ | i 3 ‘ 3 ~‘
2 a a
Therefore, we should chuose EluDr a.
Variant b:
1 1
T 1 , 1 1
11(():1‘<;:2):%:, 1><r)::x\r;:2):1”:i
1 , + 1 , 2 2 z
X X
Therefore. we can chuusc either duur 1 ur duur a.
Variant (-1
P[0:1\(;:2):u P[0:3\(;:2):1r0:1
Therel'urc, we should choose door 1.

***************Ending Page***************

***************Beginning Page***************
***************page number:15**************
. . 15
Alternative Solutlon (1)
(in Romanian)
Euhivalcnt, in cuzul vuriuntci u. punmi u Llcturlninu lnaxilnul llinh'l.‘ l’((1 I 1 \
I.‘ I 2) 3i m0 I .x \ c : 2) ar ﬁ fest suﬁcient. conform formulei lui Bayes, is
mmpamn
P(G:2 0:1) P<0:l)§i P(0:2\0:-‘$\ “(7:5)

Mai muli_ yinénd mm de relaiia (i), aceasta ravine Ia a compara

1’(G:‘Z\O: i) §i magma)‘
mpimsni poaLc ii ciLit imcdiat din tabclul dc mni Su> (vczi pi-imn linie §i
penultimu linie): r) I a este v-nni-nnin pentru care SE amine probabilitatza [a
pnstariori] maxima.
Alzfcl spus_ 0 i s osle ipoteza de probabilitate maximi a posteriori (cngl..
nmxilnuln a piniuioii prububility (MAP) hyputhcﬂs)
Absolut similar se poate proceda 5i pentru variantele I) §i i’.
Observaliez in cazuri pi-mim eel do mai sus (mu I i) Z no I 2) Z 11(0 I
:i) : i), ZpoLezzi MAP coincide cu ipoleza dc verosimilitate maximli (engl..
maximul likelihuucl (ML) hypothesis).

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
16
Alternatlve Solutlon (2)
(in Romanian)
| 2 3
ErhivalenL pentru variantele 1, 5i p putem rispunde 13 intrehare ﬁinﬁnd I.“
urmémru] ragionamenn m5 a folosi formula 1m Bayes:
logircﬂ ecu bun?) he puatc gﬁsi in apatclc “new 11mm cclc trci “g \ V '
(vczi ﬁguru ulﬁturutii). Cum filnturnu u ‘Mum, 4ch ugu cu nun-am! 2,
una dintre aceste situaﬁi (5i anume, a doua din ﬁgura) este eliminaté. \
ﬁindci in Spalale @i Esta un monstru. In cnnﬁnuare, pumm minm in
felul urmStor:
Varianta b: innucm fantoma alege uga 2 cu probabilitate 14 putem aﬁrma. (in lipsa
1

unnr Bnp. informatii) (:5 amhele variante 1 5i 3 an prohahilitiﬁ egale, 5i anume 5.
“MAM...

ﬁe us- 1, u. “mi de mm. di “We .mi» m an...“ Mum trebuie, “Mum Mmpm.“ n .5

.lmr. m. a;

he u“ = d5 1mm “Mi, i.“ “mi. d“. m mm,“ pmmipium, m, Mm. mm in “he-g5 m. a;

mm, prim-pink“ P1, m, um; u .. 1min vuslbilitdm:

.... Mm d“ m Mum, mm. a mud“ imm “I: Am.“ mm“ dc m, .....
Varianta c: 5mm cZ\ l'antoma nu a dcschis “58 a (care ar n opﬂunea corcspunzémarc
principiului P2), ci u ale! uzgu z, inseaxnnﬁ c5 nu u pm“; face ulﬂ'el, deci uzgu a reprezinm
ieg'irea.

***************Ending Page***************

***************Beginning Page***************
***************page number:17**************
17
Exemplifying
a the application of Bayes’ theorem
a the notion of MAP (hlaximum A posteriori Probability) hypotheses
CMU, 2012 spring, Ziv Bar-Joseph, HWl, pr. 1.5

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
18
Mickey tosses a die multiple tinies. hoping for a es
The sequenee of his 10 tosses is 1,114. 2. a. 3.21,. 1,0‘
Miekey is suspicious whether the die is biased towards s (or fnir).
Conduot a simple analysis based on the Bayes theorem te inform
on Miekey to what degree is the die biased?
Explain your reasoning.
Assume in gcncral every 100 diee eontain s unfair diee that are
biased towards 3 with the probability distribution of the six faces
(1.2.3.4.;15) as P:[U.1. 0.1.0.5. 0.1. 0.1111}

***************Ending Page***************

***************Beginning Page***************
***************page number:19**************
19
Snlution

Dehiiitien fur the hetieh of Maximum A pusteriuri Probability:

imp U AanmX mmn) "if (‘review i mwm FUN!) T'flil-

rig” mu PW) mu

Let us dennte:

nn’{|:s4z_5_:5251_0)*(|| :1. tin)

o H : (FD LU)‘ where FD is the fair die and L17 is the loaded die.

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
20
\u 1 m 95
P[D\F‘f)) Pmn) i Pm.“ ‘mu-"0) P(Fn)'¥'(HP(1,Fn)) P(FD)*(’) i
,:\ u luu
i 1 19
i T 3m g
m
P(mw) P[LU]’P(I|.I2. .fmv/D) Hwy!‘ (HP[|,\LD)) mm)
‘:1
liililiiil l #i #i
10 z l0 10 2 2 10 10 lu 10 IUU |u7 21 20 2“' 5T 20
In order to cnrnpare P[D\F‘D; mm) and mmm) . PfLDL it is easier to ﬁrstly apply the
In:
lxxl’(LJ\l“U) l’[l“U)>lnl‘(L) LU) 1'(LL1)<>
ln%>111%@lnlflil(\lnﬁ>7lnﬁ@*30417>*112661
Note: We rnuld have directly computed the so-called log-odds ratio:
PUDWJi P(D\FD) PUD), i . . .
In PWDD) i1" P[D\LD) P(LD) i...’ 322110130 we hAve m choose FD

***************Ending Page***************

***************Beginning Page***************
***************page number:21**************
21
Exemplifying
ML hypotheses and MAP hypotheses
using decision trees
CMU, 2009 spring, Tom Mitchell, midterm, pr‘ 2.3-4

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
22
ﬁ—¢—¢—l—<?—<?—.—l—>
11,5 22,5 335 4X
Let’! consider the 1-dimenslunal data set shown above, based on the ail-gle
realrvalued attribute X. Notice there are two classes (values of Y), and ﬁve
data points.
Consider a special type el decision lveee where leaves have probabilistic la-
bela. Each leaf nude gives the probability er each possible label, where the
probability is the fraction of points at that leaf nude with that label.
For eemple, a decision tree learned from the data set above with zero splits
wuulll say P(Y I 1] I 3/5 and P(Y I n] I 2/5. A decision tree with one split
(at X I 2.5) would say P(Y I 1) I l if X < 2 a, and P(Y I 1) I 1/3 if X 2 2.5.
Solution:
a. Fur the above data eel, draw a tree that maxi- I

mizcs the likelihood of me data. 6); 5
1W I Argmru', Prim where Nu Dr
PllD) "'I’ PlDlrl “I” 1122111’: le I r, T) lilyIllen @s n25
where v, le the label/eleeee er the in=tuncc r, N\ a
(u I15 l! I2, “:3. m :zm 73:31.75) P(Y:|):m {P/(Yﬂhﬁ

***************Ending Page***************

***************Beginning Page***************
***************page number:23**************
23

b. Consider n prior probnbility distribution P(T] over trees thnt penalizes the number

of splits in the tree.

1 who”)?
P 1' r
< 1e (4)

where T is n troe. spltmr) is tho nnrnber orsplits in r, and 1 rneons “is proportional

to”.

For the same data set, give tho MAP troe when using this prior, ml over treest

Solutionr
a nudes: L 2 ‘
o l 2 1 ’ a‘ 2‘ lilo‘
-,,vovo:ir:i:::

P(74l\l7x(a) (a) (i) 5.‘ 3125 (ltlllb

1 node:

PTDU2211| 1an"

(ii )> (E) 3(1) iﬁiv, N:- lin\
\’P<Y=ll=l? \liwsllsl/y
2 nodes: I k
rm) an l o rm l v) sl l e i , llmwl o the MAP treo is Tl.
4 4 25b

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
1H
The Naive Bayes and Joint Bayes
Algorithms

***************Ending Page***************

***************Beginning Page***************
***************page number:25**************
2;
Exemplyifying the application of
Naive Bayes and Joint Bayes algorithms;
the minimum number of parameters
to be estimated for NB and respectively .IB
CIVIU, 2008 fall, Eric Xing, HWl pr. 2

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
2!;
Consider a elossiﬁeotiori prohlemj the ta- x, X2 1' | Counts
blc of observations for whieh is given
. . 0 0 n 2
nearby A, and X2 are two binary random
, . t 11 (1 1 18
variables whieh are the observed vari- | 0 U \
ables. Y is the class label which is ob-
. t t 1 0 1 1
served for the training data given below. 1, 1 U 4
We will “SC the Noive Bayes classiﬁer ohri
l . l 0 l 1 1
the Joirit Bayes classiﬁer to classify a new _
. . . 1 1 n 2
iristooce after train-rig ori the data below, 1 1 1 18
and Compare the results.
a, Construet the Naive Bayes classiﬁer given the data above. Use
it to classify the iiistehee X, e 0. x2 e u.
b1 Construct the Joint Bayes classiﬁer given the data above Use
it to classify the instance X, :11, x, I 01

***************Ending Page***************


***************Beginning Page***************
***************page number:27**************
27

e. Compute the probabilities P0’ : llx, : u. X2 : u) for the Naive
Bayes classiﬁer (let’s deuote it PNBU' : 1le : 0X2 : 0)) and for
tlie Joiut Bayes classiﬁer (11,,30’ : llx\ : l). X, : 0))‘
Why is P\-,,(Y : llY, : u. Y2 : u) (liﬁereut frmn P1801’ : llxl I
0.x2 e 0)? (Hint: Compute P(X|.X2\YJ.)

——|
t1. What happens to the iliirerehee he- 0 t) [j 3
tween Hum’ : 1le : 0..»(2 : n) and 0 (1 1 9
13,10’ : 1 x, : 0.x2 : 0) if the table en- 1 0 n 3
tries arc ehehged to the nearby table? 1 t) | u
(Hint: Will the Naive Bayes assumption 0 1 n s
be more violated or less violated colnpated (1 l l 9
to the previous situation?) 1 1 u s

1 1 1 o

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
28
el Conlpare tlre number uf independent parameters ln the two
elnsslﬁers Instead of just two observed data variables, if tbore
were a random binary observed variables (xr. . 1r“), wbat would
be tne number of parameters required for both classiﬁers’!
mom this, wlmt can you eornment about tbe rate of growth of the
number of parameters for botlr models as a >00?

***************Ending Page***************

***************Beginning Page***************
***************page number:29**************
29
Answer
a,
17M¢<A1:u_x3:m ‘:/ m'gmm<l"()(":UV:I/] Pu'Fm/Zv) P(Y':J/)
Venn)
m "‘:" Pun :1]\Y:H) P(x,:rm':u) I’(Y:(\)
"""1 i i E l i
12 1-2 an su loo
m "‘:" P(X\ :1]\Y:1) P(X,:rm':1) I’(Y:1)
MLI: g Q y B
as ax an lUU
1),, <1», :> 17mm. : 0.x! :u):1,

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
30
b4
WA] : u X2 : m "é’ mgan mx‘ I “x; : up’ I m my I ,,)
HF“! l)

' W’ .uu" 2 12 2
I) I P(X :ux :n Y:H PS':U :‘ Z

H Y 1 ‘ ) < > 12 3i) "1U

' W ’ m,» L8 as 18
11 i PM i0,)U’U)’i FY’! afiii

‘ ‘ ‘ J < ) as so so

1'1’ < 171¢0m(4\q:0.x2 I n] :1

***************Ending Page***************

***************Beginning Page***************
***************page number:31**************
:11
4'.
PW "l' My i 1 \ x‘ i 0 x1 ,0)
p 15% P(Xy:l),X1 : n y : 11 my :1)
i 11(x, :01‘ : o \ y : um :1)+1’(x‘: o. x1 : 1n y : 011w : 0)
WM W P(Xv : my :1)-P(x, : my : 11 P(Y :1)
i 11m my n) P(){g (m 0) m/ 01+
PM] I 0W I l) Y PM; : my :1) P(Y :1)
l‘)
I I" I s mm I E
Im+m 2r,
m *m
Pm “:" P<Y :1\X\: 0 x1 : n)
,, Wm mx, *n_x2 in v *1) my i x)
i 1>(x‘ : u x2 : n \ Y : um’ :1)>1)[x‘:(1.x2 : n \ v : mmy : n)
18
1 v1 I i I §
In’, +1"! 2 > 18 20
in an

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
:12

PW # PM; because [in this cm] the conditiunul independcnce assumption
doesn‘t hold.
Indeed,

1>(x, : 0.x‘ I 0 \ Y I 0) "y i

12 n a 1 ’
1 e e 1.: I “:L'Ji if’

1(x,eu\yeu)1[)n on o) 1212 4

QM)“:0,)(1:ﬂ\!':0)#l’()(,:0\! :U]'1’(_\’1:U\)’ :0]:>

it P(X|..‘Q Y) e mt, m PnXI \ Y)
'l‘heref0re Xl and x2 are, net conditionally independent w.r.t. v.

***************Ending Page***************

***************Beginning Page***************
***************page number:33**************
33
d.
PM; I my I 1m, I um I n)
I w
i l~’[4\'\ I U X1 I 0 \ )’ I l]P(Y I11+ P(X1Il\.X; I [H Y I OJPU I 0]
F(X| I my I u) 1*(x2 I u Y I n) . PO’ I (1)4
le Imv I 1) P(X3 Inn/:1) PO’ :1)
12$ IK 15b H
Z .m :m m I¢I1I§
£32+§EE 1+1 1‘! 4
L2 12 48 KG ‘JG 48 48 48
P”; : FLY :1\Xy : ‘LXt I 0)
i l’[X\ : U X1 : 0 \ Y :1)I’()’ : 11+ I’(X1:\\.X; : (H Y : 0]!’(Y : U]
5) 36 9
T i i Q 3
I ﬁ I ﬁ I E I q- Thcrcfuru. in mi» mu PM, I Pm.
12 4% 3b 48 J8 48

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
14
1n rm, it can bi.- easily :huwn that for the newly given distribution th: coir
ditiunal independence assulnptinn (for X‘ and X2 w_r.t_ Y) holds. Therefore,
the predictions made by Naive Raye-i and Joint Hayes will raincide.

***************Ending Page***************

***************Beginning Page***************
***************page number:35**************
as

e. Fol' our daLaseL. Naive Bayes needs to compute Lhe following probabilities:

P(Y:0) Q P[Y:1):1’P(Y:0)

P(Xl:U\Y:U) 1 P[X|*l\ViuyiliPQﬂiUHfiu)

P(Xl:U\Y:l) e P[Xl* l \V*!)*I’P(Xl*U\Y* l)

P(X1:IJ\Y:D) e l~’[X1’llyi0)’l*P(X1’0H/’D)

P(Xl:ﬂlY:l) j P[Xl:l\§':1):l Pagan/:1)
Therefore, we wlll need only 5 values in order le fully construct the Naive
Bayes classiﬁer.
In the general nasm when n input attributes / variables are given, we. need m
estimate mm I'M} l Y] and 11x, ﬂy) for l e 1.1“ therefurc 2H+l values /
pal-anmtcrs.

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
as
For the Jnmt Bayex we need to estimate
P(Y:U) ¢P(Y:1):1e1><v:n)
P(X1:Q,X1:U\Y:O) eP(xr:1.x2:1\Y:n):
ﬁx, :n_x2:1\yZn) l*(P[X‘:U.X2:U\Y:l))+P(Y1:[LY1II‘YIMA
P(X‘:I,X,:U\Y:(J) mx‘:1x2:u\v:m)_
P(X\:0,X2:U\Y:l) =P(x,:1.x2:1w:1):
P(X‘:(],X2:1‘Y:l) |’(P[X‘:u X2:U\Y:1)+P(X1:U Y1:1\Y:1]*
P(X,:LX2:H\Y:1) P(Aq:1_x¢:u\v:1)).
Forrthe gerrerel case, when 1, imput variables are given, we will need to estimate my),
P(Y| - X“ \ Y) and mx, X,‘ \ 4'), where
X', e {X,.*X,)\1r e 1.”
and 4 t
[X1--- .X“) # (*Xr- --w\'~)
Thereform 2(2" e 1)+ 1 : 2" H e 1 values / parameters.
It em. be seen that Naive Bayes uses a lineal‘ rmmher of parameters (w.r.t. 1,, the mrmher
ermput attributes), while Joint Beyes uses sh exponential rmmber of parameters (wtrJt
the same u).

***************Ending Page***************

***************Beginning Page***************
***************page number:37**************
n
Exemplifying
Spam ﬁltering using the Naive Bayes algorithm
CMU, 2009 spring, Ziv Bar-Joseph, midterm, pr. 2

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
ll?‘

About 2/3 of your email is spam, so you downloaded an open
source spam ﬁlter based on word occurrences that uses the Naive

Bayes classiﬁer.

Assume you collected the following regular and spam mails to

train the classiﬁer, and only three words are informative for this
classiﬁcation, i.e., each email is represented as a 3»dimensional
binary vector whose components indicate whether the respective

word is contained in the email‘

———-

***************Ending Page***************

***************Beginning Page***************
***************page number:39**************
39

a. You ﬁnd that the spam ﬁlter nses a prior P(spam) I 0 ll Explain
(in one sentenoe) why this might be sensible.
bt Compote the Naive Bayes parameters, nsing Maximum Likeli-
hood Estimation (MLE) and applying Laplace’s rule (“add-one“),
o. Based on the prior and conditional probabilities above, give the
model probability P<spaml s) that the sentenee

is = “money for psyohology study”
is spam

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
40
Answer:
a. It is worse for regular emails in be classiﬁed us Span] than it is fnr spam email 1p be
rlnssiﬁed us regular email.
11. When estimating the Naive Bayes parameters from training data only usiug the
MLE (rnuxilnum likelihuud estimation) method wl: would have:
_ 1
P(srur1y1spuui): g :11 l (fmd'cgma'F j
:1 1 1
11(siudy1ruguiai-p I lumuucyispsuip g i 5
x
r. . i , , 1
P(f"-L\*P=““)* 8 i 1 P(m0ncy\rcgular)i 3
By applying Lupiucs1s rule (“add-one”) we g9‘:
s 1 1 2
i I I I r 1 - i i i i
P(study‘spam):: 9 I 111 1 (studyirukuiurp 1+2 i 3
a 1 o 1 1 1
l’(free\spam): a I N l’(free\rcg\1lar): i I 5
4 +1 1
i i i , 1 1 1
I (mnmwwam) 11+ 2 2 r’(m<mey\regular) i §
Notice that nus ncrurrenre of 2's at deuumiuumrs corresponds m the number pf valuPs
for each 01 (he attributes used w describe the training instances.

***************Ending Page***************

***************Beginning Page***************
***************page number:41**************
o , 41
c. Classiﬁcation of the message
a I “money for psychology study”,
using the a priori probability P(spam) i 0.1;
P(spam .~) I P[=paun \ study. moo, money)
,, NJ,“ P(siurly. ‘free, money spam) P(sprun)
i P(study,ﬁfre27money spam)P(spam) \ P(study,ﬁfree,money \ reg)P(reg)
mommy {mo money\spam) “F” l’(study\spam)1’(ﬁfrcc\spam)1’(monoy\spam)
, l i l i i
i 10 m 2 i zuu
P(s£udy, ﬁfree, moneyh'eg) “M: ““ P(study\reg)-P(ﬁfree\reg) P(money\reg)
i Z Z l i i
3 a :s 27
Therel'urc, Notice um this is a small probability‘
i i However, wZLhouL using Laplacc’s rule,
2uu m . n would be u due w the rm um the
I’ - o = i mum ‘
(“PM ’) 1 I } 4 _ u ' word study did not appear in any of the
znn m 27 m spam emails in mo training data.

***************Ending Page***************

***************Beginning Page***************
***************page number:42**************
12
Naive Bayes and Joint Bayes:
application when a joint probabilistic distribution
(on input + output variables) is provided
ClViU, 20H] spring, T. Niitcllcll, E. Xing, A. Singh, midterm pr. 2.1

***************Ending Page***************

***************Beginning Page***************
***************page number:43**************
'13

.1, 1-, U P(.¢1,;,,,;1,)

0 n n n15

0 u | 02.5
Consider the juint probability distribution 0 1 n n on
overlibnolean random variables 1,, ,2, and 0 1 1 0.118
1/ given iii the nearby ﬁgure. 1 u u 0.10

1 n 1 002

1 1 0 0.20

1 1 1 0.15
a. Express m, I 0 \ 14.1,) in tcrms of FUN-2.1, : 0) and
PUMM :1).

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
H

be Cpinplute the marginal probabilities whieh will be used by a
Naive Bayes elassiﬁerl Fill in the following tables.

vim) PM W) HI“ “:1 w

Lv I 0 L11 I 0 u I 0

1/ I 1 u I l u I 1
cl Write out an expression for the value of PQ, I ll’, I 1,12 I n)
predicted by the Naive Bayes elassiﬁeri
d‘ Write out an expression ier the value of P(;l, I 1hr, I 1.12 I 0)
predieted by the Jnint Bayes classiﬁerl
e. The expressions you wrote down for parts (e) and (d) should
be unequal‘ Explain wllyl

***************Ending Page***************

***************Beginning Page***************
***************page number:45**************
45
Answer
a. Using tba deﬁnitinn af conditional probability and than the total probability formula,
we get:
P[r\,)7.!/:U) P[I‘l,11.V/:u)
P I 0 I I
W h‘ '0 Pm V2) Pm. 'l'2,_1/:0)+P(1| thIl)
bl
V l’(y) Pm V) a, I0 ii Il Pm \v) ,1 I n it Il
V u 0 5 V 0 0 40 "15L! V u u 50 u so
V I l 0 5 V Il 0.5a MA V I l 0.54 lit-'16
Explanations:
i PM) was ooaipiitaii as a marginal probability, stating frnm the joint probability,
l,[l\..ll,l/)2
mVI m I Pm In V2 I\\._VIo)+P(1, In.” I l VIm+
+110, I 1, ,1 I u VI0)+I'(T, I1.” I l._V I 0)
I 0115+1LU5+9 l +u2Ins
I’[V I l) I lIl'(VI0) I 0.5

***************Ending Page***************

***************Beginning Page***************
***************page number:46**************
4s
11 11(1. 1 y] wax cunlputcd using again the deﬁnitiun of conditional prubability and 11w“
the 101:1 probability formula:
P(11:0,y:0) P(.1\:Oy:0)
11 u u i i
<" M’ J m/ 0) 11(11 0.1, 1\)+1’(|, 1.1, m
P11":u.1, :11] : PU, : 0,11 : 0 y :11‘;+1>(h:nwZ :1.” :0) :11 1.5+1ms :112
Pm : 1.1,:111 : 1>(1,:1,11 : 0 1/:U)+P['ry :1 1:2 : 1.1,:01 :n 1:112:11 :1
Therefore
1-( o m i o 1
H’ ‘l’ Yuma’ '
and
Pm :1 11/:11): 1 :P(|,, :111.,:u):u1-
Similarly,
1m, : 11y :1) 0214:0112;
11. :0 :1 : i:i:u.ss:~
<11 11/ 1 111:1) (,5
P(J1 :1\_1/:1) :1 P['r1 :011,:1):0:m
Pm 1 1/) was mlculated 1“ a 511111131 Way.

***************Ending Page***************

***************Beginning Page***************
***************page number:47**************
47
c. The Naive Bayes classiﬁer uses the conditional 1ndcpmldenoz “Mm-pm". Therefore:
WU r 11(1, e 1. u AHA/e1) 1m e 1y
1' 1 1, 0 i.
(1/ m 11 ) PM 1 '1 0’
1>\n:1.“:u\v:1j 1’<v:1) (m WW
i1)(.|1’l.lg emL/e1)1)(L/e1)+r(ue1..|, en (/enmq e n) i
i P('u:1\,/:1)PL'Q:0\V:1)P[V:1)
’P(.|1*l\// i UPLQ i my *1)f’(l/ ,1)+ P["\*1‘1 i (1)1’(0 i "M/ i "1PM i 0)
i uzu um us in“
inxJ n54 {WNW-0541.5’ ‘
4. Thu Juint Bayes uluasiﬁcr ducalft usl: the wmmimml independence “asuﬂtpiiun.
Thcrcfuru:
l'tu 1Q u.v 1)
P :1 :1 n :u : —
[1/ \H '~ 1 P(|,,:lzq:l>1/:l)+P[n:l rgeuuﬂn
uuz
i 0024.01 Am’

***************Ending Page***************

***************Beginning Page***************
***************page number:48**************
4x
e. The values calculaled by me Naiy Bayes and respectively clie Joint Bayes
rlassiﬁers for m, a l l i, a 1.’,2 a u) are differem beennse me ronditinnal
indspendence assumption dos not linld.
Indeed,
m.‘ n '1 on, m u l5
P’:U|,:l| :[lIiIiIHI
(" Z h‘ ) [)(I/IU) us j
while
P(r\:0‘l/:ﬂ)-P(r2:[)\l/:[)):l),10,5:02#ﬂ.{
Tlicicrcic. n an: 11 are not conditionally independent w“. '/~

***************Ending Page***************

***************Beginning Page***************
***************page number:49**************
19
Excmplifying
The computation of the error rate for the Naive Baycs algorithm
CMU, 2010 fall, Aarti smgh, HWl, pr. t2

***************Ending Page***************

***************Beginning Page***************
***************page number:50**************
30
Collsitler a sinrple learning problem or determining whether Alice and Bob
l'rern CA will g0 to hiking or not y . Hde e (PF) given the weather cuntlitiulls
x, : slimy e (T F) and x2 . Wm” é (T. F) by a Naive Bayes classiﬁer.
Using mining daia, we estimated the parallleters
mull») I 0's
P(S1HULV‘ Hill») I 0 s. P(Sinm_1/ l aHlke) I n 1
P( Windy Hiku) I 0.4. P( Windy l and“) I 0.5
Assume that ihe irne distribution of Xi. X2, and Y satisﬁes the Naive Bayes
assninpiion oi conditional independence with the above parallleters.
a. What is iha joint prohahiliiy that Alice and Bnh gn to hiking and the weaihar is
sunny and windy, that is P[Smm]/. Mimi/Jilin?
Solution:
PH'MM/ Wm'lv Hm‘) "" 9"" Pq'sinwl/imm-P(ll'lnlli/\Hilv-,) P(Hllrv:):i),?ie0.1 us :IHG.

***************Ending Page***************

***************Beginning Page***************
***************page number:51**************
h. What is the expected errur rate of the Naive Bayes classiﬁer? “1
(luformally, the expected error rate is the probability that an “obrervatidn”/instanoe
randdrnly generated according to the trite probabilistic distribution of data ir incorrectly
classiﬁed by the Naive Bayes algorithm.)
Solution:
P(‘Y].XJ,Y) I
xi x2 Y Pl'XiiY) 1’er Y>-F(v) l’rr<Xi>Xrl PtrlYlX- Xe) ND".
l" 1- 'l' 02 ilo il'i 0.060 1' 0 WHI mnespnhdihg to
F T F v.5 nulls :nmn F [1052174 income‘ [mum
F T T ()2 UJ U5:0.UAO F UiSJ'l'zWI ﬁrmer." shown in
T F F (IT-(li-Ufl :u.l1ri T unwil- ham
T F T M no‘ 05*0141] T U 31mm
T T F urns lm:l»17n F 1732232424
T T T uri (14 il5:n.1sn F HJTTMM
Note:
. <“J , l is thr inlh'llltln hint-tier.-
r'lml e Lnl ., ‘l i
A WW‘ \ "w ‘ its value is l whenever the ass
Z llvmr, \Mvi 1’(Xl.);l.Yl sot-iatrrl (:nndiﬁnn (in um res-r,
A, \,t lztolvi xn + l’) is trne. and 0
: nlusu l 0.040 i 0.115 4 0.11m: 0.435 OLhCrwisc.

***************Ending Page***************

***************Beginning Page***************
***************page number:52**************
52

Next, suppeee that we gather niere inlnrniatien aheut weather eenditiene and

intreduee a new feature denoting whether the weather is x3 Rmm/ or net.

Aysum? that eneh day the weather in CA enn he either "(tiny er Sunny. That

is, it ean nut be both Sunny and Kinny. (similarly, it ean net be a Swill]; and

aicelnv).
e. ln the abeve new eaee. are any el the Naive Bayes aeeurnptiene violated‘! Why (not)?
What ie the joint prehahility that Alice and Bob gm to hiking and the weather is eunny,
windy and net rainy, that is P(Sumly/, Wlmly. ‘Jimmy, mm?
Solution:
The conditional independenee of veriahles given the elass label assumption of Naive
Bayes is vielated. Indeed, knuwing if the weather is Simm/ eenipletely deterniinee
whether it is me/ or netr Therefore, silent and mum are elearlv NOT conditionr
ally independent given Him».

PfSll/l!ll/_ wiad”. ‘Rut/iv Hill»)
a P( innumlml/ Slmm/ “hull/y P[Sll!l!!|/_‘until/‘"1151) P(Hlkv)
@—/
i
""" """" 1'(slmm/llliie> m numb/unite) mum)
r u e u 4 u s r u 1e

***************Ending Page***************


***************Beginning Page***************
***************page number:53**************
4. What ie Lhc expechd CrrOl‘ rate when the Naive Bayes classiﬁer uses all three at- “3
tributes? Dues the performance ofNaive Bayes improve by ubserving the new attribute
Rainy’! Explain why.
Seiiitiem
mix. x, v, Y!
mm Y)
\'. \1 Y, Y 1w. xi x, n 1mm’) Pi'XMV) Pi») HWY. Xi ‘m Immxi \z X.1
f l~ l~ l‘ U U UT’: U T I (10325 l~ U 322M!‘
F F F T I! U UliU 0 N I U 04x0 F U 177M)
F F T F OUT?» U075 0 $:U472‘3S F “(MGZITI
III———-ll
F 'I' F F OUTS UT U (7325 F [)1in (U)
F IIII—_——|
I" T T I‘ U UV» U U7’: U 1K I U U223 l' U TdTHFi
I’ l 'I 'I 0.040 U U ll! U 2 I (I UUUU I‘ U 3123')’,
I l~ F I" 0.175 U17’: U T I (I 1125 'l' U ‘KNUGUT
/ 1- 1- I 0240 0240 0x Z u wet» 1 UUHYJ'H
I V- 'I I‘ U U \T'i U i I (Y U325 l- U 32258!‘
T III———-ll
T T I-' I" 0.|75 U \75 0 7 I U \225 T U JXUULIZ
i III———-ll
T T T F U 17’: (H U UEQK F U In‘)! SUI
I IIII—_——|

***************Ending Page***************

***************Beginning Page***************
***************page number:54**************
54
The new error rate is:
E,>[|UU,\" \v mu ,1 : u mm+ u um+ u 17s +(1175 : u 1', > u 411s (see question
r’).
Important Remark:
Please mam that we always compuhz the error rate with respect m P. the
true distributiom and not Muv which is the disuibution computed by Naive
Bayes by wing me comliLiunal independence a>§ulnptium
Herc ebeve, the Naive Bayes claesillcr performance drop: bcnauhc Lhc condi-
tionul independence assumptions do not hold for the correlated features.

***************Ending Page***************

***************Beginning Page***************
***************page number:55**************
How bad/naive is Naive Bayes?
CMU, 2010 spring, E4 xing, T. Mitchell, A. Singli, midterm, pr, 1.2

***************Ending Page***************

***************Beginning Page***************
***************page number:56**************
if;
Clearly Naive Bayes rnnkee whal, in many cases, are overly elreng
assumptions. Bnl even if those assumptions aren’t true, is it pose
sible that Naive Bayes is still pretty good? He're me will use a
simple eeemple lo eeplore the limitations of Naive Bayes.
Let x, and X2 lie irirdr Bernoulli(0.5) random variables, and let
Y c (1.2; be some deterministic function of the values of xl and
X,.
a. Find a function Y for which the Naive Bayes classiﬁer nae a 50%
error rate. Given the valne of Y, how are x, and x2 eorrelnleol?
b. Show llml for every fnnelion Y, the Naive Bayes classiﬁer will
perform no worse than the one above. Hint: lnere are many Y
fnnelione, but because of symmetries in the problem yon only need
to analyze a few of lnem.

***************Ending Page***************

***************Beginning Page***************
***************page number:57**************
a7
Réspuns

x1 .11 1'

u o 1
a. Consider-Z1111 y deﬁnit conform tabelului 31mm” u 1 2

1 o 2

1 1 1
Dbse'rv/aﬁe: Dam-“1 5p, considers“! valnarea 1111 1' 1mm (ﬁe 1, ﬁg z), ahlnri plltem
s5 stabilim o rcgulé mm 111C111 dacfs 11 cunoa§tom pc .11 55-1 dctcrminém p0
X2 (§1 invers).“ 1mm spus, X‘ osu: unic 11010111111131 do )1: (§i invers) dam
Hind u “1101M 1111-4111 a 1111 y. [m1 cundiﬁu dc irldcpmdmga mndigionum
este incilcati. Mai mun. in acest caz avem maxim“! posibil .12 ,Jiependenﬂi“
111m; we (1.111s variabile (in rapcvrl cu v).
P9, slide-u! urmsmr vom calcula rata erorii inregistrate de algoritmul Bayes
Naiv p0 datclc (1111 161101111 do mai SUS4

~ mm, 1 , 1 Nguln W \, m acuuiw valumr m 1,1 .11 Puntm 1 , 2, 1cg11lav>m x1 11 x2 1“. \Mon
1o11|7>lc1|1v11141¢

***************Ending Page***************

***************Beginning Page***************
***************page number:58**************
511
Bayes Naiv cstimcazh valoarca 1111 1 @1101;
1/ I ;Arg|11;1XP(X1 1 Y :1/1P1X, 1 y :1/)P(Y I 1/)
R1111
Penn-u X1 i 11. x1 i 11_ algoritmul wmpam urmétoarcle (10115 1131011:
1 1 1 1
1* PX i 1/»1 PX i 1/’1 FY"! if , if’
11 11011121111112228
1 1111 011/ 111111 011/ 2111(1/ w) 111 l
‘l’ "’ n ‘I’ i "’-222’s
Cum p, I 111, algoritlnul vu ulcgc unu dintrc B1..- cu u prubabilitutc 11¢ 11.5.
Dcuarccc vuluarcu 1111 y din 1111,01 cstc 1, inscalnnﬁ 1:13 algoritlnul vu ulcgv:
greQit cu D probabilitate .12 11 a.
Pomru cclelalu: a mun, (X, I 1121.7, :1). 121,:1111 :11) 51 (x, :1.x1 :1),
so obscrvh ugor 05 so 01111“ dc ascmcnca mod vgmc, im- algurilrnul va alcgc
pentru Y Una dintre ‘1310:1111 1 sau 2 cu o probabilitate de 0 a.
um panru accastfi (1011111110 a lui 1/ rata crorii cstc do 511%‘

***************Ending Page***************

***************Beginning Page***************
***************page number:59**************
59
h. Vom calcula rata erorii pentru ﬁecare dintre cele s "mm"; de deﬁnire a 1m v care
nu a fest studiat.
Cazul 1; Y, x2 v Em similar .1“ cazul: v
U [I 1 ‘Z
U l 1 ‘Z
l U l ‘l
i l l ‘l
. Pom“ x, I nun I u. ulgoritrnul Bump-Ma;
2 2 1
m i P(X,iﬂH/i1)P(.‘(1’0\Y’1)P(Yil)ii 11’;
m : P(X\ : u \ Y : 2)P(XQ : u \ Y : 2)P(Y : 2) : u n»u : u
Cum ])| > p2 algoritmul alege pentru Y valoarea 1, ceea 1:2 este corect.
- Pentru celelalte 3 razuri, (x, I u x2 I 1), ()q I 1, Y2 I u) 5i (X, :1 x2 I I), se
observﬁ .15 Sp, Oman areleasi valori pentru m 5i 7,2 ca mai 5115, derj algnrﬂmul Blage
(in mod corset) pemm y vale-area 1.
A§adan am obginm c5 rata erorii esua in acest caz 0v

***************Ending Page***************

***************Beginning Page***************
***************page number:60**************
no
anul2: )1] X1 1' Cw"; Y 1' 1/ 1 1' 1 1'
11 11 1 similare: 1 1 2 2 2 2 1
11 1 1 1 z 1 2 2 1 2
1 11 1 2 1 1 2 1 2 2
I I 2 I I I I 2 2 ‘Z
- Pentru x1:11_X, :11:
11,:111'.\'1:1111’ : 111’(.\'1 :1111/ :111111’ : 11: Z 33:13
11 1s 4 vs ﬁgzl
111 :P(X1 :11 1 1' :21P1xg :1111' :2)P1Y:2) :11 11- 11 :11
. Pam“ x1 :11. X1 :11
. . , , , ‘l I 3 I
,1‘:P<,\‘:111¥ :1)P(/\2:I1I :11P11:1)::.: :::
111111 1, :11:1
,1, 11.11 1111/ 21111.11 11y 211111/ 2) 11 1 I u

***************Ending Page***************

***************Beginning Page***************
***************page number:61**************
m
- Penn-u X|:l XQIm
. . , , 1 2 a 1
111*F()\|*1 vinrmru 1*111111 inf’ i if’
3:111 u >17 1
111:1“(X,:1 Y:2)P(X1:n Y:21P(Y:2):1-n 1:11
‘Penh'u Y1 1.x) 11
1 1 1 :1 1
11\:P1X1:1Y:1)P(X1:1Y:1)P(Y:l):f i i:i
J 1; .1 12 QWZ
, 1 1 I’
1;;:P(X1:1 1':2)P1X2:1 Y:!)PQ’:!):1-l 1:4
Deci rata erurii este u pentru acestﬁ deﬁniﬁe a 1111 Y.

***************Ending Page***************

***************Beginning Page***************
***************page number:62**************
m
anul a: X1 X, 1' Cazuri sirnilurc: Y Y 1'
o n 1 2 1 2
11 1 2 1 1 2
1 n 1 2 2 1
l I ‘2 1 l 1
. Pcnh'u x1 I 11.111 :11;
i11lil JulinA > Q11 1)
m i 5 5 i I’ 1'1 i 5 i i ﬁ n1 m u i (mmc
- Pam“. X‘ :11 x2 I 1:
1 1 1 1 1 t
m 5 u 5 0.11,» 5 1 5 I )1,‘ Km >,, 2 (cored)
- Pentru .11 1m 1n
1 1 1 1 1 V
[11’; 1 iii/11’?0Eil)~p1>m~>yil(corocl.)
. Pentru Y1’ 1.x2 *1:
1 1 1 1 1
,1, E 11 E 0.112 5 1 E I >111<112 >17 2 (cored)
Prin urmare, rm erorii este 11 51 in acest 1-81.

***************Ending Page***************

***************Beginning Page***************
***************page number:63**************
m

anul 4; (cel d2 la punctul u) X‘ X; Y Esta sinlilur cu cazul: Y

u 0 1 2

u 1 2 1

I n 1 I

| 1 | z
Concluzie: Doar pentru 2 rnorluri (cazul 4) d2 deﬁnite a lui V rata erorii Esme
de 51M; pemm $81918"? 14 moduri (cazurile L 2, 3) rats erorii (we u.

***************Ending Page***************

***************Beginning Page***************
***************page number:64**************
(H
Unlike Naive Bayes7
the Joint Bayes classiﬁer has 0 training error rate for
all boolcan functions
(and even all mathematically deﬁned functions cln categorical attributes);
“in-between Naive and Joint” Bayesian classiﬁers
CMU, 2004 fall‘, Tl Mitchell, z‘ Bar-Joseph, st, pr‘ 1l2

***************Ending Page***************

***************Beginning Page***************
***************page number:65**************
a;
Suppusc we have a function Y I (A /\ 13) v am v a‘), where A B 1‘ are indepenn
dent binary random variables, each having a 50% chance of being u.
a. How many parameters a Naive Bayes classiﬁer needs tn estimate (without
rnunting P( v) as a parameter if Pm) is already counted as au estimated
pal-axllcter)?
What will bc- thc- crrur rate of the Naivc Baycs classiﬁcr (asauming inﬁnitc
training data)?
h. Hclw many parameters the Joint Bayes classiﬁer needs ta estimate? What
will he the errur rate of the .lnint Bayes classiﬁer (assuming inﬁnite training
data)?
Comicntic: in cazul in carc, pcntru 0 sctarc ourccurc a vuriabilclur A, 13 gi (l,
cclc daua probabilitati calculate dc eatrc ulgaritmul Buycs Naiv in vudcrc'al
determinél'ii valnrii W, sunt egale, canvenim ca algoritrnul va lua decizia
'm'n a L

***************Ending Page***************

***************Beginning Page***************
***************page number:66**************
Answer 65
my I w! RTE-Y) P(Y I 1AA I n. R I111,‘ I 1‘)
i , PM I u B I 1,,(1I 1111' I 1,) . my I 1/)
i ‘m’ n;'1‘95ﬁ1> PM I u. B I 1,0 I 1-)
I Mg max PM I 11.2? I 11.0 I (‘\Y I y) my I ,,)
HEW“) 1
I Mg 151% P[A I 0D’ I 1,) m5 I 1111' I y] P(C I 111/ I 1,) my I y]
HI 1
Naive Bayes will need m estimate:
P(1/I<1)IP(1/I1)I1IP(YI<1)
PM I my I n) I, PM I 111' I 0) I 1 I P(/\ I my I n)
P(,1In\1/I1)I»P(,1I1\1/I1)I1IP<AIo\yI1)
Ponon)IPwI11yI11)I1I1J<qu\1'Io)
P<EI01YI1>I>P(BI1\YI1)I1IP<BIu\yI1)
me I my I u) I mc I m’ I u) I 1 I P(C I my I u;
P(cI11\1'I1>I> mcI 111'I 1)I 1 I P((7:u\Y: 1)
Altogether n is 7 parameters.
In general, fur n input binary variables it would have been 2n +1 parameters‘

***************Ending Page***************

***************Beginning Page***************
***************page number:67**************
m
113M?) | Y
U U U U U l
To culnputc the error rate we L'un tun» 0 0 1 0 l 0
struct a Boulean table of the func’ “ 1 “ Y‘ 1 0
tinn and use it to estimme prnhahiliﬁsi “ 1 1 Y’ 1 0
(=1an we assume inﬁnite ‘raining dam). 1 1: ‘1' 8 2’ h
1 l \Y 1 1 1
1 1 1 1 1 1
The estimated parameters are:
1 1 1 ' > ' i i 1 i 1
111:0]:EH111Z1JZ1411imipéi5
111111] A 1» 1w Y) M111 f 11
11 1 2 2 1 :1
1 :1 2 2 , :1 1

***************Ending Page***************

***************Beginning Page***************
***************page number:68**************
mt

The predictions of the Naive Bayes classiﬁer are tlieu as follows (assuming
that iu rasa of a tie it always predicts 1):

ll B r‘ P11111113 (‘.Y :11) Pw/(A B 111/: 1) 1M t-u.

ll 0 0 :1/1 1/2 1/1 1/2 1/1 1/2 :1/1 1/2 1 no

n 1/ 1 1/1 , .-.1/1 1/1“ >1/1 1/ no

u 1 1» 15/4 1/4 1/4 11/4 1 WW

u l l 15/4 5/4 1/1 1/4 u no

l 0 u 1/1 1/1 3/1 11/1 l no

l 0 1 1/11". :1/1 :l/t t. 1/1 1 \w

l 1 1/ 1/1 1/1 :l/t "Ya/1 1 uu

l 1 1 1/1> .-.1/1 , 11/1“ 1/1 1 no
“0m this, we can eeiupute the error rate as the number of mistakes aver the
number of possible inputs (siuee eaeli iuput is equally likely): arrnr rate :
2/); i U 25.

***************Ending Page***************

***************Beginning Page***************
***************page number:69**************
59
Z l FY: AZ.BZl,z:Z.
m and, 1,93% ) ( y 0 l)
i > _ PMZn.uZbL-Zl-l'Zyym'Zy)
’ “gyéﬁfﬁn PM Z u. B Z I’. c Z l»)
Z mg nlnx HA :uJi': I: CZ Pl)’ Z“) my Z“)
yarn/w)
Joint Bayes will need m estimate:
my Z n) Z‘ PW Z 1] Z l Z my Z n)
P(.4Z0.BZo,cZolyZm
PMIO'BIM'IWFO) ZlP(AZ1,uZl.Cle1,Zo)Z 1Z7.
PM: I_E:l cZulyZu)
PM Z n. B Z 0,1‘ Z my Z 1)
£14:anan :1‘-":1) ZlP(AZ1,BZ1.0Z1lyZ1)ZlZv..
p(,1ZlRZchul,JZ1)
Thus‘ it is 1+ 2 (2* Z1) Z 1.5. ln general, for n input binary random variables
it would have been 1+2 (2" Z 1) Z 2"H Z1.
The error rate of the Joint Bayes classiﬁer is zero (assuming inﬁnite training
data) since it Dan mudol an arbitrary complex Buolean functiun.

***************Ending Page***************

***************Beginning Page***************
***************page number:70**************
10
e. Consider a Bayes elassiﬁer that assnnies that A is independent of t‘ when
eenditiened cln B and on Y (nnlihe a Naive Bayes elassiﬁer that essnrnes that
.4. B. c’ are all independent of eaeh nther when conditioned on l’).
Show that this Bayes classiﬁer will need ta estimate fewer parameters than
the .lnint Bayes classiﬁer. hnt will still have the sarne error rate (assuming
inﬁnite training data). chinpnte the errnr rate of this classiﬁer.
Answer
Starting from y (A a 1st v aw v (‘t l/l A 1:) v (tan) /\ (4'))‘ one can infer
that A is independent 01' C when earnlitiened on u and on l:
LC: Jnst tn renvint-e ynnrsell...
By using the tt-nth table which we have already written for y , [AA my 1m
C), you can easily check the eqnellties
Pm I 1w I Ill? I t Y I t1: Pm I all? I rtl' I l” 17(1) I t5 I m I h)
if yclu analyse i for eaeh nf the [our eamhinatien of valnes for h and 1/ e
(u l), separately i the prnhahilities for PtA I ".r‘ : tl 1. PM I nl >,
m7 , ll ).

***************Ending Page***************

***************Beginning Page***************
***************page number:71**************
11
Therelore ['(A.B.Cll') I P(A.('\B.Y) Pwll') I ['(AW. Y) Pu? BY) I-(Bll/y
For any input ,4 I u. B I 1,, (‘ I t, our new Bayes classiﬁer will predict
um was,“ I sue hiss) P(A I “\B I b Y I u) Pu,‘ I MB I r, Y I nyt'B I my I l/).P(Y' I y).
Us on
which, due to the above equality, will he exactly the same as the output of
Joint Bayes.
Therefore, the error rate for new Bayes classiﬁer will he zero.
The parameters it neerls to estiniete are:
Pry I uli
mu Ioll/ I0), HuI all I l)‘
m1 I mu I o YIt»), rm I MB I o l'I l)‘ HA I ulu I 1.)’:D)t PM I u u I l. y I l),
P\Z‘:O\B I (Ll/:0), mrInlB I nyI l), P(C:ﬁ\[i I l Y:0]. PqulB I 1. \’:1).
Thus_ there are 11 parameters to estimates which is more thsn what’s required
hy Naive Bayes (1), but less than Joint Bayes (15).

***************Ending Page***************

***************Beginning Page***************
***************page number:72**************
72
LC: Just to convince yourself...
P(Y I111I1,12,
Using the Boolean table of r111 :111/ I111 I 112, 11111 I 1111 I 1) I 1,12,
y I 1111111111 vau), r1,1I111B I111’ I111 I 1,2, r1,1I111B I111’ I11I1/2,
the catilnutiuns of them.‘ pu- MIME :1 \':111:1. I>1A:n111:1>1 : 11:11.
rameters are: P11? 11111 11 v 111 11. P1!‘ 11111 11 Y 11 1.
P(C I 11111 I 1.1' I111 I 1,12, 111(1I11111 I1 v I11I1,121
The predictions of the (‘lassiﬁer are then as follows (there are n0 1195111
11111111. 1'I111 11101111’ I111 111.1111 1 I 11 HUB.) I 11-
.11 n 1* r1011’ 111 my 111 Pl)? 1' 11 PO’ 11 1,,” 1-11
u 11 11 112 11 1/2112 112 1 1,12 1/2 1 no
11 11 1 1,2 1 112 11 11 1111
11 1 11 1 1/2 . 11 1,12 . 11 110
11 1 1 1 112 11 112 11 1111
1 11 11 112 11 1,12 1 1 1 1.1,
1 11 1 1,12 1 112 11 11 1111
1 1 11 11 1,12 1 112 1 1.1,
1 1 1 11 1/2 > 1 1,12 1 110
T111, corruaponds 10 20w crrur mm.

***************Ending Page***************

***************Beginning Page***************
***************page number:73**************
71X
Computing
The sample complexity ofthc Naive Bayes and Joint Bayes Clssiﬁcrs
CMU. 2010 spring, Eric Xing, Tom Mitchell, Aarti Singh, HWz, pr‘ 1.1

***************Ending Page***************

***************Beginning Page***************
***************page number:74**************
74

A big reason we use the Naive Bayes classiﬁer is that it requires less training
data than the Joint Bayes Classiﬁer. This exercise should give yuu a “feeling”
i'er hew great the disparity really isr
Imagine that eaeh inaioeee is an independent “observation” of the inulti-
val-late random variable ,\ e (x,.. ,th where the x, are i.i.d. and Bernoulli
er pal-erneter p I 0.5.
To trnin the Joint Bayes elessiﬁer, we need Lo see every vnlue of X “enough”
times; training the Naive Bayes elassirier unly requires seeing huth values oi
x, “enough” tirnes.
Main Question: How rnuny “obscrvatiuns”/inslanncs are needed until, with
prehability l e s, we have seen every variable we need te see at less-t once’:
Note: To train the elassirlers well would require inure than this‘ hut ier this
problem we only require one observation.
Hint: Yeu may wsurt te use the i'ellowing uteqmriiiiesr

e Fur unyi :1,(l*1/li')* 5w‘

e For any events Ln. his me, U um 5 XL, mu,»

(This is called the "union bounds“ propel-Ly.)

***************Ending Page***************

***************Beginning Page***************
***************page number:75**************
Consider the Naive Bayes classiﬁer.
a. Show that if N observations have been made, the probability that a given value of
t
x, (either 0 or 1) has not been seen is Ni‘
b. Show that tr mere than NM; I 1 +1ng2 f observations have been made, then the
probability that any x‘ has not been ubscrvcd in both states is g s.
Solution:
I, )t 'bl‘ i1‘1”’2’1
a. [component , not seen m ut t ntalcs) i 5 i 5 i F i F
h. P(uny cmnpunent net seen in heth slates)
< XL, P(cc\mp0nen£ X, net seen in beth states)
i ,| \ i l i l i \ i l i 5 i p
*2,th m” m*"'W"i 7"’ E"

***************Ending Page***************

***************Beginning Page***************
***************page number:76**************
1n
Consider the Joint Bayes classiﬁer.
e. Let t he a pertirnler value of X. Show thnt after N observations, the prnhnhility
thnt we have never seen r is g WW".
2t!
d. Using the “unicm hounds“ property, show thnt if more than ivw I 2“11\(i)
observatiuns have been made, then the prehehility that any value er X has not
been seen is g e.
Solution:
c. m; not seen in N observations)
1 .t 1 2“ W" I W"
i i i ‘7M!’
d. P(any .i, nnt seen in N,“ observatinns)
g Z; P(t nnt seen in N,” ehservntiens)
< - en,” I ,i_ in,“ :Miﬂenn: :gt :i:
,L,t 2 t z t _ (ML, e

***************Ending Page***************

***************Beginning Page***************
***************page number:77**************
17
a. Let d: 2 and 5 : u I. What are the values of MM; and ME?
What about ‘I : a?
And a: m?
501mm“
2
MB 1+1‘,me 1+1ugZ-zu~ 5:2
5 I U 1 (I : 2 a 2,‘
NW: 224“ : 14an 147;
HJ
NW l+lug7ﬁ l+lug150w bti-i
5 I U l (I : 5 Q 2,)
N,” : 2W1“ : 3241.320 m max
u 1
10
NW i WW1 Tl i 1’ Inga 100: 704
5:Ul 11:1!) Q 9m
IV”, : 2*“ 11161 : 1021 1n 102mm n15; m

***************Ending Page***************

***************Beginning Page***************
***************page number:78**************
vs
The relationship between [the decision rules of]
Naive Bayes and Logistic Regression;

the case of Boolean input variables

CMU. znns falL Tom Mitchell, Andrew lVlnm-e, sz, pr. 2
CMU, 2mm fall, Carlns Cuestrin, HWI, pr. 4.1.2
CMU. 2mm ran, (1205 Gordon, HWA, pr. 1.27:1

CMU. 2ni2 fall, Tom hiitchelL zw Bar'Joseph, sz, pr. 3.;

***************Ending Page***************


***************Beginning Page***************
***************page number:79**************
79
a. [NB and LR: the relationship between the eleeisien rules]
In Tom‘s draft rhapter (Generative rmd dism'imiuative classtﬁers: Nﬂive
Bayes and logislm vegteasitm) it has been proved that when Y follows a
Bernoulli distribution and .\' i (Xin ..X4] i3 a VDCLOI" Dr Gaussian variables,
then nmler eertain aasnmptiens the Gaussian Naive Bayes elassiiier implies
that Ptl'ix) is given by the legistie innetien with apprepriate parameters lit
Se,
I
PtY:1\X): 4
1 l tannin, - Ddritxg
and therefore.
.l
m’ e nix) : “PM +Lilt “ J
1+ eavttn + Ls‘ ex.)
Consider instead the ease where Y is Beelean (rnere generally, Bernenlli) arnl
X : (X1 . Xi!) i! u vcctul‘ of Buulcun vuriublcsi Provl.‘ fur this (:an illSU that
Piyixl follows this same form anrl henee that Logistic Regression is alse the
diserirninative eennterpart te a Naive Bayes generative classiﬁer ever Beelean
features.
Note: uiserirninative classiﬁers learn the parameters ei Vii/pr) directlyt
whereas generative eiassiiiers instead [cal-ll the parameter-s ei rtx Y) and
Pit ).

***************Ending Page***************

***************Beginning Page***************
***************page number:80**************
80

Hints:
ll Simple netatien will help, Since the X,’s are Boolean variables,
you need only one parameter to deﬁne, P[X,\Y I m), for each
lils . .di
Deﬁne a‘, I Pun :1\Y:1],in which case PfX, I my :1):1IH,1,
Similarly, use H,“ tn denote P(X, : HY : n).
2. Netiee that with the above nntatinn you can represent P( YJY I
l) as follows:

PleY i 11 *Hﬁ'll can“ ‘I
except the cases when a‘. I n and X‘ I n, respectively s,‘ I l and
X! I ll Note that when x; I ] the secnnd term is equal tn 1
because its exponent is zero. Similarly, when x‘ I 0 the ﬁrst term
is equal tc 1 because its exponent is zero.

***************Ending Page***************

***************Beginning Page***************
***************page number:81**************
5011mm. 81
> . P(X:r\Y:l)P(Y:1]
P) :1A:J)”:‘ +
( LN, ., PM : HY I mm I w
i 1
1+ 11x m’ mm 0)
P[X : m I um :1) 001mm“:
1 \V'!\:|v:\]!'[y:h¢n;
1 1 PM’ :,,,\y :U]P(Y :H) z. 1m 7 l v iiul'lYiolfm
+‘XP “W x. l'[\ i MY i m ,l u Ind
m 7 my i \> w
i 1
i P(.\’\ :11 ,XA : my :owu :0]
1m" (1“ Pm .1 .M Mu UFO’ n
i v I’ yin! 41 I‘ er‘ y in!
‘+0!’ (h' m i 2:, ‘" W)
Priur probabilities are: ['(Y 11"“ w and my n) 1’ m
Alsn, enrh X, fnllnws a Bernoulli distrihminn:
mmy i 1) i MM 4m“ "U and l'()x,\Y 41) i 0;};11 film)“ "M

***************Ending Page***************

***************Beginning Page***************
***************page number:82**************
5°, 82
l
1’(Y:1\X:w) : —x,1\
If" u “(Ii/m,‘ ’ "
1 \ m 1n > , 1“ '
‘( w 2*‘ 11ml HHJUJH
i I
i 1’, , e, PH.
l+vxp 1HJ+ZL, X,lnl+[li)(,)lnim
W ‘ m liml
1
i I i, 1’ 9 , H I i H
1 \ “$41,, K1 > 27:, m1 H: >21; x, (In if 1n‘ H?»
Therefore. in urdur w rcuuh
,1
PU’ : nx : 1):1,/(1 4 t‘XD(U'n > 2mm)
,Z‘
we can set
,1
1’ v. I i H, H‘ 1*9,
m, : In W \ gm‘ 9,‘: and n!‘ :1“ H": ‘"1 14,‘: fur , :1 H4

***************Ending Page***************

***************Beginning Page***************
***************page number:83**************
m
Note
Although hcrc WG derived for P(Y\X) a form which is speciﬁc t0
Logistic Regression starting from the decision rule (better said,
from the expression of P(Y I 1\X)) used by Naive Bayes, this
docs not mean that Logistic Regression itsclf uses the conditional
independence assumption

***************Ending Page***************

***************Beginning Page***************
***************page number:84**************
PH
u [Relaxing the ecnditicnel independence assumption]
To cuptucc internetiens lit-tween reutures. tlic Lugistic Regression mullul cuu
hc supplemented with extra terrns. Ful- cxulnplc, u tccrn cuu be udded tu
capture a dependency between xi and Xe:
P(Y r lix‘ r 1
’ P 1 - txriiu, i unexixz i 21;. ‘l'iX~)
Similarly. the conditiunel independence assumptions made by Naive Beves
can be relaxed 50 that Al and )xz are not assumed to be conditionally inde-
pendent, lu this case. we can write:
PYPXXY nPXY
WW: I > l i 2‘ ll'IH r ii >
Pu’;
Provc tlisit l'ur tliie cusc, tliut P(l'\X) fullows the sniric fol-In us the logistic
regression mudcl supplemented with thc extra terrn tliut cupturcs the de
pendency between X, and X2 (and hence that the supplemented Logistic Re
gressinn model is the discriminative cnunterpnrt te this generative classiﬁer).

***************Ending Page***************

***************Beginning Page***************
***************page number:85**************
85
11mm
1. Using simple notatinn will help llcrc as well. You ncccl more
parameters than before m deﬁne P(Xl. x2, Y)‘ sﬂ leus deﬁne M I
P(X. I» X, In’ I 1;), for each 1,, and k.
2. The abovc notation can he used w represent Pm. my I k) as
follows:
Pmm' i k) i m)“ "l WWW X1) W)" W mall)" W Y»
for k E (0,1), HXl-epl for the cases when a,“ I u and xix) I 0, 0r
llm I n and xlu x2) I 0, m‘ ,imk I 0 and (1 xgxz I n, or am I 0
and(1I x,)(l I X3) I n.

***************Ending Page***************

***************Beginning Page***************
***************page number:86**************
as
Solution
"y ‘x mxwimwin
' ’ ‘ ' ’ quiwwi\>+1Ty,\u47mmTm
1

* 4‘ Plin/‘ﬂrmin 7m

‘hLXh 7 IWY :1,
i ‘
i v ‘ FUN/A!!!“ in‘)

*‘ ‘P ( “ mxw 7 “my 7 n, Cmulmum
l 1.Pxx7.»7vlf’n71y¢m

\ , HM xlwiw r1;L\I'<\v\> *ml'wim z w 7 . v 7mm 7W7";

*‘X’ m, w» 7h “(:vmx \Y:Y]I'U 7|) a “MM, 7 m r K1 m
i K 1mm» 7 n + u; mm 7
i v7" Mm :m w, w» :nv) ,

H 7 . u H , 0 u-ndllx‘)7n‘u
\

i x71 ¢1*'1|71/.Hv"'\" <4W>“‘mmﬂ\‘ Mum,‘ “WWW” W‘ Y‘)

“ ‘(‘ . ‘Em m. 7M!“ w mm“ mmw Wm ‘ MW ‘ ,,

y

7 —\7-‘ m ‘7m. 417m 4/

***************Ending Page***************

***************Beginning Page***************
***************page number:87**************
P57
Wm.
1 r " 1 a i
. ‘ ,, ‘W
u : mi‘ lni+lni
" i é km, mm
Jm Jum
u. : 1ni+lni
‘fun 3mm
7mm Mn
“2 i Ini+lni
Jun ‘1mm
a a 1 1 ‘
m 2 m i" +1“ l‘ +1“ L“ +1“ l"
Jm ‘1W m 1m
5,, 1 a,
u, I mﬁHHiUJmHIx. (Iv

***************Ending Page***************

***************Beginning Page***************
***************page number:88**************
m
Maximum Likelihood Estimation (MLE)
for the parameters of Naive Bayes
and the linearity of the model
i the case of Boolean variables i
StanfnrrL 2001 faIL Andrew Ng, HWI, pr. 4

***************Ending Page***************

***************Beginning Page***************
***************page number:89**************
P19
In this problem, we lnnk m maximum likelihood parameter estimation using
the Naive Bayes assumpﬁon. Here, the inpm features 11, 1 I I .1! in
our model are discrete, binary-valued variables_ so 1/ e (0.1). We call 'l I
(1,. 11. ...!,:)T to be the input vector. For each training example, our output
targets are a single binaryIvalue g I (0,1). Dur lnudel is then parameterized
by all, I Pgi-J I 111/ I n), 0,, I P41, I 11y I 1): and 0,, I P(., I 1]. We made] the
joint. distribution of r w) according tn
My) I (ﬂv)”[1 Iﬂv)‘ y (the exponentiation trick)
My I 0) I 1'1 PM” I 0) I 11(6me mm»
,Ii ,Ii
Pm I 11 I 1'1 Pw/ I 11 I [[uwwl I an“)

***************Ending Page***************

***************Beginning Page***************
***************page number:90**************
Exempllﬁcauon [by lelu 0mm]
Levi consider the Example | NotHeavy Smelly Spotted Smooth | Edible
“Barby "aim"? dame" " Imi-
By ﬁmly renaming 1{ I 1 D 1 ° l-
am inpm “ml-nun» H, 1 0 1 0 1 1
x\,.\l X< and respec’ 17 | 0 0 D 1 I“
Lively X“ and woudly F |—|.-
the UMP-It variable as I‘ |—|-
y, the m D: parametrri u | 1 n 0 1 |u
(a) will (‘unaist "r: n | 0 1 0 o |-
v“, : r14\\:1\):oy‘uw: 1’(x.:m/ : u, um : [ml : m' :0)‘ on : m'xl : my :11,
H“, i mx, i m' i n), H“ i PrxK i my i \>_ H.“ i P[X\*1\Yim.,ﬂni P(\'|*I\Y*1),
1/,,: rm :11.
Hy donating v i <4 n m, and aabuming um all Um ahnvu training mm“, an: prnba-
bilistically independent of each umber, Lhe verosimility function of v will be
14(0) :mw/y rum/J mu 1/) where

PUW i 9!\(1’HU)1\* 9n)[|*/q1|1#v- PUWY')’ *7u1\* 9109M] “9|qu

HUM : u HUM/ﬂu 1/<\)1/1,II,,_1’(I)W\ : (1 1mm 010M Hun/“(1 m.

HEW) "m/bn/huﬂ *UmHl i 17V) HFW) ﬂmU *1/zu)”m17n>k1 i lb)

HEW) : 9m! *RJUKY i EmeU “ﬂy WHW) : {Y i FYwWNU i "MW i MM! *h'd

***************Ending Page***************

***************Beginning Page***************
***************page number:91**************
91
Exempliﬁcation conﬂd
L(|7) i P(.<'\M] P(B'\17] PU!!!)
: enuimuimnufame” 5v1(l*91\)5u(1*49u)gv
(liﬁnwmvmmm (1*vumimmuivmvuuim
910520430“ 41°10 fay) mu , wamu fay)
emu fem , mom fay) (1*9\lx)$'10(l* mu imu , a”)
I Hitlivmlviuuivwl" M1421)’ #00420)“
9x1(l*Ru)z ﬂémliem)‘ gnufanf-ﬁnuiem’ 5:-(l*5.,)5
1(a) '2 mm!)
: 2l|n9n+lv\(li”“)*\§ln9m+2ln[l*ﬂm]+
1116“ + mu , an) ~ mom + mu , an) +
1mm +21n(li311)*2l|\19;u+3ln(li619)+
1116',“ +2h\(l *ﬂyn)*§§|||9m+2|n[l i910] +
3mm, A 5l|\[l i av)
aifWFO“; i \fo :DQS,:;
Similarly, one would w a. : g, é", : i 0'1, : g‘ 92., : Z m. : f? m. : g A, : g‘ 54., Z g.

***************Ending Page***************

***************Beginning Page***************
***************page number:92**************
Exempliﬁcation mum 92
For the following test datset
—| Nthemly Smelly Spotted Smooth Edlble
—| 0 l 1 1 7
—|— 5’
—|— 7
Naive Beyee will produce the following mules:
1 ,l
. w fl g n‘ , I A v
PLdeble:l\ll):l 1 2X l 5 :m'\j~¢ll\'u([]:l)
:Tl if? i
2" d 2‘
llndmzeelln Billie/21“ EfferZ-“iwi Q <2Q|M<lleo
llxslgllsl alz
2* J 2*
j l j * y 1 1 ,
1(Edlble:l\\l>5e:%r _i 2) J2 r) ZWZj>5QWm>Zl
3‘ e4 a‘ >4 3* a‘ 21*

***************Ending Page***************

***************Beginning Page***************
***************page number:93**************
93
a. Find me joint likelihood function mi) I 1|!Hf‘i‘P(1'(".y(" 0) in terms of the
model parameters given above. Here s repreeents me entire set of parameters
(e.,.e,",s,\.,:1... 1!).
Solution
[(51:111HPIH'RI/WH)
1:\
: \.,HP[r<*",/w 5]P(;/\" H)
.el
1|1H(HmmWinnrwhin)
e‘ P‘
I Z 1n P(|/“':H; ‘Zulm'ﬂw ,H)
I Z:[;/\"]\|5,/+(liI/"’)ln[l*5,/)’Z:ILW'11\H,VH +(1e,,§")mueww,,)]
.el H

***************Ending Page***************

***************Beginning Page***************
***************page number:94**************
94
bl Shuw tllat the pal-anletel-s which maximize the likelihood function are the
same ast hose given in the lecture nutes; i.e.,

0 , 2,:ill,p:m,,m:l,, I] , Eel Ivyldmwﬂl 0 , 21;,1WVZH
0 e i , i i i
J Zi:l‘l(l1“':il) " 2:1 [iv/Md) ” "I
Solution
The only terllls in 1(0) which are non-constant with respect m 17,‘, are those
which incluLlc H] V Tlluruful'c,
WW] o h] W , l
i i 1/ m i I i ,l
wall (m D 1-1 + <1 , 11ml m 1,»
I mL(1§'lll9,u1WlHn ‘<1 e [Pllml emullwlem)
H
i 2(1‘ 1 1i )iglirm] 1 1w l)
i we“ l, :u

***************Ending Page***************

***************Beginning Page***************
***************page number:95**************
95
Solution (mum)
Sm‘ 01H)*H'
e “gay/U i gwes
1 1
0 i Wig“, ili'miiwi Q
;(U M n in) ( l, 10%,“) u 7m)
0 I Z(Amiwlwzwi<1iﬂ>umiwzu,)
,,,
I ZHM wwlmhﬂn) :Z(r‘,‘>1<v~»n,) $021M“,
I ZUgwww'w) 5"’le ‘Aw
We than arrive at our desired result
0 £1111‘,yu1w,u)
J“ Eli‘ lw~:“»
The solution for 0,1 proceeds in the identical manner.

***************Ending Page***************

***************Beginning Page***************
***************page number:96**************
9n
Solution (mum)
To Bum fur a”,
TWM) I 07“ Z‘: (v 1M‘, + (1 I 1, 11m I m)
\ 1 y \ 1
i (/"IIMIx/Wi
;( a‘, 1I a”)
[hen setting in") I u glves us
an”
1 1
: ('li _ i ('1 i
n I Z (M1 Ia?) I <1 IMMU) I 2W I 2H,, I 2W I M‘,
.4 H H H
Therefore,
i XII! WWII»
0” m

***************Ending Page***************

***************Beginning Page***************
***************page number:97**************
97

Nata: lt c-en hc cusily shown thet thc Hcssien nnrtrix of thc lugrvcroeimility
function m) is negative deﬁnite. therefore m) is a concave function, and the
above solutions H". m. a,‘ for i e l ,n correspond tc its maximum.
c. Consider making e prediction on some new clot-e point i, using the most
lilrely cless estirnete genereterl by the Naive Beyes algorithm. show thet the
hypothesis returned by Naive Bayes is a linenr classiﬁer i i.e., if m, e n r)
and Pu, : H1) are the cluss probabilities returned by Naive Bayes‘ show that
there exists some n‘ e 1K"H such thet

P(_1/ :1\1,)2 P(t/ e nlr) ii and only if ui'[ 1r] 3 n (VT)
(Assume w is en intcrccpt tcrrn.)

***************Ending Page***************

***************Beginning Page***************
***************page number:98**************
98
Solution
‘ P(,/:1\J) P11u:1)}*<v:11 M
1 > l i) i i)
10/ ULJJQ/ (mom, (“0,10 PM HIM WW 01*‘
h l'lj',,Pw,\1/:I)P(v:l)>‘h TILL]([H,\)"[1*H/\J"")Hu >x
l'lLv Pub/*0) P01’ u) * 1'1',"1([H,u)~(1fe/n)‘*~)(14v)i
~ h, FILLMKHMWI HHW'M >0
H;',,((H,u)"(1 M)"")(l HM)’
“ a,‘ 1 H,‘ 9,,
Q ;(J,1HTW \ [1 minim) Hula,”
H UHIPMJ H P011 04/ v i
> .
c» zr/Inﬁmfl*H/\]+;lnlii7m+lnliﬁv>UQH _, u
, ,
(,4 _,—/
where
H 14H "v l/Mliﬂu)
‘ mi/Hni'. ‘ 111% 1. .1,
"" 2 PM “In, ““ I'miﬂﬂ) '

***************Ending Page***************

***************Beginning Page***************
***************page number:99**************
1 , 99
Exempllﬁcatlon conﬁd
n“ Lhc previuuhly given 1111mm we will have:
“ 14/1 0, 141“ 1412‘ pa,‘ P0“ 0
H‘ : |\\i/+lni/:ln i i i i.¢
U ,2 PM PM, (PM 1*9211 141,1, he“, 1%,)
1 *2/15 1 *1/11 I *1/11 I s 1m .‘K/es
1 ‘1/1, 1 2/5 1 2m 1 x/n 1 3/51
1/21 2/5 2/3 2/; 5/8 2 a‘ 250
:1;i1i1imi:1i:1i:lu
“(2/3 x/n ‘s/a 2,”, 5/11) “ v36 “2.13 H 8
011(1 s01“) ‘2/3 2/5 4
1 1 i 1 i i 1, 2
“‘ “en/141“) "(a/s 1/5) "a 0 58
1/3 x/s a
u‘) w; Mi 273) 1hJ s11 sons
1/3 2/5 1 ,
m 111(375 ﬁ) h1§ *an sum.
therefnre the equatinn nfthe linear separatnr correspnnding m the Naive Bayes classiﬁer
trained on the given dataset is:
hm+wz|2s|mx+mu+w n<>ozss|1suzw11+1.1)s1uuul.+um u

***************Ending Page***************

***************Beginning Page***************
***************page number:100**************
100.
Exempliﬁcation conﬁd
Checking the equivalence of the Nun/e Bayes 11mm" rule and the clusbiﬁcutiun nr-Adc
thruugh luluu'r disc'rivnznulion:
uWLri) I n ¢¢
‘
Evy} +wn<Ufnr1|*U lg’|;’1;’l¢$ in 15x (1414099ersz < v ('lhm!)
H
"Wm i u <~>
‘
2W, + u“; + u“ < 0 for I1’ lli't‘il. “in (-> *0sz (141A “gamma < u ('D'nc!)
H
I/\11("'J:l<=>
‘
2W, \ w" <n for 1\:,.,,:1,r-<:1\:n@ nzxx-[I 11 \ rum wax-‘121)
1:1

***************Ending Page***************

***************Beginning Page***************
***************page number:101**************
lUl,
Gaussian Baysian Classiﬁcation

***************Ending Page***************

***************Beginning Page***************
***************page number:102**************
1(12,
Exelnplifying the Gama-(m [Naive] Bayes algorithm 0n data frmn 1112
CMU, 2001 fall. Andrew Mama, midterm, pr. 1m

***************Ending Page***************

***************Beginning Page***************
***************page number:103**************
NM,
X
I! A
l A
Suppnse you have the nearby training set with one real-valued s r2
inpnt x and a sategnrial nntnnt Y thm has twn values. 4 B
a u
n B
7 B
a. You mnst learn frnln this data the parameters of the Gaussian Bayes
classifer. Write your answer in the following table.
11,1 I w} : my : .1) :
m I a; I my I B) I

***************Ending Page***************

***************Beginning Page***************
***************page number:104**************
10.1,
Solution (in Romanian)
Pcntru u Bilinl'd mudiilc ,H 3i M. vuln folusi formula MM Z Xi’. undo H cute
n
1
x, <1 2
numesml hum-5010: dc anchnaxnan Agadzu', M I 2+ I % I 1, m M7 I
Ziﬂx, x+4+n+rs+7 V
5 i 5 i J‘
Similar, puntru uulculul vuriungclur M qi 0”. Wm fulusi forlnulu 0;,” :
“ , , , . 1 1 , v 1
M. A§adan n5 , 5w , 1y + <2 , 011 , 1‘ m 0;, , 7m , 5)’ + (4 ,
, _ )
1
5,2,‘;2+(,;,;)2+[7,5)2\:7 2 “1:2-
J
Ponzru calculul probabimagnm my i .4) §i my i u) so ginc cont (lo faptul cé y 05w
variabilé do up Bernoulli Agadar‘ my i A) i 2/7 gi Pu i u) , 5/1.
Cum-41mm ucc>tc usﬁmZu-i, ubginum:
m :1 "11‘ :1 my I A) I 2/7
u” 5 "i: 2 my 1:) 5/1

***************Ending Page***************


***************Beginning Page***************
***************page number:105**************
105.
h. Using the notntion n : mX : 2n’ : A] and 6:10; : zll/ : B),
r Whnt is our I 2y I A)? (Answer in Lenin-l of m)
e tht is ntx I 2y : B)? (Answer in terrns of m)
e tht is v(X I 2)? (Answer in terrns of tr and it.)
i Whnt is m’ : Alx : 2)? (Answer in terms of u and at)
i How wnuld the point X I 2 he classiﬁed by the Gaussian Ewes algorithm?
(Answer in terms or u and a.)
Solution (in Romanian)
,itx :2 Y : A] : MY :zll': 4) my I ,4) I 2T"
t
,ttn ‘2.) u) 11L); zll u) my 1:) é’
t
n<x : 2) : p[X : zlY : AMPO/ :11) l 1)[X I 2w I Bl my I B) I 17m l m)
. ‘ prl’ r 4. x r 1) 2t.
I I t I i I i
"(Y 4"‘ ‘) p<X : z) 20 A s1

***************Ending Page***************

***************Beginning Page***************
***************page number:106**************
106,
Algoritmul Bayes [Naiv] gaussian va asocia punctului x : 2 eticheta y I A
dacé 110' i AM i 2) 3 my i mx i 2) (J Zn 2 53¢; n g $2
Folosind valorile estimate pentru parametrii M‘ W. 1m gi 0D la punctu] a,
vurn putuu suric:
2 i 1 2 1 2 i r 1 s
1,%1,5_i112i;l,,3
0:,‘<:‘»,.: ,1 :,,
V? ‘m ‘ V? ﬁ zﬁ
DecL
1 l 9 7
5 7, 5 7, i r- 7 r-
> { » 2 > » 4 »-'1 > >
“if/CM?‘ *Aﬁ‘ a‘ *2ﬁc'rln2ﬁ
c) I 7f: 2 Inf: i g In! 4:175 2 LIGGHT(&Lr1M')
Prin unnurc, ulguritmul Bayes [Naiv] guusaian vu asociu punctului X : 2
eticheta Y : A.

***************Ending Page***************

***************Beginning Page***************
***************page number:107**************
107.
Graphical representations
[made by Sebastian Ciobanu]
without multiplying after multiplying the p.d.f.‘s by

the ptdffs by the selection pmbabilities the selection pmbabtlities

; ‘ ‘ v Z A 5 v Z A

‘l l v z a v 1 a

l‘ ‘l ,/ \‘
i 5 l‘ ‘l l‘ 5 E / \
a l l l \
z ‘3 ‘2/ \ ; \ v ,

t / / \ \\ a / ’ \ \
'2 u 2 l s l 4 t 2 l s g

***************Ending Page***************

***************Beginning Page***************
***************page number:108**************
108,
Dbservaﬂe

Sc poatc constata rclativ “w 1:5 0mm dour; punclc do intersccgic (J, i ,8 451

2 - '1 . r .
5i r3 : 2 4m) pm", graﬁcele funcmlm :Mm 17") s1 2N0,“ Hg).

¢ ,
Toate instangele d2 test .l, situate intre aceste puncte d2 intersecﬁe (1| <
., < r2) vnr aparﬂine clasei A (annln rurha rnsie Em 5mm; deasupra calei
albastre).
lnstangele si'uate ﬁe la sténga lui I, ﬁe Ia dreams Iui l2 vor apargine clasei
u (acolo curba albastré 05w simam dcasupra cclei r05“).
Sepamloml declzional este =12 tip paring ﬁind constituit din punctele r| ‘a
.2.
LC: Mulwmesc studentului MS: Dinu Sergiu pm“, “Ema nbservaﬂie.

***************Ending Page***************

***************Beginning Page***************
***************page number:109**************
109.
Exemplifying the Gaussian [Naive] Bayes algorithm on data from [K2
CMU, 2014 falL w. Cohen, Z. BariJoseph, sz, pr‘ ac

***************Ending Page***************

***************Beginning Page***************
***************page number:110**************
110.
In a two dimensional case, we can visualize how Gaussian Naive Bayes be
haves when input features are correlated. A data set is shown iii Figure (A),
where red points are iii Class u, blue points are in Class 1. The conditional
distributions are two-dimensional Gaussians. ln (u). (a) and (D). the ellipses
represent cunditiunal distributions for caeh class. The centers uf ellipses shuw
the means, and the cuntuurs shew the boundary of two standard deviations.
a. Which of them is most likely to be the true cunditional distribution?
b. whieh of them is most likely to be estimates by a Gaussian Naive Bayes
model?
c, If we assume the prior probabilities for both classes are equal. which model
will achieve a higher swim-y On the training data’!

***************Ending Page***************

***************Beginning Page***************
***************page number:111**************
111.
7 V 1mm“. 7 (B)
/ \ J
V, \i '1 \‘i/
(C) (D)

***************Ending Page***************

***************Beginning Page***************
***************page number:112**************
112.

Solution:

a. (c) is the trntln

h. (R) corresponds to the Gaussian Nelve Bayes eitimatPs. [Lu Here follnws

the explanatiom]

Because th‘ Gaussian Naive Bayes model assume independcnce or tllo two

features conditioned on tne clan-ls labell the eetimeted model >huuld be nllgned

with the axles. Both (B) and (D) satisfy this. l>nt only in (B) the width and

height nf tlle oval. which are proportional tn the standard deviation of each

axis_ lnetolled the date.

c. (C) gives tllo lowest training error.

***************Ending Page***************

***************Beginning Page***************
***************page number:113**************
113.
Estimating the parameters for
Gaussian Naive Bayes and Full/Joint Gaussian Naive Bayes algorithms
CMU, 2014 fallt, Wt Cohen, Z. Bar-Joseph. HWZ, prl 5,3]:

Let Y e (o. l y be class lahels, and let x e 1P4" dcnoLc a d-diinensienal reatuie.

We will ﬁrst eensidei a Gaussian Naive Bayee mudel, where the eendie
tinnal distiihiitian of eaeh feature is a onendimensional Gaussian, xwlly e
hinirlwil'll), for , l. .11.

a. Given n independent training data paints, ((Xl" v”), v- .(X<"l.l'(h))),
give maximumnlikelihuod estimates (MLE) for the naranieties of the prub'sk
hilistie distribution of Xwiv. for , : 1, (I.

***************Ending Page***************

***************Beginning Page***************
***************page number:114**************
Solution: 114'
The likelihood nf me samples in (:iiiee u is
,7 ,, (i 1 (Xi: #4551
m1’ UL ' -("m,"]"') imp i’ii
" ” ,1}, i/zwf,” 2105")’
l IX)? i”):
I e m "Xv Z m
vim, i:i 2W >
and the lng'likelihond is
liiL: Mum/W‘ (XP' My i “mum
Taking me partial derivatives nmie log-likelihood, we have
um. ‘ I ‘ 1
i u<> xwl”) 000" i A“)
0,15,) g< in In lo W Z, ,U
UIHL m, I K (i2 iii 1 (i ,i 1 .1
Zoe. i x” w :Uc'nij': X" u
W} 01),, (Ow gr M, ) i I“ 1 ZK ,1) m 1
Similarly, one can derive the MLE for the paralneters in Class 1.

***************Ending Page***************

***************Beginning Page***************
***************page number:115**************
115.
b. Suppose the prior or Y is already given. How many parameters du you
need te estimate in Gaussian Naive Bayes model?
Solution:
For eaeh elass, there are 2 parameters (the mean and varianee) for each feature, there-
fun: there are 2 - 2d : h: parameters l'or all features in the twe elasses.
e. In a full/Joint Gaussian Bayes model, we assume that the eonditianal
distributien Pl(X\Y] is a multidimensional Gaussian, xll' ~ Mm .Ey)» where
u e le is the mean veeter and 2 e RM is the eevarianee matrixt
Again, suppose the prior of Y is already given. How manv parameters do you
need tu estimate in a full/Joint Gaussian Bayes model?
Solution:
For eaeh elass, there are ,1 parameters for the mean, rm] e we parameters ier the
eovarianee matriae heeause the envarianee matrix is symmetrie. 'l‘herefnn=._ the numher
of parameters is 2 111 + r101 + l)/2) e 1I(|l + a) in tetal for the two elasses.

***************Ending Page***************

***************Beginning Page***************
***************page number:116**************
116.
Proving
the mlationship hctwccn the decision rules for
Gaussian Naive Bayes and the Logistic Regression algorithm
when the covariance "latrines are diagonal and identical
Le‘, (75,:(15, for I: 1,.,.,4
ClﬂUA, 2009 spring, Ziv Bar-Joseph. HW2, pr‘ 2

***************Ending Page***************

***************Beginning Page***************
***************page number:117**************
117.

Assume a twuclass (y c (0.1)) Naive Bayes model over the d7
rlitnensional real-valued inpm spare ma", where the inpm, variables
X\Y i U 6 R’i are distributed as

Cmmmuﬂm :< pm. . 4W >. r1 :< m- . .0,‘ >1
and XW I 1 e 1R" as

Gaumanm, :< p“. . 4de >. a :< mu. .0,‘ >1
iie,, the inputs given the class have different means but identical
variance for both classes‘

***************Ending Page***************

***************Beginning Page***************
***************page number:118**************
115.
Prove that, given the conditions stated above, the conditional
probability P(Y I llX I 11,‘), where x I (X,,.,.lX,,) and , I
(1h. . 111) can be written in a simiar form to Logistic Regression:
1
I + mp0“, + H‘ > .1)

with thc parameters “it, e 1R and “i I (Humming e JR" chosen in a
suitable way.

As a consequence. the decision rule for the Caussean Bayes clas-
siﬁer supported by this model the desieh i-ule has a linear fcmi.

***************Ending Page***************

***************Beginning Page***************
***************page number:119**************
119.
Solution
,. . P(Xi1>Yi1)P(Yi1)
P Y : | Y I .)”:' é
L ‘ 2”an 1) PM I 1W I WHY : u’)
i 1
i 1+ ng i ‘\Y i (J)P(Y i n)
PIX i ‘\Y i 1]P(Y i 1)
i 1
i 1 ‘ (h) mmx : [\Y :uan' :11)
A} P(X i m’ *1)P(Y*1)
i 1
1+K>;,(1|.W
%I—m WM,

***************Ending Page***************

***************Beginning Page***************
***************page number:120**************
) 7 It WU“, 1 1’(Y:0)+‘Z’1 1'(,\',:,,y :0) 12°‘
mm”, i niﬂyin ‘:1“iPr/\3~,Y* 1)
1 ‘. 4'1",wa
PQ’ *1] H i1 \‘Xli (ii"‘*'r“‘)
i P(Y:U] ‘I (arm)2 (Hr/my
’ I“ m- ;1) +2‘: 2n? ’ 2”;
i PWIUJ " 2"’(l/xuil'x1)+[Vail/i!)
i I“ m’ i1) + 2| 20,1
Pawn) " !,(}1,0*/1'|] (Mr/1?“)
i luer; 7+?
i m'Im " (m mu d Hm M
’ “Wain ’; 203 ‘g 0; J’
@—l uH

***************Ending Page***************

***************Beginning Page***************
***************page number:121**************
121.
In conclusion,
1
P Y: x X: : i‘
with
,1
P[Y*() [If i 1f A; *1,
Note that k ‘ ‘
PO “A "j PATH")
and
PLY i nx i |1> P(Yi[1\X i.1>)<_» “~- I + w“ < u

***************Ending Page***************

***************Beginning Page***************
***************page number:122**************
122.
Since the eeeﬁieierns lr, for l e 1. . .11 de not depend on i‘, it
fnllews ‘liar, this (lecislm rnle rif Gaussian Naive Bayes [in the
conditions stated in the beginning of this problem] is a linear rule,
like in Legislie Regression
However, this relationship does not mean um there is a one-Lo-
ene correspondence between Hie parameters w, nr Gaussian Naive
Bayes (GNB) and the parameters w, of logistic regression (LR)
because LR is discriminative and therefore doesn". model P(X),
while GNB docs model P(X].
To he mnre speciﬁc, nute that the coefficients u‘, in the GNB decision rules
should be devided by ml‘. l4) in order to correspund m l-(l HA I),
which means that then they will not anymore be independent of: like the
LR cneﬁicients.

***************Ending Page***************

***************Beginning Page***************
***************page number:123**************
123.
Proving
the relationship butwccn
The full Gaussian Bayes algorithm and Logistic Regression
when 2‘, : 21
ClﬂUA, 2011 spring, Tom Mitchell, HW2, pr‘ 2a

***************Ending Page***************

***************Beginning Page***************
***************page number:124**************
124.
Lens make Lhe [olluwing assinanimlJ:
l. v is n heelenn verinhle following e Rernenlli distributiom with pnremeter
7r e m’ e l) and tline m’ e 01 e l r r.
2. x : [xnxl .AN is a vector of random variables not conditionally in-
dependent given Y‘ and 1%le Z k) follows a inhlllvminte normal distribution
NU“. E).
Note that M is the ,1 >< l mean vector depending on the value er Y, and x is
the 11X (1 covariance matrix, which docs not depend on y: Wu will write/inn.-
the rlensity of the multivariate normal rlistrihntien in vector/matrix netetien.
l l
Mini) I Winn (r50 r ,NYM rill)
Is the form nr P(Y Xi implied by such this [notrswnuive] Gauss-inn Bayes
classiﬁer [La similar to] the fhrn. nserl by logistic regression?
Derive the form he P(l’\X] tn prove your answer.

***************Ending Page***************

***************Beginning Page***************
***************page number:125**************
125.
WE start with:
) ,i i P(X\Y:1)P(Y:l]
In i 1le i HA‘Y:1)P(Y:1lvP()\\Y :ll)P(l' :0)
l l
I my e u) PinY i u) I my I illlluill I o;
l , i
+ PO’ : I) P(X\Y :1) Hm’ 1“ ['(Y |>Prxly l)
l
I my I (l) Pixll' I m
1+ tap (In 1'0’ l) +1li11<lﬂy l)
. WXW i U)
Next we Will focus cm the term ln m.
é
HY v Ol i (2W)""7\E\"Q . i . i
ln m a ln f lntaulwl a lntaulw] a (a)
immluimue
where (a) is the formulation obtained as the difference between the exponential pasts
oltwe multivariate Gaussian densities 11(le u) and Pixll/ l).

***************Ending Page***************

***************Beginning Page***************
***************page number:126**************
126.
l , 7 , 7
h) I 5H)‘ *MyE ‘(A *lnPWﬂInVE ‘(waﬂ
1 1 , _,
i (NJ *mTlY'X + @121‘) i 5w L ‘m,

As a resulL we have:

, 1

my I 1m] I 1 1 1
, 1r .
l+nxp (In f + E/Jx ‘m , 5"” 1;10+(/1K’MT)§1 u)
i I
i 1 >vale \ u‘ X)
1 . l 1

when: m, : lrrJ + Eprlldm i EHJXAW i5 a scalar,

w
and w i Y1"(m*m) is a r1 X | a parameter vector.
New that to‘; MWW I (m IHJTE'W I (Y'ﬂmm NW I E"(/m m because
2*‘ is syrnmeh'if.
()1 is symmetric because it is a cnvariance matrix and therefore, )1" is also symmetric.)
In condusinn, PmX) has the form of the logistic regression (in vector and matrix
notation).

***************Ending Page***************

***************Beginning Page***************
***************page number:127**************
121.
The quadratic nature of the decision boundary for
the Gaussian Joint Bayes classiﬁer
when E“ ¢ El
Stanford, 2014 r311, Andrew Ng, midterm. pr. 21>

***************Ending Page***************

***************Beginning Page***************
***************page number:128**************
12$.
The probabilistic distributions learned by the Gaussian Bayes Joint (GJB)
classiﬁer ean be written as:
My) : 41"(1 i WT where v‘) I m’ :11

will I v) I Wee ( a $0 a Mm» an»)

1>(1l,l/: 1) : (End/2km”? m1( a LU *1‘1>‘E(\(J' will)
The decisiun rule uf GJB predicts

1/ : l isp<|1 : l r) 2 my : illr) and 1/ : ll otherwise.
Show that il 2‘, # 2,, men the separaling boundary is quadratic in ‘I. That
is, simplify me decision rule pm, : 1 Tl 2 m : 0 T) m me lei-in
l TA: + 13H ~ (1 z l).

for seme A e llW", B e lll", C e WA, and A e n. Please clearly state your values
for A, B and c.

***************Ending Page***************

***************Beginning Page***************
***************page number:129**************
129.
Examining the log-pmbabilities yields:
mm I1\1)>1np(v I up) I» 1mg I 1m I 1mm, I up) > n I» 111w > n
i ' i 1M I 0M] i

p 8W l!(r\!/ I 1m” I 1)

I» 1n Zn

MW :ﬂlvh/ :0)
v \Zﬂm 1 I I
I I“ m I I“ W I ;(<» I “NE, ‘MI/m I (r I /‘0)T20|(" Imn) 2 u
1 I I I I I I I <b WW”
I» I§(1T(:\ ‘Izu‘).1,I2(,L,T:\ \I/Agxn‘)r+/I,TE] ‘m Inuzu'pu) HumImW z n
I 1 I I I I
<0 EM"? 1 r I1 r Iv
+1“ 1 i0 +1" \EW’ + 2(11‘, 2|, ,lu I”, E, m) 20.
1 i ‘
Horn tho above, we sec that A I EC"! I 2,‘), ET I p12,‘ Iyuzﬂ. C I lull +
Iv

‘Eu "'2 i T I‘ i I‘ . . .
111W + 30mm M I p‘ z‘ my mnhemwre, A # u, smce Eu # z, \mplles that

I, .
:5‘ # :1‘. Themmm mp. decision boundary is quadratic.

***************Ending Page***************

