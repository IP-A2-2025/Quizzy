[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What type of learning is the k-NN algorithm considered to be?
--InteriorSeparator--
Instance-Based Learning
--InteriorSeparator--
easy
--InteriorSeparator--
1
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following distances is commonly used as the metric for the k-NN classifier?
--InteriorSeparator--
(right) Euclidean distance
(wrong) Manhattan distance
(wrong) Minkowski distance
(right) L2 distance
--InteriorSeparator--
medium
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the geometric representation of decision boundaries for a 1-NN classifier called?
--InteriorSeparator--
Voronoi Diagram
--InteriorSeparator--
medium
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
In the context of k-NN and decision boundaries, what do positive and negative data points typically represent?
--InteriorSeparator--
(right) Different classes
(wrong) Distance values
(wrong) Feature scaling
(right) Categories to classify data in
--InteriorSeparator--
easy
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is a key consideration when using k-NN in high-dimensional spaces?
--InteriorSeparator--
(right) Curse of Dimensionality
(wrong) Overfitting
(right) The number of examples needed by k-NN grows exponentially with the number of features
(wrong) Underfitting
--InteriorSeparator--
hard
--InteriorSeparator--
15
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the formula to calculate the cumulative distribution function (c.d.f.) of d* for t ∈ [0,1] when p = 1, where d* represents the distance from the origin to its nearest neighbor?
--InteriorSeparator--
Fₙ,₁(t) = 1 - (1 - t)ⁿ
--InteriorSeparator--
hard
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does 'p' represent in the context of a p-dimensional ball when discussing the curse of dimensionality in k-NN?
--InteriorSeparator--
The dimension of the space.
--InteriorSeparator--
medium
--InteriorSeparator--
16
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the formula for the cumulative distribution function of d* for the general case when p ∈ {1, 2, 3, ...}?
--InteriorSeparator--
Fₙ,ₚ(t) = 1 - (1 - tᵖ)ⁿ
--InteriorSeparator--
hard
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the median of the random variable d*, denoted as tmed(n, p), as a function of both the sample size n and the dimension p?
--InteriorSeparator--
tmed(n, p) = (1 - 2^(-1/n))^(1/p)
--InteriorSeparator--
hard
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is the impact on the volume of the minimal sphere containing the nearest neighbor of the origin as the value of p (dimension) increases, according to the study?
--InteriorSeparator--
(right) The volume grows very fast.
(wrong) The volume decreases.
(wrong) The volume remains constant.
(right) Most of the training instances are closer to the surface of the unit ball than to the origin O
--InteriorSeparator--
hard
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
The k-NN classifier works best when a test instance has what kind of neighborhood in the training data?
--InteriorSeparator--
A dense neighborhood.
--InteriorSeparator--
medium
--InteriorSeparator--
26
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the term for the true conditional probability distribution for points in class y, denoted as py(x)?
--InteriorSeparator--
P(X = x | Y = y)
--InteriorSeparator--
medium
--InteriorSeparator--
28
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Given p0(x), p1(x), and θ, what is the formula for calculating q(x) = P(Y = 1 | X = x)?
--InteriorSeparator--
q(x) = (p1(x) * θ) / (p1(x) * θ + p0(x) * (1 - θ))
--InteriorSeparator--
hard
--InteriorSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
The Joint Bayes classifier assigns a data point x to which class?
--InteriorSeparator--
(right) The most probable class
(wrong) A random class
(wrong) The least probable class
(right) argmaxy P(Y = y | X = x)
--InteriorSeparator--
medium
--InteriorSeparator--
30
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the probability that example x will be misclassified using the Joint Bayes classifier, in terms of q(x)?
--InteriorSeparator--
ErrorBayes(x) = min{1 - q(x), q(x)}
--InteriorSeparator--
hard
--InteriorSeparator--
30
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Given some test data point x and its nearest neighbor x', what is the expected error of the 1-nearest neighbor classifier, i.e., the probability that x will be misclassified, in terms of q(x) and q(x')?
--InteriorSeparator--
Error1_NN(x) = q(x)(1 - q(x')) + (1 - q(x))q(x')
--InteriorSeparator--
hard
--InteriorSeparator--
31
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the asymptotic case, what is the asymptotic error for the 1-nearest neighbor classifier at point x, in terms of q(x)?
--InteriorSeparator--
lim Error1_NN(x) = 2q(x)(1 - q(x))
--InteriorSeparator--
hard
--FlashCardSeparator--
32

--FlashCardSeparator--
Single
--InteriorSeparator--
In the context of the asymptotic error of 1-NN, what is the relationship between 2q(x)(1 - q(x)) and ErrorBayes(x)?
--InteriorSeparator--
2q(x)(1 - q(x)) ≤ 2ErrorBayes(x)
--InteriorSeparator--
hard
--InteriorSeparator--
33
--FlashCardSeparator--

--FlashCardSeparator--
True/False
--InteriorSeparator--
True or False: The Cover & Hart’ upper bound for the asymptotic error rate of 1-NN holds in the non-asymptotic case (where the number of training examples is finite).
--InteriorSeparator--
False
--InteriorSeparator--
medium
--InteriorSeparator--
34
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
According to Kulkarni and Harman (2011), what is a tighter upper bound for E[lim Error1_NN]?
--InteriorSeparator--
2E[ErrorBayes](1 - E[ErrorBayes])
--InteriorSeparator--
hard
--InteriorSeparator--
35
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the properties of Kn-NN when kn/n converges to 0 for n approaching infinity?
--InteriorSeparator--
(right) Universally consistent learner
(wrong) Non-parametric
(right) Its performance approaches that of Joint Bayes
(wrong) Dimensionally dependent
--InteriorSeparator--
hard
--InteriorSeparator--
36
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does it mean for a learning method to be universally consistent?
--InteriorSeparator--
Its performance approaches that of Joint Bayes when the amount of training data grows
--InteriorSeparator--
medium
--InteriorSeparator--
37
--FlashCardSeparator--

--FlashCardSeparator--
True/False
--InteriorSeparator--
True or False: There is a universal convergence rate for learning methods, meaning one method can always beat out all others.
--InteriorSeparator--
False
--InteriorSeparator--
medium
--InteriorSeparator--
37
--FlashCardSeparator--

--FlashCardSeparator--
True/False
--InteriorSeparator--
True or False: According to the material, the ML field continues to be exciting, and the design of good learning algorithms and the understanding of their performance is an important science and art.
--InteriorSeparator--
True
--InteriorSeparator--
easy
--InteriorSeparator--
38
--FlashCardSeparator--

--FlashCardSeparator--
True/False
--InteriorSeparator--
True or False: After being mapped into the feature space Q through a radial basis kernel function (RBF), the 1-NN algorithm using unweighted Euclidean distance may be able to achieve a better classification performance than in the original space
--InteriorSeparator--
False
--InteriorSeparator--
hard
--InteriorSeparator--
40
}], role=model}, finishReason=STOP, citationMetadata={citationSources=[{startIndex=4916, endIndex=5096, uri=https://courses.cs.washington.edu/courses/cse446/13sp/homework/homework3.tex}]}, avgLogprobs=-0.13913774754776312}]