***************Beginning Page***************
***************page number:1**************
1
Dec1s10n Trees
Contains:
ID3 alga; ex. :54, an, 2, 4. 12, 1", 47. 15, 19. 20,
AdaBoost alga: ux. 22. 23. 24. 25, 2s. 21, 2:5. 10. 29
[rum the 2023f version of the ML exeruhe book by L. Ciurtuz at a1,

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2
Exemplifying
how to compute information gains
and how to work with decision stumps
CMU, 2013 r311‘ w‘ Cohen E, Xing, Sample questions. pr, 4

***************Ending Page***************

***************Beginning Page***************
***************page number:3**************
ii
Tilmny wunth tu knuw huw w du well fur ML exam. Hu collects thus’: 01d
statistics and decides to use decision trees to get his model. He uuw gets 9
data points, and two feahlres: “whe'hsr stay up isie before exam" (s) and
“whether attending all we classes“ (A). We already know the statistics is as
below:
Sir/hill) I [a+. 1e]
511/ (5+) : was} 514$") I n+2’
Sm! [14+] : \5*.li].b'rl(.4i) I [ﬂ+.3*}
Supposc we an‘ guing Lo use the feaLurc to gain must anormatiun ui ﬁrst
split. which feature shuuld we choose? How much is the information gain?
You may use the following approximations:
N :i a r
his” 1.511 z 32 281

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
4
[5+,4—l [5+A-l
+ g g, +5 gI
13+,2-1 [2+,2-1 15+,1-1 [0+.3-1
H=l H=0
J . ",J sHJ 4M5 9 4 QJ s r 4 ,
Hm”) I H[u+.4I] I H (é) I H (g) I §10g1 3 I 510g, 3 I loggoI §10g§u I é logQ-l
I zlngZaI 510g» I .5 I I5 + zlngZaI 510g?) I 11.991016
‘ m/ 5 4
H175 I H3+.2I+ H2+.2I:.
(a ‘ ) ‘1 l 4‘) [ Iumrm) '“I' H(1111)I HlanS) I u 001215
5 - r V4
I § 0 910%” 5 1: 0 983561 16mm) ‘Ii Hm”) I HMIHA) I 0 5511-28
H(nH\A) "'I/ f; H 5+.1I] + f) H[U+ xI I, lawns) < 10mm) Q H(a1!\5)> me)
I g 0 050022 +3 0 I 0 mm

***************Ending Page***************

***************Beginning Page***************
***************page number:5**************
Decision stumps;
entropy, mean conditional entropy, and information gain:
some very convenient formulas to be used
when working with pocket calculators
Sebastian Ciolianu, Lava“ cam-m, 2017

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
r»
Consider tho dcciaiun atunio given in the nearby image. Wk]
The synlbclls "livid i» and f represent munts oornputed
irorn a training dataset (not provided). As you see, the
label (or, output variable), here denoted by Y, is binary, 0 l
and so is the attribute (0L input variable) A. Obviously,
u :l + r and b: 4+ /, ‘CHH |=+H
a. Prove that the entropy of [the output variable] eorrespouding to the pm‘-
tittoo associated to the test uode in this decision stump is
l (ll+1i)<'+'> _
matte} 1 f H’ logZ fawn lru e ll and [1% 0
bl Derive a similar formula fur the entropy, for the case when the output
variable has three values, and the partition associated to the test node in the
deoision stump would he lo+ level.
(Note that there is no link between the. last i and the o oount in the above
deeision stump.)

***************Ending Page***************

***************Beginning Page***************
***************page number:7**************
7
c. Assume that for the abnve given decision stump we would have [all counts]
t’ (I r and f different from u. Prove that the mean conditional entreny eorre
spnnding to this derision stump is
1 (t'+tl]"d (we/WI
Hen,‘ humor i1 ,, i i
“‘ ' u+t °*'( m1" r'r/
t1. Now suppusc thet oue or the eouute t,tl.r and I is u; ror example, 1005
consider r I n, [nfer the formula for the mean eonditienel entropy in thie
case.
e. Prove the following formula for the informntion gnin corresponding tu
the above given decision stump. assuming that all aim .tl. n and J are strictly
positive:
1 (a t WM mi“ :"f/
[(;vlt2t1 tumult : i1‘ i i i -
" ' ' ‘ a+b °*’( MI)" (r+d)‘*" (r'+j)‘*/

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
a

WARNING!
A serious problem when using the above formulas on a poeket calculator is
tlle ruet tllut tl-e lnternul eupueity of representutlon for intermediate resulte
eun be everl-lewn.
For example, a pocket calculator Sharp EL-ESIVH can represent the number
515"“ but not 57"1 similarly, the calculaLur made available by tlle Linux Mlnt
operating system [see the Amesson'ee menu] cun represent 179"“ but net
1mm’
ln such overﬂow cases‘ you should use tlre basic / general formulas ror cn-
tropier and tne lnt'er-rnutlen gain, beeuure they rnuke u better- use uf the In};
runetlen.

***************Ending Page***************

***************Beginning Page***************
***************page number:9**************
9
Answer
a.
11 11 b 11
11 _,,:, i411 i i4 1 i
[M ’ 1 (“'11 “"111+11+u+11 “1”? 11+h)
1 11 11 1 11 ~ 11
if uni 111.1,i if 11, i 1,i'>
11+11(“ (“1+th 1117"“) 11+11(l°"1(11+11) +°g‘(11+11))
1 11“ I1" 1 11" 11“
if 111i 11* filvwi
11+11( “(11411011 + "~01qu "+21 ‘*'(1+11)11<1
1 (11+r1;""'
I i1 i
11+!’ "g1 11“ 11"
b.
11 11 11 11 1 1
H ,1’~:ii11ii.|1i_i|1i
W’ ‘l (“+11% °'”111+11+1-+11+b+1 ""111+11+1, u+l1+1 ""lewv)
1 11 11 11 1, 1 1
Iiihi 10* 11*
11+11+11(°"1(11+11+1)+ °"1(11+11+1)Jr “52(11+11+11))
1 11" r1" 1‘
I fi111i11i11i
11+11+1-(""1111+11+1-)" + "“2(11+11+1)'>4r "5111’11411-1)
, 1 1 11" 11"1< 1 1» 111+11+1-)"""'
11+11+1 "‘2<11+11+111~H'1< 11+11+11 U"? 11111111

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
1. m
W ‘+d . i r‘+[ i
Hum/MM," m H[r+.1l HT’), H[l+./ ]
“In 1 10 (‘+1()<"‘+L,+/f 1 [U] [(,+j)r\l
’ “+1, M1! °"* u W “+1, L/f ‘*2 w [T
i 1 ((-+(I)"“ [wt/Y”
1i|+1110"1(ﬁ if!)
‘1
r' ¢ I
IIWWMW I m-II[<'+ 1*]
1+! r {if I r+[
I i- i iii 1 ,i
ﬁn, ((+f 1"?“ I 1+f "*2 f)
M4 1 n+1 (+1‘
’ “+Vuyf("1°"1 ‘ +11% j >
1 (r+[]“/
‘1+!’ W2 w fr
“I I 1 w
i I [u+b)"" 1 (,Mlyw (1,’f)‘
Inwmm, , m>lﬂglwim10g1(ﬁ (4.” )
i 1 MAW“, W a" ">f'
i u \ 1)"“"1( a“ r," u \ aw" (v \ If”)

***************Ending Page***************

***************Beginning Page***************
***************page number:11**************
n
Important REMARKS (in Romanian)

1. iiimimii majorh'ahla raii-iiliimiimlm de hlllenar M an rimma 1% vi
immune 1H §i lg, in formulclc prezcntatc 5a“ dcdusc la punctclo 1| ii ar ﬁ
d0 iiom 55 schimbiiii baza logaritmului. AccasLa rcvino , p(‘ iaiiga inlocuiroa
iiii 10;) cu iii suu i; , iii iiimiiigimi mcmbiiiiiii drupt cu 1,1“2, rcspuctiv 1/1g2.
2. iniiiicac, la aplicarea algoiiimiiiiii 1133. Fermi alegerea Celui mai
bun atribut de pus in nodul Bimini este suﬁcient is calculirn EnLeriiIE
tondiﬁnnale medii, va ﬁ suﬁcient sri tnmparém produsele d8 forms

(‘+d)z~ii ((4+/]¢+l (I)

imp ivy
penmi cumpaQii up. decizie consideraﬁ la nodul respectiv 5i 55 alegem minimul
dime aceste produse-

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
12
Exemplifying the application of the ID3 algorithm
on a toy mushrooms dataset
CMU, 2002(1) spring. Andrew IVIoore, midterm example questions, pr‘ 2

***************Ending Page***************

***************Beginning Page***************
***************page number:13**************
13
You are stranded on a deserted island. Mushrooms of various types grow widely all
over the island, but nu uthcr foud is anywhere tu bL‘ found. Sumo uf the mushroumn
have been determined a! puisunuus and uthcrs as not (determined by yuur furmcr
companions‘ trial and error). You are me only ene remaining on the island. Yuu have
the following data tn nonsider:
—| NolHeaI/y Smelly Spotted Swmolh | Edzbls
——|—!-
1; 1 11 1 11 1
f ' O 1 D 1 1
—| 11 o 11 1 I“
—|—Iu
—I-—-—-II-
1,- 1 11 11 1
H O 1 D O
——|—I
1' | | n | 7
\l 1 1 D O 1'
You know whether 0r not mushruoms A thrnugh H are poisonous, but you do not know
about U through II‘.

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
1\
For the oed questions, consider only mushrooms A through [It
a, whnt is the entropy of Edible?
ht whieh attribute should you choose as the root of a deoision tree?
Hint: You can ﬁgure this out hy looking at tho data without explicitly
computing tho information gain of all four attributes.
c. whet is the information gain of the attribute you ohose in the previous
question?
(L Build a ID3 dooision tree to ol-sssiiy mushrooms ss poisonous or not.
o. Classify mushrooms U, v and W using tho dooision troo os poisonous or
not poisonous
t1 If tho mushrooms A through H that you know are not poisonous suddenly
became scarce, should you oonsider trying U, v and u'? Whioh ono(s) and
why? 0r if nono of thom. thon why not?

***************Ending Page***************

***************Beginning Page***************
***************page number:15**************
11'!
a,
, (hf :z s s s a a n x
11m,” I 11[$+,si] : *8-10g2 8 i 8-1098 : 8-1‘ng 3 + 8 Jug; n
3 3 ‘- ' 3 7
: gvsi51%“é05*51%;:3ig-Iugzsié-Iogg
z 09511

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
lﬁ
b4
[73:57-1 @+,5—] [31-55] [-35-]
V \ V \ V \ V \
[l+,2—] [24-3-1 [2+.3—] [1+,2—] [24-3-1 [|+.2—] [1+‘2—] [1+.3—]

***************Ending Page***************

***************Beginning Page***************
***************page number:17**************
l7
c.
HWWH, “2' gH[2+,2i]+%H[l+.‘$i]:%‘1+%<%1“g211+%1ug2%)
: i ¢ 1 ZlungIf; 210g,3~(10(150
IGWM, ‘i’ HM, i Pin/SW”,
: (I 9544 i 0.9056 : (I 0-188

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
l8
cL
my 3 s
HWMW :1 §H[1+.2I]+EH[2+.3I]
- § 110 5+310w§ +9 91o L510 9
*83g21 3”22 35g’2 5'223
31:2‘2 52r23rigu'
I g<§10g15+510525 3'1)+8(Jlngla 51+510gzd 510,,23)
:s , z 5 r :5 t z
3 2 r, a 2
: I “If , ,ri’ M"
810g} 8mm}, 810g“; 8
a 4 k
: gloglﬁIgzUUSll
I Ian/WWW dé’ Hmmm I HWHHW : 0,0544 , 0.9512 : 0,0032.
[GD/NOWHHy I I(;ll/§mw”1/ I [GU/SWIM I 0-0032 < I(;l!/€mumh I 0 0488

***************Ending Page***************

***************Beginning Page***************
***************page number:19**************
19
Important Remark (in Romanian)
in 10¢ Si n calculat el'ecLiv aconlc cﬁgtiguri dc inl'urmagie, pcntru n dctcrmina aLribuLul
cal mai .,bun"7 at ﬁ fest suﬁcient is cnmpnnam valorile entropiilor condiginnnle medii
"mannmn 5i [IN/Nulllrmv:
[GH/vamw/A > [GU/A'nlllrm v a Hu/sWnnn < H!),r'\'nle1mvy
n : r 1
¢= giglnglv3< £10115’ E a llidlnglli<§lngl§il
‘b 1n < 310315 ¢ 314ng ac» 14s<11m1ms 4 17518(adev.)
in mod alternativ, Qinénd cunt Ele formulele d2 la prublema UAIC, 2011 fan, s. Ciobanu,
L. canntnn, plllem proceda rhiar mm‘. simpl'u relaﬁv la calcule (nn dam‘ aini, rn'i d? Hm
m M ave'm de-u face cu u'n numdv‘ mm d5 instan§e):
41 4‘ 3" 55 4‘ , , .
H , Wn<H mmﬁ<>i i (\i i<>i<P<> ﬂag‘ r’<>2“’<s~‘
‘M V H/u 2121 3; 21 21); 3i J ’ °
Q m» 2m < 27 2', 12a (adev.)
V WA
"72‘ >< 5 l2;

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
20
Nude l: Smooth : 0
[1:2,] [2:2,] |2i2il
GochnvyE <§mclly E (Spmlcd E
V N V N 0/ N
[O+_l—] [2+,1—] [2+,O—] [0+_2—] [l+,l—] [l+,l—]

***************Ending Page***************

***************Beginning Page***************
***************page number:21**************
21
Nude 2; Smooth : 1
[lip] “LH [Il3il
@018“in (Emmy i2 (ipulled i2
V N V N V N
llLJj [ohm [mail [moi] [my] [0%]
<w> A A A A

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
22
The resulting IDS ‘IYee
[3+.5-]
U g \\1
\ [F (Smooth : a AND Smelly : 0) on
[2&1 mm (5mm : 1 AND Smelly : 1)
@y > THEN mum;
/ ELSE #1541th
V \ V \
[2+/\0_] [mail 10+} | [NAIF]
m A A m
Clumﬁumun of 1m instances:
Smooth : 1, Smelly : 1 :> Edible : 1
V Smooth, : 1, Smelly : 1 :> Edible : 1
ll‘ Smuobh, = 0, SllleHy = 1 f/ Edible = 0

***************Ending Page***************

***************Beginning Page***************
***************page number:23**************
23
Exemplifying the greedy character of
the IDS algorithm
CMU, 2003 fall, T. Mitchell, A‘ Moore, midterm, p1". {La

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
2\

Fic atributclc binarc do intrarc 14.8.6‘, atributul dc 105m 1' 51 urmétoarclc
exemple de antrenament:

A 12 (‘ 1'

1 1 o 0

l U l 1

0 1 1 1

n 0 1 n
a. Determinaﬁ arborele de decizie calculat (is algoritmul ID3. Em acest
arborc dc dccizic consistent cu datclc do antrcnamcnt?

***************Ending Page***************

***************Beginning Page***************
***************page number:25**************
Réspuns
Nodul 0; (rédécina)
[2+,2*] [2+,2*] [2+.2*]
[l+.l*] [l+.l*] [1+.l*] [ltli] [0+,l*] [Z+,l*]
Nod l
A C)
Se observi imediat 05 primii doi “compa§i de decizie” (engL decision
stumps) an IG : 0, in Limp cc al trcilca (Yornpas do 11mm arc IG > u. Prin
urmzu'e, in nodul 0 (radacina) vom pune atributul c.

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
2H
Nndul 1: Avem de clasiﬁcat instangele cu C : 1, deci alegerea se face intre
atributele A §i B‘
[2+.l—] [251-1
D B
0 1 o |

[111*] moi] [Mk] [um

@ A A
Cele doué entropii condiﬁunale medii sunt egale:

2 1

HW I H,,'U:§H[1+li]+gH[|+.0*]
A5adar, putcm alcgc oricarc dintrc cclc douﬁ atributc. Pcntru ﬁxarc, i1
alegem pe A

***************Ending Page***************

***************Beginning Page***************
***************page number:27**************
27
[2+,z-]
Nodnl 2= La acest nod nu mai avem disponibil
decél. atributul B4, deci i1 vom pune pe acesLa, n l
|0+H [2+»I—]
Arbm'ele IDS complet este reprezentat in ﬁgura {Q
alﬁtumti. <1 l
. . . ‘ . [In-1 [MU-l
Prm construcgle, algorltmul 1m este cansnstent
cu datele de antrenament dacﬁ afrestea smut con- 0 A
gimme (m, nccontradictorii). In cazul BOSCH‘. 0 '
se veriﬁcé imediat c5 datele de antrenament sum [0+v|_] [l+.0-]
(:(msistente. A />

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
28
b‘ Existé un arbure d9 decizie cle adéncime mai micé (decét cea a arborelui
1133) consistent cu datele de mai sus? Dacé da, ce concept (logic) reprezinté
acest arbore?
Réspuns:
Din (1m se observﬁ (:5 atributul de ie§ire
Y rcprczintii dc fapt functia logica .4 XOR my]
u.
Reprezenténd aceasté funcgie ca arbore 0 '
de decizie? vom obgine arborele alfaturat‘ [leil [my]
Acest arbore are cu un nivel mai pugm a a
dcczit arborclc construit cu algoritmul 0 I O I
IDS’ [0+_l’] [H'IF] [l+.l'|—] [0+_l’]
Prin urmare, arbomle 1m M este up- A A A A
tim (1m punctul (1e vedere a1 numéirului ° M M 0
dc nivcluri.

***************Ending Page***************

***************Beginning Page***************
***************page number:29**************
29
Accasta cstc o consecintﬁ a caractcrului “greedy” al algoritmului
1133, (13mm faptului 1:5 la ﬁecare nemgae alegem ,‘Vcel mai bun“
ambm in mpon cu criteriul (‘instigului do informatic.
Se §tie c5 algoritmii de up “greedy” nu granteazé obginerea opti-
mului global.

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
1m
Exemplifying the application of the ID3 algorithm
in the presence of both
categorical and continue attributes
CMU, 2012 fall, Eric Xing, Aarti Singh7 HWl, pr. 1.1

***************Ending Page***************

***************Beginning Page***************
***************page number:31**************
31
As or September 2012. 800 extrasolar planets have been identiﬁed in our galaxy. sheet-
seeret surveying epaeeships sent to all these planets have established whether they are
habitable for humans or not. but sending a spaeeship te eaeh planet is expensive. ln
this problem, you will eome up with deeision trees to prediet if a planet is habitable
based only on features observable using teleseopes,
a. ln nearby table you are given the data Sim Orbit Habitable Coum
from all son planets surveyed so lar, The fea_ Big Near Yee 20
turea obeerved by teleeeope are Size (“Big" or Big Felt Yea 110
"Sm'elll“), and Orbit ("Ne-alt“ or “lel-“). Eaeh Small Near Yes 139
row indieates the valuea of the features and Small Far Yes 4r.
habitahility, and how many times that set oi Rig Near No 130
values was observed. Sn_ ior example, there Big Fal' No so
were 20 “Big” planets “Near” their star that sinall Near No 11
were habitable. Small Far Nu 255
Derive and draw tbe derision tree learned by ma on this data (use the maximum
information gain eriter-ion rer splits, dolﬂ do any pruning). lvlahe sure to elearly mark
at eaeh notle what attribute you are aplitting on, and whieh value currcspunds te whieh
braneh. By eaeh leufnude of the tree. write in the number alhabitable and inhabitable
planets in the training data that belong to that node.

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
:12
Answer: Level 1
H(374/XO(\) Hum/800)
[374m426-l [374+,426>]
g; (01.)
B S W
[wonlsui] [ls4+,zev] [159M411 [2|5+,285’]
Hug/351 H(92/225) Hk47/IOO) H(43/|UO)
mummrlmsmy £ u(%)+% "(51)
‘s0 '1;
Z i ( , i 7y, : _, m 10.1wa .qu : unz
an 0mm +81) nu u new ( v n x
.1 n 1 ,1‘;
H(Ha!w!/|Mr\01'M) : g H(W)+; H(W) mumzwmmm : uuom
~‘ u" E w, u
g n m4+8 n mas n nm

***************Ending Page***************

***************Beginning Page***************
***************page number:33**************
u
The ﬁnal decision tree
[374LA2H
VW
l|g%1§l»\ [ l84f12j§j\
(Orbn/> (‘om’)
N/N N/N
[zomsoil [170+,10’1[|19+.|1’1 [45+‘2ssil

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
3t
b- For just 'J of the planets‘ a third -m——
feature, Tempemtuw (in Kelvin degrees), Rig Far 205 N0
has been measured, es shown in the Big Neal‘ 205 No
nearby table, Big Near 260 Yes
Rule all the etvp! from pnrt a on tliie llutu Big Near asn Yet-
using all three features. For the Temper’ small Par 2415 Nu
alum feature. in each iteration you must small Far 260 Yes
maximize over ell possible hinnry thresh- smell Nee,- 260 Yes
olding splits (such BS I‘ g 250 vs. I‘ e 25m Small Near 380 No
for example). Srnnll Near 380 No
According to your decisinn tree, would a planet with the features (Bigv Near, 280) he
predicted to he habitable or not habitable?
Hint: You might need te use the fellhwing velues 0f the eutrupy funetien for e Bernoulli
variable 0f parameter ,e
H(1/'li):[)9182,H[2/5):HHTU9, Hun/215) : 09759. HUJ,’1L)H):HUK38, Hun/3r.) : u 9941-,
HUT/IOU) u 9074.

***************Ending Page***************

***************Beginning Page***************
***************page number:35**************
33
Answcr
Binary threshold splits for the continuous attribute Temperature:
o . o
o . o
o . .
4§—¢—¢—0—0—>
zos 2225 260 320 330 “mrewm

***************Ending Page***************


***************Beginning Page***************
***************page number:36**************
ilﬁ
Answer: Level 1
FHA/9) PIN/9) H(4/9) FHA/9)
[4m] [LS-1 [Als-l [Ms-1
< Sm: < Orbit > T<:232 S <T<:320
y S [/XN Y N \/ N
[2+_2*] [2+‘3*] [H.Zil [3+.J*] [0+.3*] [44.2% [3+,3*] [1+v2i]
H:l H(2/5) PHI/37 1-1:! A H(l/3\ 1-1:] PHI/37
H10
>
>
MMWM z 1+1“(2)41HWZWM ‘°‘”"“""""““ "(W M“
,,\,,,,,,M,W‘5w, 7 gngys‘ WWW 7 Wm

***************Ending Page***************

***************Beginning Page***************
***************page number:37**************
Answer: Level 2 i“
HUB) HUB) HU/l)
[4+.2—] [4+1—] [4+,2—]
B ! ! S F ! ! N Y! !N
[Z+,O*] [2+,2*] [l+,0*] [ltli] [3+v0~] [Mia]
A H:l A H(Z/S) A HUB)
H=D H=U H=O
g >
>
>
>:
Note:
le plum lmcs indicate that both thr surlﬁc rundltiurml cntmpws and thrir ruclﬁcwnts
(weights) in the average collditiunal entropies satisfy the indicated relationship. (For exe
ample, "(2/5) > "(l/"5) and m > s/u.)
The dotted lines indicate that nﬂly the JPCctflc condltwnal cntmlhcs Satisfy the indicated rela'
tinnship, (For example, “(z/m : l a “(2/3) but 1/0 < 5/0.)

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
3%
The ﬁnal decision tree:
[4+,5—]
Q<=1329
V \N
"M" ‘Til c. According m your decision tree,
A (“ﬂab would a planet with the features
‘ i“ (Big, Near, 280) be predicted to be
Y \I habitable or not habitable?
[3+.(1—] [Hi-l Answer: habitable‘
A < >
V \S
[ltUi] [Okli]

***************Ending Page***************

***************Beginning Page***************
***************page number:39**************
m
Excmplifying
the application of the IDS algorithm
on continuous attributes,
and in the presence of a noise.
Decision surfaces; decision boundaries.
The computation of the LOOCV ernw
CMU, 2002 fall7 Andrew Moore, midterm, pr. 3

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
in

sirnnnse we ere lesrning a clessiner with binary innnt velnes v I u end 1 U
Y L There is one real-valued input x4 The training data is given in the 2 U
nearby table‘ 3 0
Assume thnt we lenrn a derision tree on this dntn. Assnnne thnt when i ‘1
the decision tree splits on the real-valued ettrihnte x_ it pnts the spht f l
tllmshold hslrwny between Lhc nttrilnltes that surround the spliL. For . l
eramples using infurrnution gain es splitting criterion: the decision tree f4 l
would initially choose to split et X I a, whieh is halfway between x I 4 h 5 U
and Y : b dntnpnints. 9 l

lu l
Let Algorithm ma he the method of lenrning e decision tree with nnly two leaf nodes
(itch only one split).
Let Algorithm DT' he the method of lenrning a decision tree fully, with no prunning.
n. What will be the training sci mw for DT2 nnd respectively DT‘ on our dntnl
h. What will he the lennc-one-eni cmss-Imlirllllion error (LOOCV) fnr DT2 end re
spectively DT‘ on our dete?

***************Ending Page***************

***************Beginning Page***************
***************page number:41**************
41
. tmmmg data: [133 tree:
4—o—o—o—o—§—o—o—o-o-o—o—>
01234567x910x
[515+]
. . . .. / ‘\
. discretlzatlon / decision thresholds: < kw
0/ \Nu
1 2 3 4 a 1 x 9 l0 X \
5 815 [4-vu+] [1-v5+]
875 'i'
e compact representation ofthe ms tree= 11/ ’\\Nu
\
12 3 45 a 1 x 9 l0 X W3“ [L121]
El A Gus?‘
odecision “surfaces”: D“? \N“
A new [Cl-‘2+1
5 $25815 X A

***************Ending Page***************

***************Beginning Page***************
***************page number:42**************
42
um 1.-

[slaw [55+] [55+]
‘/X<5\‘ (XQK I?‘ ‘(<3 7?‘
[44H] [1’.5+| [44+] 114+] vim] wig“

Q < Q

IG Compuﬁﬂﬂons :
Lm/z [115+] [ﬁg
66.2?‘ \/x<s 1?‘
[my] [PM [14+] [04+]
10-0191 10 0109
Damian “MW/mt’: 4%

s x 15 x 75

***************Ending Page***************

***************Beginning Page***************
***************page number:43**************
u
xﬂumn ;9;¢;¢;»
s R25 x15
- + - +
H 4o—o—o—>
ID3, LOOCV: 4 5 x 25 x 75
Decision surfaces , + , +
X:6 49—¢_¢—.
5 5 s 25 8.75
XZX ;9;¢;¢;,
LOOCV error: 3/10 5 775 “7”
xﬂ 5 ;§++—+,
5
X:9 4§—¢—¢—~’ * i *
s x 25 Q 25

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
'H
DT2
[5*.5+]
Du; g Nu
[4-4l)+] ll—‘S+]
Decixion "surfaces":
i +
5

***************Ending Page***************

***************Beginning Page***************
***************page number:45**************
0T2, LOOCV 41"
IG Conlputations
[415+] [515+] [4AM
Xh<§ 5 @(ilﬁ X375
Case 1: X:1 2 m4 D“ N“ D/ N“ D“ N"
[3am] [|*,S+I [3*.3+] Ilalﬂ Nah] [UaZH
ggé
<
<
151M gin [54+]
\X_</5 5 @315 75 X<8 75
case 2: x:s‘ 74 3 m Nu Dy Nu m Nu
[4—.0+] [l—v4+] [4-,Z+] [|—‘Z+] [5—.2+] [ﬂ—.2+]
Q § 4

***************Ending Page***************

***************Beginning Page***************
***************page number:46**************
1r
DTZ, CVLOO ’
1G computations
(cont’d)
lib?“
/X<s\
Case a: X:S.5 m7 i KN“
Him] [wow
[514+] l§14+1 [§;»4+]
KM?‘ (NR Qm4§ 25
Gabe 2; X:9,10 D; $1“ m/ $1“ 13/ ﬂu
[441+] \PAH l4<3+1 iii-H] [5<3+1 lU’.l+|
Q < 4
< <
CVLOO error: 1/10

***************Ending Page***************

***************Beginning Page***************
***************page number:47**************
17
Applying IDB on a dataset with two continuous attributes:
decision zones
Liviu Ciortuz, 2017

***************Ending Page***************

***************Beginning Page***************
***************page number:48**************
19$
XZ
Consider the training dataset in the
nearby ﬁgure, 4 o O O
x. and X1 are considered countinous at-
tributes. 3 o
Apply the ID3 algorithm on this dataset. a _ O _
Draw the resulting decision treer '
Make a graphical representation of the l O -
rieeisien areas and decision boundaries
determined by Ina‘ 0 X
0 l 2 1 4 i l

***************Ending Page***************

***************Beginning Page***************
***************page number:49**************
49
Solution
Level l:
[1:511 L423 it»?! l3L-5jl Hﬁjl
(XMS/Z) (>0 <9/2) (Xxx/D (x109 (map
, ‘(,1 \, ,1 _ i, \, i \, i
[2+‘uil lzivsil [n+1 {AH [um [MM [MIA [~34 szix [ML]
A Ham H( 1/3» H( 1/3) H:l Ham st) Hz 1/4» Ht lm A
1L 164L091 1L
H:O > 1-H:
m=o x19 \ ‘ _
‘ Z 111T) m
)lHLPWl-lrzm \ “m i:§N||w<v~uvllA\/1r mmwmnum

***************Ending Page***************

***************Beginning Page***************
***************page number:50**************
50
[4*i2’] [4*i7’] [4*i2’] [442*]
Y N Y N Y N Y N
[2+v0-1 [z+.2-] [z+.2-] [z+.n-] [l+.l-] [3+,1-] [3+,z-] [I+,(1—]
/\ H:l H:l H:l H(I/4) H1Z/i)
L § 3 lGA) 04 Z f 3
H=0 1-H) H:ﬂ
16207109
lG=0 251 e 16:0 251 ‘
>
um . |= 2/3 i Mm ]:II\~11\ mm» H|v| . I: s/n "(z/s)
NOLES!
I‘ when working with nt least 2 rontinnons nttrihutes, the split thresholds rnnst hr recom-
puted at eaeh new iterationi because they niav change. (Fur instance, here ahuves 4 replaces
4.5 as a threshold l'or x..)
2. In the enrrent stage, i.e., fur the enrrent node in the ma tree you niav choose (as test)
either x, < 5/2 or xi < 4.
a. Here above we have an example of reverse relationships between weighLed and respectively
unsweighted specific entmpleat H pail a HLH H but a‘ Hi2+ zel < €»H[K+,2a].

***************Ending Page***************

***************Beginning Page***************
***************page number:51**************
31
The ﬁnal decision tree: Dawn" at?“
[405*]
(£2379 X2
~77; e e
j/ \ 4 o o o
[5:277] [mm n
(X.@ 3 . Q
\/ “\N 6
/ ‘v 2 . o .
[2+(Vl [Ztlil
L (W) . @ o -
/iv\
V N
// \\
Mi] Mr. 0 X
/ /
A Q U l 2 3 4 5 l

***************Ending Page***************

***************Beginning Page***************
***************page number:52**************
52
Other criteria than IG for
the best attribute selection in ID3:
Gini impurity / index
and Misclassiﬁcation impurity
CMU, 2003 fall, T‘ IVIitchell, A‘ ]\4(10re, HWl, pr. 4

***************Ending Page***************

***************Beginning Page***************
***************page number:53**************
33

Entropy is a natural measure to quantify the impurity of a data sot. The
Decision mm learning ulgurithxn uaus Entropy as a aniitting criteriun by tui-
culating the information gain to decide the naxt attribute tn partition the
current node.
llowcver, there are oLher impurity measures that cuuld be med as tho split-
ting criteria tun. Letai investigate two uf them.
Assume the current node u has t classes (nil. .ri.

- Ginilmpitv'ity: ![71) iazfsmlthi

o Misclassiﬁmtmn Impumiy: ,(ii) : i a maKfil PUJ.
a. Assume nude 1| has twn classes, ti and (a. Please draw a ﬁgure in which
the three impurity measures (Entropy, Gim' and Misclassiﬁcatzml) are repre
anntari as the funcﬁnn nf PM).

***************Ending Page***************

***************Beginning Page***************
***************page number:54**************
a1
Answer
S
L'mmm/(p) whim,’ (1 in 11,“)(1 *1’)
mm’) I 1 p" (1 1)]2 :2141 I!)
aly~011M/(,,> g
i 1’(|’m_ fnrptw 1,1)
i 1’ ,1. for ,1’: W1. n g _ PM!
i ,7 for w: [may I 312535,‘,
i If]! Turpin/ll] 5
no n2 n. H na w
g

***************Ending Page***************

***************Beginning Page***************
***************page number:55**************
h. New we (‘an deﬁne new splitting rriteria based nn the Gim' end Mtsclnssiﬁ-
cntian impurities, wliieh is enlled Dmy-of-Impurity in some literatures. Thet
is the diﬁerenee between the impurity of the eurrent nede end the weighted
sum er the impurities er ehildren.
Fer the hinsry eetegdry splits, Dmp-nf-Imymrity is deﬁned ns
Me) ml) e 1’(n,]![m]’I’[1l,)rln,)

where in and it are the lert and rcspecLIvely the right child er nede e after
splitting.
Please ralnulaie the nmp_df.rmpdettp (using hdth amt end chlnssiﬁl'atio'n
Impurity) for the renewing example data set in wliieh C is the elsss variable
te be predicted.

A m in it, (I) lie d2

c t. e, t1 Q te ti

***************Ending Page***************

***************Beginning Page***************
***************page number:56**************
an
Answer
[214*]
r'\
1,1‘
E
u‘ \0:
[Zak] [U+_3i]
E E
mm; 1,: 2/11: 1/.21Q
.7 ,
,(UJ : 21(1,l)::.5:i
1(1511110 1111., ,
2,5 ii 1‘ QAwP”; 7”’?w:
111)’23113)338’g o a: <1 J J
1(1) : n
nlmlMg-mmm; 7, : 1/‘; < 1/2 Q
1
110) 1, Z
' 1 11 1
2 1 :A1[0):”i i:<
: ,,:, a
1(1) 1 1 3 a _ 1 a
,11) i n

***************Ending Page***************

***************Beginning Page***************
***************page number:57**************
37
c. We eheese the attribute that can maximize the Dmp-of-Imp'urit'” ta split a
uade. Please create a date set and show that on this data set Misclnssiﬁcntion
Impumy based Ami) euuldm determine whieh attribute should be used for
spiittiug (e,g., Am‘) e 0 for an the attributes), but Information Gain and
Gim' Impurity based Ami) can.
Answer
A "i u, m 1|! hi “1 u;
"i
a 4 .

Ennupv: Aim) a Hue seie ;H' i+_2ai+ ¥HH+HPX , 011007 u;

2 2 .‘1 l 1 'l l l 11] 2 I;
Jieeeee-e fave’ ,, :2iaii
(‘m 2{7 (l 7) [7 a; (1 J) 7 i<l In} {10 [2L + 23]}

W i7

Z 1 i a i u-

(m ail) $ '

2 a 1 i l
ADM 0: ,ee,, ee,_
1m with H Aim) 7 (7 n+1 4) 0

***************Ending Page***************

***************Beginning Page***************
***************page number:58**************
ax
Note: A [quite bad] property
[Q +,C; *I
lrc,<(‘,,r“(<r“§andr“;<r“; Q
(Wm. (1‘ : (x; + C; and C2 : c; + 05y “A
um. the Dmprnjrlmpm'm/ based m MMWZ/an is n. '
lc{+.cﬁ—1 Lc[+.c5-1
Prouf
MM , m i c4441; v; + 2W C;
' ' "1+6; P. Ha cw“; (n +6; (Lg/yr;
i c', c'; Ac; i c', c', i n
’ (‘1 >0 (‘l \ (a i (‘1 M‘, (w >0 i

***************Ending Page***************

***************Beginning Page***************
***************page number:59**************
as)
Exemplifying
pre- and post-pruning of decision trees
using a threshold for the Information Gain
CIVIU, 2005 spring, Carlos Guesti'in, midterm. pr, 4
[adapted by Liviu Ciortuz]

***************Ending Page***************

***************Beginning Page***************
***************page number:60**************
(if)

Starting from the data in the following table, the 1133 algorithm
huilrls the decision tree shown nearby.

l» u X l’ f \.

0 u 0 0 f \ \11

0 l 0 l s" 1L

1 11 11 1 / A

l l 0 o Kw) 11m

1 1 1 1 9,>*\1 9/”41

A A A A

a. One idea for pruning such a decision tree would be to start
at the root, and prune splits for which the information gain (or
some other criterion) is less than some small 51 This is called top-
down pruning. What is the decision tree returned for s : 0.00017
What is the training set error for this tree‘!

***************Ending Page***************

***************Beginning Page***************
***************page number:61**************
m
Answer
We will ﬁrst angnnent the given ducieiun tron: with
information! regarding tin.- data partitions (he, the [RH
nnrnher of positive and, respectively, negative inr ®
stances) which were assigned to each test nnde during
the npplientien ole3 algorithm. y \
The infnrnlaﬁnn gain yielded hy the nttrihnte X in the m1" I'm"
l) l
u[‘l+.~ze]e 1/5 0*4/5 l 0971*08 0171>5
[H.li] [lnli]
Therelere this node will not he eliminated from the
tree. (I l n l
The information gain f0!‘ the QLLlibulO V (in the leL- [Oklil Il+10r1H+JJr1 [Oriel
hand side child of the root node) is:
A A A A
H[2l=2] 1/2-1 1/2-l:1 1:00,
50, the whole lert subtree will be cut err and replaced by a decision ®
node: as shuwn nearby. The training error predneed by this tree is u \
2/5. l l
’ U ’ l

***************Ending Page***************

***************Beginning Page***************
***************page number:62**************
m
b. Another option would be to start at the leaves, and prune
subtrees for which the information gain (or some other criterion]
ofa split is less than Solne small 5. In this method, n0 ancestors 0f
children with high information gain will get pruned, This is called
bottom-up pruning, What is the tree returned for 5 I 0,0001?
What is the training set error for this tree’?
Answer:
The information gain of V is [C(Y. V) i I). A step later, the infor-
mation gain 0f H‘ (for either one of the descendent nodes 0f V) is
[C(Y. n') : 1. So bottom-up pruning wnn't delete any nodes and
the tree [given in the problem statement] remains unchanged.
The training error is U.

***************Ending Page***************

***************Beginning Page***************
***************page number:63**************
m

c. Discuss when you would Want to choose bott0m—up pruning
over top-down pruning and vice versal
Answer:
Top-down pruning is computationally cheaper When building
the tree we can determine wheu tu etup (no need for real pruning),
But a, we saw ttiIHl<iWii pruning prunes tuu much.
On the other hand, b0lt0m»up pmning i5 more ewycnsivc since W0
have to ﬁrst build a full tiee i which can be exponentially large

and theii apply pruning. The second problem with bottom-up
pruning is that superﬂuous attributes may foolish it (soc CMU,
2009 fall, Carlos Guestrinl, HWl, prl 2.4), The thii-d problem with
it is that in the lower levels of the tree the number 0f examples in
the subtrcc gets smaller so information gain might be an inappro-
pi-iate criterion for pruning, so one would usually use a statistical
66M instead.

***************Ending Page***************

***************Beginning Page***************
***************page number:64**************
[H
Exemplifying
XZ-Bascd Pruning of Decision Trees
CMU, 2010 fall, Ziv Bar-Joseph, sz, pr. 2.1

***************Ending Page***************

***************Beginning Page***************
***************page number:65**************
Iii
ln elass, we learned a decision tree pruning algorithm that iter-
atively visited snbtrees and used a veh'dhtioh dotoeet to deeide
whether to remove the subtree. However, sometirnes it is desir—
ahle to prune the tree arter training on all of the availahle data.
One snoh approaeh is based OTL ateh'etteel hypothesis toetthg.
After learning the tree, we visit each internal node and test
whether the attribute split at that node is aetnally unco1Teluted
with the class lobele
We hypothesize that the attribute is independent and then use
Peoraeh/s chi-square test to goneratc a test etotiett'o that may
previde evidenee that we should reject this "hull" hypothesis, 1f
we rail to i-ejeet the hypothesis, we prune the snhtree at that node,

***************Ending Page***************

***************Beginning Page***************
***************page number:66**************
hr»
a. At earh internal node we can create a cnntingeney table for the training
examples that pass through that node 0n their paths tn the leaves. The table
will have thc r class labels asseciatcd with thc columns and the t valucs thc
split attribute associated with thc rows.
Earh entry 0,, in the table is the number at times we ehserve a training
sample with that attribute value and label, where t is the row index that
corresponds to an attributc value and J is thc column index that corresponds
to a class labcl.
1n order ta calculate the chirsquarz test statistics we need a similar table uf
expected cnunts. The expected caunt is the number at ebservetians we wclulrl
expect if the class and attribute are independent.
Derive a formula for each expected count 1:. J in the table.
mm: What is thc probability that a training cxarnplc that passes through
thc node has a particular label? Using this prnbability and thc independence
assumptien. what can yuu say abeut how many examples with a speciﬁc
attribute value are expected to alsu have the class label’!

***************Ending Page***************

***************Beginning Page***************
***************page number:67**************
67
bl Given theee two tables for the split, you can nclw calculate the chirsquare
test statistic

e,’ 'lotrw
X a ZZ E,
with degrees of freeduln (1 1)(<» 1).
You can plug the test statistic and degrees of freedom into a software package”
or ah Dnline calculator" tn calculate e “value. 'lypically, if 1, < um we reject
the null hypnthesis that the attribute and class are independent and say the
split is statistically signiﬁcant.
The decision tree given on the next slide was built from the data in the table
nearby.
For each of the 3 internal nudes in the decision tree. show the “value for the
split and state whether it is statistically signiﬁrant.
How manly internal nodes will the tree have ii‘ we prune splits with 1' 3 u us?
" \ ~i‘ lrllvauttlilH m MA'l‘l..\l! w (‘llllllWlM (1H m lmvl
‘l {limewillmllM,‘(litter/Mlleluau-W dismount“; 1-,.»er (‘FHL'mlm “View

***************Ending Page***************

***************Beginning Page***************
***************page number:68**************
r111
Input:

x A x x 01 MM
1 | 11 11 11 Q
1 11 1 11 1 /"/ \\‘\
11 1 11 0 11 my‘ “QM
1 11 1 1 1 A
11 1 1 1 1 0Q‘ A
11 11 1 11 11 / \
1 11 11 11 1 117,1»1 117m
11 1 11 1 1 A Q
1 11 11 1 1
1 1 11 1 1 11 '
1 1 1 1 1 107,211 114M
11 11 11 11 11 A

1 a

***************Ending Page***************

***************Beginning Page***************
***************page number:69**************
m
Idea
While traversing the IDs tree [usually lu bottom-up manner],
remove the nodes fur which
there is not enough (“signiﬁcant”) statistical evidence that
there is a dependence between
the values of the input attribute tested in that node and the values
of the output attribute (the labels).
supported by the set uf instances assigned tu that nude.

***************Ending Page***************

***************Beginning Page***************
***************page number:70**************
10
Contingency tables
i i , u 1 1
0v, cum i u Class’ 1 Ml pm n) E 5 PM‘ H §
xk :u 4 z Q 1 ‘ 2
\1 | 0 u mam“ im i $ i I P(Cln.w*1\y* 7
. 1 1
rm m \l n) K 5
0AM?“ Cluss:0 Cluss:1 ,7(\\:“M:Uy:l
\ h z
x, in .s n i’ x A,
x.:x v 2 I’(Clnss:0\4\\:l\): I’
<» s
P(Chzss: 1 \x\ :n>:ii
. 2
1><x1:u\‘\,:u.x‘ :1): £
UAJ\\'|:HY\:1 Chm:n Class:l ‘7 l><\'z HM u X‘ n é
x2 :u u z 5‘ 1
x2 :1 1 ‘, mam“ m v, n x‘ n 3
/'<01w:|\x.:u.\‘:\,:3‘

***************Ending Page***************


***************Beginning Page***************
***************page number:71**************
11
The reasoning that leads to the computation of

the expected number of observations

P(A :1.C :1) : PM : i) - P(C:j)
PM I 1) I L170“ and P(C I _,) I L191
ine. ‘I ,- L 1.
P(AI1;0I]) “I” M

EM: [.C: j] : IV‘ PM :i.C':j)

***************Ending Page***************

***************Beginning Page***************
***************page number:72**************
12
Expected number of observations
E1, C!rL§5:U 171.155 : 1 BMW,“ 1711;55:11 01115.1 : 1
x1 11 2 -1 X1 11 2 1
x1 i 1 2 -1 X1 i 1 2 1
Emmuxk, cm“: 11 Class = 1
X» :11 3 i
~ 1s :1
1 2
A» 1 , ,
‘ :s 11
51,1111 :11 01.15,:111-
. 1 , 1
1V’ 12. P1X, i111’ E §1 1110111511411’ gﬁ
, I 1
11111211 :11 01115:“) I IV-P(X, :11) P10111145: 11) : 12 3 i :2

***************Ending Page***************

***************Beginning Page***************
***************page number:73**************
-> A _ 73
x‘ Statlstlcs
1i ‘ ' (ll/*3)?
X i Z Z EU
H H
,, (4 i 2)2 (u i 2)} (2 i 4);’ (a i 4)’ 7 _
,< I i i i I ; 2 1 1 : r
A \1 2 + 2 + 4 + 4 + + + >
2 (3*?)2 (143)? mi‘): (2*|)z ,
x mm, i 2 + 2 + 1 + 1 *5
2 z z 1
0 i Z 1 i l 2 i i 0 i Z ,
2 3 .1 s s 4 2' .
X \1\M:"\'\:‘ I ﬁJrer'ilJrﬁIQTI';
p-values: 0.0143. 0 0832, and respectively 002133.
The ﬁrst one 0f these p-values is smaller than l) (15, therefore the root mule
(21,) cannot be prunncd.

***************Ending Page***************

***************Beginning Page***************
***************page number:74**************
H
Chi Squared Pearson lesl stalistics
_ ki‘
_ k : 2
g _ k = a
_ k : A
_ k Z 5
10 k : 9
n c
a
3"
I
1 Y
o
n 2 4 s s
,3 7 Pearson‘s cummaiwe \Esx stamshc

***************Ending Page***************

***************Beginning Page***************
***************page number:75**************
7;
Output (pruned tree) for 95% conﬁdence level
{X3}
n / ’ \\\
A/ \7\
/L\ Q

***************Ending Page***************

***************Beginning Page***************
***************page number:76**************
7!»
The AdaBoost algorithm:
pseudo-code and some basic pmpewties
CMU, 2015 fall, Ziv Bar-Joseph, Eric Xing, HW4, pr‘ 2.1
CIVIU, 2009 fall, Carlos Gucstrin, HW2, pr‘ 341

***************Ending Page***************

***************Beginning Page***************
***************page number:77**************
17
Cnnsider 7r) training examples S: {iii m) , ,(ei. yin), where i, e A’ and yr e (el +1)‘
Suppose we have a weak iemnnig algorithm A which produces a hyputhesis h w l l i i l)
given any dielribulien D ni exuniplee.
AdaBoost ie un iterative ulgnrinini which wnrlre ax follnwu:
\
- Begin wiih a unil'errn distribution 0,0; e w i: 1,. .m.
. At eueh iteration r e l, .Te "'
e run the weuk learning alga. .4 on the distributinn D, und pruduce the hypothesis 11,;
Nah: (1); Since A is a weak leerning algorithm‘ the produced llyputllesis ii, at round z is
ullly slightly better lhun randuln gueueing. euy, by a nuirgin 1.:
1 l
e, e ('7Yp,(hg) e Prieniy eri,(i,1]< E end it e 3 er,
Nriie (2); "at u certain iteratiun k g T nie weulr cleneiiiei- 4 ennnul pruduee n hypnlheii,~
benei- than random guessing (i.e., 1, e n) ur if it can unly pruduce u perreei hypothesis
(he, 5i e u), then the AdaBuuM. ulgurinini uhuulrl he shipped.
l l , n
. compute the weight ni e alnrii
e updeie the distribution
0mm e z‘ um) ("W/‘MW for i e l. .m. where z, is the nul'nlalizer (2)
. in thr nnd, dnlivcr HT e mu (Bimini) n; thr learned hypothrsis, which will act as n
weighted mlljan'iy vale.

***************Ending Page***************

***************Beginning Page***************
***************page number:78**************
vs
Important Remark

The nheve formulation er the AllaBousL algurilhm states no restriction en
the h, hypothesis dclivcl-ull by the wenle elessilier A et iteration I. except tlnit
st < 1/2.

lleweven in another reminlntien of the AdaBoost algorithm (in a more gen»
oral setup; see for insLance MIT, 2006 fall. Tommi Jaakkola, IIWAe problem
a), it is ieqneeted / reccornlnended thnt hypetheeis ll, be chosen by (eppwm:
ﬁnitely) menimimg the [criterion of] Ille'ighted training error cm a whole class
of hypotheses like. for instsnee, decision trees er depth 1 (decision stumps).
1n Lhih prubluln we will net be concerned with sneli u mqnesl, but we will
Conlply with it for ihstsnee st CMU, 2015 fall, Ziv BeeJeseph, Eric Xing,
l-lw4, pr2.6, when showing hew Adchmst works in pisetiee.

***************Ending Page***************

***************Beginning Page***************
***************page number:79**************
19
Prove the follclwing relationships:
l. z, : w“ [1 e s,) +t'" -s; (a euusequerree from (2))
1,. Z9 I 2 /s,(l e 5t) (a Consequence from 1.. and the value stated for m iu
the AdaBoust pseudo-code)
he u < 2, < l (u consequence derivable from u.)
y. ,e l1"! my, ,e h,(.rr)). l.e., the mistake set
1L‘.L71Al[l): 5'0)
' . 1 e c “:" Mu, : mm). Ler, the correct set
2(1 e 5,)
(a consequence derivable from (2) and HY)
u. s, >5, Q m <<t,
(a cclnsequence frnm the velue steterl for (v, lu the AdaBonst pseudo-code)
m w DUN/It] I l/Zv where (‘Hum (In) “'2 Prue‘ ({HVHUl) % yd]
(u euusequeuee derivable from (2) and L1.)

***************Ending Page***************

***************Beginning Page***************
***************page number:80**************
so
Solution
t. Since Z. i=- the normalization factor fur the distributiun DH. we can write:
Z» Eu,(t)w“'"""""' Elm/Mm + E 0mm: (1 i 5,) w" +5, a" (a)
,Zt ,(r ,(M
no l 1 i ‘r .
H. Since m :’ 21“ , 1t folluws that
1‘ lift , list
(u 5 “ ; " g, 11* 5,
t t \1 t i 4)
v <
and i
f'" I L I if" (q,
w \f 1 i t, ‘
st), i
‘ 5 mi ,
Z,’[i*£;) V/‘iy'ﬁ' V'Vi':2\/s,(li:v) (b)
, :1 :1
1 :
NoLe that f’ > 1 because s, e (u. 1/2); met-Drum (t, > 0.
it

***************Ending Page***************

***************Beginning Page***************
***************page number:81**************
81
m.
'th sernnd ordPr funcﬁnn m x 4,1 reaches its maximum valuP for g, i |,'2, and the
JT
maximum is 1/4. Since 5, ;<u.1/2J_n follows from (a) mm z, > 0 and z, < 2v‘ 3 , 14
m. Based on (z). we can write:
1 w for 1 >1 V11
D,+.<11:Zj~m11 { M,“
Thorofurc.
1 < 1 fl , 5 u
[G M Q Dmh) :i m’) w" i‘ : MU) ‘if’ i i
Z, 2\/L](1iiy] Vs, 1;,
v 1 , my 1 v3 D41)
C D :i 1) '"‘:i.u, i:iv
‘E i’ m") z, 1(1) ’ WHO a; (‘J \/1 ,5‘ mfg’)

***************Ending Page***************

***************Beginning Page***************
***************page number:82**************
82
1’ V
u Starting from the deﬁnition u; i 1n V, f’, we can write:
in
‘1 i I J1 i I
m <n, ¢1n J ”' <1nJ ‘/
v \,
Further on. since both 1n and V/ functions are strictly increasing, n follows um
I i 2‘ 1 ,5 a ;
m < n, Q r < V / J“ :,(1 a) -::,(1 5,) ‘Q r, 9n< a 71¢: a > r,
w. h. is easy m soc than
mum/1') 2mm MW,» Z 70w ' 7 Z MOM
v’! 16M ( I rf‘!
W
1 m 5' ~
: Z ;, - r‘ (O
By substituting (a) and [4) into (1), wn will gut:
H (h) 15 1 flit; 1
1*’ N, f v if P ‘i: *
D ' A! ’ 2VL,(1<,) ' \/ a, 2

***************Ending Page***************

***************Beginning Page***************
***************page number:83**************
all
The AdaBoost algorithm:
why was it designed the way it was designed, and
the convergence of the training error, in certain conditions
CMU, 2015 fall, Ziv Bar-Joseph, Eric Xing, HW4, pr‘ 2425
CMU, 2000 fall, Carlos Guestrin, lrlwz, pr‘ all
CMU. 2005 spring, Tom Mitchell, Carlos Guestrin, llwz, pl: 11.3

***************Ending Page***************

***************Beginning Page***************
***************page number:84**************
Pi 1
The objec'tve of this exercise is te prove that the training error mil-(111) of
the AdaBuost algorithm decreases aL a very lziet rate. lull‘ iii Certain cases it
converges to u.
N01215:
14 For the pseudo-code or the AdaBoosL algorithm, see CMU, 2015 fall. Ziv
Burrloscph, Eric Xing, HW4, przl
24 llere we will assume (similarly to Clvlu, 2015 fall. ziv liar-Joseph Eric
Xihg, HW4, pr2.1) that AlluBuueL Inukc> mi Yuslhcbio'n Uh the l.X llypulhcsia
delivered by the weak classiﬁer ll at iteration t, Except that r, t' (ti. 1/2].

***************Ending Page***************

***************Beginning Page***************
***************page number:85**************
PU
er
e, Show that urvill) : (o. 11,1, zl) will”, wliere m) :gf,,sl,li,(l).
bl shew lliul 1:11,(H1) g 11,1‘ ZH where (le(H/] :' m 2:, lmmsw re llle
lrsiuing error produeed by AdaBonst.
c. Obviously, we wuulll like le vninivrnzc lesl sel error pruducell by Adelaoesl,
but il is lnird lo du so directly We llius sellle for y1nedily optimizing lire
upper bound on the learning error found at part ll. Observe that 2,. ,zH
ere rlvlvlllllllml 1w llle lirsl f’ l llerellous, end we eeuner Clmllgr' KIWI}! el lwmlon r
A greedy step we Hm take lo uuuluuze the LHlellg ~l>t eirer bmllld (m round Y h le
ruiuuuire zvv Prove lliel llle value or (u lliel minimizes z, (among all possible
l le e
vnlues for m) is indeed u, : 51“in (see lire previous slide).
d
d4 Show llrel 111:,2, 5e 121m.
e. From purl l end d, we knvw the lruining error deereeses at exponential
rate with res-peel le Tl Assume lliel lnere is a number e > n such llrnl e g e,
for l I l ,T. (This e is called n gonmniee er Pmpincal e 40911‘: lenmrlln'zrip.)
l-low many rnnnds nre needed ld neiiieve. a lrnining errer 5 > u’! Plense express
in big-O nolslion, 7‘ : 0p).

***************Ending Page***************

***************Beginning Page***************
***************page number:86**************
2m
Solution
a. We will PXpand [5(1) racursivPly:
1
11mm Z Z'Dy(1\v’“"""""‘
1
I DNUZTFWIM.“
I l
I U7, WM” y,,,‘,1i,7n,‘,‘h,‘.
1 1m Zhlr 21'
Z DA’) 11 "y" y, WM,»
HMZ/
Z épwnm

***************Ending Page***************

***************Beginning Page***************
***************page number:87**************
M7
y
b. W'c will make u>u uf the fact that the expunentiul
loss function upper bound; the U'l loss functiun. Le. ‘
1W“) (F2 1M e"
"TWHH I m Zlkv'llmqﬂ 9%,‘
é 52mm’) Hz,:Zny,+.</v1'lzy
I (van>)>(1'[z,)
_,_
.
1
: Hzp
H

***************Ending Page***************

***************Beginning Page***************
***************page number:88**************
at
r. We will start frnm the eqnatinn
z, a5, w + [I is‘) t

which has been proven before. Note that the right-hand side is eenatant with
reepeet te 1, (the errer produced by 1n, the hypetheate produced by the weak
clusaiﬁcr A at the enrrent etepy
Then we will preeeett as penalty, colnputing the partial derivative w.r.t. an

1) e ee . a.

it‘, r etiam r 'ptme, wauaei) t 'teo

om

1a e I’: i [is
m,’ (we); , | i5, We‘ e pi' :72,“ emf‘ WU I 51“;
a, at . 5,

1a r

Note that i’ > l (and therefore u, > u) heeattae e, 5(1) my
e,
. . 1 l Ft . .
1t een also he nnrnedinteiy ehuwn that ‘ti : 5 1n i ie mdccd the vulul: rer
t,
whirh we reach the mtnttnmn of the expression 5, w + (1 ,5’) z and
thererere uf z, too:
1 , 1 1 i
t, ‘:1'" amen) 11"" >0etel'" a i‘ ><1<=>m> airtig.
q 2 (i

***************Ending Page***************

***************Beginning Page***************
***************page number:89**************
so
i em:v4
Plots of three Z(3) functions, Z i Ew:Z/5
1
ZIH>::.;2+<14,)3
w ‘
when: V E
‘12" w, (Q being “00(1) here) \
and , w X N 3 ‘W //
1c implies that “ \XJ/
/1 Z \
V 5, \ //
2mm.) : H : Zy/5,(i i 5,) g *1», )/,//
, 4*‘
‘ 1 i g, ‘
“W I 1mm“: I" 1i
\ 5, ‘
1 2 a 4 5
he'a

***************Ending Page***************

***************Beginning Page***************
***************page number:90**************
90
i
d. Making use hr the isihiiiihship (h) which was already
proven, and using Lhe facL that l’ r 5 r' ‘ [or all i E 1R, :7.
we can write:
1 i is‘
HZ» [[2 i/siﬂsswl n X
izi iii
i
: H \ll 11,9
iii
7
s H N h Iv‘
iii
,, ,,. I
i H W" #711 i HF»,
izi izi
Z P422 v’

***************Ending Page***************

***************Beginning Page***************
***************page number:91**************
91

c.

4—L:L='J—>

U [A "I IIZ
R'om the result obtained at parts 1, and ,1, we got:
» 1 1 1 ' l
m-(HT) S ,~ 15H“ 5 v 2" , fr 1*)“ i1
Therefore,
. '2 ‘ 2 , '2 1 I 1
(IT!,(HI)'\ ; ‘f 2T1 11“; =» 2n > 11,5»1212 >111’ @1- » w 1“

1 1
Hence we "cud I: o in,’ .

T ;
Nate: n ruuum um m ~(11, ) a 0 a! T a x.

***************Ending Page***************

***************Beginning Page***************
***************page number:92**************
$12
Excmplifying the application of AdaBoost algorithm
CMU, 2015 fall, Ziv BariJoseph, Eric Xing, uw4, pr‘ 246

***************Ending Page***************

***************Beginning Page***************
***************page number:93**************
93
xi

Consider the trhihirrg rihtheet in the nearby ﬁg’ i
urei Run T : .x iterations of AdaBoust with decir r “e ,le ‘7Q
sinn Stillnps (axis-aligned eephrrithre) he the base _
learners‘ Illustrate the learned weak hypotheses * ‘2
I1, in this ﬁgure and ﬁll in the teble given below‘ 1 H- r4» \“l
u-"m the pwudu-oodc er the AllaBouat algorithm. ite- cMu, , r e ‘ .
2015 Li“. Ziv Burrluwuhr Elil- Xillgv HWAr pr 2'1. I'lcvhn: K )
lead the Impullilnl 11mm (ll-L lullnwrs th-t Paluducnde!) “ x

H i 1 1 A t r
"l mu) m2) 0,13; um) ms) um) 04$) mo) <'W~(1I]

l
2

I---——-——
NOiC: The In‘ m‘ this exercne I: tn hEID yuu undent-nd how Ari-Bean wurks m Drlctice.
It .. “were dun they "Meetindrhz thin were, y,“ Wm.“ impiemem i mm“. /
Mew... mt "huh"; (in. "WM Mm” m, meme, by .. Kim ewe“. mm,‘ w r t ..
ccllam probabilistic dlitnbutmn (It) deﬁned on the tlamlﬂg datum! Later 0n ynn will extend
the mgm... it. e “"7"th Wmuhew “Meme.

***************Ending Page***************

***************Beginning Page***************
***************page number:94**************
94
Solution
Unlike the. graphimal reprazentatinn that we need nntil nnw for derision
stilinps (ne trees of depth I)_ here we will work with the following nnnit't-
tcul representation: for a continuous attribute x taking values i 5 Iii and for
nny threshold rt e lR, wl: enn Llcﬁnl: twn dcuiaiun etnrnpr:
l nigh ml itrze

Wm , x] { 1 n 'r<.~ ninl “time i) {1 i‘ r < t
For eenvenienne, in the eeqnel we will ilennte the ﬁrst tleeieien etinnp with
x g .i and the second with x < it
Accnnling tn the important Remark that follows the AdaBoost pseudo-code
[scc CMU, 2015 full, Ziv Bur-Jusuph, Eric Xing, HWA, pr. 2.1], tit ouch itera-
tien (t) the weak algorithm A selects the/e decision etnrnp which. amnng all
decisinn etnmpe, has the minimum weighted lmtnivlg ermr w.r.t. the cnrrent
distributinn (m) on the trnining dntn.

***************Ending Page***************

***************Beginning Page***************
***************page number:95**************
95

Notes
When applying the 1133 algnrithm. for eaeh enntinunus attrihute x, we used a
separation threshold for each pair of examples (it-mt [nu-W01 with W, it <
0 such that i, < Int, but no Ij e max) for which I, <11 < In!‘
We will preeeed similarly when applying Adaneost with deeislpn sLumps and
continuous attributes,
ln the case ol 103 algorithm, there is a theamttcal result stating that there is
no need te eensider ether threshelds fur a eontinnous uttrihute x apart from
thus-e situated heteen puirs uf successive velues (n < fr+l) having uppesite
labels (1/, # !/v+l), heeause the lulermation Gain (1c) for the other thresholds
(i, < rm, with 1,,’ UM) is provahly less than the maximal lc lor Y.
LC: A similar result can he proven, which allows us to simplify the application
ol the weak classiﬁer (.4) in the framework of the AduBoost algorithm.]
Moreover, we will eensider also a threshold situated eutslde the interval er
valuols Lnkcn by the attribute x in the training detetet. [The deeision >Lulnp5
corresponding tu this “outside” threshold can he esseeiated with the deeisiun
trees or depth n that we rnet in ether problems]

***************Ending Page***************

***************Beginning Page***************
***************page number:96**************
95
Iteration t : l:
Thcrcioro at this stsgc (itct thc ﬁrst iteration of Adsuoost) thc thmsltalds for
thc two continuous variables (x, and x1) corresponding to thc two coordinstcs
of thc trnining instances (n. ...n.) arc
r r
o l, 3, nnd i‘ fnr x‘, nnd
Z 2 2
n1,1‘,3 and i for Xo.
2 2 2 ‘2 “
1
an can easily scc thnl we can get rid or thc “outside” thrcshohi 5 [or x‘,
hoeonse the decision stomps corresponding to this threshold act in the same
l
os the decision stomps associated to the. “outside” threshold 5 for x,.
'rhe derision stumps corresponding to this keratin“ together with their as-
soeioted weighted training errors are shown on the next slide. When ﬁlling
thosc tabels, we have used thc cqnnlitics 4'7YL),[4Y1 z ,1 e l’ trinxXr < or)
and, sirnilnrly, m~i,,,(4\’2 3 a] : 1 arr”, (X1 < a). fur nny threshold a and cvcry
iteration t I 1.2 , These equalities are easy to prove.

***************Ending Page***************

***************Beginning Page***************
***************page number:97**************
97
Y i i §
—._—
| 2 | z z
—lI-
5 7 l
mm()\i;~/ 5 5 3
2 2 Z Z
>1 4 I x12 1,12
"""‘<\‘\J'1+‘FA f'
m ,i 2 > u 5 2 7
7 ‘(X i j o v; 4»
H can hP Seen aim ‘he minlmnl weighted twining error (5‘ i 2/0) is ohmined fm- the
decision stumps x, < 3/2 and x) < 7/2. Therefore we can choo>e m : WW (2 X1) as
hm hypothesix ‘AL iteration f I 1; the corresponding Rpm-hm i=- the line x2 I 2 The
h, hypothesis wrongly classiﬁes the illsbancvs i. and i7,‘ Then
1 2 5 i 1 i 5, 5‘ z z 5 ‘
‘l E’§ Randi“ EMT invi<li§> § hiwqiuzu

***************Ending Page***************

***************Beginning Page***************
***************page number:98**************
9x
Nuw the algorithm must get u new distribution (0,) by altering the uld unc (0,) so
that the next iteration concentrates more on the misclassiﬁed instances.
I 1 E ‘ 1 2 x
, "13.7%,",
1 7,. "HM Z’ 'r \*'v 0"“ )
mm : fDHUU‘ w ' ' : e
41 M; 1 1 J7 V r
\1/7 71 6-\J‘El0!1t(l,v}
Rmncmbcr that zl is a normalization [new l'ur vi. —
So, X2
/ ‘g4 lg4 ‘g’
1 _ $2 t1 NH . ‘ ‘x ‘s *1 ,
a Ja 2 a i x' l‘ h
2t u('\‘7+ v2) 9 us, _ o ,‘
t t2
Therefore, 3 Xlgt Q: X124
t t 4 t
e m m4
u t r2 1 | X0 ‘0
i a ta i m 4.’ . 5 9
2\/ﬁu\/7 14 "H J)
mm : 9 1 a 1 t
9 1
> J I t ' 4r. X
2m'rh 1 WM) n | 1 .1 t s |

***************Ending Page***************

***************Beginning Page***************
***************page number:99**************
99
Note

If, instead of m,” (é , x1) we would have taken, as hypoLhesis m, the doci-
sion stump W” (g i A‘) the subsequent calculus would have been slightly
dimmm (although hath decision sinmps have the same minimal weighted

Z l
training error, q); 1i nnd n, would have been allocated the weights 1. while

' 1
i, §i n, would have been allocated the Weights E‘
(Therefore, tho uutput of AdaBuust lnuy nut l".- uniquely uninriiiiiicul)

***************Ending Page***************

***************Beginning Page***************
***************page number:100**************
100,
Iteration 1 : Z:
| n u
2 2 2
i 2 z z z u
('7YIWJ(X\<‘) i H ﬁ+3+ﬁ T4
, m \2 5
-II—|
1 t 1 1
‘ ‘2 2 z z
i i vs Its 2 i x 2 i
whens» U Trieﬁ T’U’ﬁ I’?
, in 15 t. l
r'v‘! ,s _, > s i i i s
’ 1‘ > l4 2x H z
Note; According to the theoretical resnit presented et pert (z or CMU, 2015
fall, Ziv BeeJeseph, Erie Xing, HW4. pr. 2.1. computing the weighted error
rate df the deeisien stinnp [corresponding to the test] \Q < 7/2 is now superr
ﬂuous, hﬁrause Chi; decision Shlmp has bBGn H1058“ BS optimal hypothesis at
the previous iLeraLion. (Nevertheless, we had piaeed it into the Label, for the
sake oi‘ at Lhoruugh presentation.)

***************Ending Page***************

***************Beginning Page***************
***************page number:101**************
101,
Nuw the 11m hypothesis is r.2 I w" (5’ i X‘); the ﬂux-responding >cpuratur
i> the 1111c x‘ I l.
2
2 1 1 1 5
51 i numb 11,}J*U*;*ul4s ﬁ"1ii’?’ﬁ
11ft» 1 1 1 1
n1 1111; 1,,1(1,;) ; 111w 0m
\' :2 \/ 1 1
1 1
-D 111 11/111, :1“
171(1) : Z, 172W [1‘ 'lv' ' 'I 1
‘ M, z 47,111 1% othurvciw
1

1 1 1
i i T f’ I'LI T
Z214 W» “UH in}
1 1 1

i i1i 1111's“.
7.» 4 fr, ‘ ’
i l v3 1‘ as 9)
Z; 11 u" '

***************Ending Page***************

***************Beginning Page***************
***************page number:102**************
1112,
1 1 1 1 1 r, I VB 12+12 24 21/6
21:“. .2. 12. w: 1 1,: >: >:_mUT
11 \/E 1 f1, 11 11% 21/6 1 11w 14w 1
11
2
X2 .i
1/51 1g; “$1
7 1 1 1 1 1 1 X
if i i 11», 1.21 . ‘ 6 1 *
zﬂuﬁ 24 f1e(25(17} 12-1 +h|
1 1,
7 1 1 1 r
n , i i i i fume 1/. 1m 1/41 1/4
1(1) gﬂ 4 ﬂ 411 ' { J) z 1|- 1} ‘1'
7 1 1 1/41 1/1
276 U \m 1 011»1e(¢.121 1 150 x9.
11
<1 l 2 1 J § X!

***************Ending Page***************

***************Beginning Page***************
***************page number:103**************
10.1.
Note
If the property H from CMU, 21115 fall. Ziv BaeJoseph, Eric Xing. HW4, pr.
2.0 were used (ins'ead dr me deﬁnition (2)), men there would he dd need 1d
wmpme 2,, end M1) would he computed directly:
01(1) i 1 i 1 .
mi‘) e” ‘I 7 *11 10140.11)
mm ) 1 11
him:%:l f1)!‘r&‘43}
1:‘ 9 2 : 4
Y‘
and
7 l l
if: i 1 e 11.1.01
D111) ind/Jiluni 1211 21 °“'( M
2047,)’2 b *12 "‘ i 7 1 7
. ive:< 1011mm};
01(1) : 1 I 12 1 1a
1721"] i H i 1 V
2Q e 2 i ,1 11111L(?&,'|).
7

***************Ending Page***************

***************Beginning Page***************
***************page number:104**************
lHl,
Iteration f : 3!

i Z 2
—.—_i
--I—|

. 7 2 ‘ii
(yr/“(Avid l2 1 ifs
2 2 2 Z
V 7 T Z l Yi T l ] T T
Hwy/U i i iiiii 1i in liii
-—-| m n

***************Ending Page***************


***************Beginning Page***************
***************page number:105**************
105,
u
The new best hypothesis i,- I.K : mm x‘ 2); the corresponding separator i,- the line
‘Y
x‘ I 2.
xz 1,2 h]
+ _ _ +
1 l ‘ 1 4 \o to OX
L :1’ _,‘\:2ii:i:, K 11 Y’
x MM I; I‘), 21+}1 21 8 ‘ h‘
1 A;
1 I ‘K
m I I
2 a n 2 v,- Xlo n“
f ‘ﬁ
:1’; c I I i O 0
m : 1n Ji’:lnc 1 i *:ln\/7:Ufl73 ' ‘i X‘!
\‘ 7.‘ \‘ a n
o
U l 2 Q 4 ‘ X]

***************Ending Page***************

***************Beginning Page***************
***************page number:106**************
um,

Finally, ener ﬁlling our results in the given table. we get:
.|—,>,e‘ M) M M M, We W We,“
1 2/" ln\/T/2 m 1/21 m m m 1,21 m 1,’:1 m 2/0
I'M‘ mﬁ m1 ml 1,'\| 1'|-1/|| um 1m 2,")
x 1,1“ mﬁ 1'21 1/21 1/21 ms 7pm 1/21 1/21 m m u
Note: Thu folluwing table hclps yuu understand huw mum was curnpuLcLl;
remember um n, ( , ‘1 "2 em, (111,‘ (1/ m L ,)).

1 u; H ‘I I! I, 1', H‘ '7 1x e,

1 0020 +1 +1 *1 +1 +1 *1 *1 +L +1

II we --—-HI *1

II m II-I-HIIIHII +|

-|_-II-IIH=I- +'

***************Ending Page***************

***************Beginning Page***************
***************page number:107**************
107.
Note: By using the nntntinn r, I tn t .t, 2), we can write
r - r t
H,[1,) n ~H[YL(1H VJ; "Wé i 1,1) +1“ i/(i mm; i i, l1 +1“ i/"t Yin/m)“, i 3))
Remark (1)
One can immediately see that the [test] in- X1 hz h,
stance [1 l) will be clanilicd by the hypuﬂl- e e
e>is H, learned by AdaBoo>t as negative 4 X10 A60 o‘7
(since m t (w m : lmzu - llqui n rm < hl
u). 1 Xi‘
- e e
Aﬂer making Ether silnilar calculi, we 2 A. ‘o .A
can conclude that the derision znnﬂs and ' 4 “
the decision boundaries produced by Ad- l e 5 "9
aBoost for the given training data will be
as indicated in the nearby ﬁgure. l!
0 l 2 1 4 5 XI

***************Ending Page***************

***************Beginning Page***************
***************page number:108**************
lax.
Remark (2)

The execution of Aoeseest eeuh-l eontinue (if we would have tekeu initinlly
T > x...). although we have already obtained (‘in-(Hi) I u at iteratinn t I X. By
eloboroting the deteils, we would see ‘hat for r , 4 we would obtoiu us opiimol
hypothesis Xi < 7/2 (which has already been seleeted et iteietiuu I e l). This
hypothesis produces now the weighted training error e, I l/G. Thereroie,
o1 e lu ﬁ, and this will be added te m e lu ‘fl/2 in the new eutput H‘. ln
this wny. the cnnﬁdence in the hypothesis Y2 < 7/2 would he strengthened.
5e, wl: elieuhl keep in miinl that Adelaeeet euu eeleet several times u eeituiu
weak hypothesis (but never et eenseeutiye iteraticlns, cf. CMU, 24115 fall. E.
Xing, z. BonJeseph, HWA, pi. 2.1).

***************Ending Page***************

***************Beginning Page***************
***************page number:109**************
. ‘ 10!‘),
Graphs made by MSc student Sebastian Clobanu (201s fall)
The variaLiun 01-3! Wm. /= The two upper bounds of the empirical
error of Hy :
§ \‘ 2 ‘v’
A
“
a ‘ m
q u, h
= h
r/rii u
'1’ ‘ / ‘n
é = \ w ‘A ‘
s ‘ " n “wig-,3)
w / = r w‘ .
2 \ ‘\ 111%‘ \
u \ \
: H \ \ \
\ 1: ‘\ri:;ii
D m 2° an .0 a w 20 so on

***************Ending Page***************

***************Beginning Page***************
***************page number:110**************
nu,
AdaBoost and [non] empirical w-weak learnability:
Exempliﬁcation 0n a dataset from 1R
CIWU, 2012 fall, Tom IVIitchcll, Ziv BarJoscph, ﬁnal, pr. 8.3-0

***************Ending Page***************

***************Beginning Page***************
***************page number:111**************
111,
1n this prehlem, we study hew Ade
aBoost pertsrms en a very Sims
ple elssstﬁeetion nrohlein shown In U l j t 4 § n r
thc nearby ﬁgure. "
We nse decisien stump fer ench week hypothesis h,. Decision stump classiﬁer
chooses e constant velne ~ and clnssiﬁes all points where l > s as one cless
and ether points where 'r 5 s as the other class.
a. tht is the initial weight thst is assigned to esch data point?
b4 shew the decision boundary for the ﬁrst decision stump (indicate the
positive and negative side of the decision boundary).
e. Cirele the peint whese weight inerenses in the bees-ting process.
d. Write. down the weight thst is sssignerl tn eeeh data point elter tne ﬁrst
iteration of boosting algorithm.
c. Can boosLing algorithm perfectly clessil'y all the training examples? Ir nc,
brieﬂy explain why. If yes. whst is the minimnm number er iterations’!

***************Ending Page***************

***************Beginning Page***************
***************page number:112**************
112.
Answer
Withuut nutside Lhreshuld:
With outside threshold:
. 1 I 1 =
' Y I 1 m "I m 1;;
"' v1 19 m ‘ V , ‘ 5
' err‘ l/.\
| \ 5
err: 1/: I 1 I Z ‘
w m "1 1/1
. r I 2 . “ V ‘ V ‘
m "1 l/Z I!!! :rr. W. m. m
\ r» s ~ : a
‘ ' ' ‘ ‘ ' If) h‘ 1m l/K
err: 1/1 m1 m m: m ‘ ,
1 \ ‘
- r : a m m .m 1/2
‘1'? "J "’ "1 . I I '1. V
‘ ' 3M l/l " 1/2
| \ 5 , ,
‘ V V > ‘ V \ » s
err: 1/3 en‘ l/Z In: l/iv ' ’ ’ '
m- m .m w

***************Ending Page***************

***************Beginning Page***************
***************page number:113**************
Without outside threshold: 113»
With 0"“de “Hughuld: :\/;\‘,'x < m : \n\ﬁl: u um n, MI‘! : \H
07*‘ 01*) ~A’|,'|’/HA’\HV§’V7“H
5| i 1,’; 4 (n i lln/‘Z i 03405. (mum i —||||m|||
1': M + + , ,
.1 2 , —|—-—
I2 :1,'-1:’"! :“H/—":"-51“—" HZ; (L:v7":~]Qk'|:‘VY\/1:H(NVV7:LV\
—|||||g|||
m , i \ >
= ' H’ :1'3 7 =Hu U:'
II- H / ﬂllll
HIM) |-- "M" ‘ >
"*3 HZ‘ q:‘,IAQHFM/ﬁzmm
I»! i 1/1, QM , m J54) 8017 —|I|IInIII
‘H ‘ 7 ﬁrrv~Lll_,*|L
m , , i \ \
‘ + ,
"~‘ ' J’ ’ "Wm" “ —|—-—
"J + + i l I ﬂ ll mtl I d
t m w cm mm, m w ms a ,, an
mt k ,) |-- “w... WM {£me u. W," MW, wk.“ “w
‘in, a; ,1 M“ MW‘ he +.
nmmnmmvi mm“, Tip

***************Ending Page***************

***************Beginning Page***************
***************page number:114**************
with mite-Me threshold without outside threshold 114’
5 5
Graphs made by ﬁ 5%
Sebastian Ciobanu ‘E : ‘X ‘

***************Ending Page***************

***************Beginning Page***************
***************page number:115**************
11*»,
Seeing AdaBuost as an optimization algoTithm,
w.r4t. the [negative] exponential loss function
CIVIU. 2008 fall‘ Eric Xing‘, HW3. pr‘ 4141
CMU, zoos fall, Eric Xingﬁ midterm, p12 51

***************Ending Page***************

***************Beginning Page***************
***************page number:116**************
llli,
At (‘,MU, 2015 fall, Ziv Bar-Joseph, Erie Xing_ HW4_ pr. 2.1_s_ pert ll, we have shown
that in Adauoost, wc try to [indircctly] minimize the training error writ-(11) by sequen-
tiully minimizing ita uppur buund 1-1,‘, l Zl, Le. at each itcr-ntiun l (1 g l g T) wc chuoat-
m so as to minimize Z. (viwed as a function uf m).
Hurl: yuu will scl: that nnotht-r wuy tn explain AdaBoust is by aoqut-nti-nlly minimizing
the negative exponential lass:
1 f l l r
.I'gi ~e, ,"é'i -a‘ r, 'll‘
r m gum v ml >> m gum v gm w >1 l 7
Thnt is to say, nt tin.- l-th itcr-ntiun (l g l 5 T) wc wunt tu chuusl: bCEiLlL‘! tho uppruprintc
classiﬁer In the corresponding weight n, su that the overall lass J, (accumulated up to
the ﬁ-Lh iteraﬁnn) is minimized.
Prove that thia- [new] strategy will lead to the same update rule for 1n uaad in AdaBoost,
l 1
Le" (ll I Einfif’.
Hint: You can use the fact that Dill] 'x vxp( Yin/Mug), and it [LC: the proportionality
factor] ran he viewed as ronstani when we try to optimize J, with respect to m in the
l-th iteration

***************Ending Page***************

***************Beginning Page***************
***************page number:117**************
117,
Solution
At the I-Lh iteration. Wu haw
1 1 ,7.
J, e Eztspteyjtm) e W2v><11<ﬂlt(Zm'hl'ﬁd) *I/tm/MUJ)
tel ‘e, t :1
1 1 H
I "1272“ 1/, fvev[n])>r,xp( l/,mhv(1-,)): "12.021120 13m) mm Maw»)
e Em» vxp[*//,mh,(|,)) "”' mm CMU,1015 (all, z‘ Bar-Jnscvh, e. we "wt, pr. 271.5, Wt t)
,et
Further OIL We can rewrite J,’ as
J,’ Em’) 0x])(*t/,mh,(|,]] ZUAtMAMﬂu) + Z Uy[!)\'xp[m)
,et 15C ts.“
I (1d,) W" +4, t (9)
where :7 is the set of Examples which are currently classiﬁed by m, and ;\I is the set of
examples which are miseelsssiﬁed by 7t,.

***************Ending Page***************

***************Beginning Page***************
***************page number:118**************
11x.
The rcluﬁun (9) is identical with the cxprusﬂun (:x) from part n uf CMU, 2015
ran, Ziv Bame-gph, Eric Xiiig, HW4_ pi. 2.175 (see the solution). Therefore,
1 1 L
when V, ii ﬁxed, J, will reach its miiiimiim for (i, I E iii f’.
:1

***************Ending Page***************

***************Beginning Page***************
***************page number:119**************
110,
i In
i 052:"?
i WM
Graph made for the J} functions c.
by Sebastian Ciobanu v v
N
for CMU. 2015 fall. m
z. Bar'Juseph, E. Xing Q ‘ l
uw4A pr. 2.5 ~_ ‘
(Not-“ion; 1:“ w g 1 l
1 2 a 4 5
beta

***************Ending Page***************

***************Beginning Page***************
***************page number:120**************
120.
Important Remark Input: (QM/q)‘ \ the training dalaset. T the number
(in Romanian) of itcvatinns u» be executed, u i an set or Wrak hypothrics,
w 1/] (-Xmiw') i the exponential l0» funuﬁun.
nus rescriem expl'esia Procedurs;
('J} Bub forma \muahze the new.“ M‘) : 0 and
If“, ‘ I (‘m rm‘) ‘akmmm dismbuzion mm: 1mm”: v. .wv
VT‘ fon:1wTdo
> 1 Compute
se Dbmvss CA am; ﬂxﬁm “L
m atunci a minimiza (‘X- UM") idawn“;1LHal/WHIP‘[TVFHMIJJ
pmin ‘I, in rapuﬂ. cu Lﬁ
[alegerea \ui] m revine la "KW
a lninirniza ,, (we "u 2 Updatelhedassmer11(1):], \(l)+m/n(.|)
depinde d2 m)‘ and {alum m: MW mnnbnnun, 1m.
_ and (or
In consecinpd, algorilmul m m m d .0. r I "r v ,r
AdaBonst Poatr ﬁ m " ‘ ass ‘ 5g M )y
1 L 1 1
stra‘l‘::;:ezviﬁﬂ'l§ The inmmon is that, at each step, the algorithm greedily adds
a mam!“ / Pied» a hypnthrsi» n e n m nm ".1"an hypn'hrxia m minimizr‘ mp
new)” Y‘ MM“

***************Ending Page***************

***************Beginning Page***************
***************page number:121**************
121,
AdaBoost algorithm]: the notion of [voting] margin;
some properties
CMU, 2016 spring, W, Cohen. N4 Balcan, HW44, pr‘ as

***************Ending Page***************

***************Beginning Page***************
***************page number:122**************
Despite that model mmplerity inereeses with each iteration, AdaBoost nines not usually 122’
mun-ﬁt. The reasnn behind this is that the model becomes more “conﬁdent“ as we
increase the number of iteratinns. The “conﬁdenee” can be expressed mathematically
as the [voting] mamm. lzeeall that after the algorithm AdaBoost terminates with '1'
iteratiens the [entpnt] el-arsilier is
r
lly-[ll e Wit (Zn, mt )).
v:i
Similarly_ we ean deﬁne the intermediate weighted classiﬁer after It iterations as:
t
nit.) alj/Yl(Zuylll[l))
iei
As its mitpnt is either ,, or l, it does not tell the eenﬁtlenre nf its judgement. Here,
witlient changing the decision rnle, let
t
Him We (Em/11m) .
lei
where ti, : E"), so that the weights on each weak classiﬁer are normalized.
1'41“,

***************Ending Page***************

***************Beginning Page***************
***************page number:123**************
12.x.
Dciinc thc margin nitci thc k»th iteration as [thc Mun nq thc [nurnlulizcd]
wcightc nr In voting cncicctiy lninus [thc innn ni] thc [nunnalizud] weights ni
n, voting incurrectly.
annu) Z 2 a, i Z (1,.
“WW ,nnw
a. Lat Mr) “I” 2L1 a, Mr). Shnw thm warming” : [1,f‘(r,] for all training
instances in with i i. Hm.
bi If MWWA (1,) > MMWA (17,1, which of thc sample: r‘ and i, will icccivc a
higher wcight in iteration t- - 1?
1 ,
Hint: Uic thc relation Dt+l(t] : H‘ z 'vxp[ mm») which was proan
m (:l I
at CMU‘ 2015 fall. z. Bar-Joseph‘ ht Xing, Hw4_ pr. 242.

***************Ending Page***************

***************Beginning Page***************
***************page number:124**************
12.1,
Solution

a, We will prove the equality amamhg frurn na- right hand side:

A a

1mm) I 11/! 2m Mm) : 26, 71/‘ mm : Z A. Z‘ m
H H ‘away, (away,
Amy/M“)
b. Accurding w n“.- wlauaaahap already pruvcn at part a,
AIVUUXYIAUR) > :‘Imqwﬂ'lJ Q Wm» >1/11A(J;] ‘='
mm) < u, M1,) Q mm u, m») < wt u, mm)

Based uh the given Hmt, n follows that BMW) < DHUJ.

***************Ending Page***************

***************Beginning Page***************
***************page number:125**************
125.
Important Remark

It can be shown that boosting tends to increase the margins of
training examples sec the rclatinn (8) at CIVIU, 2008 fall, Erie

Xing, st, pr. 4.11 i, and that a large margin on training
examples reduces the generalization error.

Thus we can explain why, although the number of “parameters” of

the model created by AdaBoust increases with ‘2 at every iteration

i therefore complexity rises i, it usually (100an ovcrﬁt.

***************Ending Page***************

***************Beginning Page***************
***************page number:126**************
12h,
AdaBoost: a sufficient condition for 7-wcak loarnability,
based on the voting margins
CIVIU, 2016 spring. W. Cohen, N. Balcan, HW4, pr. 3.1.4

***************Ending Page***************

***************Beginning Page***************
***************page number:127**************
127.

At CMU, 2015 fall, z, BereJeseph, El Xingl HW4, pr. 245 we eneenntereel
the nntien of einpirieel q'weak learn-ability. When this eenrlitien i ~, ( a, rer

ii I
all t. where q, ‘I’ 2 e, with e, being the weighteel training errer predneed by
the weak hypetliesie ll, i is “ch it etienree that AdaBousL will drive down
the tt-einlhg error qttlehly. However, this cundiLion does net huld all the time.
1n this prehlem we will prove a suﬂicient comiztx'rm rer empirical weak learn’
ehility [te hold]. This conditinn refers to the netien of voting memm which
was presented in CMU, 2016 spring, w. Cnhen, N. Belt-en, l-lw4, pr. 3.3.
Nrnnely, we will pruvu ttnit
if there is a eenstent H > ti sneh that the [vuting] margins of all training
instenees ere lower'boundecl by H at each iteration of the Adchlnst elgerithni,
then the preperty nr empirieel 1-weak leerhehility is --gnerenteeil", with e, e
0/2.

***************Ending Page***************

***************Beginning Page***************
***************page number:128**************
12x,
[Formallsatlon]
Suppose we are given a training set s : ((1.41). .(.|,,,,ym)}, such thnt for
some weak hypotheses ln. v . . . ln rnnn the hypothesis space u. and some non-
neg-ntive eoel-lisients oi . . . . . in with 21;", I 1, there exists H > n such th-nt
l
wlZe,l»,<m) 2 a. vol ti) e s
y:l
Note: ossonling to CMU, 2010 spring, W. Cohen, N. Balcan, HW4, pr. 3,3,
A l
MZHMAMI e Marvin, < n) e ll ( t) where fl (n) “I” X (ti/('1 l)
l:¥ ,e.
Key idea: We will show thst irthe eondition above is satisﬁed (fur a given k),
then for any distribution D over s, there exists a hypothesis m e (h, NJ“)
l e
with weighted training error nt inoet E e 5 over the distribution ut
it will lollow that when the condition above is satisﬁed loi any l, the training
e
set s is empirically a-weak lenrnnble, with s‘ : 3.

***************Ending Page***************

***************Beginning Page***************
***************page number:129**************
12o,
e, Shuw that i if the whtmhm stated above it met i Lhcrc exists u week
hypothesis m from (n, .m} such thet Ewhlwtttm] 2 H.
Hint: Taking expectation under the same distrahhtinh tines hnt t-hhhee the
inequality conditionm
b4 Show that the inequality uwth/W t) z 0 is equivalent to
I f7
Prwn[vt#lh(1v)l< 2 If
mum
meaning that the wexghted training error cf 2,, ‘e at meet ., e 2, and therefore
> 9 ‘
'Wt i 2.

***************Ending Page***************

***************Beginning Page***************
***************page number:130**************
_ 130,
Solutlnn
u. SinuL‘ m2,‘ (t,/t,[m] z H a mum) g u fur , I 1, .m, it follow! (auturding tu tht-
Hint) that
A
Ewnh/JAUJ] 2 H whm Mn] “:" )jww.) (m)
,i.
oh the other side. E~~rv[u,h;(rt)] z e "4 211, mm‘) in“) 2 u.
Suppose, Uh contrary, that EM, humid] < e, that is ZZZ‘ mm) m) < a fur 1 : 1. ,A-t
Then 2:1”me . my)“, < va‘ for r : 1, k. By summing up these inequatinns for
I I 1, .L- we get
A L t t
22%!“(19 uh) m < Zu h, Q Z u,D(/)(Zm<1,w) < ﬂZm Q
M H I,‘ H M I,‘
ZNMJMJM <11)
M
because g§,,:1§a/‘[m“:" XL‘ “mm.
Tho incquaLion (11) can be wriLLcn m, Embmmuﬂ < 9. Obviously. it Conn-adieu, Lhc
relationship (10). Therefore: the previuus Suppusiﬁﬂn i=- false‘ 1h conclusiun, there exist
[Q {1, .1) such that EM, J/Jwtriﬂ > H-

***************Ending Page***************

***************Beginning Page***************
***************page number:131**************
131,
Solution (conﬁd)
b. w¢ already said um EpJ/[T/y/Ayhy] z H a 211‘ 1/, M(r,) 0m g H.
Since 1/, q 1, \ 1) and Mn) <( 1, \ 1) for l I 1.. m andl:1 kwe have
2mm] 0m 25 Q Z 0m Z mm 2 H =» <1 a) I, 2 HQ
l S H l H
why/Q25, show-1 s 534411100111: Y5

***************Ending Page***************

***************Beginning Page***************
***************page number:132**************
with outside threshold without mlLside lhreshnld 132'
Graphs made by ' '
Sebastian Ciobanu n t
\ ‘ t
for CMU‘ 2012 rant a: ‘ K, t ‘
'1‘. Micuhcll, z. Bar-Juauph, i ' t ’ t t
ﬁnal. pr. s.“ n t
[with outside tiimimid]
rm (7MU, 2006 spring, 2 M
cm“ Guestriii, ﬁnal, pl’. s i _ A’
i .
, I
v \
\
\

***************Ending Page***************

***************Beginning Page***************
***************page number:133**************
1.1a,
AdaBoost:
Any set of consistently labelled instances from 1R
is empirically q-weak learnable,
using decision stumps
Liviu (Jim-tun fnllowing
Stanford, 2016 fall, Andrew Ng, John Dnchi, HWZ, pr. (Lube

***************Ending Page***************

***************Beginning Page***************
***************page number:134**************
131,

Ai CMU, 2015, z. Bar-Joseph, E. Xing, HWA, pr‘ 2.5 we encounmred uie

neiien of empirical q-woak loarnahility. When this condition i ~, g 1i for all

/_ where 1,, ‘I 5 e r,‘ with e, being ilie weighted ireining error produced by

the weak hypeihesis ii, i is mel, ii Ensures that AdaBoost will drive down

the training errnr quickly.

In iliis problem we will assume that our inpni niirilniie veciers i e 1R‘ mini is,

they are one-dimensional, and We will show that [LC] when iliese vectors are
consimntly labelled, decision SLulnpS based on Lhresholding pruvidc a weak-

learning gnmniee (1).

***************Ending Page***************

***************Beginning Page***************
***************page number:135**************
135.
Decision stumps: analytical deﬁnitions / formalization
Thresholding-hased derision stumps een he seen as functions indexed by e
threshold ll and sign we, such than
l il'st i *1 ifIZx

“*m ’{ fl iIJ < e “'“l W4‘) ’{ l 1L: < >1

Therefore, m‘ +(r) I 4mm‘
Key idea for the proof
We will show that given a consistently lnhellerl training set s I
will). .uvnymly with i, e 1R and y, e {*1.+l} for I : l. .nl, there
is some ~, > 0 such than for any distribution 11 deﬁned on this lreining set
llrere i5 a threshold e r lR for which
l 1
l-rnmlm +) g 2 i ~, or (‘TYYWVUMri S 2 r '1.

where ermine“ +) and MVWALL ,) denote tile weiglned training error of (Em +
and rerpeelively e, e. computed according m lire distribution p.
We will start hy analysing "WNW +J and mmillmr).

***************Ending Page***************

***************Beginning Page***************
***************page number:136**************
1m,
Cnnvention: In Olll' nmhleni we will assume tlint our training instances
i, l,“ E IR are distinrn Moreover. we will eeeuiue (without lees of gen-
erelity but this makes the proof notationally simpler) tliet
i. < .i!< < in
a. Shaw tlnit, given s, fur any distribution p deﬁned on this training eet, fur
each mu € (0.1 Hm): and
for any - I [rm Mm) in case mu 3 l, and respectively for any - I (Ix. H) in
case mt, : 0v
it follows tliet
“7"”,le I erllnw..<nll:§+§ 2mm I Z mu
lei lei emu
_,—/
and .nt ,(Hm,
Minute] I Zr, menu.» I 2 I 2 21m I Z 11ml I 2 I 2mm"),
,Ii ,Ii ,Imei
Notc: Treat sums over empty sets of indices as new Therefuie, 21L‘ a‘ I n
fur any In, and similarly 2;,“ ‘ l Ill I n,

***************Ending Page***************

***************Beginning Page***************
***************page number:137**************
137.
Comment
After having determined that
1 1 1 1
(new), 1) E seamen) and minim, J E i 51m“).
v _ v _ 1 1
we Win look at the inequalities 117701y<wei> < 2 is‘ and (in/mm e) ( 2 is‘.
1
The ﬁrst inequality gives 5 {(11111) < i», a {(7/10) < in, while the seennd one
1 ,
gives 7mm) g s» Q 1W“) 2 2').
Putting them tegether, it suggests us te study whether given the tint-eset
s tiieie i> '1 > n such that fur nny distribution: n en s wc can ﬁnd a value
nu, e {111 . .|Y|)S\1Ch as 1][mn)\> 2-,.

***************Ending Page***************

***************Beginning Page***************
***************page number:138**************
138,
b. Prove that, given s, fur any set of probabilities 7, on the training set
(therefore I!‘ 3 0 and 21;, p, 1) we ran ﬁnd 1m, e (u. .m} so as
iftmun e 5 where mm a Zuu a Z up‘
‘er ‘enter
Hint: Evaluate the expression Wm") e [(mu e 1) .
Note: Based on the properties [to be] proved at parts t and 11, it renews
rv l
inuueriiateiy that q 'I' T ean he taken as a guurnutee for the. empirirlll Im'ek
.m
lumnaln'lity of the data-set S. using AduBoust with dccieiun stumps.
1.‘. Can yuu give uu upper buunLl uu the number uf Hireahultlcd decision
stumps required te achieve zero error on a given training set?

***************Ending Page***************

***************Beginning Page***************
***************page number:139**************
13!‘),
Solution
a. We perform several algebraic steps.
Let wquﬂ) I I if t 3 u, and ~xr/n(1) I fl otherwise.
Then
llweumn I 1<WWH w”) I 1w owl WSW)’
Where the symbol L‘ , denotes the well known indicator fmlctwn.
Thus we have
e t
><l .. anlsl 5 anlslel . Km
leuﬂm r; s 211‘ IWH ,m, s21), lb,‘ tutored,» s Z 11, wheel) + Z p, M9,,
‘:1 ,er ,. <. ‘.24
Because 1, g ~ lor l g In“ and l, > ,~ fur l > W we have
twptotl) : £11, 1t” ml, — X, Ah lll/f l»
rel emu

***************Ending Page***************


***************Beginning Page***************
***************page number:140**************
1.10,
Solution (conﬂd)
Nuw wc “11.1w a m, obxurvutivn:
wu have
1 + y 1 i 1/
1111,41’ T 111111 111/7,“ i T‘
buuuuau 1/ e(*1.1).
Consequently,
1 +111 1 , V,
111111111111.” Z 2,1,1? 1 Z ,,, T
1,1 P1111111
: 2211+ 221m i 1, Z 1111/1
v,1 1,1 ~ 11,111 11
1 1 1 1
, 5+ 4211112111111)’ 5+ 511111111
The 11151 1,111 one equality folluws beuuu>e 2111,11 Z 11
T110 1:ch for o, , 1> Bylnlnutric w 111111 unL'. :u wl: 011111 Lhc argument.

***************Ending Page***************

***************Beginning Page***************
***************page number:141**************
1 1.11,
Solutlon (conﬁd)
1n Fur any "n, r, (1,,...m) we have
mm , Hm“ , 11 Z1111‘ , Z w, , Z (M', + Z 1/11» WWW
"I Mn,“ H m
Therefore, 1/1”“,1411111411: MUHAMW:21“ for n11 ~11, e (1~ m)-
l
Bccausc XI! 11, : 1, there Inuat bu at 10m unc index mg, with pm‘; z i.
m
Thus we have
1 / 2 ,
Kw) Mm 1H 2 m (ll)

and SO it llluSL b0 the £650 that at least one 0f

/.n')1\- l nr fm' 1) > i (rs)

<1 1m <1, ,7” >
holds (scc proofun the next slide)
Depending on which one nnhnse two inequatinns in (1x! is true, we would then "return“

1
m; or 1,4,n1. (Nate: If 1/ [1%le z i and "q, I 1‘ men we have L0 consider an “011151110”
W

threshold, N < ‘1-)

***************Ending Page***************

***************Beginning Page***************
***************page number:142**************
1.12.
Proof of (13)
Rndlchio all absurdum:
1 1
Suppusc l/lwfil < i AND 1W1, i lll < i. This is equivalent lu say thuL
in in
l , , l
if ‘ < i l4
m < Mill m < >
AND 1 1
W n , ' i l i.
H! Q j( “U )Q m
le lulu double inequality is equivalent w
l 1 ,
*’Q*f[l!|;,il)<i (15)
w in
By summing up the double inequalities (14; and (15). it follows that
2 2
,in 'i "l <<
m [("M mi, > W
or otherwise said, ‘ﬂu/m i will) i I)‘ < F whlch Contradlvts (12)!

***************Ending Page***************

***************Beginning Page***************
***************page number:143**************
1.1.x.
Summing up
At each itcl-etion I executed by AdaBuust,
o a probabilistic distribution p (ocnotcd as u, in cmu, 2015‘ zt Bar-
.ioscpn, E. Xings "W4, pr. 2.1»5) ls in use;
s let pert I: of the present exercise we proved lhntl
there is at least one 1m, (better denoted now/)1 in l0. . .m} snch that
‘ i- l s i 1i i i l
l/(nml z m Wicrc /(VVH)) gm ‘i;*‘l/[l
e [tlle preoi made nt pert it of the present exercise irnplies that]
ior any ~ e [rm 1mm) ii in‘, 2 l, and respectively for any s e (eec. |,) if
Hill : 0,
l 1 l W, l
l’Y‘HlV)l[C", l l g 5 e1, or ﬁlmywr )5 5 es where 1, 5.
As a consequence, AdsRoost can choose st eeen iterntion a weak hypothesis
l
1,; h'l'l'l >~,:i.
(l) nrw n: y, 2m

***************Ending Page***************

***************Beginning Page***************
***************page number:144**************
1-1-1,
Solution (conﬁd)
1
c. Boosting takes at most % ismiinns w achieve zern [training] error, as
shown at CMU, 2015, z. Banlansnnh, E. Xing_ HWA, pr. 2.5, sn wiih decisinn
sinnins wn will avhieve zero [training] error in at ninst 21mm” iinnniinns of
boosting.
Earh iteration 0f hnosting introduces a single nnw weak hypothesis. sn at
innsi 2mm m Lhresholded decision suimps are necessary.

***************Ending Page***************

***************Beginning Page***************
***************page number:145**************
11'»,
A generalized version of the AdaBoost algorithm
MIT7 2003 fall, Tommy Jaakkola, HW4, pr. 241-3

***************Ending Page***************

***************Beginning Page***************
***************page number:146**************
1'1“,

Here we derive a boosting nigimthm from a slightly more geneml perspective
than thc AdaBoost algorithm in CMU‘ 2015 fall‘ z. Bar-Joseph, E4 Xing,
nw4. pr, 2.1-5, that will be applicable [or a class 0/1058 functions including
the expunentiul one.
The ganl is to generate discriminant functions of the rorrn

ml) rilll(|.01]+v inn/mm.
where both .1 belong to 1R", u arc paramctcrs, and you can assume that the
weak claasiﬁers m r, a) are decision stumps whose predictions are i1: any other
set or weak learners would be ﬁne withent modiﬁcatinn.
Wc successively add components to the ovcrall discriminant function in a
nnrnncr that will scparutc the cathnntion uf [uie punurmtcv'a of] the wen/c
clnaeij-ici-e fruln tho cciting uf the votes n to the cxtcnt possible,

***************Ending Page***************

***************Beginning Page***************
***************page number:147**************
l l 1.17.
A useful deﬁnltlon
Lot’: start by deﬁning a 50L of useful Loss functwma. The only mmelwn we place 0n
the loss ls that it should be a monatomcally decreasing and dlﬁemnhable function of us
urgunlan The asgnnlenl ln our cuntuxt is in j, (1,) so that the mun: the discriminant
function agrees with the i1 label l/h the smaller the loss.
The simple Pzponenticll lass we have already considered [at emu 2:115 fall, z. Ban
Joseph E. Xing_ l-lw4, pr. 2.1-5]_ l.e.,
Lwﬂl/JN-IJJ sl‘><l)1*l/,lv'("l))
certainly conforms to this notion.
l
4n s‘ mil i
as “5&3, ‘ n a
a
And sa does the leglalls lass 25
2
Lmﬂv, ml,» ‘ 5
I IHU amen/Tull» 1 ,ﬂg
4-3-2401235

***************Ending Page***************

***************Beginning Page***************
***************page number:148**************
1-18,
Remark
Note that the lneistic lnss has a nire interpretatinn as a negative. log-
nrnhahility. Indeed, [recall that] for an additive lngistic Mgrﬂssin'n Inﬁdel
l
lllP[l/:1‘l‘,m]: 1111 > mm I) :l\l(l l (:xp( 2))

where z Mimi I 1+ +ll'rt'11(1) and we cinit the bias ternt (my) for simplicity.
By rcplncing tht- ntlrlitivc culnbinatiun of basis functions mm) with the
combination of weak classiﬁers (mi +1»), we have an additive logistic regression
mode! where the weak classiﬁers serve as the basis functinns. The difference is
that both the basis functions (weak classiﬁers) and the memcients multiplying
them will be estimated. ln tlic logistic regression model we typically envision
a ﬁxed set of‘ basis functions.

***************Ending Page***************

***************Beginning Page***************
***************page number:149**************
14"),

Let us now try to derive the boosting algorithm in a manner thnt enn neee-
modate any loss [enetwn er the type discussed abovet in) this end, at the
element iteration (t) we suppose that we have already included re1 component
classiﬁers

[Mir] e mli<eml + .,.+<neili(r,iei)s (1h)
and we wish to add another h[r H). The entimntion cn'tt'nzm for the overall
discriminant. functiom including the new eempnnent with votes n, is given by

Mn a) e E;Lo~\(y‘/,e|(r,] +y,l)7|(1,.ﬂ))
Nete that we explicate only how the objective depends on the i-heiee of the
last component and the corresponding votes since the parameters of the I e 1
previoue culnponents alung with their votes have already been set and wun’t
be mudiﬁecl further.

***************Ending Page***************

***************Beginning Page***************
***************page number:150**************
150.
We will first try Lo ﬁnd the new component or parametere 0 so as to maximize
its putential in reducing the empirieal lass, patential in the aenae that we ean
euhsequently adjust the votes to aetually reduee the empirical less. Mere
preeisely, we set 9 so as tn minimize the derivative
t7 l m i7
mm" miter, I 5; ﬁllies-(allele) + i/aihirmnte"

R glam/l in» , l/lu Mum) mun] "e0

‘ﬁlm I < >1 !( w '1')

m H i/ i i 1 i

0L0»[:]
h u : J i.
w ore ( 1 02
0

Note that this derivative Etliilimine" precisely eeptures the amnunt hy
which we would etart te reduce the empirical less if we gradually inereaseel
the vute (a) ier the new component with parameters a. Ivlinimizing this re
dllcﬁnn seems like a sensible esiimatinn rriterinn fnr ‘he. new nompnnem nr
ll. This plan permits us to ﬁrst set 0 and then subsequently uptiiniae a to
actually minimize the elllpirica] loss‘

***************Ending Page***************

***************Beginning Page***************
***************page number:151**************
151.

Laﬁs rewrite me algnrﬂhm slighily to make ii lnnk more like a boosting ul-
gm-imm First, let's deﬁne the following weights nnd normalized weights on
the training examples:

111w Z (iL(|/,],,,(r,)), icr and

V ‘ 1W"

ii'j' : ﬁ fni- 1:1 .1”

2/’! "J

Tiicic weights an! guaranteed w bu llUIkKltguLiVL‘ since the loss function i; a
decreasing function of its argument (its derivative has m be negative or zero).

***************Ending Page***************

***************Beginning Page***************
***************page number:152**************
152,
Nuw we can rewrite Lhc cxprussiun (17) as
v 1 “l l»
mm“ em, : i; FL l, will
'4.‘
1 (Z 7m "
: W H ) Z+A/,Il(l,‘9)
i ,lll Y
"' l .4 21 ‘b
l ‘w w)
, “in, ) g", will”)
By Ignoring mu Inultlplicatwc BUIIECHHL (Lu, i 2/ ll y ', which i, constant at
m
iteration l)_ we will estimate 11 by minimizing
filigmyllllipn) (m)
‘:1
where the normalized weights 111"‘ sum to 1. (This is the same as maximizing
the weighted agrcblllclll with Lhe labels, Lu, ill‘ li'f'wmm, 0).)

***************Ending Page***************

***************Beginning Page***************
***************page number:153**************
Some remarks (by lelu Clortuz) ‘
1. Using some familiar notations. we Ear. write
Zli',"‘,l/m(i, H) I Elijml/JIMVH) l Zlivful/JYUV, H)
iii yen Mi
I Eli?” 2111"’ :1 2:,
l4‘ leu
“fl ME
a r, I 2(1 le1"‘r,li(i, ell)
,:i
2. Because Hy") 3 0 and ill‘ lif“ i l, it follows that if; lifhlmi, a) : H4“
ilrerefore
l Quill?“ 2‘(>\l¥i',\"):l Zli;“‘l/,ll(r,-€,)<: I), l2], and so
,il
W
Hil Ml
i (l ,2?‘ “Jul/11K!‘ m) a L) +ll.

***************Ending Page***************

***************Beginning Page***************
***************page number:154**************
1T1,
We are now ready tn ‘Est the steps er the boosting algorithm in a form similer m the ‘
AdaBoost algnrithm given at CMUT 2015 fell, z. BarrJDseph. E. Xing, Hw4, pr. 2.175.
s 1
[Assume W)“ : e and fl;(l1,):\1 for l :1 . m]
m

Step 1: Find any classiﬁer m 0]) that performs better iliaii chance willi respect v.0 me
weigliied lr-eiriirig crrur:

e 1 l fr”) / D '1‘)

He; r2 limit.» t.)

‘ vil
Step 2: Set me voles (ll for ilie new eempeiiem by minimizing the overall empirical
loss:
mes» I e 21,0»(y, [Hm + yi(\!l('|'l-Hl>)» and so ii, : merrier/mil. 9,1
m he
,el
Ship 3; Ruculnputc the nunnulizud weigliis fur ilie next iteration according w
li',"“> : er, liL(l,,/,el(r,) + .1, a, mm») for , :1 . m, (2m
_,_

where <», is eliesen sci that 2,”; lij'm I 1.

***************Ending Page***************

***************Beginning Page***************
***************page number:155**************
155,
One more remark (by Livin Ciortuz),
now concerning Step 1:
Normally there shuuld be such s, <1 m, 1/21 (in fact: some corresponding 8,),
because if for some ll wn would have a, a (1/2 l), then we snn lnkr. w : ih,
and the resulting 5; would belong to (0.1/2).
The are only two exceptions. which correspond m the case when lur any
hypothesis h we would have
i either 5, i l/zl in which case 2,5‘, Hf” i 2M, Hf”
i or 51 E m ll_ in which case eiﬂler h nr ll’ ill is a perfect (therefore not
wwk) classiﬁer for the given u-nlulug unln.

***************Ending Page***************

***************Beginning Page***************
***************page number:156**************
Exemplifying Step 1 on data from 15“
CMU, 2012 fall, T. Mitchell, zl Bar-Joseph, ﬁnal, prl sla-e
[graphs made by MSc student Sebastian Ciobanu, Fll, 2015 [all]
Iteration 1
z n a‘ H
; a ;‘ a
é 2*
g a‘ ‘ ‘ ‘ g ?
7w 4 o 5 w 40 '5 u 5 w
M. M

***************Ending Page***************

***************Beginning Page***************
***************page number:157**************
157,
[graphs made by MSG student Sebastian Ciobunu, F11, 2013 fall]
Iteration 2
5 2‘
*5? e é i a
i ° If a‘ “ —\
i 2 ‘ ‘ é : ‘
7W '5 n 5 m 4n '5 n 5 m
M M

***************Ending Page***************

***************Beginning Page***************
***************page number:158**************
153,
[graphs made by MSG student Sebastian Ciobunu, F11, 2013 fall]
Iteration 3
z " F g‘ N
i ‘ é i =
a ‘ ‘ a u
U a L

***************Ending Page***************

***************Beginning Page***************
***************page number:159**************
15"),
e. Show thnt the three steps in the elgorithrn correspond exnetly to AdnBoost
when the loss innetion is the exponential less Luss(z) e exp(*;].
Mere precisely, show that in this case the setting of m based on the new
weak classiﬁer and the weight npolnte to get 63"“) would be identical to
AdaBoost. (In CMU, 2015 roll, z. Bar-Joseph. E. Xing, HWAt pr. 2.1-5,
WW" corresponds te Dist (or)
Solution

For the ﬁrst pert, we will show that the inininiizntion in Step 2 of the general
algorithm (LI-IS below), with I.ti>~(:) i F", is the same as the, minimization
performed by Adeuoost (RllS below), i.e. that

- v ' ”‘ ~\ i‘ I '1 ’ w "' 7'”) l Pt

"‘bfgﬂZVJ-lohull]! it 0+ ll/M i-M) “*RE'AZI'“ wilt ‘(/l"[l¢.i7l))-
with (from Adsuoost)

Weller onion/rerun»

where lie. is a nurmalizixtiun eenst-nnt (weights snrn to 1).

***************Ending Page***************

***************Beginning Page***************
***************page number:160**************
Solution (with) 160'
Evaluating the nhjertiva in LHS gives:
Ewen/,1, .<|,1+~i,,h<r,.ﬂ11>
e,
e Emmett/Pint») tin,(em/,r.[t(.un) '2” fin,“‘vxp(enq,mt, 0,);
,e| (‘"1 I’!
e n' / <~ u ‘ " .
“J 2 (,y + Z, (‘H
wilziymihzmﬂi ichiymlhuik ti
which is proportional to the objective minimized by AdaBoost (see CMU,
zoos full. Eric Xing, HWB, pl‘. 4.1.1). Therefore, niinilnizing w.r.t the value
of a is the same for both algorithms.
For the second part, note that the weight assignment in Step a of the general
algnriihm (for stage 1) is
W“ e *rt (mu/mm ,1, \‘Xi’[*(/»[1(It>)»
which is the same as in AdaBoost (soc ClVlU, 2015 fallv Z4 Bar-Joseph‘ E.
Xing, HWA, pi. 2.2).

***************Ending Page***************

***************Beginning Page***************
***************page number:161**************
161,
b. shew that for any valid less funetien erthe type discussed eheve, the new eernpenent
mi» 0,) just added at the i-th iteration would have weighted training error exactly 1/2
relative tu the updated weights WY“).
Solution g
. . . . ~ _ UM" 0,)
At etage i, m is chosen ta minimize ./,(u til), he. to solve ‘a I it. In general.
in
0 _ l t) e
(Muted; e m g ("0L(i»(Ui_/rel(fv] t l/tlllm mm»
I m ZtlLU/lf/rlui) ~ '/i"7l(rr-Hi))J/JY(-ln it) a Z", time it)
lei‘— MM, P‘
7 I‘!

so that we must have li,"*“yl/i(.i, 0]) , 0 Then‘ the weighted training error lel- mum)
(relative td the updated weights ll ,”*“ determined by in) can be edniputed in a similarly
way te (wit

l atrial, » l 1

e e 1 e e e

2(1 g“, h, mi, m1) 2(1 0) 2

_,_,
n

***************Ending Page***************

***************Beginning Page***************
***************page number:162**************
EMU, 2mm fuzz, Eric King, HWH, pr. 4.1.1 162,
CMU, 2005 fall, Em Xihg, midtmn. pr. 5.1
ci Now, suppose that we change the objective function to J, I 21m,‘ e Mn) )1 and we
still wnnt tn optimize it sequentially." Whnt is the new update rule for m’?
Solution
We will conlputc the duiivutivt- uf J, : 212M‘ i Mm)" Wit. it, nud set it to ZCI'O tu
ﬁnd tht- vuluc of m,
01/1 62m ,(y, i M-IJF 60/, *M'J)
i I iii : , s . if
0», on, 24”‘ Mm) a",
We also know thet
[VIM] It i[|i)+m!n[h)
In this equetien, t l is independent ofm. Substituting this in the derivative equntion,
we get
0.1, out s Jteiirqenmi1t>>
R e 22w, inMT * 2;11/r/1<~'J1(*!ulu))
t LC Nuiv tiut (Hi *MJJ): :[y‘\1’Vv,!(Ii]J‘1 : [iii/mu» : 4171,11 whmv z, :1], mm 'nu» [uni-Linn
(v :11 .Mhnmhh ind (nun-n u is iiminnngmq x,“ ,nid inniueuwn ii Kin)

***************Ending Page***************

***************Beginning Page***************
***************page number:163**************
163,
Solution (cont‘d)
Setting the derivative m zero, we get
:74 I 0 I gm I nun/mm I rm 2w, Iwwm I I, \<.|,nm<~) I 0

2w. I/,,.<.|,11/~,<.|,1 I "/Zhﬂm Wm I M

,7‘ ,i, 24w»
‘WK

Im I 7,, gm I nemnwy)

***************Ending Page***************

***************Beginning Page***************
***************page number:164**************
m4.
MIT‘ 2006 fen, Tommi Jaukkala, um, pm a.“
MIT, 2009 Ian, Tomvm Jauklcala, uws, pm 2.1
d4 Show um if we use the logistic 1m instead [of the exponential loss] me
unnormalized weights "1”" are buunded by 1.
Solution
MAM . My d H
n, was deﬁned as *11L(y,f,(1,)),wlihr".[:] : Fonu w q). Therefore.
‘he l 1 e
n, ‘Y e T e i <1 where :, *JIVM.”

***************Ending Page***************

***************Beginning Page***************
***************page number:165**************
et When using the logistic less, whet are the normalized weights, WY“)? Express the 16*"
weights as a lenetien er the agreements will‘), where we have already included the
ch weak learner.
Wlmt can you say eheet the resulting nurmalized weights fur exampleii tlmt are clearly
misclassiﬁecl in cumparison tn these that ere jest slightly misclassiﬁecl by the enrrent
ensemble’!
If the training elete contains mislabeled examples, why do we prefer the logistic loss
over the exponential less, Less-(s) e exples)?
Solution
The normalized weights are given by ‘if’ ‘Y I t», “Xpri'l' Mr») with the normalizae
l lrxvl IMAM)
g i i
tien constant 1, : (211‘ “pr with,» ) ,
i l l exh( l/tfllriD
[Answer from MIT, 2:111 fell, Leslie P. Knelhling, l-lws, pr. 1.1]
For clearly miselsssiﬁeel examples, l,‘ M it) is a large negetive, 50 wk" is elese te [and
less then] l, while for slightly miselessiferl examples_ "'le is elhse ln [nnrl greater than]
l/z Thus, the normalized weights ier the twe respective cases will be in a retie of at
mest 2 v ls itc. a single elesrly misclassifcd entlier will never be worth more than twe
eempletely nneertsiin peints. This is why heusting with legistie less limetien is rehnst
to Outliers.

***************Ending Page***************

***************Beginning Page***************
***************page number:166**************
166,
Solution (cont‘d) in Romanian

LC: Penu'u ultima part2 de la punctul w nu am gash deloc raspuns la Mn:

insa pm géndi astfel:

in razul funcﬁei de piPrdere logistice, dacé avem un I, care BI‘ avea de drept

eticheua 1,, w, dar se considerﬁ (in mod eronat) v, *1, pierderea we de
apl'oxhnativ mm dchK 1m.) > u.” in vrcmc cc in cazul runqzei do pienlcre

[negativ] expunengiale pierderea este mp(h(1,)) cure ESLE in general mun mai

mare decin Mm» Cuzurile simetrice (MM) g n Q; upoi 1/, Z 1 , .1) 52

trateazi in mod similar.

***************Ending Page***************

***************Beginning Page***************
***************page number:167**************
1m.
L. Cim'tuz. 2020
r. Suppose we use logistic loss. What is the update rule for 0,?
Solution (in Romanian); initially written by Stefan Matcovici (MSC student)
in Inc 55 minimizam J, in “pm s“ argumentul (L vnm inrerra 55 minimizsm
in mpm cu I» o mnmlne mysv'ioard psmm difemuta dinms J, si 1,1‘, Unde
J; l :“':" null],s§¢<v<:‘ 1,40» Q). (Spit Llcuscbirc us .1, curl: dupinllc 1.1!: <2, J,’ ‘
nu depinde d2 1v. Prin “mass, a minimiza J, in raport cu (v este echivalent
4'" a minimiza '1, i 1,1‘ in mp0" m H.)

***************Ending Page***************

***************Beginning Page***************
***************page number:168**************
168,
J, 1p.:;ln(1mp< MM») gm m1 Uva71(lv])):;m‘Lt:;l()i:/Il}£€?li)))

I i1" 1+ ex #wmﬁ)’ qwjtm+ expw MI»)

,7, HexvtiuvrP-Lm)
, .. “PPM/Vin»HWPPy-JMMJJ
’ 2‘ (H 1*“P(*%/:4("~J) )
, “vwkfwrm , wmm) , Mmiwmvm
’ 2M” \HwW/HWJ )
i “pm/,1,’\(i-awwiynhm) , cwillmm)
* 21'4"’ “vxpwl/Pwmg) )
i n exppw, \(I,))[0‘<p(*y.uh[z.))il]
’ Z,‘ (H wwww m») )
i cxp<fylnh(z,])il i 6x1)(*//,nh(r,))il
, 21..(1+1i4) igln<l+W> (21>

“WM; (7,»

g Z W mum |||(l +1) g z \1; > ,1 (22)

***************Ending Page***************

***************Beginning Page***************
***************page number:169**************
1G‘),
Remarcagi faptul CE lugaritmnl din expresia [211 existi, intrucét
"YPFl/yu/XLIJ) iv
‘ > l Q (‘XD( M017! r.) L> UXD Ulyil 4,) l
thm/HWHY < > (’ ( >
<> “KPPWJ'MD > *K-Xprwmuyu
W _,_
inrgalitatl‘ ﬂdvviratﬁ pentru V1»
Expresia (22) De care Locmai am obginut'o mai SUE esLe mammcﬂ mpmoam (engL,
“ppm hound) pmm: /, i 1,1, pl‘ can‘, n Wm minimizu 'l'n rapt)", ru 111.

***************Ending Page***************

***************Beginning Page***************
***************page number:170**************
170.
Un (‘alcul Simplu no arat-i C5
d V 1‘ i l 1
i||.[1+<')iii,iiiﬂ,i <21)
(1V l +1 ‘ % + ‘ 1 ‘ + l
sum din mum c5 w)" : (1140/, MW). Conform rclaﬂci (21;, rnzultﬁ ca
l 1
“41/ , , , ,
' < me/ij mm+1> mm], “n+1
iar
4 \
u "' Z 1,7, i,
‘ "wu/yfmvhl) +1
“nan ‘,7. we cunmma do normalizarr a pnndcrilnr M"

***************Ending Page***************

***************Beginning Page***************
***************page number:171**************
171,
Prin urmara
J, , 1,1. s £%(w<wwrm>i 1)
,leww'j' ‘(0H1
i \ ~ U, v ‘ ‘
i W Zn, (owl Wm“); 1;
a 2W" uwkymu,»izﬁy"
,4 ,4 l
A§adan a minimiza marginea superioarii pentru A 1,1‘ ravine la a minimiza (in
raport cu Q) Bxpresia 21;‘ xi,” vxy>( mun»). pe care am imam“; 5i la upti'
mizarna rostului [nrgaﬁv] cxpnnnnﬁul‘ Pun-m mmhid“ ra qi in razul ﬁlncﬁni a“
cost logiscice putem alege a, i 2mg.

***************Ending Page***************

***************Beginning Page***************
***************page number:172**************
172,

MIT, 2mm‘ fnzz, Tommi Junk/cola, HW4, [77‘. m
g. Suppose again llml we use logistic los> and the training set is linearly
separable. We would like to nss a linear support VECLO!‘ machine (no slack
penalties) as a lmss classiﬁer [Lo nr nny linear separator consitent with the
training dam]. Assuma ilnn ilns generalized AdaBnnsl algnl'ilhm minimizes
the s‘ Weighuad error at Step L ln the ﬁisi boosting iteratiom What would
the resulting n‘ be?

***************Ending Page***************

***************Beginning Page***************
***************page number:173**************
17.x.
Solution
hi Step i, we pick [1,. We wish te ﬁnd 1/, te minimize WWW
e
Equivalently, this m is chosen to minimize the weighted sum: 2:. 1 :
s , s l
n22, lv,“‘H/,n(i,,,li), where WP“ I i rec nil t’ I 1,2“ .w. 1f the training
m
set is linearly seperehle with offsets then the no-slack svm problem is ieesi-
hle, Hence. the base elnssilier in this ense will thuzs he an aﬂlnc (linear with
el-rset) sepnmter may), whieh sntislies the ineqmility Mil, 9,) 2 l fur nll
1 I 1,2“ m.
l
In Step z. we pick e. to minimize Mm a‘) : e EL union,‘Jew/‘litip/m) I
m
l
m 2;!‘ L(mi/,7|\ rpm). Nete thnt mm e.) is a snm uf terms tlmt ere strietly
m
decreasing in m (as Mil ,.0,) 3 l); therefore‘ it itseliis else strictly decreasing
in mi lt follows thet the heesting slgerithm with logistic loss will take it, ee
in order te minimize men/h),
This mshes sense becnuse if we can ﬁnd n hese clessiﬁer that perfectly sepa-
rntes the dete, we will weight it es miieh es we ran te minimize the boosting
loss. The lessen here is simple: when doing heesting, we need te use base
ehissiiiers that ere not puwcrful enengh te perfeetly sepsmte the shits.

***************Ending Page***************

