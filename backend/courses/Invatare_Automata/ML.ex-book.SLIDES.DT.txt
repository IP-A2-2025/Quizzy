***************Beginning Page***************
***************page number:1**************
1.
Decision Trees
Contains:
ID3 algo.: eX. 34, 36, 2, 4, 12, 10, 47, 15, 19, 20,
AdaBoost algo.: ex. 22, 23, 24, 25, 26, 27, 2s, 70, 29
from the 2023f version of the ML exercise book by L. Ciortuz et a1.

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2.
Exemplifying
how to compute information gains
and how to work with decision stumps
CMU, 2013 fall, W. Cohen E. Xing, Sample questions, pr. 4

***************Ending Page***************


***************Beginning Page***************
***************page number:3**************
3.

Timmy wants to know how to do well for ML exam. He collects those old
statistics and decides to use decision trees to get his model. He now gets 9
data points, and two features: “whether stay up late before exam” (S) and
“whether attending all the classes” (A). We already know the statistics is as
below:

Set(all) I [5+,4—]

Set(S+) I [3+, 2-], Set(S—) I [2+,2-]

Set(A-l—) I [5+,1—],Set(A—) I [O+,3—]
Suppose we are going to use the feature to gain most information at ﬁrst
split, which feature should we choose? How much is the information gain‘?
You may use the following approximations:

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
4.
[5+,4—] [5+,4—]
H=1 H=O
no . no . 5 Sim. 4 def. 5 9 4 9 5 4
H(all) :t H[5+,4—] :t H (5) : H <5) : §log2 5 + 510g2 Z :10g29 — 510g25 — 510g24
5 8 8 5
def. 5 4
H(all|S) : — -H[3+,2—]+—-H[2+,2—] z... def.
g 4 9 IG(all, 8) z Hm”) — H(all|S) I 0.007215
I 5 - (1970951 + 5 '1 I 0983861 IG(all, A) déf' H(all) _ 151071104) I 0.557728
H0004) déf' 3105+, 1_] + g -H[0+, 3—] I . .. IG(all, 8) < 10001.14) <5 15107108) > 1510004)
4
: g - 0.650022 + 5 - 0 : 0.433348

***************Ending Page***************


***************Beginning Page***************
***************page number:5**************
5.
Decision stumps;
entropy, mean conditional entropy, and information gain:
some very convenient formulas to be used
when working with pocket calculators
Sebastian Ciobanu, Liviu Ciortuz, 2017

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
6.
Consider the decision stump given in the nearby image. [a+,b—]
The symbols a,b,c,d,e and f represent counts computed
from a training dataset (not provided). As you see, the
label (or, output variable), here denoted by Y, is binary, O 1
and so is the attribute (or, input variable) A. Obviously,
a I c + e and b I d + f. [9411-] le+,f—l
a. Prove that the entropy of [the output variable] corresponding to the par-
tition associated to the test node in this decision stump is
1 (a —|— b)“+b
Ha ,b— I —lo —ifa Oandb O.
b. Derive a similar formula for the entropy, for the case When the output
variable has three values, and the partition associated to the test node in the
decision stump would be [a+, b—, 0*].
(Note that there is no link between the last c and the c count in the above
decision stump.)

***************Ending Page***************


***************Beginning Page***************
***************page number:7**************
7.
c. Assume that for the above given decision stump we would have [all counts]
c,d, e and f different from 0. Prove that the mean conditional entropy corre-
sponding to this decision stump is
1 c+dc+d e-l-feJFf
Hnode|attribute : —1Og2 % ' % '
a + b ccd 66 f
d. Now suppose that one of the counts c,d,e and f is 0; for example, let’s
consider c : 0. Infer the formula for the mean conditional entropy in this
case.
e. Prove the following formula for the information gain corresponding t0
the above given decision stump, assuming that all a, b, c, d, e and f are strictly
positive:
1 a + b ‘1+1’ ccdd eeff
[Gnodemttm'bute I —10g2 % ' —d ' — .
a + b aab (c —|— d)c+ (e + f)€+f

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
8.

WARNING!
A serious problem when using the above formulas on a pocket calculator is
the fact that the internal capacity of representation for intermediate results
can be overflown.
For example, a pocket calculator Sharp EL-531VH can represent the number
5656 but not 5757. Similarly, the calculator made available by the Linux Mint
operating system [see the Accessories menu] can represent 179179 but not
180180.
In such overﬂow cases, you should use the basic / general formulas for en-
tropies and the information gain, because they make a better use of the log
function.

***************Ending Page***************


***************Beginning Page***************
***************page number:9**************
9.
Answer
a.
a a b b
H ,b—:—(—-l — —-1 —)
[a+ 1 a+b Og2a+b+a+b Og2a+b
1 a b 1 a a b
I —— -1 — —>:——<1 <—> 1 <—b>
a+b(a Og2a+b+ Og2a+b a+b 0g2 a+b +0g2 a+b>
1 (1 (La +1 bb ) 1 1 aa-bb
I —— 0 — 0 — :——-0 —
a+b g2(a+b)a g2(a+b)b a+b g2(a+b)a+b
1 (a+b)a+b
I —-1 —.
a-l-b 0g2 aa-bb
b.
H[+b*](“10 a+b10 b+610 C)
a _C :_—. — —. — —. —
’ ’ a+b+c g2a+b+c a+b+c g2a+b+c a+b+c g2a+b+c
— ——1 (10 (—a )aHO (—b W10 (—C )6)
_ a+b+c g2 a+b+c g2 a+b+c g2 a+b+c
1 (1 aa +1 bb +1 cc >
I —— O — O — O —
a+b+c g2(a+b+c)a g2(a+b+c)b g2(a+b+c)c
1 aa-bb-cC 1 (a+b+c)a+b+C
a+b+c (c1,—|—b—|—c)a+b+c a+b+c aa-bb-cC

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
c. 10.
d6. C+d €+f
Hn0d|atribut :f a—+b ' H[C+7 d_] + a—+b ' H[€—|—, f—]
_ a-l-b M g2 CC-dd (1+5 M Ge'ff
_ 1 1 ((c+ d)c+d . (6+ f)e+f)
_ a-l-b'Og2 cC-dd 66-ff '
d. +f
e
Hn0d|atribut I m ' H[€+v f_]
6+f ( 6 6+f f 6+f
a+b e+f 0g2 e e+f g2 f)
1 6+ e+f
I g'QPngTfﬁ'logQT)
_ L105; w.
— a-l-b 2 66-ff
e.
[Gnodmtm'but I a—+b'10g2w—a—+b' Og2 (W 66,fo
1 1 (a+b)a+b cC-dd . ee-ff )
I a+b'0g2( aa-bb (c+d)c+d (6+f)@+f'

***************Ending Page***************


***************Beginning Page***************
***************page number:11**************
11.
Important REMARKS (in Romanian)
1. intrucét majoritatea calculatoarelor de buzunar nu au funcgia 10g2 cz'
functiile 1n §i 1g, in formulele prezentate sau deduse la punctele a — e ar ﬁ
de dorit sii schimbéim baza logaritmului. Aceasta revine — pe léngé inlocuirea
lui 10g2 cu 1n sau 1g — la inmuligirea membrului drept cu 1/ 1112, respectiv 1/ 1g 2.
2. intrucét, la aplicarea algoritmului ID3, pentru alegerea celui mai
bun atribut de pus in nodul curent este suﬁcient séi calculém entropiile
condiigionale medii, va ﬁ suﬁcient s5 compariim produsele de forma
(C + d)c+d (6 + f)e+f

ccdd . 66 f f (1)
pentru c0mpa§ii de decizie considerati la nodul respectiv §i sﬁ alegem minimul
dintre aceste produse.

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
12.
Exemplifying the application of the ID3 algorithm
on a toy mushrooms dataset
CMU, 2002(?) spring, Andrew Moore, midterm example questions, pr. 2

***************Ending Page***************


***************Beginning Page***************
***************page number:13**************
13.
You are stranded on a deserted island. Mushrooms of various types grow widely all
over the island, but no other food is anywhere to be found. Some of the mushrooms
have been determined as poisonous and others as not (determined by your former
companions’ trial and error). You are the only one remaining on the island. You have
the following data to consider:
——|—|
——|—|
n|—|—
——|—|
u|—|u
—|—II-
—|—l“
—|—l-I
—|—lu
——|—|
——|—|
——|—|
You know whether or not mushrooms A through H are poisonous, but you do not know
about U through W.

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
l4.
For the a—d questions, consider only mushrooms A through H.
a. What is the entropy of Edible?
b. Which attribute should you choose as the root of a decision tree?
Hint: You can ﬁgure this out by looking at the data Without explicitly
computing the information gain of all four attributes.
c. What is the information gain of the attribute you chose in the previous
question?
d. Build a ID3 decision tree to classify mushrooms as poisonous or not.
e. Classify mushrooms U, V and W using the decision tree as poisonous or
not poisonous.
f. If the mushrooms A through H that you know are not poisonous suddenly
became scarce, should you consider trying U, V and W? Which one(s) and
Why? Or if none of them, then why not?

***************Ending Page***************


***************Beginning Page***************
***************page number:15**************
15.
a.
def. 3 3 5 5 3 8 5 8
HEdible I H[3+>5—] I —g'10g2é—§'10g2g:g-10g2§+g-10g25
3 3 5 5 3 5
I g'3—g'10g23+g‘3—g'10g25=3—é'10g23—g-10g25
m 0.9544

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
16.
b.
[3+,5—] [3+,5—] [3+,5—] [3+,5—]
wm @m m m
0 X 0 \ O \ O \
[1+,2—] [2+,3—] [2+,3—] [1+,2—] [2+,3—] [1+,2—] [2+,2—] [1+,3—]

***************Ending Page***************


***************Beginning Page***************
***************page number:17**************
17.
C.
de, 4 4 1 1 1 4 3 4
HO/Smooth :f gH12+72_1+ gH11+13—1 I 5 '1+ é (Z lOgQ I + 1 10g2 g)
1 1 1 3 3 1 1 3
I _ _ _-2 _-2__1 3 :_ _ 2__1 3
2+2(4 +4 4%) 2+2< 4%)
1 3 3 3
I _ 1__1 :___1 ~.
2+ 80g23 2 80g23 09056
def.
[GO/Smooth I HEdz'ble — HO/Smooth
I 0.9544—0.9056:0.0488

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
18.
d.
de. 3 5
HO/NotHeavy :f gH[]-+a 2—] + gH[2+a 3—]
3 11 3+21 3 +5 21 5+31 5
:——0——0———0——0—
83g21 35522 85g22 5&3
3 1 2 2 5 2 2 3 3
3 2 5 3 2
I — l — — — l — —l — —
8<0g23 3)+8(0g25 50g23 5)
3 2 5 3 2
: g10g23— g+élog25— élog23— g
5 4
I —1 5——%0.9512
8 Ogg 8
=» [GO/Noweavy d2‘ HEdible — HO/Nothy I 0.9544 — 0.9512 I 0-0032,
lGO/NotHeavy I [GO/Smelly I [GO/Spotted I 00032 < [GO/Smooth I 00488

***************Ending Page***************


***************Beginning Page***************
***************page number:19**************
19.
Important Remark (in Romanian)
in loc sa ﬁ calculat efectiv aceste cagtiguri de informatie, pentru a determina atributul
cel mai ,,bun“, ar ﬁ fost suﬁcient sa comparam valorile entropiilor condiigionale medii
HO/Smooth §i HO/NotHeavy:
IGO/Smooth > IGO/NotHeavy <:> HO/Smooth < HO/NotHea'uy
3 3 5 1
(I) — — —10g23 < —10g25— — ¢> 12—310g23 < 510g25—4
2 8 8 2
(I) 16 < 510g2 5 + 310g2 3 (I) 16 < 11.6096 + 4.7548 (adev.)
in mod alternativ, ginand cont de formulele de la problema UAIC, 2017 fall, S. Ciobanu,
L. Ciortuz, putem proceda chiar mai simplu relativ la calcule (nu doar aici, om' de cdte
om' m1, a/vem de-a face cu un numiir mare de instante):
44 44 38’ 55 48
H eed<H mi<l>—-—<—-—<:>—<55@48<33-55@216<33-55
O/Ntd O/U§ 2222/ 33 22/ 2233/ 33
@64- 210 < 27- 25-125 (adev.)
V W
1024 >3 . 8 . 125
V
1000

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
20.
Node 1: Smooth I O
[2+,2—] [2+,2—] [2+,2—]
w @
[O+,1—] [2+,1—] [2+,0—] [0+,2—] [1+,1—] [1+,1—]

***************Ending Page***************


***************Beginning Page***************
***************page number:21**************
21.
Node 2: Smooth I 1
[1+,3—] [1+,3—] [1+,3—]
[1+,1—] [O+,2—] [O+,3—] [1+,O—] [1+,2—] [O+,1—]

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
22.
The resulting ID3 Tree
0 \
/ IF (Smooth z O AND Smelly I O) OR
[2432-] [1+’3_] (Smooth = 1 AND Smelly = 1)
@ @ THEN Edibile;
ELSE ﬁEdible;
O \ O \
[2+i0—] [0+,2—] [O+,3—] [1+,0—]
Classiﬁcation 0f test instances:
-| Smooth I 1, Smelly : 1 :> Edible : 1
-| Smooth = 1, Smelly = 1 i Edible = 1
-| Smooth I 0, Smelly : 1 :> Edible I O

***************Ending Page***************


***************Beginning Page***************
***************page number:23**************
23.
Exemplifying the greedy character 0f
the ID3 algorithm
CMU, 2003 fall, T. Mitchell, A. Moore, midterm, pr. 9.21

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
24.

Fie atributele binare de intrare A, B , C , atributul de ie§ire Y §i urmétoarele
exemple de antrenament:

A B C Y

1 1 O O

1 O 1 1

O 1 1 1

O O 1 O
a. Determinaigi arborele de decizie calculat de algoritmul ID3. Este acest
arbore de decizie consistent cu datele de antrenament?

***************Ending Page***************


***************Beginning Page***************
***************page number:25**************
25.
Raspuns
Nodul 0: (rédécina)
[2+,2—] [2+,2—] [2+,2—]
[1+,1—] [1+,1—] [1+,1—] [1+,1—] [0+,1—] [2+,1—]

Se observﬁ imediat c5 primii doi “compagi de decizie” (engl. decision
stumps) au I G I 0, in timp ce al treilea compas de decizie are I G > 0. Prin
urmare, in nodul 0 (rédéciné) vom pune atributul C.

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
26.
Nodul 1: Avem de clasiﬁcat instangele cu C I 1, deci alegerea se face intre
atributele A §i B.
[2+,1—] [2+,1—]
w A
[1+,1—] [1+,0—] [1+,1—] [1+,0—]
Cele douéi entropii condigionale medii sunt egale:
2 1
Agadar, putem alege oricare dintre cele douﬁ atribute. Pentru ﬁxare, i1
alegem pe A.

***************Ending Page***************


***************Beginning Page***************
***************page number:27**************
27.
[2+v2_]
Nodul 2: La acest nod nu mai avem disponibil
decéit atributul B, deci i1 vom pune pe acesta. w
[0+,1—] [2+,1—]

Arborele ID3 complet este reprezentat in ﬁgura A
aliituraté. O 1

. - . . [1+’1_] [1+,0—]
Prln c0nstruc§1e, algorltmul ID3 este conszstent
cu datele de antrenament dacé agestea sunt con- a A
sistente (i.e., necontradictorii). In cazul nostru, 0 1
se veriﬁcii imediat c5 datele de antrenament sunt [0+,1—] [1+,0—]
consistente. A A

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
28.
b. Existéi un arbore de decizie de adéncime mai micé (decét cea a arborelui
ID3) consistent cu datele de mai sus? Dacé da, (:e concept (logic) reprezinté
acest arbore?
Réspuns:
Din date se observé cé atributul de ie§ire
Y reprezintii de fapt functia logicé A XOR [2+,2_]
B.
Reprezenténd aceastﬁ functie ca arbore 0 1
de decizie, vorn obtine arborele aléiturat. [1+,1—] [1+,1—]
Acest arbore are cu un nivel mai putin a a
decét arborele construit cu algoritmul 0 1 0 1
ID3' [O+,1—] [1+,O—] [1+,O—] [O+,1—]
Prin urmare, arborele ID3 ml, este op- Q A A Q
tim din punctul de vedere al numérului
de niveluri.

***************Ending Page***************


***************Beginning Page***************
***************page number:29**************
29.
Aceasta este 0 consecintii a caracterului “greedy” a1 algoritmului
ID3, datorat faptului (:5 la ﬁecare iteraigie alegem ,,cel mai bun“
atribut in raport cu criteriul cégtigului de informaigie.
Se §tie (:5 algoritmii de tip “greedy” nu granteazii 0b§inerea opti-
mului global.

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
30.
Exemplifying the application of the ID3 algorithm
in the presence of both
categorical and continue attributes
CMU, 2012 fall, Eric Xing, Aarti Singh, HWl, pr. 1.1

***************Ending Page***************


***************Beginning Page***************
***************page number:31**************
31.
As of September 2012, 800 extrasolar planets have been identiﬁed in our galaxy. Super-
secret surveying spaceships sent to all these planets have established whether they are
habitable for humans or not, but sending a spaceship to each planet is expensive. In
this problem, you will come up with decision trees to predict if a planet is habitable
based only on features observable using telescopes.
a. In nearby table you are given the data --——|
from all 800 planets surveyed so far. The fea- Big Near Yes 2O
tures observed by telescope are Size (“Big” or Big Far Yes 170
“Small”), and Orbit (“Near” or “Far”). Each Small Near Yes 139
row indicates the values of the features and Small Far Yes 45
habitability, and how many times that set of Big Near No 130
values was observed. So, for example, there Big Far No 3O
were 2O “Big” planets “Near” their star that Small Near No 11
were habitable. Small Far No 255
Derive and draw the decision tree learned by ID3 on this data (use the maximum
information gain criterion for splits, don’t do any pruning). Make sure to clearly mark
at each node what attribute you are splitting on, and which value corresponds to which
branch. By each leaf node of the tree, write in the number of habitable and inhabitable
planets in the training data that belong to that node.

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
32.
Answer: Level 1
H(374/8()O) H(374/800)
[374+,426—] [374+,426—]
[190+,160—] [184+,266—] [159+,141—] [215+,285—]
H( 1 9/ 35) H(92/225) H(47/ 100) H(43/ 100)
. . 35 19 45 92
H(Habztable|5'zze) _ 8—0 -H (5) —|— Q -H (ﬁ)
I g .0_9946 + gig 419759 I 0.9841 IG(Habz'table;S1lze) I 0.0128
. . 3 47 5 48
H (Habmblelmbzt) I g - H (W) + g - H (W) IG(Habz'table; 0150:) I 0.0067
I g - 0.9974 + g - 0.9858 z 0.9901

***************Ending Page***************


***************Beginning Page***************
***************page number:33**************
33.
The ﬁnal decision tree
[374+,426_]
B ! ! S
[190+,160—] [184+,266—]
[20+,130—] [170+,30—] [139+,11—] [45+,255—]

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
34.
b. For just 9 of the planets, a third Habitable
feature, Temperature (in Kelvin degrees), Big Far 205 N0
has been measured, as shown in the Big Near 205 No
nearby table. Big Near 260 Yes
Redo all the steps from part a on this data Big Near 380 Yes
using all three features. For the Temper- Small Far 205 No
atu'r'e feature, in each iteration you must Small Far 260 Yes
maximize over all possible binary thresh- Small Near 260 Yes
olding splits (such as T g 250 vs. T > 250, Small Near 380 N0
for example). Small Near 380 No
According to your decision tree, would a planet With the features (Big, Near, 280) be
predicted to be habitable or not habitable?
Hint: You might need to use the following values of the entropy function for a Bernoulli
variable of parameter p:
H(1/3) I 0.9182, H(2/5) z 0.9709, H(92/225) I 0.9759, H(43/100) I 0.9858, H(16/35) I 0.9946,
H(47/100) I 0.9974.

***************Ending Page***************


***************Beginning Page***************
***************page number:35**************
35.
Answer
Binary threshold splits for the continuous attribute Temperature;
O Q O
O Q O
O Q '
4|—|—|—|—|—>
205 232.5 260 320 380 Temperature

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
36.
Answer: Level 1
H(4/9) H(4/9) H(4/9) H(4/9)
[4+,5—] [4+,5—] [4+,5—] [4+,5—]
[2+,2—] [2+,3—] [1+,2—] [3+,3—] [0+,3—] [4+,2—] [3+,3—] [1+,2—]
H:1 H(Z/S) H(1/3) H:1 A H(1/3) H:1 H(1/3)
H=0
187
IG Habz' able; Size I H — — 0.9838
H(Habitable|Sz'ze) I g + 5 ‘11(2) I § + é ~ 0.9709 I 0.9838 ( t ) (400)
9 9 5 9 9 I 0.9969 — 0.9838
H(Habz'tableITemp g 232.5) = 319%) = g 99192 = 0.6121 = 0-0072
IG(Habitable; Temp g 232.5) I 0.3788

***************Ending Page***************


***************Beginning Page***************
***************page number:37**************
Answer: Level 2 37'
H(l/3) H(l/3) H(l/3)
[4+,2—] [4+,2—] [4+,2—]
[2+,0—] [2+,2—] [1+,0—] [3+,2—] [3+,0—] [1+,2—]
A H=1 A H(2/5) A H(1/3)
H=O H'=O H=O
Note:
The plain lines indicate that both the speciﬁc conditional cntropies and their coeﬁ'icients
(weights) in the average conditional entropies satisfy the indicated relationship. (For ex-
ample, H(2/5) > H(1/3) and 5/6 > 3/6.)
The dotted lines indicate that only the speciﬁc conditional entropies satisfy the indicated rela-
tionship. (For example, H(2/2) : 1 > H(2/5) but 4/6 < 5/6.)

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
38.
The ﬁnal decision tree:
[4+,5—]
[0+’3_] [4+’2_] c. According to your decision tree,
A w would a planet with the features
(Big, Near, 280) be predicted to be
Y \V habitable or not habitable?
[3+,0—l [1+,2—l Answer: habitable.
B ! ! S
[l+,0—] [0+,2—]

***************Ending Page***************


***************Beginning Page***************
***************page number:39**************
39.
Exemplifying
the application of the IDS algorithm
on continuous attributes,
and in the presence of a noise.
Decision surfaces; decision boundaries.
The computation of the LOOCV error
CMU, 2002 fall, Andrew Moore, midterm, pr. 3

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
40.

Suppose we are learning a classiﬁer with binary input values Y : O and 1 0
Y : 1. There is one real-valued input X. The training data is given in the 2 0
nearby table. 3 0
Assume that we learn a decision tree on this data. Assume that when 4 0
the decision tree splits on the real-valued attribute X, it puts the split 6 1
threshold halfway between the attributes that surround the split. For 7 1
example, using information gain as splitting criterion, the decision tree 8 1
would initially choose to split at X : 5, which is halfway between X : 4 8-5 0
and X : 6 datapoints. 9 1

10 1
Let Algorithm DT2 be the method of learning a decision tree with only two leaf nodes
(i.e., only one split).
Let Algorithm DT* be the method of learning a decision tree fully, with no prunning.
a. What will be the training set error for DT2 and respectively DT* on our data?
b. What will be the leave-one-out cross-validation error (LOOCV) for DT2 and re-
spectively DT* on our data?

***************Ending Page***************


***************Beginning Page***************
***************page number:41**************
41.
0 training data: ID3 tree:
—|—o—o—o—o—|—o—o—o—o—o—o—>
012345678910X
[5_’5+]
o discretization / decision thresholds: @ﬂ
Da Nu
1 2 3 4 6 7 8 9 10 X
5 8.25 [410+] [1-,5+]
o compact representatlon of the ID3 tree: Da Nu
1 2 3 4 5 6 7 8 9 10 X [0-,3+] [112+]
@ A @-
0 decision “surfaces”: Da Nu
;§—+414;, [PM [0-H
5 8.25 8.75 X A A

***************Ending Page***************

***************Beginning Page***************
***************page number:42**************
42.
Level 1:
[5—,5+] [5—,5+] [5—,5+]
D215 5 Nu D21! E Nu D21! ENu
Level 2: [1-,5+] [1-,5+]
D21! ! Nu D21! ! Nu
[013+] [112+] [113+] [012+]
IG: 0.191 IG: 0.109
Decision "surfaces": —_|;|;|+—>
5 8.25 8.75

***************Ending Page***************


***************Beginning Page***************
***************page number:43**************
43.
X=1,2,3,7,10: ;);¢;¢+—>
5 8.25 8.75
X=4. ——7+7;7+—,
ID3, LOOCV: 4.5 8.25 8.75
Decision surfaces _ + _ +
X=6= —§—¢—l—>
5.5 8.25 8.75
ng. ;7;7;7+—,
LOOCV error: 3/10 5 7'75 8'75
X=8.52 ;’++—+>
5
X=9: ;7;7—_7+—,
5 8.25 9.25

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
44.
DT2
Dag g Nu
[410+] [115+]
Decision "surfaces":
— +
5

***************Ending Page***************


***************Beginning Page***************
***************page number:45**************
4 .
DT2, LOOCV 5
IG computations
[4—,5+] [4—,5+] [4—,5+]
@5 @ @
Case 1: X:1 2 3 4 Da Nu Da Nu Da Nu
[3-,0+] [1—,5+] [3-,3+] [1—,2+] [4—,3+] [0-,2+]
[5—,4+] [5—,4+] [5-,4+]
Case 2: X26, 7, 8 Da Nu Da Nu D21 Nu
[4—,0+] [1—,4+] [4-,2+] [1—,2+] [5-,2+] [0-,2+]

***************Ending Page***************

***************Beginning Page***************
***************page number:46**************
46.
DT2, CVLOO
IG computations
(cont’d)
[4—,5+]
Case 3: X:8.5 ngu
[4—,()+] [0-,5+]
[5—,4+] [5-,4+] [5—,4+]
@ @.25
Case 2: X29 10 Da Nu Da Nu D3 Nu
[4—,0+] [1-,4+] [4—,3+] [1-,1+] [5-,3+] [O-,1+]
CVLOO error: 1/10

***************Ending Page***************


***************Beginning Page***************
***************page number:47**************
47.
Applying ID3 on a dataset with two continuous attributes:
decision zones
Liviu Ciortuz, 2017

***************Ending Page***************

***************Beginning Page***************
***************page number:48**************
48.
X 2
Consider the training dataset in the
nearby ﬁgure. 4 O O O
X1 and X2 are considered countinous at-
tributes. 3 o
Apply the IDS algorithm on this dataset. . O .
Draw the resulting decision tree. 2
Make a graphical representation of the 1 O '
decision areas and decision boundaries
determined by IDS. 0
0 1 2 3 4 5 X1

***************Ending Page***************


***************Beginning Page***************
***************page number:49**************
49.
Solution
Level 1:
[2+,0—] [2+,5—] [2+,4—] [2+,1—] [1+,1—] [3+,4—] [3+,2—] [1+,3—] [4+,2—] [0+,3—]
i i H(2/7) H(1/3) H(1/3) H=1 H(3/7) H(2/5) H(1/4) H(1/3) i i
IG=0.091
9; > _ 16:0.378
H[Y| . ] = 7/9 H(2/7) H[Y| . ] = 5/9 H(2/5) + 4/9 H(1/4) H[Y| . ] = 2/3 H(1/3)

***************Ending Page***************

***************Beginning Page***************
***************page number:50**************
[4+,2—] 5 [4+,2—] g [4+,2—] [4+,2—]
[2+,0—] [2+,2—] [2+,2—] [2+,0—] [1+,1—] [3+,1—] [3+,2—] [1+,0—]
1 l: 1 i: IG=0.04
IG=0.251 g = IG=O.251 g
H[Y| . ] =2/3 H[Y|.]=1/3+2/3 H(1/4) H[Y| . ] =5/6 H(2/5)
Notes:
1. When working with at least 2 continuous attributes, the split thresholds must be recom-
puted at each new iteration, because they may change. (For instance, here above, 4 replaces
4.5 as a threshold for X1.)
2. In the current stage, i.e., for the current node in the ID3 tree you may choose (as test)
either X1 < 5/2 or X1 < 4.
3. Here above we have an example of reverse relationships between weighted and respectively
un-weighted speciﬁc entropies: H[2+,2—] > H[3+,2—] but g ~H[2+,2—] < g - H[3+,2—].

***************Ending Page***************


***************Beginning Page***************
***************page number:51**************
51.
The ﬁnal decision tree: . .
De01s10n areas:
[4+,5—]
@ X2
(9 G)
Y w 4 o o o
[4+,2—] [0+,3—] _ _ _ _ _ _ _ _
@ A 3 '
G‘)
Y \w G)
2 o o o
[2+,0—] [2+,2—]
A @ 1 ® O .
Y N
[0+,2-] [2+,0-] 0 X
A A 0 1 2 3 4 5 1

***************Ending Page***************

***************Beginning Page***************
***************page number:52**************
52.
Other criteria than IG for
the best attribute selection in ID3:
Gini impurity / index
and Misclassiﬁcation impurity
CMU, 2003 fall, T. Mitchell, A. Moore, HWl, pr. 4

***************Ending Page***************


***************Beginning Page***************
***************page number:53**************
53.

Entropy is a natural measure to quantify the impurity of a data set. The
Decision Tree learning algorithm uses entropy as a splitting criterion by cal-
culating the information gain to decide the next attribute to partition the
current node.
However, there are other impurity measures that could be used as the split-
ting criteria too. Let’s investigate two of them.
Assume the current node n has k: classes 01,02, . . . ,ck.

o Gini Impurity: Kn) I 1 — 253:1 P2(c,).

0 Misclassz'ﬁcatz'on Impurity: i(n) I 1 — maxi/$21 P(c,-).
a. Assume node n has two classes, cl and 02. Please draw a ﬁgure in which
the three impurity measures (Entropy, Gi'ml and Misclassiﬁcatz'on) are repre-
sented as the function of P(cl).

***************Ending Page***************

***************Beginning Page***************
***************page number:54**************
54.
Answer
Emmy (p) I —p10g2 p — (1 — p)10g2(1 — p)
Gimp) I 1—P2—(1—P)2:2P(1—P) .0
MisClassz'ﬂp) I q:
_ {1 — (1 —p), for P Q [0;1/2)
1 — p: fOI' p € [1/251] g — Entropy
: p: fOI‘ p € [0; 1/2) : aliZICIassif
1—p, forp€[1/2;1] g
0.0 0.2 0.4 0.6 0.8 1.0
p

***************Ending Page***************


***************Beginning Page***************
***************page number:55**************
55.

b. Now we can deﬁne new splitting criteria based 0n the Ginz' and Misclassz'ﬁ-
cation impurities, which is called Drop-of-Impur'ity in some literatures. That
is the difference between the impurity of the current node and the weighted
sum of the impurities of children.
For the binary category splits, Drop-of-Impurity is deﬁned as
where n; and n71 are the left and respectively the right child 0f node n after
splitting.
Please calculate the Drop-of-Impurity (using both Ginz' and Misclassiﬁcation
Impurity) for the following example data set in which C is the class variable
to be predicted.

------|

-----|

***************Ending Page***************

***************Beginning Page***************
***************page number:56**************
56.
Answer
E
a1 a2
[2+,1—] [0+,3-]
Gini: p:2/6: 1/3:>
1 1 2 2 4
3(0) I 2-_(1__):_._:_
3 3 3 3 9 4 3 4 4 2 2
_ 3 3 _3 3-9
3(2) : O
Misclassz'ﬁcatz'on: p I 1/3 < 1/2 :>
. 1
2(0) I Pzg
1 1 1 1
1 I 1__:_
Z() 3 3 3 2 3 6
3(2) I 0

***************Ending Page***************


***************Beginning Page***************
***************page number:57**************
57.
c. We choose the attribute that can maximize the Drop-of-Impurity to split a
node. Please create a data set and show that on this data set, Misclassiﬁcation
Impurity based Ai(n) couldn’t determine which attribute should be used for
splitting (e.g., Ai(n) I 0 for all the attributes), but Information Gain and
Gini Impurity based Ai(n) can.
Answer
"———-III
"———-III
. 3 4
Entropy: Ai(0) I H[2+, 5—] — (?H[1+, 2—] + ?H[1+, 3]) I 0.006 75 0;
2 2 3 1 1 4 1 1 10 2 3
G":2 — 1—— ——-— 1—— —-— 1—— I2 ——— —
W {7< 7) l7 3< 3)+7 4< 4)“ {49 [21+2sl}
10 17
I 2 — — — °
(49 84) 7g 0’
2 3 1 4 1
M'l ' t':A' I—— —-— —-—I.
isc asszﬁca ion i(0) 7 (7 3 + 7 4) 0

***************Ending Page***************

***************Beginning Page***************
***************page number:58**************
58.
Note: A [quite bad] property
[Cl +,C2 —]
If 01 < 02, Ci < Cg and of < Cg
(withC1 :Ci-l-Cf and Cg:Cé+C§), a a
1 2
then the Drop-of-Impum'ty based on Misclassz'ﬁcatz'on is O. 1 1
[C1+,C2 —] [Cf +,C§—]
Proof
01+02 01+02 M 01+02 W
_ L _ M _ L _ i _ 0
_ 01+CZ 01+02 _ 01+02 01+02 _'

***************Ending Page***************


***************Beginning Page***************
***************page number:59**************
59.
Exemplifying
pre- and post-pruning of decision trees
using a threshold for the Information Gain
CMU, 2006 spring, Carlos Guestrin, midterm, pr. 4
[adapted by Liviu Ciortuz]

***************Ending Page***************

***************Beginning Page***************
***************page number:60**************
60.

Starting frorn the data in the following table, the ID3 algorithm
builds the decision tree shown nearby.

V W X Y 0 ® 1

O 0 O O \

0 1 0 1 ﬂ A

1 0 0 1 0 1

1 l O 0

1 1 1 1 0 Q 1 0 Q 1
a. One idea for pruning such a decision tree would be to start
at the root, and prune splits for which the information gain (or
some other criterion) is less than some small a. This is called top-
down pruning. What is the decision tree returned for e = 0.0001?
What is the training set error for this tree?

***************Ending Page***************


***************Beginning Page***************
***************page number:61**************
61.
Answer
We will ﬁrst augment the given decision tree with
informations regarding the data partitions (i.e., the [3+;2_]
number of positive and, respectively, negative in- ®
stances) which were assigned to each test node during 0 1
the application of ID3 algorithm. / \
The information gain yielded by the attribute X in the [HQ-1 [1+;0—]
root node is: X A
O 1
H[3+; 2—] — 1/5 - 0 — 4/5 - 1 I 0.971 — 0.8 I 0.171 > 5.
[1+;1—l [1+;1—]
Therefore, this node will not be eliminated from the
tree. Q 1 Q 1
The information gain for the attribute V (in the left- [0+;1—] [1+;0—] [1+;0—] [0+;1—]
hand side child of the root node) is: A A A A
H[2+;2—]—1/2-1—1/2-1:1—1:0<6.
So, the whole left subtree will be cut off and replaced by a decision ®
node, as shown nearby. The training error produced by this tree is K x
2 / 5.

***************Ending Page***************

***************Beginning Page***************
***************page number:62**************
62.
b. Another option would be to start at the leaves, and prune
subtrees for which the information gain (or some other criterion)
of a split is less than some small e. In this method, no ancestors of
children with high information gain will get pruned. This is called
bottom-up pruning. What is the tree returned for e : 0.0001?
What is the training set error for this tree‘?
Answer:
The information gain of V is [C(Y; V) I O. A step later, the infor-
mation gain of W (for either one of the descendent nodes of V) is
IG(Y; W) I l. So bottom-up pruning won’t delete any nodes and
the tree [given in the problem statement] remains unchanged.
The training error is 0.

***************Ending Page***************


***************Beginning Page***************
***************page number:63**************
63.
c. Discuss when you would want to choose bottom-up pruning
over top-down pruning and vice versa.
Answer:
Top-down pruning is computationally cheaper. When building
the tree we can determine when to stop (no need for real pruning).
But as we saw top-down pruning prunes too much.
On the other hand, bottom-up pruning is more expensive since we
have to ﬁrst build a full tree — which can be exponentially large
— and then apply pruning. The second problem with bottom-up
pruning is that superﬂuous attributes may foolish it (see CMU,
2009 fall, Carlos Guestrin, HWl, pr. 2.4). The third problem with
it is that in the lower levels of the tree the number of examples in
the subtree gets smaller so information gain might be an inappro-
priate criterion for pruning, so one would usually use a statistical
test instead.

***************Ending Page***************

***************Beginning Page***************
***************page number:64**************
64.
Exemplifying
XZ-Based Pruning of Decision Trees
CMU, 2010 fall, Ziv Bar-Joseph, HWZ, pr. 2.1

***************Ending Page***************


***************Beginning Page***************
***************page number:65**************
65.
In class, we learned a decision tree pruning algorithm that iter-
atively visited subtrees and used a validation dataset to decide
whether to remove the subtree. However, sometimes it is desir-
able to prune the tree after training on all of the available data.
One such approach is based on statistical hypothesis testing.
After learning the tree, we visit each internal node and test
whether the attribute split at that node is actually uncorrelated
with the class labels.
We hypothesize that the attribute is independent and then use
Pearson’s chi-square test to generate a test statistic that may
provide evidence that we should reject this “null” hypothesis. If
we fail to reject the hypothesis, we prune the subtree at that node.

***************Ending Page***************

***************Beginning Page***************
***************page number:66**************
66.
a. At each internal node we can create a contingency table for the training
examples that pass through that node on their paths to the leaves. The table
will have the c class labels associated with the columns and the r values the
split attribute associated with the rows.
Each entry Om- in the table is the number of times we observe a training
sample with that attribute value and label, where 2' is the row index that
corresponds to an attribute value and j is the column index that corresponds
to a class label.
In order to calculate the chi-square test statistic, we need a similar table of
expected counts. The expected count is the number of observations we would
expect if the class and attribute are independent.
Derive a formula for each expected count Em- in the table.
Hint: What is the probability that a training example that passes through
the node has a particular label? Using this probability and the independence
assumption, what can you say about how many examples with a speciﬁc
attribute value are expected to also have the class label?

***************Ending Page***************


***************Beginning Page***************
***************page number:67**************
67.
b. Given these two tables for the split, you can now calculate the chi-square
test statistic T C
2
2 _ (Om — Em)
X — ZZZ E.

. . '1, J

2:1 3:1 ’
with degrees of freedom (r — 1)(c — 1).
You can plug the test statistic and degrees of freedom into a software package“
or an online calculatorb to calculate a p-value. Typically, if p < 0.05 we reject
the null hypothesis that the attribute and class are independent and say the
split is statistically signiﬁcant.
The decision tree given on the next slide was built from the data in the table
nearby.
For each of the 3 internal nodes in the decision tree, show the p-value for the
split and state whether it is statistically signiﬁcant.
How many internal nodes will the tree have if we prune splits with p Z 0.05?

a Use 1-chi2cdf(x,df) in MATLAB or CHIDIST(X,df) in Excel.
b (https://en.m.wikipedia.org/wiki/Chi-square_distribution#Table_of_.CF.872_value_vs_p-value.

***************Ending Page***************

***************Beginning Page***************
***************page number:68**************
68.
Input:

[4-,8+]
X1 X2 X3 X4
1 1 0 0 0 Q
1 0 1 0 1 0 1
0 1 0 0 0 [4-,2+] [0-,6+]
1 0 1 1 1 A
0 1 1 1 1
0 0 1 0 0 0 1
1 0 O O 1 [3—,0+] [1—,2+]
0 1 0 1 1 A
1 0 0 1 1
1 1 0 1 1 O 1
1 1 1 1 1 [0-,2+] [1-,0+]
0 0 0 0 0 A A

***************Ending Page***************


***************Beginning Page***************
***************page number:69**************
69.
Idea
While traversing the ID3 tree [usually in bottom-up manner],
remove the nodes for which
there is not enough (“signiﬁcant”) statistical evidence that
there is a dependence between
the values of the input attribute tested in that node and the values
of the output attribute (the labels),
supported by the set of instances assigned to that node.

***************Ending Page***************

***************Beginning Page***************
***************page number:70**************
70.
Contingency tables
6 1 1
OX Class : 0 Class : 1 I I I I _ I I _

4 N:12 P<X4 O) 12 27 P<X4 1) 2
X4IO 4 2 :> 4 1 2
X4I1 O 6 P(ClassI0)IEI§, P(Class:1)I§

3 1
( 1 0| 4 0) 6 2
OX1|X4IO Class I O Class I 1 P(X1 z 1|X4 z 0) z;
N=6
X1 IO 3 0 :> 4 2
2
P(X2:0|X4:07X1:1):§
OX2|X4:0,X1:1 Class I O Class I 1 P(X2 I 1 | X4 I O,X1 I 1) I é
X2 :0 O 2 N::>3 1
X2:1 1 0 P(ClassIO|X4:(),X1:1):g
2
P(ClassI1|X4:O,X1:1):g

***************Ending Page***************


***************Beginning Page***************
***************page number:71**************
71.
The reasoning that leads to the computation of
the expected number of observations
P<A:@,0:j> :szw-szj)
P(A :1) I L6} J6 and P(C I j) I Liv k”

_ , inde . (EC: Oi,k) (2T: 0kg)

P(A:2,C:J) :p—k1 N2 k1 9
E[A:i,C:j] :N-P(A:i,C:j)

***************Ending Page***************

***************Beginning Page***************
***************page number:72**************
72.
Expected number 0f observations
EX4 Class I O Class I 1 EX1|X4IO Class I O Class I 1
X4 I 0 2 4 X1 I 0 2 1
X4 I 1 2 4 X1 I 1 2 1
EX2|X4207X121 Class: O Class I 1
2 4
X I 0 — —
2 3 3
1 2
X2 I 1 _ _
3 3
EX4(X4 I O, Class I O) :
1 1
N I 127 P(X4 I O) I 5 §i P(ClassI 0) I § I>
1 1
N-P(X4I0,ClassIO)IN-P(X4IO)-P(ClassIO)I12- §§ I2

***************Ending Page***************


***************Beginning Page***************
***************page number:73**************
2 . . 73.
X Stat1st1cs
2 _ T C (0m — Em?
2:1 1:1 ’
2 (4 — 2V (0 — 2V (2 — ‘02 (6 — ‘02
I : 2 2 1 1 I
X X4 2 + 2 + 4 + 4 + + + 6
2 _ (3—2)2 (1—2)2 (0—1)2 (2—1)2_
X X1|X4z0 — T+T+f+f—3
2 2 2 2
0 _ 2 1 _ 1 2 _ 1 0 _ g
2 _ 3 3 3 3 _ 4 27 _ 3
X X2|X4=0,X1=1 — T+f+T+T—§'Z—
3 3 3 3
p-values: 0.0143, 0.0833, and respectively 0.0833.
The ﬁrst one of these p-values is smaller than 0.05, therefore the root node
(X4) cannot be prunned.

***************Ending Page***************

***************Beginning Page***************
***************page number:74**************
74.
Chi Squared Pearson test statistics
O.
‘Q k = 9
q; o
E
§
O
c\i
O
c>_ ,
O
0 2 4 6 8
X2 - Pearson’s cumulative test statistic

***************Ending Page***************


***************Beginning Page***************
***************page number:75**************
75.
Output (pruned tree) for 95% conﬁdence level

***************Ending Page***************

***************Beginning Page***************
***************page number:76**************
76.
The AdaBoost algorithm:
pseudo-code and some basic properties
CMU, 2015 fall, Ziv Bar-Joseph, Eric Xing, HW4, pr. 2.1
CMU, 2009 fall, Carlos Guestrin, HW2, pr. 3.1

***************Ending Page***************


***************Beginning Page***************
***************page number:77**************
77.
Consider m training examples S : {($1,3/1), . . . , (mm,ym)}, where sci E X and y,- E {—1,+1}.
Suppose we have a weak learning algorithm A which produces a hypothesis h : X —> {—1,—|—1}
given any distribution D of examples.
AdaBOOSt is an iterative algorithm which works as follows:
1
o Begin with a uniform distribution D1(i) : —, 2' : 1,. . . ,m.
0 At each iteration t : 1, . . . ,T, m
o run the weak learning algo. A on the distribution Dt and produce the hypothesis ht;
Note (1): Since A is a weak learning algorithm, the produced hypothesis ht at round t is
only slightly better than random guessing, say, by a margin 'yt:
1 1
et I 6TTDt(h/t) I PI'wNDthJ 31$ ht(ac)] < 5 and 7t I 5 — at.
Note (2): If at a certain iteration t g T the weak classiﬁer A cannot produce a hypothesis
better than random guessing (i.e., 7t : 0) or if it can only produce a perfect hypothesis
(i.e., at z O), then the AdaBoost algorithm should be stopped.
no . 1 1 —
0 compute the weight at :t 51H —€t;
6t
o update the distribution
1 _ . .
Dt+1(z') I 7 - Dt(z') - e aty,ht(w,) for t : 1, . . . ,m, where Zt is the normalizer. (2)
t
o In the end, deliver HT : sign (23:1 atht) as the learned hypothesis, which will act as a
weighted majority vote.

***************Ending Page***************

***************Beginning Page***************
***************page number:78**************
78.
Important Remark

The above formulation of the AdaBoost algorithm states n0 restriction on
the ht hypothesis delivered by the weak classiﬁer A at iteration t, except that
6t < 1/2.

However, in another formulation of the AdaBoost algorithm (in a more gen-
eral setup; see for instance MIT, 2006 fall, Tommi Jaakkola, HW4, problem
3), it is requested / reccommended that hypothesis ht be chosen by (approazi-
mately) maximizing the [criterion of] weighted training error on a whole class
of hypotheses like, for instance, decision trees of depth 1 (decision stumps).
In this problem we will not be concerned with such a request, but we will
comply with it for instance at CMU, 2015 fall, Ziv Bar-Joseph, Eric Xing,
HW4, pr2.6, when showing how AdaBoost works in practice.

***************Ending Page***************


***************Beginning Page***************
***************page number:79**************
79.
Prove the following relationships:
2. Zt I 6-“t - (1 — at) —|— co” ‘875 (a consequence from (2))
22. Zt : 2\/5t(1 — at) (a consequence from 2., and the value stated for at in
the AdaBoost pseudo-code)
222.0 < Zt < 1 (a consequence derivable from 22.)
D I TLO . . I o
%, 2 € M :t {2|yz- 75 ht(a:¢)}, 1.e., the mlstake set
. . 6
221. Dt+1(2) I Dt (2)
m, 2 € C nét' {2|yZ : ht(;ci)}, i.e., the correct set
(a consequence derivable from (2) and 22.)
U. €i><€jz>021<04j
(a consequence from the value stated for oat in the AdaBoost pseudo-code)
v2. 67°7°Dt+l(ht) I 1/2, where 6TTDt+1(ht) nit PrDt+1({a:Z-|ht(a:i) 75 yz})
(a consequence derivable from (2) and 22.)

***************Ending Page***************

***************Beginning Page***************
***************page number:80**************
80.
Solution
2. Since Zt is the normalization factor for the distribution Dt+1, we can write:
Z2 I ZDMWWWU I Z thw + Z awe”: (1 _ at) - (rat + at - eat. (3)
2:1 2'60 2€M
n0 . 1 1 — .
22. Since 02,; :t — ln—€t, 1t follows that
2 8t
1 1 — 5t 1 — 5t
— 1n — 1n — 1 _ 6t
6042 : 62 6t : 6 6t : — (4)
552
and 1
_ 52
at : — : 2 /—. 5
6 fiat 1 — 6t ( )
So,
s /1 — a
\l — 2 2
1 _
Note that —€t > 1 because at E (0, 1/2); therefore at > 0.
52

***************Ending Page***************


***************Beginning Page***************
***************page number:81**************
81.
The second order function et(1 —st) reaches its maximum value for at I 1/2, and the
1
maximum is 1/4. Since at G (0,1/2), it follows from (6) that Zt > O and Zt < 2\/g : 1.
2'1). Based on (2), we can write:
. 1 . 60“, for z' € M
0mm) — 211m)- { for 7; Q a
Therefore,
_ _ 1 _ (4) 1 _ \/1 — at Dt(z')
2€M¢D z:—-Dz-e°‘t:—-Dz-—:—
. . 1 . _ (5) 1 . \/€—t Dt(i)
2€CI>D 2:—-Dz-e at:—-Dz-—:—.
H10 Zt t() 2\/et(1—et) t0 \/1—5t 2(1—5t)

***************Ending Page***************

***************Beginning Page***************
***************page number:82**************
82.
1 _
v. Starting from the deﬁnition at I ln l / —€t, we can write:
5t
/1 — Z- l1 — -
04¢ < Oij <I>l11 —€ < lIl —€J
62' 8]‘
Further on, since both 1n and \f functions are strictly increasing, it follows that
< @1—8i<1_€j6“<3>>0 (1 )< (1 )<:> < I >
041' a- — — 6' —5¢ el- —e- 5-—6-- ei—6-- 51' 6-
2 62- 69- .7 J J /€f /€f J
m'. It is easy to see that
m . (2) 1 . at 1 . at
6TTDt+1(ht) I EDt+1(Z)'1{yZ-¢ht(mi)} I Z 7Dt(z)e I 7 Z Dt(z) e
i=1 iGM t t iGM
‘xi
1 6*’
I _ . . at 7
Zt 5t 6 ( )
By substituting (6) and (4) into (7), we will get:
:> (h ) 1 a 1 /1 — at 1
67”?“D :—-5-6t:—.5. —:_
t+1 t Zt t 2\/€t(1 —€t) t 5t 2

***************Ending Page***************


***************Beginning Page***************
***************page number:83**************
83.
The AdaBoost algorithm:
why was it designed the way it was designed, and
the convergence of the training error, in certain conditions
CMU, 2015 fall, Ziv Bar-Joseph, Eric Xing, HW4, pr. 2.2-5
CMU, 2009 fall, Carlos Guestrin, HW2, pr. 3.1
CMU, 2005 spring, Torn Mitchell, Carlos Guestrin, HW2, pr. 1.1.3

***************Ending Page***************

***************Beginning Page***************
***************page number:84**************
84.
The objective of this exercise is to prove that the training error errS(HT) of
the AdaBoost algorithm decreases at a very fast rate, and in certain cases it
converges to 0.
Notes:
1. For the pseudo-code of the AdaBoost algorithm, see CMU, 2015 fall, Ziv
Bar-Joseph, Eric Xing, HW4, pr2.1.
2. Here we will assume (similarly to CMU, 2015 fall, Ziv Bar-Joseph, Eric
Xing, HW4, pr2.1) that AdaBoost makes no restriction on the ht hypothesis
delivered by the weak classiﬁer A at iteration t, except that at € (O, 1/2).

***************Ending Page***************


***************Beginning Page***************
***************page number:85**************
85.
-1
a. Show that DT+1(z') I (m - {[le Zt) e—y@'f(‘”i), where fa) I 2le athta).
1
b. Show that errS(HT) g Hthl Zt, where errS(HT) nét' — 2111 1{HT($i)$éyi} is the
m
training error produced by AdaBoost.
c. Obviously, we would like to minimize test set error produced by AdaBoost,
but it is hard to do so directly. We thus settle for greedily optimizing the
upper bound on the training e'r'ro'r found at part b. Observe that Z1, . . .,Zt_1
are determined by the ﬁrst t — 1 iterations, and we cannot change them at iteration t.
A greedy step we can take to minimize the training set error bound on round t is to
minimize Zt. Prove that the value of at that minimizes Zt (among all possible
1 1 — e
values for at) is indeed at I 51H —t (see the previous slide).
6t
d. Show that {[le Zt g 522.11%.
e. From part b and d, we know the training error decreases at exponential
rate with respect to T. Assume that there is a number y > 0 such that *y g Vt
for t : 1, . . . ,T. (This *y is called a guarantee of empirical y-weak: learnability.)
How many rounds are needed to achieve a training error e > O? Please express
in big-(9 notation, T : (9().

***************Ending Page***************

***************Beginning Page***************
***************page number:86**************
86.
Solution
a. We will expand Dt(2') recursively:
- 1 - —a ~h (an)
DT+1<Z) : _ D110) 6 Tyz T 1
ZT
1
: D ' _ _O4T yi 7124151)
TQ) ZT 6
1 1
: D _ ' —(IT_1 yi hT_1(113i) _ —O¢T 3/1 hT(xi)
T 1(2) ZT—1 e ZT 6
thl Zt
: éj" €—yi H1171‘).
m ' thl Zt

***************Ending Page***************


***************Beginning Page***************
***************page number:87**************
87.
Y
b. We will make use of the fact that the exponential
loss function upper bounds the 0-1 loss function, i.e. 1
1{w<0} 3 @_’”= 11x41} e‘X
H — 1 m 1
6TTS< T) _ E; {yif(a:¢)<0} 0 X
< li€_yif(wi)
_ m1:1
b 1 m T m T
: — ZDT+1(1') -m - Hzt : ZDT+1(1') Hzt
m 1:1 1::1 1:1 1:1
m T
1;:1 1:1
W
1
T
: Hzt.
1:1

***************Ending Page***************

***************Beginning Page***************
***************page number:88**************
88.
c. We will start from the equation
Zt : at - eo‘t + (1 — at) - 6-0“,
which has been proven before. Note that the right-hand side is constant with
respect to at (the error produced by ht, the hypothesis produced by the weak
classiﬁer A at the current step).
Then we will proceed as usually, computing the partial derivative w.r.t. at:
é 041: 1 —04t _ 0 (It 1 —04t _ 0
8—04t(6t.€ +( —€t)'€ )— @Et'e —( —€t)'€ —
1 — 1 — 1 1 —
@st-(eatf I 1—5t<:)620‘t I —€t <:>2o¢t :ln—€t @ozt I —ln—€t.
1 _
Note that —€t > 1 (and therefore oat > O) because at € (O, 1/2).
5t
. . 1 1 — 5t . .
It can also be Immedlately shown that at : 51H — 1s 1ndeed the value for
51:
which we reach the minimum of the expression at - co“ + (1 — at) - e_o‘t, and
therefore of Zt too:
1 — 1 1 —
(st-60“ —(1—et)-e_o‘t >O<i>620‘t ——€t >O<I>o¢t > —ln—€t.
8t 2 5t

***************Ending Page***************


***************Beginning Page***************
***************page number:89**************
89.
— eps=1/10
— eps=1/4
Plots 0f three Z(/3) functions, j — ePS=2/5
1
2(6):5t'5+(1—5t)'g
c\!
Where 1-
6 nét' eo‘, (04 being free(!) here)
and at is ﬁx. N 2 i
It implies that l
O ; :
/1 — i i a
5t : i :
1 2 3 4 5
beta

***************Ending Page***************

***************Beginning Page***************
***************page number:90**************
90.
y
d. Making use of the relationship (6) which was already
proven, and using the fact that 1 — a; g (Tm for all :13 € R, e-x
we can write:
T T
Hzt I [12-Jan-59 0 X
12:1 t=1
— ﬁ 2 1 1 1
— 2 7t 2 7t
15:1
T
I H V 1 — 4%2
t=1
T
g H \/6—4%2
15:1
T T 2
I H i I (6-271?)2 I H €_2'Yt
t:1 t=1
: 6-2 23:1 71?.

***************Ending Page***************


***************Beginning Page***************
***************page number:91**************
91.
e.
Y
—|—L:§l—>
0 8t Yr 1/2
From the result obtained at parts b and d, we get:
6TT5<HT) S e—22;r:1%2 g €—2TV2 I (6—2’Y2)T Z ;T
(@212)
Therefore,
° 2 2 2 1 1 1
errS(HT) <5 1f —2T'y < 1ne<i>2T7 > —lne<:>2T’y >1ng @T> Flu;
'Y
1 1
Hence we need T I (9 (—21n —).
y a
Note: It follows that 6TT5(HT) —> O as T —> oo.

***************Ending Page***************

***************Beginning Page***************
***************page number:92**************
92.
Exemplifying the application 0f AdaBoost algorithm
CMU, 2015 fall, Ziv Bar-Joseph, Eric Xing, HW4, pr. 2.6

***************Ending Page***************


***************Beginning Page***************
***************page number:93**************
93.
I I I I X2
Cons1der the tra1n1ng dataset 1n the nearby ﬁg-
ure. Run T I 3 iterations of AdaBoost with deci- 4 x30 x60 x70
sion stumps (axis-aligned separators) as the base
3 x .
learners. Illustrate the learned weak hypotheses 2
ht in this ﬁgure and ﬁll in the table given below. 2 x10 x40 x80
(For the pseudo-code of the AdaBoost algorithm, see CMU, 1 x O x .
5 9

2015 fall, Ziv Bar-Joseph, Eric Xing, HW4, pr. 2.1. Please
read the Important Remark that follows that pseudo-code!) O X

0 1 2 3 4 5 1
Dt<1> 1w Dt<3> M Dt<5> Dt<6> 1W 1w Dt<9>
III----------
III----------
III----------
Note: The goal of this exercise is to help you understand how AdaBoost works in practice.
It is advisable that — after understanding this exercise — you would implement a program /
function that calculates the weighted training error produced by a given decision stump, w.r.t. a
certain probabilistic distribution (D) deﬁned on the training dataset. Later on you will extend
this program to a full-ﬂedged implementation of AdaBoost.

***************Ending Page***************

***************Beginning Page***************
***************page number:94**************
94.
Solution

Unlike the graphical reprezentation that we used until now for decision
stumps (as trees of depth 1), here we will work with the following analit-
ical representation: for a continuous attribute X taking values x € R and for
any threshold s G R, we can deﬁne two decision stumps:

5in(a:—s)— 1 ifst and sin(s—a3)— —1 ifst

g _ —1ifx<s g _ 1 ifat<s.

For convenience, in the sequel we will denote the ﬁrst decision stump with
X Z s and the second with X < s.
According to the Important Remark that follows the AdaBoost pseudo-code
[see CMU, 2015 fall, Ziv Bar-Joseph, Eric Xing, HW4, pr. 2.1], at each itera-
tion (t) the weak algorithm A selects the/a decision stump which, among all
decision stumps, has the minimum weighted training error w.r.t. the current
distribution (Di) on the training data.

***************Ending Page***************


***************Beginning Page***************
***************page number:95**************
95.

Notes
When applying the ID3 algorithm, for each continuous attribute X, we used a
separation threshold for each pair of examples (513,-, yd), (x7;+1,jg,-+1), with yinl <
O such that x,- < 331+1, but no x,- € Val(X) for which x,- < x,- < $i+1.
We will proceed similarly when applying AdaBoost with decision stumps and
continuous attributes.
[In the case of ID3 algorithm, there is a theoretical result stating that there is
no need to consider other thresholds for a continuous attribute X apart from
those situated beteen pairs of successive values (xz- < xi+1) having opposite
labels (y,- 75 yi+1), because the Information Gain (IG) for the other thresholds
(sci < mHl, with y, : yi+1) is provably less than the maximal IG for X.
LC: A similar result can be proven, which allows us to simplify the application
of the weak classiﬁer (A) in the framework of the AdaBoost algorithm.]
Moreover, we will consider also a threshold situated outside the interval of
values taken by the attribute X in the training dataset. [The decision stumps
corresponding to this “outside” threshold can be associated with the decision
trees of depth 0 that we met in other problems.]

***************Ending Page***************

***************Beginning Page***************
***************page number:96**************
96.

Iteration t : 1:
Therefore, at this stage (i.e, the ﬁrst iteration of AdaBoost) the thresholds for
the two continuous variables (X 1 and X2) corresponding to the two coordinates
of the training instances ($1, . . . ,xg) are

1 5 9

o 5, 5, and 5 for X1, and
1 3 5 7
o —, —, — and — for X2.
2 2 2 2
1
One can easily see that we can get rid of the “outside” threshold 5 for X2,
because the decision stumps corresponding to this threshold act in the same
1

as the decision stumps associated to the “outside” threshold 5 for X1.
The decision stumps corresponding to this iteration together with their as-
sociated weighted training errors are shown on the next slide. When ﬁlling
those tabels, we have used the equalities eert (X1 Z s) : 1 — eert (X1 < 5)
and, similarly, eert(X2 Z s) I 1 — eert(X2 < s), for any threshold s and every
iteration t I 1, 2, . . .. These equalities are easy to prove.

***************Ending Page***************


***************Beginning Page***************
***************page number:97**************
97.
1 5 9
——.__
4 2 4 2 2
5 7 1
—1II-
1 3 5 7
8 _ _ _ _
!]I--_
4 1 3 4 2 1 1 2
—1.§+§—§ §+§-§
5 5 2 7
> _ _ _ _
—._-I1
It can be seen that the minimal weighted training e'r'ro'r (51 I 2/9) is obtained for the
7
decision stumps X1 < 5/2 and X2 < 7/2. Therefore we can choose hl I sign (2 — X2) as
best hypotheszs at 1terat10n t I 1; the correspondlng separator 1s the 11ne X2 I 5. The
hl hypothesis wrongly classiﬁes the instances x4 and $5. Then
1 2 5 1 1 — 51 2 2 \ﬁ
I ———I—andaI—1n—Iln 1—— :—I1n —%0.626
11 2918 12 61 \/( 9)9 2

***************Ending Page***************

***************Beginning Page***************
***************page number:98**************
98.
Now the algorithm must get a new distribution (D2) by altering the old one (D1) so
that the next iteration concentrates more on the misclassiﬁed instances.
1 1 2
— - — - \/: for 2' € {1,2,3,6,7,8,9};
. _ 1 . —a -h (x) _ Zl 9 7
172(1) — —D1(@)(@ 1 )1” 1 Z —
Z1 V 1 1 \/? f _ E {4 5}
\/2/7 Z1'9' 2 orz , .
Remember that Z1 is a normalization factor for D2.
So, X2
1414 1/(14 1/(14
1 2 7 2 14 4 x3 x6 x7 _
3 x2‘
Therefore, 2 xll°14 x1é4 961/014
1 4 8
1/4 1/14
9 1 2 1 1 o 0
— _ _ _ _ : _ f ' 4 5 . X5 X9
. 2T4 9 \ﬂ 14 0r@¢{,},
D20) I 9 1 7 1 0
—-—- —:— f'E45. X
2m9\/;40H{,} 0123451

***************Ending Page***************


***************Beginning Page***************
***************page number:99**************
99.
Note
7
If, instead of sign <5 — X2) we would have taken, as hypothesis hl, the deci-
5
sion stump sign (5 — X1), the subsequent calculus would have been slightly
different (although both decision stumps have the same — minimal — weighted
2 1
training error, 5): $8 and x9 would have been allocated the weights Z’ while
1
x4 si x5 would have been allocated the weights E.
(Therefore, the output of AdaBoost may not be uniquely determined!)

***************Ending Page***************

***************Beginning Page***************
***************page number:100**************
100.
Iteration t : 2:
S 1 5 2
_ 2 2 2
4 2 2 2 2 11
lO 12 3
X > _ _ _
_II—1
_1I--
S _ _ _ _
—._———
(X <> i 1+3_E 2+i_§ Ll
6m” 2 5 14 4 14_28 4 14_14 4_2
10 l5 6 1
> _ _ _ _
_I---1
Note: According to the theoretical result presented at part a of CMU, 2015
fall, Ziv Bar-Joseph, Eric Xing, HW4, pr. 2.1, computing the weighted error
rate of the decision stump [corresponding to the test] X2 < 7/2 is now super-
ﬂuous, because this decision stump has been chosen as optimal hypothesis at
the previous iteration. (Nevertheless, we had placed it into the tabel, for the
sake of a thorough presentation.)

***************Ending Page***************


***************Beginning Page***************
***************page number:101**************
101.
5
Now the best hypothesis is hg : sign (5 — X1); the corresponding separator
is the line X1 : g.
2 1 1 1 5
82 D2({x8’x9}) 14 7 0 3 :12 2 7 14
/1— l 1 1
042 I 1n —€2:1n (1——):—:ln\/6:0.896
82 7 7
1 D <'> 1 'fh < >
- 1 ' —a 'h (at) 22 2 \/6 2 y
D30) Z —'D2(Z)'(6 2W 2 ’ I
Z2 V 1 , .
1/\/6 Z— -D2(2) - J5 otherW1se
2
1 1 1
— - — - — f ' 1 2 3 6 7 '
1 1 1
: _-—-— forz'Q 4,5;
1 1
—-—- f ' 9 .
22 14% or2€{8,}

***************Ending Page***************

***************Beginning Page***************
***************page number:102**************
102.
1 1 1 1 1 5 1 \/6 12+12 24 2/6
Z2 : 5._._+2._._+2._.\/6:—+_+_:—:—:_%0_7
14 \/6 4 \/6 14 14/6 2/6 7 14/6 14\/6 7
h
2
X2 +_
7 1 1 1 1/24 1/24 1/24
4 x0 x0 x0
—._._:_ f ' 12 . 3 6 7 —
2/6 14 \/6 24 GHQ’ ’3’6’7}’ v.24 + hl
3
D(') 7 1 1 7 f 'e{4 5} 1/24 X2 7/48 1/4
: —-—-—:— orz '
32 2J6 4 \/6 48 a 7 2 x1. x40 x8’
7 1 1 7/48 1/4
—._. 6:_ f ' _ o o
2% 14 \/_ 4 0r2€{8,9} 1 x5 x9
0
0 1 2 3 4 5 X1

***************Ending Page***************


***************Beginning Page***************
***************page number:103**************
103.
Note
If the property 2'1) from CMU, 2015 fall, Ziv Bar-Joseph, Eric Xing, HW4, pr.
2.6 were used (instead of the deﬁnition (2)), then there would be no need to
compute Z1‘, and Dt(2') would be computed directly:
D1(i) 1 1 ,
— I — I _ f 4 5 -
2(1—€1) 9-25 14 ormﬂ, },
192(1) I D . 9
1 1
&:—:_ {OH-Q45}
251 g. 2 . 2 4
9
and
7 1 1
—-—:— f e 1 2,3,6,7;
192(1) _D2(z)_lD(Z_) _ 12 14 24 OH {’ }
2(1—€2>— 2.? _12 2 _ 7 1 7 f'e{45}
. — - — I — or , ;
193(1) I 1 7 12 4 48 Z
— I — I — f 8 9 .
262 2 . 1 4 or 2 € { , }
7

***************Ending Page***************

***************Beginning Page***************
***************page number:104**************
104.
Iteration t I 3:
3 l 5 9
—.—_—
2 2 7 2 2 7 1 21
X — —:— — — 2-— 2-—:—
—124+4 12l24+ 48+ 4 24
5 2 3 1
--\I—
S l 5 5 Z
—._———
7 7 2 1 23 7 1 1 7 7
X — — — —:— 2-— —:— 2-—:—
--44+44+4 48 44+44 4 48 44
5 25 2 17
-\I—--

***************Ending Page***************


***************Beginning Page***************
***************page number:105**************
105.
The new best hypothe51s lS hg I szgn (X1 — 5); the correspondlng separator 1s the llne
9
X1 I 5.
X2 h2 h3
+ - - +
1 1 3 1 4 x0 x0 Ox
I P I 2 . _ _ I _ I _ 3 6 7 -
53 D3({£U1,332,$7}) 24 + 24 24 8 + h1
3 x2.
1 1 3
W3 : — — — : —
/1 — 1 1 1 O 0
043 :111 —€3:1n 1—— :—:1n\/?:O.973 x5 x9
63 8 8
O
O 1 2 3 4 5 X1

***************Ending Page***************

***************Beginning Page***************
***************page number:106**************
106.

Finally, after ﬁlling our results in the given table, we get:
Il-I-I-I-I-I-
II-l W-I-I-I-II-
Il-I-I-I-I-I-
Il-I-I-I-I-III
Note: The following table helps you understand how errS(HT) was computed;
remember that HT(:132) dif' sign (23:1 04,; lithe”).

III-IIIIIIII

II 0-626

II 0-896

II 0-973

III-IIIIIIII

***************Ending Page***************


***************Beginning Page***************
***************page number:107**************
107.
Note: By using the notation xi : ($¢,1,513¢,2), we can write
. 7 . 7 . 5 _ 9
H3($1) I Szgn 1n 5 - sign(§ — mm) + ln\/6- sign(§ — $131) + lnﬁ- sign(ati71 — 5) .
Remark (1)
One can immediately see that the [test] in- X2 h2 h3
stance (1, 4) Will be classiﬁed by the hypoth- 9 9
esis HT learned by AdaBoost as negative 4 x30 x60 0x7
(since —oq +042 - 043 : —0.626+0.s96 — 0.973 < hl
G) Gr)
After making other similar calculi, we 2 x‘ x o . x
can conclude that the decision zones and 1 4 8
the decision boundaries produced by Ad- 1 (9 x50 ox
aBoost for the given training data will be 9
as indicated in the nearby ﬁgure. O
O l 2 3 4 5 X1

***************Ending Page***************

***************Beginning Page***************
***************page number:108**************
108.
Remark (2)

The execution of AdaBoost could continue (if we would have taken initially
T > 3...), although we have already obtained 67°7“S(Ht) I 0 at iteration t I 3. By
elaborating the details, we would see that for t I 4 we would obtain as optimal
hypothesis X2 < 7/2 (which has already been selected at iteration t I 1). This
hypothesis produces now the weighted training error e4 : 1/6. Therefore,
044 : 1n J5, and this will be added to 041 : ln \/7/2 in the new output H4. In
this way, the conﬁdence in the hypothesis X2 < 7/2 would be strengthened.
So, we should keep in mind that AdaBoost can select several times a certain
weak hypothesis (but never at consecutive iterations, cf. CMU, 2015 fall, E.
Xing, Z. Bar-Joseph, HW4, pr. 2.1).

***************Ending Page***************


***************Beginning Page***************
***************page number:109**************
. . 109.
Graphs made by MSc student Sebastlan Clobanu (2018 fall)
The variation 0f at w.r.t. t: The two upper bounds 0f the empirical
error of HT:
311. °°.
8
$2
0% g‘ T eXP(—2;Yt2)
w L12‘
3 o
<5 err (HT)
C>_
0 10 20 30 40 0 10 20 30 40
iteration T

***************Ending Page***************

***************Beginning Page***************
***************page number:110**************
110.
AdaBoost and [non] empirical "y-weak learnability:
Exempliﬁcation on a dataset from R
CMU, 2012 fall, Tom Mitchell, Ziv Bar-Joseph, ﬁnal, pr. 8.a-e

***************Ending Page***************


***************Beginning Page***************
***************page number:111**************
111.

In this problem, we study how Ad-
aBoost performs on a very sim-
ple cla551ﬁcat10n problem shown 1n I I x

0 l 2 3 4 5 6
the nearby ﬁgure.
We use decision stump for each weak hypothesis hi. Decision stump classiﬁer
chooses a constant value s and classiﬁes all points where a: > s as one class
and other points where a: g s as the other class.
a. What is the initial weight that is assigned to each data point?
b. Show the decision boundary for the ﬁrst decision stump (indicate the
positive and negative side of the decision boundary).
c. Circle the point whose weight increases in the boosting process.
d. Write down the weight that is assigned to each data point after the ﬁrst
iteration of boosting algorithm.
e. Can boosting algorithm perfectly classify all the training examples? If no,
brieﬁy explain why. If yes, what is the minimum number of iterations?

***************Ending Page***************

***************Beginning Page***************
***************page number:112**************
112.
Answer . .
_ , Wlthout out51de threshold:
Wlth 0uts1de threshold:
0 t I 1 : h
' t I 1 ' 1/3 1 1/3 1/3
h
1 1/3 1/3 1/3 1 , 1 3 5
err: 1/3
1 3 5
+ _
err: 1/3 ' t I 2 : h
1/4 1/4 2 1/2
0 t : 2 I 1 3 5
1/4 h; 1/2 1/4 err: 1/2 err: 1/4
1 3 5 0 t I 3 : h
+ - - + + - 1/2 3 1/6 1/3
err: 1/2 err: 1/4 err: 1/4
1 3 5
. t I 3 3 err: 1/3 err: 1/2
h
1/6 1/3 3 1/2 . t : 4 2 h
3/8 1/s 4 1/2
l 3 5
+ - - + + - 1 3 5
err: 1/3 err: 1/2 err: 1/6 i + + i
err: 1/2 err: 3/8

***************Ending Page***************


***************Beginning Page***************
***************page number:113**************
Without outside threshold: 113-
0 o . . t : 1 l
Wlth outs1de threshold. 61 I 1/3 :> a1 I 1n\/§ z 0,3465? 6TT$(H1) z 1/3
‘15:13 “5:2:62:1/4;»@2:1n\/§:0.5493
61 I 1/3 I> 041 I ln\/§ I 0.3465, errS(H1) I —|---
1 3 — + +
0 t I 2 : —--|—
52 I 1/4:>O‘2 :1n\/§I()_5493 “=3; 83:15:51 IagIlnﬂI03465Ioz1
-|--- —l---
O41 — — a1 — + +
2 043 — + +
---|- —--l—
.1523; “:4;54:3/8ea4I1m/5/3zo2554
63 z 1/6 a 043 :1n \/5 I 0.8047 —l-I-
-|--- $1 l i +
2 — _
061 _ _ _ 043 _ + + I> errS(H4) _ 1/3
044 + + —
04 — + + I> 6TT’ H I O
2 S( 3) —--|—
043 + + —
It can be easily proven that the signs of x1 and
-|--- 51:3 will always be opposite to each other, while the
sign of m2 will always be +.
Therefore errs(HT) I 1/3 for any T € N*.

***************Ending Page***************

***************Beginning Page***************
***************page number:114**************
with outside threshold Without outside threshold 114‘
O 10 20 3O 4O O 10 20 3O 4O
Graph? mad;e by iteration iteration
Sebastlan Clobanu
g 2 eprZéytz)
z:- ijzi wit-22!?)
err (HT) errs(HT)
O 10 20 3O 4O 0 10 20 3O 40
T T

***************Ending Page***************


***************Beginning Page***************
***************page number:115**************
115.
Seeing AdaBoost as an optimization algorithm,
W.r.t. the [negative] exponential loss function
CMU, 2008 fall, Eric Xing, HWS, pr. 4.1.1
CMU, 2008 fall, Eric Xing, midterm, pr. 5.1

***************Ending Page***************

***************Beginning Page***************
***************page number:116**************
116.
At CMU, 2015 fall, Ziv Bar-Joseph, Eric Xing, HW4, pr. 2.1-5, part d, we have shown
that in AdaBoost, we try to [indirectly] minimize the training error errS(H) by sequen-
tially minimizing its upper bound H321 Zt, i.e. at each iteration t (1 g t g T) we choose
at so as to minimize Zt (viwed as a function of at).
Here you will see that another way to explain AdaBoost is by sequentially minimizing
the negative exponential loss:
d f 1 m 1 m T
e . t.
JT I g ZP/Xpewmm “é 5 ZQXPPMZQMW <8)
i=1 1:1 t:1
That is to say, at the t-th iteration (1 g t g T) we want to choose besides the appropriate
classiﬁer ht the corresponding weight at so that the overall loss Jt (accumulated up to
the t-th iteration) is minimized.
Prove that this [new] strategy will lead to the same update rule for at used in AdaBoost,
1 1 — e
i.e., at I — ln —t.
2 6t

Hint: You can use the fact that Dt(i) o< eXp(—y¢ft_1(xZ-)), and it [LC: the proportionality
factor] can be viewed as constant when we try to optimize J7; with respect to at in the
t-th iteration.

***************Ending Page***************


***************Beginning Page***************
***************page number:117**************
117.
Solution
At the t-th iteration, we have
1 m 1 m t—1
J I — — i i I — — i /h ’ i — z‘ h i
t mgeXm yft(sv)) mgeXp< y (2041: t($)> yat 15(97 )>
1 m 1 m t—1
Z E 2exp(—%ft_1(wi)) -exp(—y¢04tht($¢)) I E 2072111 Ztl) ‘1715(1) ' €XP(—%04tht(98¢))
OC Z 1915(2) ' GXp(—yiOéth/t (2%)) nét. J£(see CMU, 2015 fall, Z. Bar-Joseph, E. Xing, HW4, pr. 2.1-5, part b)
i=1
Further 0n, we can rewrite Jt’ as
J; I Z Dt(z') -exp(—y¢atht(aji)) z Z 1m) eXp(—ozt) + Z 1m) eXp(ozt)
2':1 16C iGM
z (1 — at) - 6-0“ —|— 6t - cat, (9)
where C is the set of examples which are correctly classiﬁed by ht, and M is the set 0f
examples which are mis-classiﬁed by ht.

***************Ending Page***************

***************Beginning Page***************
***************page number:118**************
118.
The relation (9) is identical with the expression (3) from part a of CMU, 2015
fall, Ziv Bar-Joseph, Eric Xing, HW4, pr. 2.1-5 (see the solution). Therefore,
1 1 —
When st is ﬁxed, Jt Will reach its minimum for at I 5 ln €—6t.
1:

***************Ending Page***************


***************Beginning Page***************
***************page number:119**************
119.
j — eps1=2/9
— eps2=1/7
— eps3=1/8
c\!
Graph made for the J; functions Q i
by Sebastian Ciobanu ‘- \
N \‘\
for CMU, 2015 fall, 00 ‘
Z. Bar-Joseph, E. Xing o‘ \ I
HW4, pr. 2.6 '
(Notation: 5:60‘) <0 \\\ \xq: : I _’,/"/
C5 \ |\“~--?--:--""
q: \\\\:\~ i : ‘___—""”ﬂ
o . ~-—'-o-—---“
1 2 3 4 5
beta

***************Ending Page***************

***************Beginning Page***************
***************page number:120**************
120.
Important Remark Input: {(zzti,y,-)}Z-:17___7m — the training dataset, T — the number
(in Romanian) of iterations to be executed, H — the set of weak hypotheses,
$(y,y’) I exp(—yy') — the exponential loss function.
Daca rescriem expresia Procedure:
(9) SUb forma Initialize the classifier f0(a:) I 0 and
6_at + 6 (eat e_o‘t) calculate the distribution D1(z') : 1/m for 2' : 1, . . . ,m
t — 7
T fortzltono:
1. Compute
se observa ca daca ﬁxam _ m
at atunci a minimiza ex- (ht, at) I arg mlnhEHyaeR Z My“ ft_1(x,-) + ah(:z:,-))
presia Jt in raport cu mm,—/
[alegerea lui] ht revine 1a _ _ Jt(h,o¢)
a minimiza 8t (care nu 2. Update the classn°|er ft(m) : ft_1(a:) +o¢tht(x)
depinde de at) and calculate the new distribution, Dt+1
A end for
Kidcrignsecznta, ailzgorflitmul return the classifier sign (fT(a;))
a oos poa e re-
or'rnulat ca un a1 oritm
{1e 0 timizare secvingﬁiala” The intuition is that, at each step, the algorithm greedily adds
a cozs9turilor / pierder- a hypothesis h 6 H to the current hypothesis to minimize the
ilor“ Jt. ” ¢_riSk'

***************Ending Page***************


***************Beginning Page***************
***************page number:121**************
121.
AdaBoost algorithm: the notion of [voting] margin;
some properties
CMU, 2016 spring, W. Cohen, N. Balcan, HW4, pr. 3.3

***************Ending Page***************

***************Beginning Page***************
***************page number:122**************
Despite that model complexity increases with each iteration, AdaBoost does not usually 122'
overﬁt. The reason behind this is that the model becomes more “conﬁdent” as we
increase the number of iterations. The “conﬁdence” can be expressed mathematically
as the [voting] margin. Recall that after the algorithm AdaBoost terminates with T
iterations the [output] classiﬁer is
T
HT($) I sign (Z at ht(x)> .
t:1
Similarly, we can deﬁne the intermediate weighted classiﬁer after k iterations as:
k:
Hk($) I sign (Z at ht(at)> .
12:1
As its output is either —1 or 1, it does not tell the conﬁdence of its judgement. Here,
without changing the decision rule, let
k:
Hk(:v) I sign (Z 07.5 ht(x)> ,
tIl
a
where at I ij—t so that the weights on each weak classiﬁer are normalized.
t’I1 at/

***************Ending Page***************


***************Beginning Page***************
***************page number:123**************
123.
Deﬁne the margin after the k-th iteration as [the sum of] the [normalized]
weights of ht voting correctly minus [the sum of] the [normalized] weights of
ht voting incorrectly.
Marginﬂm) I Z (it — Z (it.

tzht(w):y tzht(:l:);éy
a. Let fﬂx) nét' 2:le 64.; ht(a:). Show that Marginkﬁvi) I yz- fk<$¢) for all training
instances xi, with z' : 17 . . . ,m.
b. If Marginﬂxi) > Margink(xj), which of the samples $2- and xj will receive a
higher weight in iteration k —|— 1?

l

Hint: Use the relation Dk+1(z') I W -eXp(—yZ-fk(xZ-)) which was proven

m- t:1 t
at CMU, 2015 fall, Z. Bar-Joseph, E. Xing, HW4, pr. 2.2.

***************Ending Page***************

***************Beginning Page***************
***************page number:124**************
124.
Solution
a. We will prove the equality starting from its right hand side:
k; k
yifk(33i) I yi Zéthdﬂii) I 20-41% yz' ht(33i) I Z 541: — Z 071:
tzl 15:1 tzht(a:i):yi tzht(:13i)75yi

: Margin/Ami).
b. According to the relationship already proven at part a,

Margindwi) > Margin/Ami) <1> m fk(~’13i) > yj fk<33j) <1>

—Z/¢fl<:($¢) < —yj fk:($j) 4:) eXP(—y¢ fk:(113¢)) < eXP(—yj fk:($j))-

Based on the given Hint, it follows that Dk+1(i) < Dk+1(j).

***************Ending Page***************


***************Beginning Page***************
***************page number:125**************
125.
Important Remark

It can be shown that boosting tends to increase the margins of
training examples — see the relation (8) at CMU, 2008 fall, Eric

Xing, HWB, pr. 4.1.1 —, and that a large margin on training
examples reduces the generalization error.

Thus we can explain why, although the number of “parameters” of

the model created by AdaBoost increases with 2 at every iteration

— therefore complexity rises —, it usually doesn’t overﬁt.

***************Ending Page***************

***************Beginning Page***************
***************page number:126**************
126.
AdaBoost: a sufficient condition for y-weak learnability,
based 0n the voting margins
CMU, 2016 spring, W. Cohen, N. Balcan, HW4, pr. 3.1.4

***************Ending Page***************


***************Beginning Page***************
***************page number:127**************
127.

At CMU, 2015 fall, Z. Bar-Joseph, E. Xing, HW4, pr. 2.5 we encountered
the notion of empirical 'y-weak learnability. When this condition — 7 g y, for

, 1
all t, where 'yt déf 5 —et, with at being the weighted training error produced by
the weak hypothesis ht — is met, it ensures that AdaBoost will drive down
the training error quickly. However, this condition does not hold all the time.
In this problem we will prove a suﬂicient condition for empirical weak learn-
ability [to hold]. This condition refers to the notion of voting ma'r'gin which
was presented in CMU, 2016 spring, W. Cohen, N. Balcan, HW4, pr. 3.3.
Namely, we will prove that
if there is a constant 9 > O such that the [voting] margins of all training
instances are lower-bounded by (9 at each iteration of the AdaBoost algorithm,
then the property of empirical y-weak learnability is “guaranteed”, with *y :
6 / 2.

***************Ending Page***************

***************Beginning Page***************
***************page number:128**************
. . 128.
[Formahsatlon]
Suppose we are given a training set S I {($1,y1), . . . , (mm,ym)}, such that for
some weak hypotheses h1,.. . , hk from the hypothesis space H, and some non-
negative coefficients 041, . . . , 04k with 2L1 043- I 1, there exists (9 > O such that
k
ME wit-(1183)) Z 9, meyi) € S-
3:1
Note: according to CMU, 2016 spring, W. Cohen, N. Balcan, HW4, pr. 3.3,
k k
. not.
yAZ (why-(333)) I Margmﬂxi) I fk(x3), Where fk(333) I Zoqhﬂ-(azi).
3:1 3:1
Key idea: We will show that if the condition above is satisﬁed (for a given k),
then for any distribution D over S, there exists a hypothesis h; € {h1, . . . ,hk}
1 <9
with weighted training error at most 5 — 5 over the distribution D.
It will follow that When the condition above is satisﬁed for any k, the training
set S 1s emplrlcally y-weak learnable, Wlth y I 5.

***************Ending Page***************


***************Beginning Page***************
***************page number:129**************
129.
a. Show that — if the condition stated above is met — there exists a weak
hypothesis hl from {h1, . . . , hk} such that E¢~D[yihl(xZ-)] Z 9.
Hint: Taking expectation under the same distribution does not change the
inequality conditions.
b. Show that the inequality Eire D[yihl (2:1)] Z 6 is equivalent to
1 6
P ¢~ i h 1 < — — —,
1“ ply 7g l($)l_2 2
eerULZ)
meanlng that the welghted tralnlng error of h; 1s at most 5 — 5, and therefore
€
'Yt Z 5

***************Ending Page***************

***************Beginning Page***************
***************page number:130**************
_ 130.
Solutlon
a. Since yZ(2§:1 djhj(:cZ)) Z 6 (I) nyk,(xZ) Z 6 for 2' : 1, . . . ,m, it follows (according to the
Hint) that
k:
EZND11ZfZ(1Z)] 2 e where 11(11) “2' 201,1,(11). (10)
3:1
On the other side, EZND[yZ-hl(a:Z-)] Z 9 62g 2:11 yZ-hl(:1:Z-) -D(z') Z 6.
Suppose, on contrary, that EZ-ND[yZ-hl(:vZ-)] < 6, that is 2:11 yZ-hl(xZ-) D(1l) < Q for l I 1,. ..,l<:.
Then 2211 yZ-hl(wZ-) - D(7l) - 01; < € - 01; for l : 1,. ..,k. By summing up these inequations for
l:1,...,l<: we get
k m k m k k
ZZyZhl(xZ-) -D(1)-O1Z < Z9 - O1, e 21,0(1') (Z 1111,1011) < 92011:
1:1 1:1 1:1 1:1 1:1 1:1
Zy,f,,(1,) 13(1) < e, (11)
1:1

because 221:1 01, I 1 si fk<$1) nit 2le 01th (atZ)
The inequation (11) can be written as EZ-ND[ny;Z(aUZ)] < (9. Obviously, it contradicts the
relationship (10). Therefore, the previous supposition is false. In conclusion, there exist
l€ {1, . . .,k} such that EZ-ND[yZ-hl(xZ-)] Z €.

***************Ending Page***************


***************Beginning Page***************
***************page number:131**************
131.
Solution (cont’d)
b. We already said that E¢~D[yihl(a:¢)] Z t9 (I) 2:11 yz- hﬂxi) - 13(2) Z 9.
Since yi E {—1, +1} and hl(a:¢) 6 {—1, +1} for 2' I 1, . . . ,m and l I 1, . . . , kt, we have
2111mm) -D(@') 2 a e Z 0(a) _ Z 0(a) 2 64:) (1 _gl) _€l 2 6e
1:1 ilyizhﬂwz‘) iiyﬁéhﬂw)
1 Q e. 1 Q
1—2€ZZ@<:>2€Z§1—Q<I>61§5—562:; 6T7"D(hl)§§—§.

***************Ending Page***************

***************Beginning Page***************
***************page number:132**************
with outside threshold without outside threshold 132'
Graphs made by F. F-
Sebastian Ciobanu “a m_

lﬁloQQQQQQQ'M'Avaaeeaev.v».

for CMU, 2012 fall, 5 q é Q
T. Mitchell, z. Bar-Joseph, E E °
ﬁnal, pr. 8.a-e u? “I

[with outside threshold]
for CMU, 2006 spring, g
Carlos Guestrin, ﬁnal, pr. 3 g o

***************Ending Page***************


***************Beginning Page***************
***************page number:133**************
133.
AdaBoost:
Any set of consistently labelled instances from R
is empirically y-weak learnable,
using decision stumps
Liviu Ciortuz, following
Stanford, 2016 fall, Andrew Ng, John Duchi, HW2, pr. 6.abc

***************Ending Page***************

***************Beginning Page***************
***************page number:134**************
134.

At CMU, 2015, Z. Bar-Joseph, E. Xing, HW4, pr. 2.5 we encountered the

notion of empirical y-weak learnability. When this condition — y g 7t for all

t, where 'yt dIf 5 — at, w1th at belng the weighted tralnlng error produced by

the weak hypothesis ht — is met, it ensures that AdaBoost will drive down

the training error quickly.

In this problem we will assume that our input attribute vectors a: € R, that is,

they are one-dimensional, and we will show that [LC] when these vectors are
consitently labelled, decision stumps based on thresholding provide a weak-

learning guarantee (*y).

***************Ending Page***************


***************Beginning Page***************
***************page number:135**************
135.
Decision stumps: analytical deﬁnitions / formalization
Thresholding-based decision stumps can be seen as functions indexed by a
threshold s and sign +/—, such that
l if m Z s _ —l if x Z s

$5,433) _{ —1 ifg; < 5 and Q5843”) _{ 1 1m < 5.

Therefore, gbs,+(x) : —gbs,_(at).
Key idea for the proof
We will show that given a consistently labelled training set S I
{($1,y1),...,(a:m,ym)}, with 131- € R and yi € {—1,—|—l} for 2' I l,...,m, there
is some 'y > O such that for any distribution p deﬁned on this training set
there is a threshold s € R for which
1 1
errorp<¢s,+> s 5 — v or wwpws,» s 5 — v,

where errorp(gbs,+) and errorpwsr) denote the weighted training error of ¢s,+
and respectively gbsr, computed according to the distribution p.
We will start by analysing errorpwsu) and errorpwsr).

***************Ending Page***************

***************Beginning Page***************
***************page number:136**************
136.
Convention: In our problem we will assume that our training instances
$1,...xm € 1R are distinct. Moreover, we will assume (without loss of gen-
erality, but this makes the proof notationally simpler) that
331 <332 <...<33m.
a. Show that, given S, for any distribution p deﬁned on this training set, for
each m0 G {0,1,...,m}, and
for any s € [$m0,xm0+1) in case m0 Z 1, and respectively for any s € (-00,:c1) in
case m0 I 0,
it follows that
6 .
errorp(¢s,+) I 2P1-1{y,¢¢57+(wi)} I § + 5 (2191291 — Z MP1)
1:1 1:1 i:m0+1
m,—/
and not.: f(m0)
d f m 1 1 m0 m 1 1
6 .
@TTOTp(¢s,—) I 2P1'1{y1¢¢5,_(w1)} I 5 — 5 (2111191 —‘ 2 11/1191) I 5 — §f(m0)-
1:1 2:1 z:m0+1
Note: Treat sums over empty sets of indices as zero. Therefore, 2?:1 a,- : O
for any (1,, and similarly 2,11%, a, : 0.

***************Ending Page***************


***************Beginning Page***************
***************page number:137**************
137.
Comment
After having determined that
1 1 1 1
errorp(¢s,+) I 5 —|— §f(m0) and errorpwsr) I 5 — §f(m0),
. . . . 1 1
we w1ll look at the inequalltles errorp(¢sa+) 5 5 — y and errorp(q587_) g 5 — 'y.
1
The ﬁrst inequality gives §f(m0) g —'y (I) f(m0) § —2'y, while the second one
_ 1
gives —§f(m0) g —'y (I) f(m0) Z 2'y.
Putting them together, it suggests us to study whether given the dataset
S, there is y > O such that for any distribution p on S we can ﬁnd a value
m0 € {O,1,...,m} such as ]f(m0)] Z 27.

***************Ending Page***************

***************Beginning Page***************
***************page number:138**************
138.

b. Prove that, given S, for any set of probabilities p on the training set
(therefore pl- Z O and 221113 I 1) we can ﬁnd m0 € {0, . . . ,m} so as

1 m0 m

1:.
\f(m0)! 2 E’ Where f(m0) n; 2mm — Z yipi-
1:1 i:m0+l

Hint: Evaluate the expression |f(m0) — f(m0 — 1)|.
Note: Based on the properties [to be] proved at parts a and b, it follows
immediately that 'y dif 2— can be taken as a guarantee for the empzmcal week

m
learnab'ility of the dataset S, using AdaBoost with decision stumps.
c. Can you give an upper bound on the number of thresholded decision
stumps required to achieve zero error on a given training set?

***************Ending Page***************


***************Beginning Page***************
***************page number:139**************
139.
Solution
a. We perform several algebraic steps.
Let signﬁi) I 1 ift Z 0, and sign(t) I —1 otherwise.
Then
1{¢.,+<w>¢y} I lemme-ea} I 1{y-s¢gn<w—s>§0}>
where the symbol 1{ } denotes the well known indicator function.
Thus we have
— +
X1 Xm0($) S Xm0(5)+1 Xm
def. m m
i=1 1:1 incl-<5 21:21-25
Because sci g s for 2' g m0 and 95¢ > s for 2' > m0, we have
m0 m
@TTOTp(¢s,+) I 21%" 1{y¢:+1} + Z Pi'1{y.-:—1}-
1:1 i:m0+1

***************Ending Page***************

***************Beginning Page***************
***************page number:140**************
140.
Solution (cont’d)
Now we make a key observation:
we have 1 + 1
y — y
1{y=+1} I T and 1{y=—1}: T’
because y € {—1, 1}.
Consequently,
m0 1 + yr]: m l — yi
errorﬂqésﬁ) — 2291' ' T +. Z Pi ' T
1:1 z:m0+1
1 m 1 m0 1 m
I 52Pz+§2piyi — 5 Z szi
1:1 1:1 i:m0+1
1 1 m0 m 1 1
— 5 + 5 (219191 —_ Z 191%) — 5 + 5160710)-
121 z:m0+1
The last but one equality follows because 2111191‘ I 1.
The case for gbsr is symmetric to this one, so we omit the argument.

***************Ending Page***************


***************Beginning Page***************
***************page number:141**************
. 141.
Solutlon (cont’d)
b. For any m0 € {1, . . . ,m} we have
m0 m m0—1 m
f(m0) — f(m0 — 1) I 2mm — Z yipi — Z yipi + Z yipi I 231%me-
i=1 i=m0+1 1:1 1'sz
Therefore, |f(m0) — f(m0 — 1)| : 2|ymolpm0 : 2pm0 for all m0 € {1, . . . ,m}.
1
Because 2211191- : 1, there must be at least one index m6 with 29mg Z —.
m
Thus we have
2
|f<me> — f(m6 — 1>| 2 a» <12)
and so it must be the case that at least one of
VWW>i w WM—m>i (m
0 _ m 0 _ m
holds (see proof on the next slide).
Depending on which one of those two inequations in (13) is true, we would then “return”
1
m6 or mf)—1. (Note: If |f(m6—1)] Z — and m6 I 1, then we have to consider an “outside”
m
threshold, s < x1.)

***************Ending Page***************

***************Beginning Page***************
***************page number:142**************
142.
Proof of (13)
Reductio ad absurdum:
1 1
Suppose \f(mf))\ < — AND |f(m6 — 1)| < —. This is equivalent to say that
m m
1 , 1
__ < < _ 14
m f<m0> m < >
AND
—l<f(m' —1)<i
m 0 m'
The last double inequality is equivalent to
1 , 1
—— < — — 1 < —. 15
m f (m0 ) m ( )
By summing up the double inequalities (14) and (15), it follows that
2 , , 2
—— < — — l < —,
m f(m0) f(m0 l m
2
or otherwise said, |f(m6) — f(m6 — 1)| < —, which contradicts (12)!
m

***************Ending Page***************


***************Beginning Page***************
***************page number:143**************
143.
Summing up
At each iteration t executed by AdaBoost,
0 a probabilistic distribution p (denoted as Dt in CMU, 2015, Z. Bar-
Joseph, E. Xing, HW4, pr. 2.1-5) is in use;
o [at part b of the present exercise we proved that]
there is at least one m0 (better denoted m0 (19)) in {0, . . . ,m} such that
1 def. m0 m
|f(m0)| Z E’ Where f(m0) I Zyipi — Z yipi
1:1 izm0+1
0 [the proof made at part a of the present exercise implies that]
for any s G [scmo,wm0+1) if m0 Z 1, and respectively for any s G (—oo,a71) if
m0 I 0,
<¢ ><1 <¢ ><1 h ml
5 _—— or $__——, ere :—,
errorp 7+ 2 'y errorp 7 2 'y W *y 2m
As a consequence, AdaBoost can choose at each iteration a weak hypothesis
_ 1
(ht) for Wthh *yt Z y I —.
2m

***************Ending Page***************

***************Beginning Page***************
***************page number:144**************
144.
Solution (cont’d)
_ 1nm . . . . .
c. Boostlng takes at most W 1terat10ns to achleve zero [tralnlng] error, as
V

shown at CMU, 2015, Z. Bar-Joseph, E. Xing, HW4, pr. 2.5, so with decision

stumps we will achieve zero [training] error in at most 2m21nm iterations of
boosting.

Each iteration of boosting introduces a single new weak hypothesis, so at

most 2m21nm thresholded decision stumps are necessary.

***************Ending Page***************


***************Beginning Page***************
***************page number:145**************
145.
A generalized version 0f the AdaBoost algorithm
MIT, 2003 fall, Tommy Jaakkola, HW4, pr. 2.1-3

***************Ending Page***************

***************Beginning Page***************
***************page number:146**************
146.

Here we derive a boosting algorithm from a slightly more general perspective
than the AdaBoost algorithm in CMU, 2015 fall, Z. Bar-Joseph, E. Xing,
HW4, pr. 2.1-5, that will be applicable for a class of loss functions including
the exponential one.
The goal is to generate discriminant functions of the form

fT(:c) I a1h(ai; (91) —|— . . . —|— aTh(:c; €T),
where both a‘ belong to Rd, t9 are parameters, and you can assume that the
weak classiﬁers h(a:; 9) are decision stumps whose predictions are :|:1; any other
set of weak learners would be ﬁne without modiﬁcation.
We successively add components to the overall discriminant function in a
manner that will separate the estimation of [the parameters of] the weak
classiﬁers from the setting of the votes a to the extent possible.

***************Ending Page***************


***************Beginning Page***************
***************page number:147**************
O C 147.
A useful deﬁnltion
Let’s start by deﬁning a set 0f useful loss functions. The only restriction we place on
the loss is that it should be a monotonically decreasing and diﬁerentiable function 0f its
argument. The argument in our context is yi fT(:I:,) so that the more the discriminant
function agrees with the :|:1 label yi, the smaller the loss.
The simple exponential loss we have already considered [at CMU, 2015 fall, Z. Bar-
Joseph, E. Xing, HW4, pr. 2.1-5], i.e.,
LOSS(yifT(37i)) I eXP(—y¢fT(IB¢))

certainly conforms to this notion.

" W '7 ""4 ibgsigm'as»""_‘""*

r r , ~ r r , r ~ r , , r 315' sigma(z) ‘ '
Andsodoesthe logistic loss 73,7,,,,;,,,2,_757

-4 I-s -2 -1 ' o 1 2 3 4

Z

***************Ending Page***************

***************Beginning Page***************
***************page number:148**************
148.
Remark
Note that the logistic loss has a nice interpretation as a negative log-
probability. Indeed, [recall that] for an additive logistic regression model
1P< 11> 1 1 1<1+ <>>
—n :zc,w:—n—:n eX —z,
y 1 + eXp(—Z) p

where z I w1¢1(a:)+. . .+wT¢T(:c) and we omit the bias term (wO) for simplicity.
By replacing the additive combination of basis functions (oz-(33)) with the
combination of weak classiﬁers (Mas; (91)), we have an additive logistic regression
model where the weak classiﬁers serve as the basis functions. The difference is
that both the basis functions (weak classiﬁers) and the coefﬁcients multiplying
them will be estimated. In the logistic regression model we typically envision
a ﬁxed set of basis functions.

***************Ending Page***************


***************Beginning Page***************
***************page number:149**************
149.
Let us now try to derive the boosting algorithm in a manner that can acco-
modate any loss function of the type discussed above. To this end, at the
current iteration (t) we suppose that we have already included t—1 component
classiﬁers
ft—1<33) z ozlh(:c;§1) +...+at_1h(w;ét_1), (16)
and we wish to add another h(:l:;9). The estimation criterion for the overall
discriminant function, including the new component with votes oz, is given by
1 m
Jt(06, 9) : E 2:1 Loss(yift_1(ati) + yi oz h(a:i; 6)).
1,:
Note that we explicate only how the objective depends on the choice of the
last component and the corresponding votes since the parameters of the t — 1
previous components along with their votes have already been set and won’t
be modiﬁed further.

***************Ending Page***************

***************Beginning Page***************
***************page number:150**************
150.
We will ﬁrst try to ﬁnd the new component or parameters 9 so as to maximize
its potential in reducing the empirical loss, potential in the sense that we can
subsequently adjust the votes to actually reduce the empirical loss. More
precisely, we set 9 so as to minimize the derivative
a 1 m a
gJKOQ H)|o¢:0 I E g %LOSS(Z/ift—1($i) + 3/1‘ 04 h($¢; 9))|a=0
1 m
I 5 ZldL(y¢ft—1($i) + w a h<wte>>~yih<wiw>1|azo
1:1
1 m
I — Z dL<Uift—1(xi)) 1111' 1mm), (17)
m 1:1
8Loss
where dL(z) n25. —(Z) .
dz
(9
Note that this derivative 8—Jt(04,6)|a:0 precisely captures the amount by
04
which we would start to reduce the empirical loss if we gradually increased
the vote (04) for the new component with parameters 6. Minimizing this re-
duction seems like a sensible estimation criterion for the new component or
9. This plan permits us to ﬁrst set 6 and then subsequently optimize a to
actually minimize the empirical loss.

***************Ending Page***************


***************Beginning Page***************
***************page number:151**************
151.
Let’s rewrite the algorithm slightly to make it look more like a boosting al-
gorithm. First, let’s deﬁne the following weights and normalized weights on
the training examples:
Wit) I —dL(yift—1($i))a £01‘ and
These weights are guaranteed to be non-negative since the loss function is a
decreasing function of its argument (its derivative has to be negative or zero).

***************Ending Page***************

***************Beginning Page***************
***************page number:152**************
152.
Now we can rewrite the expression (l7) as
8 l m (t)
(9a t<04, )|a_0 m g Z 31 (5U )
m (t)
1 <2 <t> W
I —— W- ) 2+1” Min-'9)
J (t) ’
m j 1:1 23' Wj
1 (t) m ~ (t) .
I -E(ZWJ. )ZW yih(a:i,6).
y i=1
1
By ignoring the multiplicative constant (i.e., — 23- WJ-(t), which is constant at
m
iteration t), we will estimate Q by minimizing
_ m ~ (t) , _.
Em whom), <18)
i:1
where the normalized weights Wit) sum to 1. (This is the same as maximizing
the weighted agreement with the labels, i.e., 2:11 l/l/i(t)y¢h(a3i; 6).)

***************Ending Page***************


***************Beginning Page***************
***************page number:153**************
. . . 153.
Some remarks (by L1V1u Clortuz)
1. Using some familiar notations, we can write
Z W1(t)y¢h(wi;9) I Z WPyZ-hm; 9) + Z WftM-Mwi; @)
1:1 1'60 16M
160 16M
1—a e
:> 5t I l 1 _ ivy-(ﬂthﬂét)
2 .—1 2 z 11
2. Because Wig) Z 0 and 2:11 W1“) : 1, it follows that 2:11 Wi(t)yih(xi; €) € [—1,+1],
therefore
1 _ (EEC WP - 22m WP) I 1 _ Z WWW; 6}) e [0, +2], and so
1:1
_,—/
€[—17+1l
1 m ~ (t) . A
5 (1 — 21:1 W1- mama) e n+1]-

***************Ending Page***************

***************Beginning Page***************
***************page number:154**************
154.
We are now ready to cast the steps of the boosting algorithm in a form similar to the
AdaBoost algorithm given at CMU, 2015 fall, Z. Bar-Joseph, E. Xing, HW4, pr. 2.1-5.
~ 1
[Assume W10) I — and f0(xi) I O for 2' I 1, . . . .m.]
m
Step 1: Find any classiﬁer Mat; (92¢) that performs better than chance with respect to the
weighted training error:
_ 1 m ~ (t) . A
51: — 5 (1 — 2W1‘ y¢h(513z,9t)> - (19)
Step 2: Set the votes at for the new component by minimizing the overall empirical
loss:
A 1 m A _ A
Jt(04,9t) I E ;Loss(y¢ ft_1(xZ-) +yZ-ozh(:tZ-;9t)), and so at I argrlnzlg Jt(04,@t).

Step 3: Recompute the normalized weights for the next iteration according to

” (H1) _ , , , _. A - _

W1‘ _ —ct - dL(yZ ft_1(xl) —|— yZ at Mail, (9,5)) for 1, _ 17 . . . ,m, (20)

*,—/
3/1‘ ft (1%‘)

where ct is chosen so that 2:11 Wjﬁl) I 1.

***************Ending Page***************


***************Beginning Page***************
***************page number:155**************
155.
One more remark (by Liviu Ciortuz),
now concerning Step 1:
Normally there should be such 6t € (0,1/2) (in fact, some corresponding 6}),
because if for some h we would have 5t 6 (1/2,1), then we can take h’ I —h,
and the resulting a; would belong to (0,1/2).
The are only two emceptions, which correspond to the case when for any
hypothesis h we would have
— either at I 1/2, in which case ZieC Wigs) I 216M W710i)
— or at € {0,1}, in which case either h or h’ I —h is a perfect (therefore not
weak) classiﬁer for the given training data.

***************Ending Page***************

***************Beginning Page***************
***************page number:156**************
. . 156.

Exempllfylng Step 1 0n data from

CMU, 2012 fall, T. Mitchell, Z. Bar-Joseph, ﬁnal, pr. 8.a-e

[graphs made by MSc student Sebastian Ciobanu, FII, 2018 fall]
Iteration 1
"5| ‘El

—1O —5 O 5 1O —1O —5 O 5 10

theta theta

***************Ending Page***************


***************Beginning Page***************
***************page number:157**************
157.
[graphs made by MSc student Sebastian Ciobanu, FII, 2018 fall]

Iteration 2

v. <r.

ti o. ti o.

"2| ° ‘El °
I E
is ¢

C? U c?

<r. <r.

9 9

-10 -5 0 5 10 -10 -5 0 5 10
theta theta

***************Ending Page***************

***************Beginning Page***************
***************page number:158**************
158.
[graphs made by MSc student Sebastian Ciobanu, FII, 2018 fall]

Iteration 3

<r_

5 s c\!

% % O

I ' E

is ¢ O.
<r.
CI’

c?

-10 -5 0 5 10 -10 -5 0 5 10
theta theta

***************Ending Page***************


***************Beginning Page***************
***************page number:159**************
159.
a. Show that the three steps in the algorithm correspond exactly to AdaBoost
when the loss function is the exponential loss Loss(z) I eXp(—z).
More precisely, show that in this case the setting of at based on the new
weak classiﬁer and the weight update to get Vl/Z-(Hl) would be identical to
AdaBoost. (In CMU, 2015 fall, Z. Bar-Joseph, E. Xing, HW4, pr. 2.1-5,
WYH) corresponds to Dt+1(z').)
Solution

For the ﬁrst part, we will show that the minimization in Step 2 of the general
algorithm (LHS below), with Loss(z) I e_z, is the same as the minimization
performed by AdaBoost (RHS below), i.e. that

m m

. A . ~ t A
are £5313 21 LOSS(yift-1($i)+ 041mm; 90) I are {$18 21 W; )eXp(—04yih($i§ 60),
with (from AdaBoost)
Wit) I 615-1 'eXP(—yift—1(93i)),

where ct_1 is a normalization constant (weights sum to 1).

***************Ending Page***************

***************Beginning Page***************
***************page number:160**************
Solution (cont’d) 160'
Evaluating the objective in LHS gives:
Z LOSS(y1f1_1(w1-) + ay1h(w1; 91))
1:1
_ m r (22> 1 m~<1> _-
— ZGXP(—y1f1_1(931)) eXP(—@Z/1h($119t)) — JZWZ' eXp(—043J1h($1191>)
1:1 _ 1:1
1 ~ (1:) _ ~ <1)
1H l EA ( 1 Z 1
which is proportional to the objective minimized by AdaBoost (see CMU,
2008 fall, Eric Xing, HW3, pr. 4.1.1). Therefore, minimizing w.r.t the value
of 01 is the same for both algorithms.
For the second part, note that the weight assignment in Step 3 of the general
algorithm (for stage t) is
WW) I —ct -dL<1/1f1<11>> I (1 - eXp<—.11-f1<11>>,
which is the same as in AdaBoost (see CMU, 2015 fall, Z. Bar-Joseph, E.
Xing, HW4, pr. 2.2).

***************Ending Page***************


***************Beginning Page***************
***************page number:161**************
161.
b. Show that for any valid loss function of the type discussed above, the new component
h(sc; 6t) just added at the t-th iteration would have weighted training error exactly 1 / 2
relative to the updated weights Wz(t+1)'
Solution A
A d
At stage t, at is chosen to minimize Jt(oz, Qt), i.e. to solve w I 0. In general,
a
8 A 1 m a A
$11404, 9t) I R g %L055(yift—l<$i) + yZ-ahw; 91))
1 m A A m ~ A
I E Z dL(y¢ft-1(£U1:) + yiah($i§ 91)) yZ-hm; 91s) O< Z Wi(t+1)yih(xi§ 91:),
1:1 Wle-l) 1:1
—c—t
so that we must have Wi(t+1)yih(zci;ét) I 0.Then, the weighted training error for Mandi)
(relative to the updated weights thﬂ) determined by at) can be computed in a similarly
way to (19):
1 m ~ (1+1) A 1 1
5(1 _ 2W2. mug/1691)) z 5(1 _ 0) I 5.
1:1
\_,—/
0

***************Ending Page***************

***************Beginning Page***************
***************page number:162**************
CMU, 2008 fall, Eric Xing, HW3, pr. 4.1.1 162.
CM U, 2008 fall, Eric Xing, midterm, pr. 5.1
c. Now, suppose that we change the objective function to Jt : 2211(311 — ft(aci))2 and we
still want to optimize it sequentially.“ What is the new update rule for at?
Solution
We will compute the derivative of Jt I 21-11(yz- — ft(£cz))2 W.r.t. at and set it to zero to
ﬁnd the value of at.
(Wt @2in — ft(w¢))2 m (9(yi — ft($i))
8% (Mt 2 <y m >> (Mt
We also know that
ft($i) I ft—1($¢) + atht(37i)-
In this equation, ft_1 is independent of at. Substituting this in the derivative equation,
we get
3J1: m 8(3/1' — ft—1(513i)— atht($¢)) m
3041: gt? ft<$ )) 8% gw ft<$ ))( 1%(9? ))
a LC: Note that (yi — f(mi))2 I [yz(1 —yift(xq;))]2 I (1 —yZ-ft(mi))2 I (1 — z¢)2, where 27; I yift(a:i). The function
(1 — z)2 is derivable and convex; it is decreasing on (—oo, 1] and increasing on [1, +00).

***************Ending Page***************


***************Beginning Page***************
***************page number:163**************
163.
Solution (c0nt’d)
Setting the derivative to zero, we get
8Jt m m
8—04t I 0 (I) 2% — ft<$¢>>ht($1) I 0 <i> 2&1 (IthtW) — ft-1($i))ht($i) I 0
izl 2'21
m m T” 1' — - i h i
Em — item-whim I at Z him e at I M
1:1 izl Zizl ht (331)
1
1 m
(I) at I E 2%‘ — ft—1($¢))ht($i)-

***************Ending Page***************

***************Beginning Page***************
***************page number:164**************
164.

MIT, 2006 fall, Tommi Jaakkola, HW4, pr. 3.a

MIT, 2009 fall, Tommi Jaakkola, HW3, pr. 2.1
d. Show that if we use the logistic loss instead [of the exponential loss] the
unnormalized weights WZGH) are bounded by 1.

Solution
WY“) was deﬁned as —dL(yZ~ft(asi)), with dL(z) It 8—(1n(1 + e_z)). Therefore,
z
e_zi 1

***************Ending Page***************


***************Beginning Page***************
***************page number:165**************
e. When using the logistic loss, what are the normalized weights, l/l/Z-(H-l)? Express the 165'
weights as a function of the agreements yift(a3,-), where we have already included the
t-th weak learner.
What can you say about the resulting normalized weights for examples that are clearly
misclassiﬁed in comparison to those that are just slightly misclassiﬁed by the current
ensemble?
If the training data contains mislabeled examples, why do we prefer the logistic loss
over the exponential loss, Loss(z) I exp(—z)'?
Solution
The normalized weights are given by WétH) I ct - X1My—2ft($z)),with the normaliza-
1 1 + @XP(—yift($i))
. eXP(—y¢ft(fEi)) >
t10n constant ct I ( Ti — .
2P1 1 + 6Xp(—yift($i))
[Answer from MIT, 2011 fall, Leslie P. Kaelbling, HW5, pr. 1.1]
For clearly misclassiﬁed examples, y,- ft(x,-) is a large negative, so WYH) is close to [and
less than] 1, while for slightly misclassifed examples, VIC-(H1) is close t0 [and greater than]
1/2. Thus, the normalized weights for the two respective cases will be in a ratio of at
most 2 : 1, i.e. a single clearly misclassifed outlier will never be worth more than two
completely uncertain points. This is why boosting with logistic loss function is robust
to outliers.

***************Ending Page***************

***************Beginning Page***************
***************page number:166**************
166.
Solution (cont’d) in Romanian

LC: Pentru ultima parte de la punctul e nu am gasit deloc raspuns la MIT,
insa pot gandi astfel:

in cazul functiei de pierdere logistice, daca avem un 5132' care ar avea de drept
eticheta yz- I +1, dar se considera (in mod eronat) yz- I —1, pierderea este de
aproximativ fﬂxi) daca fﬂxi) > O,“ in vreme ce in cazul funcgiei de pierdere
[negativ] exponentiale pierderea este €Xp<ft($i)) care este in general mult mai
mare decat ft(a:Z). Cazurile simetrice (ft(:1;,) g O §i apoi yd I —1 —> +1) se
trateaza in mod similar.

a Vedetj graﬁcul fundgiei de pierdere logisticd din enun§ul problemei de fa§a.

***************Ending Page***************


***************Beginning Page***************
***************page number:167**************
167.
L. Cio'r'tuz, 2020
f. Suppose we use logistic loss. What is the update rule for at?
Solution (in Romanian); initially written by Stefan Matcovici (MSc student)
ln 10c sa minimizam Jt in raport cu argumentul a, vom incerca sa minimizam
in raport cu 04 0 margine superioard pentru diferenta dintre Jt §i Jt*_1, unde
Jt*_1 :nét' minhEH,a€R+ Jt_1(h,04). (Spre deosebire de Jt care depinde de 04, Jt*_1
nu depinde de 04. Prin urrnare, a minimiza Jt in raport cu 04 este echivalent
cu a minimiza Jt — JZ‘_1 in raport cu 04.)

***************Ending Page***************

***************Beginning Page***************
***************page number:168**************
168.
Jt - Jzil I 2mm + eXp<—yZ-ft<xi>>> — 2mm + exp<—yift_1<xi>>> I £2111 —11+:f§(’fyf}fj§fi)))
z 21“ 1 + eXE§_%f> £511 @1213?!LMZ-ftixfig) + €Xp(—yift(£131))
I gm <1 + 6Xp(_yift_l(iilljgﬁfiiljf)?WfHW))
I gm (1 + eXp(—y¢ft-1($i)i Tgigg/iogjgfjiQSXPPZ/ift-1(11%)))
g i; w ﬁindcﬁ 1n(1 + z) g ZNZ > -1. (22)

***************Ending Page***************


***************Beginning Page***************
***************page number:169**************
169.
Remarcagi faptul c5 logaritmul din expresia (21) existéi, intrucét
eXp(—yw¢h(%)) — 1
—>—1 <:> eX — iahxi —1>—eX Z- _1 51:1- —1
exp(yift_1(a:i)) + 1 p( y ( )) My f1: ( ))
41> 6Xp(—y¢04h($i)) > —eXP(yift—1($i)),
W _/—/
>0 <0
inegalitate adevéiratii pentru Va.
Expresia (22) pe care tocmai am 0b§inut-0 mai sus este marginea superioa'ré (engl.,
upper bound) pentru Jt — JZ‘_1 pe care 0 vom minimiza in raport cu a.

***************Ending Page***************

***************Beginning Page***************
***************page number:170**************
170.
Un calcul simplu ne aratﬁ 05
(9 _ —e_z 1 1
—ln(1+e Z):—_z:——:—z—. (23)
6g 1 + e i + 1 e + 1
6-Z
Stim din enunt c5 Wit) : —dL(yZ-ft_1(:ci)). Conform rela§iei (23), rezultéi CS1’
W-(t) : _< _ 1 > : 1
Z eXP(yift—1(ilYi)) + 1 eXP(y¢ft—1(33i)) + 1,
iar
~ 1
wit) I c _ . —7
1 t 1 exp<yift_1<wi>> + 1
unde ct_1 este constanta de normalizare a ponderilor Wig).

***************Ending Page***************


***************Beginning Page***************
***************page number:171**************
171.
Prin urmare,
1t — 1:21 s i éwxpeyiahw» — 1)
1.21 @Xp(yift-1(IB¢)) + 1
_ 1 m ~ (t)
— m g WZ- ‘(6XP(—yiO¢h($i)) — 1)
oc Z Wit) ~eXp(—yiozh(xZ-)) — Z Wit).
1:1 i=1
1
A§adar, a minimiza marginea superioaré pentru Jt — Jt*_1 revine la a minimiza (in
raport cu a) expresia 2:11 Wit) 'QXp(—y¢O£h(QZ¢)), pe care am intélnit-o §i la opti-
mizarea costului [negativ] exponential. Putem conchide cii §i in cazul functiei de
1 1 —
cost logistice putem alege at I 51m —6t.
6t

***************Ending Page***************


***************Beginning Page***************
***************page number:172**************
172.

MIT, 2006' fall, Tommi Jaakkola, HW4, p1“. 3.b
g. Suppose again that we use logistic loss and the training set is linearly
separable. We would like to use a linear support vector machine (no slack
penalties) as a base classiﬁer [LC: or any linear separator consitent with the
training data]. Assume that the generalized AdaBoost algorithm minimizes
the at weighted error at Step 1. In the ﬁrst boosting iteration, what would
the resulting d1 be?

***************Ending Page***************



***************Beginning Page***************
***************page number:173**************
173.
Solution
dJ 9
In Step 1, we pick 61. We wish to ﬁnd 61 to minimize %la:0.
04
Equivalently, this 61 is chosen to minimize the weighted sum: 261 — 1 :
m ~ ~ 1 . . .
—ZZ-:1W,(O)yih(xi;6), where W50) I — for all z I 1,2,...,m. If the tralnlng
m
set is linearly separable with offset, then the no-slack SVM problem is feasi-
ble. Hence, the base classiﬁer in this case will thus be an afﬁne (linear with
offset) separator h(-;61), which satisﬁes the inequality y,h(a3,~;¢91) Z 1 for all
1L: 1,2,...,m.
1 m
In Step 2, we pick a1 to minimize Jt(oq, 91) I — 21-21 L(y¢h0($,)+ozly,~h(a:,; 91)) I
m
1 m . .
— 22:1 L(o¢1y,-h(x,-; (91)). Note that Jt(o¢1, H1) 1s a sum of terms that are str1ctly
m
decreasing in 041 (as y,h(a3,-; 91) Z l); therefore, it itself is also strictly decreasing
in 041. It follows that the boosting algorithm with logistic loss will take 041 : oo
in order to minimize Jt(a1, (91).
This makes sense, because if we can ﬁnd a base classiﬁer that perfectly sepa-
rates the data, we will weight it as much as we can to minimize the boosting
loss. The lesson here is simple: when doing boosting, we need to use base
classiﬁers that are not powerful enough to perfectly separate the data.

***************Ending Page***************


