[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What is the primary goal of Support Vector Machines (SVM)?
--InteriorSeparator--
To find an optimal separating hyperplane that maximizes the distance from the closest points of either class.
--InteriorSeparator--
Easy
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to slide 6, under what conditions is a set S considered linearly separable?
--InteriorSeparator--
(right) There exist w ∈ Rd and w0 ∈ R such that yi(w·xi + w0) ≥ 1 for i = 1,...,m.
(wrong) There exists w ∈ Rd such that w·xi ≥ 1 for i = 1,...,m.
(right) Each point xi belongs to either of two classes, with label yi ∈ {-1,+1}.
(wrong) Each point xi belongs to either of two classes, with label yi ∈ {0,1}.
--InteriorSeparator--
Medium
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the geometric margin also known as?
--InteriorSeparator--
maximal margin
--InteriorSeparator--
Easy
--InteriorSeparator--
8
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the key components of the primal form of a linear SVM?
--InteriorSeparator--
(right) Minimizing ||w||^2
(wrong) Maximizing ||w||^2
(right) Subjecting the minimization to the constraint yi(w·xi + w0) ≥ 1 for i = 1,...,m.
(wrong) Subjecting the minimization to the constraint yi(w·xi + w0) ≤ 1 for i = 1,...,m.
--InteriorSeparator--
Medium
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the purpose of using Lagrange multipliers in the primal form of linear SVMs?
--InteriorSeparator--
To transform the constrained optimization problem into an equivalent "dual" form.
--InteriorSeparator--
Medium
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is maximized in the dual form of a linear SVM?
--InteriorSeparator--
(wrong) Σi=1 to m αi + ½ Σi=1 to m Σj=1 to m αiαjyiyj xi · xj
(right) Σi=1 to m αi - ½ Σi=1 to m Σj=1 to m αiαjyiyj xi · xj
(right) Subject to Σi=1 to m yiai = 0
(wrong) Subject to Σi=1 to m yiai ≥ 0
--InteriorSeparator--
Hard
--InteriorSeparator--
11
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What are support vectors?
--InteriorSeparator--
The closest points to the optimal separating hyperplane needed to define it.
--InteriorSeparator--
Easy
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the purpose of introducing slack variables (ξi) in linear SVMs?
--InteriorSeparator--
To allow for a small number of misclassified points when the data is not linearly separable.
--InteriorSeparator--
Medium
--InteriorSeparator--
13
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to slide 14, what are the constraints in the generalised OSH?
--InteriorSeparator--
(right) yi(w⋅xi + w0) ≥ 1 - ξi
(right) ξi ≥ 0
(wrong) yi(w⋅xi + w0) ≤ 1 - ξi
(wrong) ξi ≤ 0
--InteriorSeparator--
Medium
--InteriorSeparator--
14
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What role does the parameter C play in the soft margin SVM?
--InteriorSeparator--
It acts as a regularizing parameter, balancing the number of misclassified points and the minimum distance (margin).
--InteriorSeparator--
Medium
--InteriorSeparator--
15
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
In Non-Linear SVMs, what is a key advantage of mapping data points into a higher-dimensional space?
--InteriorSeparator--
(right) A linear separator is more likely to be constructed
(wrong) Computation becomes simpler
(right) It allows for the construction of non-linear decision boundaries in the input space
(wrong) Memory requirements decrease significantly
--InteriorSeparator--
Hard
--InteriorSeparator--
16
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the main advantage of using a kernel function K in non-linear SVMs?
--InteriorSeparator--
It allows computing the separating hyperplane without explicitly carrying out the map into the higher space.
--InteriorSeparator--
Medium
--InteriorSeparator--
18
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are common classes of kernel functions used in SVMs?
--InteriorSeparator--
(right) Polynomial
(right) RBF (radial basis function)
(wrong) Exponential
(wrong) Logarithmic
--InteriorSeparator--
Easy
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In practice, how are kernel functions tested for SVMs?
--InteriorSeparator--
By testing kernel functions about which we know that they correspond to the dot product in a certain space.
--InteriorSeparator--
Medium
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to slide 22, what condition must a symmetrical function K satisfy to represent a dot product (Mercer's Theorem)?
--InteriorSeparator--
(wrong) ∫∫ K(x, x') f(x) f(x') dx dx' < 0
(right) ∫∫ K(x, x') f(x) f(x') dx dx' ≥ 0
(right) For any function f such that ∫ f^2(x)dx is finite.
(wrong) For any function f such that ∫ f^2(x)dx is infinite.
--InteriorSeparator--
Hard
--InteriorSeparator--
22
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
If K1 and K2 are kernels over X × X, with X ⊆ R^n, is K(x,y) = aK1(x,y) also a kernel, where a ∈ R+?
--InteriorSeparator--
Yes
--InteriorSeparator--
Easy
--InteriorSeparator--
23
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the context of hand-written character recognition, what is the final output of SVMs, according to the diagram in slide 24?
--InteriorSeparator--
sign(Σ αi yi K(xi, x) + w0)
--InteriorSeparator--
Medium
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
In the XOR exercise (slide 25-28), given K(x,x') = (x · x' + 1)^2, what is the dimensionality of the space after mapping using Φ(x)?
--InteriorSeparator--
(wrong) R2
(right) R6
(wrong) R3
(wrong) R4
--InteriorSeparator--
Hard
--InteriorSeparator--
25
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the optimal separation hyperplane for the XOR problem, as found in the slides?
--InteriorSeparator--
-x1x2 = 0
--InteriorSeparator--
Hard
--InteriorSeparator--
27
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to the concluding remarks, what are the cons of using SVMs?
--InteriorSeparator--
(right) Require both positive and negative examples.
(wrong) Simple implementation
(right) Need to select a good kernel function.
(wrong) Low memory requirement
--InteriorSeparator--
Medium
--InteriorSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How can SVMs be adapted for multi-class classification?
--InteriorSeparator--
Using a one-against-the-rest approach, constructing a hyperplane between each class and the remaining classes.
--InteriorSeparator--
Medium
--InteriorSeparator--
30
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are SVM implementations mentioned in the presentation?
--InteriorSeparator--
(right) LIBSVM
(wrong) TensorFlow
(right) SVMlight
(wrong) PyTorch
--InteriorSeparator--
Easy
--InteriorSeparator--
31
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
According to slide 32, what does the SMO algorithm do?
--InteriorSeparator--
It selects pairs αi and αj to update and reoptimize W(α) with respect to them, holding other α's fixed.
--InteriorSeparator--
Hard
--InteriorSeparator--
32
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
In the update equations for the SMO algorithm, what does the parameter η represent?
--InteriorSeparator--
(wrong) η := w ⋅ xi + w0 - yk
(right) η := x'i + x'j - 2xi ⋅ xj
(wrong) η := Ei - Ej
(wrong) η := max(0, αj - αi - C)
--InteriorSeparator--
Hard
--InteriorSeparator--
33
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the context of the SMO algorithm, what geometric shape do the Lagrange multipliers lie on due to the inequality and linear equality constraints?
--InteriorSeparator--
A diagonal line segment within a box.
--InteriorSeparator--
Hard
--InteriorSeparator--
34
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to the derivation of the SMO algorithm's update equations, what is the expression for ∂W/∂α2 in terms of y, s, x1, and x2?
--InteriorSeparator--
(wrong) ∂W/∂α2 = -s + 1 + ½η(γ - sα2) + ½ηα2 + sγx1 + sγ (x1 - x2) + ½α1x1 + y2α2
(right) ∂W/∂α2 = -s + 1 + ½η(γ - sα2) - ½ηα2 - sγx1 - sγ (x1 - x2) + ½α1x1 - y2α2
(wrong) Where s = y1 * y2 and W is a matrix
(wrong) Where s = y1 / y2 and W is a matrix
--InteriorSeparator--
Hard
--InteriorSeparator--
36
--FlashCardSeparator--
}], role=model}, finishReason=STOP, avgLogprobs=-0.1666981940030959}]