***************Beginning Page***************
***************page number:1**************
1.
Instance Based Learning
Based 0n “Machine Learning”, T. Mitchell, McGRAW Hill, 1997, ch. 8
Acknowledgement:
The present slides are an adaptation of slides drawn by T. Mitchell

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2.
Key ideas:
training: simply store all training examples
classiﬁcation: compute only locally the target function
inductive bias:
the classiﬁcation of query / test instance :rq will be most similar
to the classiﬁcation of training instances that are nearby
Advantages:
can learn very complex target functions
training is very fast
don’t lose information
robust to noisy training
Disadvantages:
slow at query time
easily fooled by irrelevant attributes

***************Ending Page***************


***************Beginning Page***************
***************page number:3**************
3.
Methods
1. k-Nearest Neighbor;
Distance-weighted k-NN
2. A generalization of k-NN:
Locally weighted regression
3. Combining instance-based learning and neural networks:
Radial basis function networks

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
1. k-Nearest Neighbor Learning 4'
[ Evelyn Fix, Joseph Hodges, 1951; Thomas Cover, Peter Hart, 1967 l
Training:
Store all training examples
Classiﬁcation:
Given a query / test instance xq,
ﬁrst locate the Ak nearest training examples $1, . . . , 56k,
then estimate f(:cq):
0 in case of discrete-valued f : 3Q” —> V,
take a vote among its k nearest neighbors
k:
f (11(1) <- argmaXZ loo-aw}
vEV i:1
where 1{ .} is the well-known indicator function.
o in case of continuous-valued f,
take the mean of the f values of its k nearest neighbors
m e 1T

***************Ending Page***************


***************Beginning Page***************
***************page number:5**************
5.
Illustratrlng k-NN; Voron01 Dlagram
o
_|_ o
o
o
_|_ o
Note that Above: The decision surface induced
1-NN classiﬁes xq as + by 1-NN for a set of training examples.
5_NN class1ﬁes qu as _ The convex polygon surrounding each training
example indicates the region of the instance space
closest to that examples;
1-NN will assign them the same classiﬁcation as
the corresponding training example.

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
6.
When To Consider k-Nearest Neighbor
0 instances rnap t0 points in 3Q”
a less than 2O attributes per instance
0 lots 0f training data

***************Ending Page***************


***************Beginning Page***************
***************page number:7**************
7.
Efficient memory indexing
for the retrieval of the nearest neighbors
kd-trees ([Bentley, 1975] [Friedman, 1977])
Each leaf node stores a training instance. Nearby instances
are stored at the same (or nearby) nodes.
The internal nodes of the tree sort the new query atq to the
relevant leaf by testing selected attributes of :Eq.

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
8.
k-NN: The Curse of Dimensionality
Note: k-NN is easily mislead when X is highly-dimensional,
i.e. irrelevant attributes may dominate the decision!
Example:
Imagine instances described by n I 20 attributes, but only 2 are rel-
evant to the target function. Instances that have identical values for
the 2 attributes may be distant from xq in the 20-dimensional space.
Solution:
0 Stretch the j-th axis by weight zj, where Z1, . . .,zn are chosen so to
minimize the prediction error.
0 Use an approach similar to cross-validation to automatically choose
values for the weights 21, . . . , 2n (see [Moore and Lee, 1994]).
0 Note that setting 23- to zero eliminates this dimension altogether.

***************Ending Page***************


***************Beginning Page***************
***************page number:9**************
9.
A k-NN Variant: Distance-Weighted k-NN
We might want to weight nearer neighbors more heavily:
o for discrete-valued f: ﬂacq) e argmaxvev 2le 1026(0, f(xz))
where
_ 1
wi I d(a:q,xi)2
d(:13q,:zzz) is the distance between xq and $2-
but if azq I 3131' we take f(a:q) e ﬂat/L)
A k
o for continuous-valued f: f(:1:q) e W
Remark: Now it makes sense to use all training examples
instead of just k. In this case k-NN is known as Shepard’s
method (1968).

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
. . , 10.
A llnk to Bayes1an Learmng (Ch. 6)
k-NN: Behavior in the Limit
Let p(a:) be the probability that the instance :1: will be labeled
1 (positive) versus 0 (negative).
k-Nearest neighbor:
o If the number of training examples —> oo and k gets large,
k-NN approaches the Bayes optimal learner.
Bayes optimal: if p(:c) > 0.5 then predict 1, else O.
Nearest neighbor (k : 1):
o If the number of training examples e oo,
1-NN approaches the Gibbs algorithm.
Gibbs algorithm: with probability p(m) predict 1, else 0.

***************Ending Page***************


***************Beginning Page***************
***************page number:11**************
11.
2. Locally Weighted Regression
Note that k-NN forms a local approximation to f for each
query point ZEq
Why not form an explicit approximation f (:13) for the region
surrounding :cq:
o Fit a linear function (or: a quadratic function, a multi-
layer neural net, etc.) to k nearest neighbors
f: wO + w1a1(:13) —|— . . . + wnan(a';)
where a1(a:), . . . , an(:z:) are the attributes of the instance :13.
0 Produce a “piecewise approximation” to f, by learning
w07w17"'7wn

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
Minimizing the Error in Locally Weighted Regression 12'
o Squared error over k nearest neighbors
1 A
Elm) E 5 Z (fa) — fa)?
xG k nearest nbrs of asq
o Distance-weighted squared error over all neighbors
1 A
are) E 5 Zea) — fa)? mama)
mQD
where the “kernel” function K decreases over d($q, x)
o A combination of the above two:
1 A
Egaq) E 5 Z (fa) — fa)? Kama)
$6 k nearest nbrs of :cq
In this case, applying the gradient descent method, we obtain the
training rule wj e wj + ij, where
ij I W Z K(d(ivq, Iv))(f(f1¢) — f<$))@j(l‘)
:cG k nearest nbrs of wq

***************Ending Page***************


***************Beginning Page***************
***************page number:13**************
13.

Combining instance-based learning and neural networks:

3. Radial Basis Function Networks

o Compute a global approximation to the target function f , in terms of
linear combination of local approximations (“kernel” functions).

0 Closely related to distance-weighted regression, but “eager” instead
of “lazy”.

o Can be thought of as a different kind of (two-layer) neural networks:
The hidden units compute the values of kernel functions. The output
unit computes f as a liniar combination of kernel functions.

0 Used, e.g. for image classiﬁcation, where the assumption of spatially
local influencies is well-justified.

***************Ending Page***************


***************Beginning Page***************
***************page number:14**************
14.
Radial Basis Function Networks
f ( x) Target function:
Q k
f($) I wO + Z quu(d(atu, x))
uzl
W0 W] The kernel functions are commonly cho-
a a a a sen as Gaussians:
1 2
l _ ——2d (HZwiE)
Wk/ Ku<d<wm w» I e
)(/\ The activation of hidden units will be
. . . close to O unless 90 is close to mu.
61106) a (X) a (X) . . .
2 ” As 1t Wlll be shown on the next sllde, the
at are the attributes describ- two layers are trained separately (there-
ing the instances, fore more efﬁciently than in NNs).

***************Ending Page***************

***************Beginning Page***************
***************page number:15**************
15.
Training Radial Basis Function Networks
Q1: What JZU to use for each kernel function Ku(d(a3u, 113)):

o use the training instances;

o or scatter them throughout the instance space, either uni-
formly or non uniformly (reﬂecting the distribution of
training instances);

o or form prototypical clusters of instances, and take one Ku
centered at each cluster.

We can use the EM algorithm (see Ch. 6.12) to automati-
cally choose the mean (and perhaps variance) for each Ku.
Q2: How to train the weights:
o hold Ku ﬁxed, and train the linear output layer to get w,

***************Ending Page***************


***************Beginning Page***************
***************page number:16**************
16.
Theorem
l Hartman et al., 1990 l

The function f can be approximated with arbitrarily small
error, provided

— a sufficiently large k, and

— the width 03 0f each kernel Ku can be separately speci-

ﬁed.

***************Ending Page***************


***************Beginning Page***************
***************page number:17**************
17.
Remark
Instance-based learning was also applied to instance spaces X 75 31B”, usually
with rich symbolic logic descriptions. Retrieving similar instances in this
case is much more elaborate. It is
This learning method, known as Case-Based ReaSOIliIlg, was applied
for instance to
o conceptual design of mechanical devices,
based on a stored library of previous designs
[Sycara, 1992]
o reasoning about new legal cases,
based on previous rulings
[Ashley, 1990]
o scheduling problems,
by reusing/ combining portions of solutions to similar problems
[Veloso, 1992]

***************Ending Page***************

