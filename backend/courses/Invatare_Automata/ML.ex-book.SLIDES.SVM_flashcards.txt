[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What are the three main categories of Support Vector Machines (SVMs) discussed?
--InteriorSeparator--
Hard margin SVM, Soft margin SVM (C-SVM), and Other SVM-like Optimization problems.
--InteriorSeparator--
easy
--InteriorSeparator--
1
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the formula for the perpendicular distance of a point x0 to a hyperplane defined by wa + b = 0?
--InteriorSeparator--
d(x0, x*) = |w * x0 + b| / ||w||
--InteriorSeparator--
medium
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following is part of the primal form of the SVM optimization problem?
--InteriorSeparator--
(right) Minimizing 1/2 * ||w||^2
(right) Subject to yi(w * xi) >= 1 for all i
(wrong) Maximizing ||w||
(wrong) Subject to yi(w * xi) <= 1 for all i
--InteriorSeparator--
medium
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
Explain the geometrical interpretation of transforming the SVM optimization problem from equation (4) to equation (5).
--InteriorSeparator--
Corresponds to multiplying equations of linear separators of the training set by a positive (supra-unitary) constant, so that the minimum geometrical distance to training instances becomes 1.
--InteriorSeparator--
hard
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
Explain the geometrical interpretation of transforming the SVM optimization problem from equation (5) to equation (6).
--InteriorSeparator--
Corresponds to multiplying equations of linear separators of the training set by a positive (sub-unitary) constant, so that the minimum geometrical distance to training instances becomes 1.
--InteriorSeparator--
hard
--InteriorSeparator--
10
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the key components obtained from solving the SVM optimization problem in its primal form?
--InteriorSeparator--
(right) A linear model y(x) = w * x + w0 for classification
(wrong) A non-linear model for classification
(right) Support vectors, which are the closest instances to the separator
(wrong) Only the weights 'w' for classification
--InteriorSeparator--
medium
--InteriorSeparator--
14
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What are Lagrange multipliers (αi) also referred to as?
--InteriorSeparator--
Dual variables
--InteriorSeparator--
easy
--InteriorSeparator--
15
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
What is Slater's condition and why is it important in the context of SVM?
--InteriorSeparator--
Guarantees that there exists w and w0 such that (w * xi + w0)yi - 1 > 0 for all i. Ensures strong duality, meaning the primal and dual problems have the same optimal value.
--InteriorSeparator--
hard
--InteriorSeparator--
16
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to KKT conditions, what relationship can be deduced for any i where αi > 0?
--InteriorSeparator--
(right) w * xi + w0 = yi
(wrong) w * xi + w0 > yi
(wrong) w * xi + w0 < yi
(wrong) αi = 0
--InteriorSeparator--
hard
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
How is the dual form (D) of the SVM problem related to the primal form (P)?
--InteriorSeparator--
The dual form maximizes a function LD(α) subject to constraints like αi >= 0 and Σ αiyi = 0. The solutions to (D) are linked to (P) via relationships like w = Σ αixiyi.
--InteriorSeparator--
hard
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How is a new instance xnew classified using the SVM model?
--InteriorSeparator--
Classified as positive if f(xnew) = w * xnew + w0 >= 0, and negative otherwise.
--InteriorSeparator--
easy
--InteriorSeparator--
22
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What operations make using kernel functions possible in the context of SVM?
--InteriorSeparator--
Scalar product operations
--InteriorSeparator--
easy
--InteriorSeparator--
23
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What do slack variables (ξi) in C-SVM allow?
--InteriorSeparator--
Allowing some training data points to be misclassified.
--InteriorSeparator--
easy
--InteriorSeparator--
25
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
How does Slater's condition apply to the C-SVM optimization problem (P')?
--InteriorSeparator--
It is satisfied when there exists w ∈ Rd, w0 ∈ R, and ξ ∈ Rm such that (w * xi + w0)yi - 1 + ξi > 0 and ξi > 0 for all i.
--InteriorSeparator--
hard
--InteriorSeparator--
26
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the Lagrangian of the C-SVM problem, what do the Lagrange multipliers correspond to?
--InteriorSeparator--
αi >= 0 correspond to the constraints (w * xi + w0)yi >= 1 - ξi, and βi >= 0 correspond to the constraints ξi >= 0.
--InteriorSeparator--
medium
--InteriorSeparator--
27
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
What is the main difference between the dual form (D') for C-SVM and the dual form (D) for hard-margin SVM?
--InteriorSeparator--
C-SVM has an additional constraint, αi <= C, which limits the importance of any single training example in determining the optimal separator.
--InteriorSeparator--
hard
--InteriorSeparator--
30
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
In C-SVM, how can w0 be obtained from the relationships derived?
--InteriorSeparator--
(right) w0 = yi(1 - ξi) - w * xi, with ξi = 0 if αi < C
(wrong) w0 = w * xi - yi
(wrong) w0 is independent of support vectors
(wrong) w0 = yi - w * xi
--InteriorSeparator--
hard
--InteriorSeparator--
30
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
In C-SVM, how are support vectors identified given a solution to the dual problem (D')?
--InteriorSeparator--
Support vectors are the instances xi for which αi > 0. If αi ∈ (0, C), then yi(w * xi + w0) = 1.
--InteriorSeparator--
hard
--InteriorSeparator--
32
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the context of C-SVM, how is a new instance x' classified?
--InteriorSeparator--
Classified positively if w * x' + w0 >= 0, and negatively otherwise.
--InteriorSeparator--
easy
--InteriorSeparator--
33
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
How is the parameter Λ related to C in the unconstrained formulation of SVM using the hinge loss function?
--InteriorSeparator--
Λ = 2C
--InteriorSeparator--
hard
--InteriorSeparator--
38
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the key features of the unconstrained formulation of the C-SVM optimization problem?
--InteriorSeparator--
(right) Absence of restrictions on variables.
(right) Balancing simplicity (reflected by the term ||w||^2) and generalization (reflected by the term Λ Σmax(1 - yi(w * xi + w0), 0)).
(wrong) Only simplicity is pursued
(wrong) Presence of restrictions on variables
--InteriorSeparator--
medium
--InteriorSeparator--
38
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the Sequential Minimal Optimization (SMO) algorithm, what optimization strategy is used?
--InteriorSeparator--
Coordinate ascent
--InteriorSeparator--
medium
--InteriorSeparator--
41
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
What are the formulas for updating α1 and α2 in the SMO algorithm?
--InteriorSeparator--
α2_new,unclipped = α2_old + y2(E1 - E2) / η and α1_new,unclipped = α1_old + y1y2(α2_old - α2_new,unclipped) where η = -||φ(x1) - φ(x2)||^2
--InteriorSeparator--
hard
--InteriorSeparator--
41
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
How do you determine the bounds L and H for clipping α2 in the SMO algorithm if y1 != y2?
--InteriorSeparator--
L = max(0, α2 - α1) and H = min(C, C + α2 - α1)
--InteriorSeparator--
hard
--InteriorSeparator--
46
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
What are the complementarity (KKT) constraints used for stopping the SMO algorithm?
--InteriorSeparator--
NOT [(αi < C AND yiEi < -tol) OR (αi > 0 AND yiEi > tol)] for i = 1, ..., m, where tol is a positive constant.
--InteriorSeparator--
hard
--InteriorSeparator--
48
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
What is a sufficient condition to achieve zero training error in a SVM with RBF kernel, assuming distinct training instances?
--InteriorSeparator--
The RBF kernel parameter γ must be small enough, specifically γ < a / √ln(m-1) where ‘a’ is the minimum distance between any two training points and m is the number of training instances.
--InteriorSeparator--
hard
--InteriorSeparator--
62
--FlashCardSeparator--

--FlashCardSeparator--
Hard
--InteriorSeparator--
In one-class SVM with Max Margin, what is the primal optimization problem attempting to achieve?
--InteriorSeparator--
It seeks to find a direction w ∈ Rd that maximally separates the data from the origin.
--InteriorSeparator--
hard
--InteriorSeparator--
83
--FlashCardSeparator--
}], role=model}, finishReason=STOP, avgLogprobs=-0.17565178185125763}]