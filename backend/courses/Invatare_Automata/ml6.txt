***************Beginning Page***************
***************page number:1**************
1
Bayesian Learning
Based 0!! “lVIacllille Learning", Tl Mitchell, McGRAW Hilll, 1997, ch, e-
Acknowledgement:
The present slides are an adaptation of slides drawn by Tl Mitchell

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2
Two Roles for the Bayesian Methods in Learning
ll Provides practical learning algorithms
by cornllining prior knowledge/prub-abilities witli observed data:
I Naive Bayes learning algorithm
e Expectation Nlaximization (EM) learning algorithm (scheme):
learning ill the presence of unobserved variables
I Bayesian Belief Network learning
2r Provides a useful conceptual framework
0 Serves for evaluating ether learning algorithms, eg.
concept learning tlirongli general-to-speciﬁc hypotheses- ordering
(FINDS. and CAblllllATl-JELlMlNATloN),
neural networks, liniar regression
o Provides additional insight illto Occam’s razor

***************Ending Page***************

***************Beginning Page***************
***************page number:3**************
PLAN 3
1i Basic Notions
Bayes’ Theorem
Deﬁning elasses of hypotheses:
Nlaximum A posteriori Probability (MAP) hypotheses
Maximum Likelihood (ML) hypotheses
24 Learning MAP hypotheses
2.1 The lirnte force MAP hypotheses learning algorithm
2,2 The Bayes optimal classiﬁer;
2,3 The Gihhs classiﬁer;
2.4 The Naive Bayes and the Joint Bayes classiﬁers.
Example: Learning over text date using Naive Bayes
2,5 The Minirnnrn Description Length (MDL) Principle:
MDL hypotheses
si Learning ML hypotheses
3,1 ML hypotheses in learning real—valued riinetions
3.2 ML hypotheses in learning to prediet probabilities
3,3 The Expectation Maximization (EM) algorithm
4. Bayesian Belief Networks

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
4
Basic Notions
. Pruduct Rule:
probability iif a (iiiijiiiicliiiii iif two events A and B:
PM /\ m : mummy) Z P(U\A)P(A)
. Bayes‘ Theorem:
, , PWHWH)
P(A\B) W
. Theorem of total probability:
if events A‘ , . , . , A“ are mutually exclusive,
with 27:‘ Pill) *1, then
ma) I Z mama/3M)
,Zi
in particular
PLB) i Pwllnpml + PLBhAWhA)

***************Ending Page***************

***************Beginning Page***************
***************page number:5**************
Using Bayes’ Theorem for Hypothesis Learning
P D MP l1
P(h\n):i( P‘(I))( l
I P(D) I the (prior) probability of training data D
0 PM) I the (prior) probability of the hypothesis h
I P(D\h) I the (a posteriori) probability of D given ll
I P(/7\D) = the (a posteriori) probability of h given D

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
r»
Classes of Hypotheses

Maximum Likelihood (ML) hypothesis:

the hypothesis that best explains the training data

IIVL I argmax P(D\h,)
Mal-1'

Maximum A posteriori Probability (MAP) hypothesis:

the most probable hypothesis given the training data

I“, H, I argumethD) I argmax W I argnmx P(D\!1)P(h)
hgu 1,5” PM“ hgu

Note: If P(/1‘)I P(h,J).Vi,j7 then 1731.41’ I thL

***************Ending Page***************

***************Beginning Page***************
***************page number:7**************
7
Exemplifying MAP Hypotheses

Suppose the follnwing data characterize the lab result for cancernsuspect
people‘

PWW) n 0.00s Pffv'um'r'r') n00<12 h, fun/11,1172 n new,

P(+\t/mtr‘r) n 0.08 P[*\l,lwu:l) A102 D n 4+. eyPtD t It,).P(D t 1,2)

P(+\ WINFM):UU3 PH mulmltl') :th

Question: Should we diagnoze a patient , whose lab result is positive as
having canccr?
Answer: N0.
Indeed, we have te ﬁnd urgnIHX(P(I'm/I'M H). P(ntmn>er\+)}.
Applying Bayes theorem (for D = {Hy

P(+ t ("mm )P(t,ttm'r'r) n 0.08 >< 0.003 n 0.0070 7 ,.

P(+ t were)mnmmr) I 0.0:; >< 0.902 I 0 0298 9 W" * “W”

(We can infer ﬁnance?" \ +) I % : 21%)

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
a
E Learning MAP Hypothesis
2.1 The Brute Force MAP Hypothesis Learning Algorithm
Training:
Choose the hypothesis with the highest posterior proba-
bility
hMAP : argmax [KM/J) : argmax I’(l)\h)l’(h)
1ng hex-1
Testing:
Given r, compute hamphc)
Drawback:
Requires to compute all probabilities P(D\h) and PM).

***************Ending Page***************

***************Beginning Page***************
***************page number:9**************
2.2 The Bayes Optimal Classiﬁer: 9
The Most Probable Classiﬁcation of New Instances

So far we’ve sought lump, the most probable hypothesis given
the data I).

Question: Given new instance .r i the classiﬁcation of which
can take any value r, in some set V i, what is its most
probable classiﬁcation?

Answer: P(1'/\D) : EMU,P(1»J\}1,)P(ll,lD)

Therefore7 the Bayes optimal classiﬁcation of z is:
argmax Z P(1:,\h,)P(/:1\D)
‘V9’ mg!

Remark: IIMAPM) is not the most probable classiﬁcation of l1‘!

(See the next example.)

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
10
The Bayes Optimal Classiﬁer: An Example
Let us consider three possible hypotheses:
Pmm) I 0.4, FUND) : 0.3. PWW) I 0.3
Obviuusly, In,” : h‘.
Let’s consider an instance :1 such that
211(1) i +. 112(1) i i, 114(1)’ i
Question: What is the most probable classiﬁcatinn of 1,7
Answer:
PHMFQ P[+\h,>:1
P( MM) :1‘ m 4 v12) : n
P(i\71,):1. P(+\h_,) : O
Z P(+\71,)P(/1,\D) : 01and Z msmmmn) : 0.6
my h,cH
therefore
argnmxEIKFJVVJPUMUJ : i
4,10" Merl

***************Ending Page***************

***************Beginning Page***************
***************page number:11**************
ll
2.3 The Gibbs Classiﬁer
[Opper and Haussler7 1991]
Note: The Bayes optimal classiﬁer provides the best rennltr, but it can be
expensive if there are many hypotheses.
Gibbs algorithm:
1. Choose one hypothesis at random, according to FWD)
‘Z. Use this to Classify new instance
Surprising fact [Haussler et a1, 1994];
If the target concept is selected randomly according to the HMD)
distribution, then the expected error of Gibbs Classiﬁer is n0 worse
than twice the expected error of the Bayes optimal classiﬁer!
El‘ I I "I (atoll S YElﬂ'ml'ao,,».opt,mii1l

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
12
2.4 The Nalve Bayes Classd-"ler
When to use it:
a The target function l takes value frDm a ﬁnite set v I (m. all)
o lVInderate or large training data set is available
o The attributes < 1l| . ,uu > that describe instances are conditionally independent
WAKL w the given classiﬁcation:
mum) 11M): H will»)
The most probable value of 1m is:
F 11 .11». .1,“ l- P l»
WA” a alglll,le(ll,lu,./.IH1b,) a “gum M
Us Us PM”) n”)
a mglnrlAP(lll,llg . uﬂlﬂ,)P<l;,) a mglllﬂxHP(rl,il/'V)P(|'J) = ME
we" wav !
This is the suculled decision rule of the Naive Bayes classiﬁer.

***************Ending Page***************

***************Beginning Page***************
***************page number:13**************
13
The Joint Bayes Classiﬁer
WW : mgnmx [Xv/‘11,411 ..n,,):.
UH‘
I 1mm“ mm’, , 11,,\1‘,)P(1',]:mgmnxPQHJu . "WI/)2" pm

***************Ending Page***************


***************Beginning Page***************
***************page number:14**************
14
The Naive Bayes Classiﬁcation Algorithm
NAI\’E,BAYE$,LEARN(uwuplss)
for each value 1', of the output attribute
PM» <~ estimate P(L',)
for each value a, of each input attribute a
15(a,\'v,) <~ estimate P(a,\1‘l,)
CLAssIFY,NEW,INsTANCE(r)
0M, : argmaxwéy 15(1)!)1115! 15(aﬂzrj)

***************Ending Page***************

***************Beginning Page***************
***************page number:15**************
1r,
The Naive Bayes Classiﬁer: Remarks

1. Along with decision trees, neural networks7 k-nearest neigh-
bours, the Naive Bayes Classiﬁer is one of the most prac-
tical learning methods.

2. Compared to the previously presented learning algorithms,
the Naive Bayes Classiﬁer does no search through the hy-
pothesis space;
the output hypothesis is simply formed by estimating the
parameters 111:1). P(a,\z:,).

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
16
The Naive Bayes: An Example
Consider again the Plug/Tennis example, and new instance
(om/wk : sulLTrimp : (7)07‘ HUM/W : I!'/_r;h.W1m/: mung)

We compute:

PM“) i % i n s4 Pm») i % i n as

kisnvww) i g i u 33 P(stmng\no) i 1; i 0'00

PW“) PWMWM) PVWHH'IU PUuyhh/m) PU/W'th/M) i 0 17053

P(n0) P<m\m) P(('Iml\no) Pmmw) mnmmhw) :uuzuo'

~> UN, : no

***************Ending Page***************

***************Beginning Page***************
***************page number:17**************
17
A Note 0n The Conditional Independence
Assumption of Attributes
P(u1‘u2...1l,vll',) : HP(u,\U,)
/
It is often violated in practice ...but it works surprisingly well
anyway.
Note that we don’t need estimated pesterinrs [SQ/WI) t0 be
correct; we only need that
arguinxf’(v,) HPQMUJ : arg11|z\xP('u,)P(al . v ﬂank’)
mu" I w,<;\~'
[Domingos 1Q Pazzani, 1996] analyses this phenomenon.

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
l8
Naive Bayes Classiﬁcation:
The problem of unseen data
What if nene of the training instances with target value l’, have the at-
trihute value 11,?
It follows that Philly) e o, and 15(1',)1L15(ll,\1',) in
The typical solution is to (re)deﬁne Phi, l,-,). for each value r, of tn:
‘ m l m1) _
P(a,,|vj) <~ W’ wlieie
0 n is numher of training examples for which u : U1,
e n, numher of examples for which t» : ti, and (1:11,
0 1) is a prior estimate for malls)
(for instance. if tlie attribute n has Iv values, tlien [I s i)
o in is a weight given to that prior estimate
(i.e. nnrnher 0f “virtual” examples)

***************Ending Page***************

***************Beginning Page***************
***************page number:19**************
19
Using the Naive Bayes Learner:
Learning to Classify Text

a Learn which news articles are of interest

Target concept Intm'eslinr]? : Duczmwnl ~> {+., i}
I Learn to classify web pages by topic

Target concept Cnfsgnry : Dnmlmmt ~> {(1. . . Han}

Naive Bayes is among most elfective algorithms

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
Learning to Classify Text: Main Design Issues 20
1r Represent eaelr rloeument by a veetnr of words
e one attribute per word position in doeument
2r Learning:
0 use training examples to estimate m i 1. Pk), P(do(\ i ), whale)
e Naive Bayes eemlitienal imlepemlenee aseumptiom
‘VIN/1”‘(ﬁk)
mantle/i I H m, : iii liy)
,si
where P(a, : u-ili,) is probability that word in position i is ti, given ry
a Make one more assumption:
Vi, in Pin, r will,» r Pm," , Hilly) a Plum
Lei attributes are (not rmly imlep. but) also identically distributed

***************Ending Page***************

***************Beginning Page***************
***************page number:21**************
21
LEARN,NA[VELBAYES,TEXT(Emamples, Vocabulary)
ll Collect all words and other tukens that occur iu EHHIIpIIW
Von/ilmlmi/ <~ all distinct words and other tokens in FTIITIIFIIiQ‘
24 Calculate the required PM) and PUFMPJJ probability terms
For each target value it in v
(Ir/rat‘, < the subset of EJ'117771711'5 for which the target value is 1U
, Mimi
P(‘ I) P TEumipihil
Tm, <~ a single doc, created by concati all members of 11m,
ii < the total number 0f words in 1m,
For each word lrl in Vornhlllm y
m <~ the number (if thnes Wnﬂl in occurs iu TFI'n
v v 'H ii .
P(U,;, h“) e m (here we use the VH-eSUl'nale)

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
22
CLAssIFv,NAIVEBAYESJEXNDW)
[mm/1m H all wurd positiuns in Dm that Cuntain tokens frunl 17"(111111117'11-‘1/

***************Ending Page***************

***************Beginning Page***************
***************page number:23**************
Appllcatlon: Learnlng to Classlfy Usenet News Artlcles
Given 1000 training documnnts from each of the ‘20 newsgroups, loam to
classify new documents according to which newsgrnllp ii came from

compcgraphics misccforsalc
comp,osims-windowsimisc recanios
c0mp.sys.ibm.pc.hz\rdwarc rccmnotorcyclcs
comp.sys,mac.hardware remsporhbaseball
conlp,wind0wsix recisporhihockey
aiicnihcisna sci.space
cncrcligicnnchriniian suiicrypt
talk.rcligi0n.misc sci.clcctronics
talkpﬂlitichnideaSt Scimed
talkipoliticsimisc
talkpoliticsguns
Naive Bayes: 89% classiﬁcation accuracy (having used 2/1; of each group
fci- training; eliminated rare words, and ihc 100 lnosl. freq‘ words)

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
. 2\
Learnlng Curve for 20 Newsgroups
ZONeWS
TOO
90 7/?’
/é‘/,k ,,
so /i/7" V * "
70 ;;,,:%N Bayes %
60 'y FREE; "W
so
4o ,4”:
,+""/’
30 ,1,
7%
20
1D
O
‘DO ‘000 10000
Accuracy vs. Training set size

***************Ending Page***************

***************Beginning Page***************
***************page number:25**************
2.5 The Minimum Description Length Principle b
Oceanfs razor: prefer the shertest hypethesis
Bayes analysis: prefer the hypethesis limp
limp I eipnbnix lam/twat) I AYEXZHXUUQ2 Pmlh) + lug: P(h,))
I mgnnﬂI log‘ P(D\/1)Ilh;, P(h))
Ich
Interesting faet from the Inferrhatien Theory:
The eptirnal (sherteet expected coding length) code for an event
with preh-ahility p is the one using I log, 11 bits.
So we can interpret:
I 10g) P01): the length ef h under the eptirnal eede
lug) mule); the length ef u given h under the optimal code
Therefere we prefer the hypothesis h that minimizes...

***************Ending Page***************


***************Beginning Page***************
***************page number:26**************
2n
Bayes Analysis and the MDL Principle
We saw that a MAP learner prefers tiie hypothesis n that Ininimlzes
L001) + LL-l(Dlh), when‘ Lair) is the description length 0f i, under
encoding (7
)1411)y,:ill‘gllill\(Lpl(ll)+ L(,-!(Dl/i))
fvcll
Example: 11 : decision trees, L] : training data labels
0 Ln (h) is tiie number of bits to describe tree n
o LCJLDHI) is the number of bits to describe D given ll
In literature, the application of MDL to practical problems often include
arguments justifying the choice of tiie encodings (I1 and (‘ll

***************Ending Page***************

***************Beginning Page***************
***************page number:27**************
27

For instance:

unwlli) : u if examples are classiﬁed perfectly hy h,

and both the transmitter and the receiver knuw ll,

Therefore, in this situation we need nnly tn rleserihe exeeptinnsr Sn:

)Lum, i ﬂl'gmlMlmmlhﬂl] +1wrmmrsslrrwfn-nnone)
hQH

In general, MDL trades eff hypothesis size for training errors:

it might select a shorter hypothesis that rnnkes few errors ever a lnnger

hypothesis that perfectly classiﬁes the data!
Consequence: In learning (for instance) decision trees, (using) the MDL

principle can work as an alternative to pruning.

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
28
The MDL Principle: Back to Occam’s Rasor
MDL hypotheses are not necessarily also the
best/MAP ones‘
(For that, We should know all the probabilities P(D\h)
and FUN.)

***************Ending Page***************

***************Beginning Page***************
***************page number:29**************
29
l3 Learning Maximum Likelihood (ML) Hypothesis
3.1 Learning Real Valued Functions:

ML Hypotheses as Least Suquered Error Hypotheses
Problem: Consider learning a real-valued
, target function f X A 1R from D, a training
/ set consisting ofoxamplcs<l,.1l,), i : 1. 7H

>54; with
, /af~°“ 1,, assumed ﬁxed (to simplify)
;;// ,1, noisy training value <1, e ﬂit) + t‘,
1‘, is random variable (noise) drawn indc~
pendently for each 1,. according to soule
, ' Gaussian distribution with Inean:ll.

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
:10
Proposition
Considering 11, a certain class of functions h v X A 1R such that
/|(.r,) I f(r,) and assuming that r, are mutually independent given
h,
the maximum likelihood hypothesis MM is the one that minimizes
the sum of squared errors:
111/ . W 2
1,, : digUMXP D r, :ai>!|nn '1‘ em‘,
UL M < \ > BM” g] )J

***************Ending Page***************

***************Beginning Page***************
***************page number:31**************
Proof 31
Note: We will use the probability density function:
w , l
PM) L! llmHUZPﬂm S I < I" +0
hm : argnhmP(D\/1) :a!’gmm(Hp(ll,\/|) “1'”Mgnmnp(<>,\h)
W M H M H
hurlm
i rugumx Mir J, I, i (“mm “1,41 L, n
ht” EN /( N) it” EM ( H)
1 fitmmll 1 1(rl,*71(1,))2
i at 1mm iﬁ i iargmax luiﬁ i
fin ‘llMiZWﬂ’ W ‘2 WW 2 n J
1 d,*11<1»,))2 v
: ar "m ﬁ i :m‘ 1|!le i ‘1,41 r, ‘
ﬁw§2(n ay§< <>>
: Hr mm [if], r, 2
a, 2 I >>

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
12
Generalisations...
1i Similar derivations can be performed starting with other assumed
noise distributions (than Gaussians), producing dinereni resultsi
24 It was nssnrnod that
n the noiso affects only my), and
h no nniso was recorded in ilin attribute vnlnes for the given 0x-
amples 'I,.
Otherwise the analysis becomes signiﬁcantly more ooinplexi

***************Ending Page***************

***************Beginning Page***************
***************page number:33**************
33
3.2 ML hypotheses for Learning Probability Functions
Let us (:nnsider n nun-rieterlninistic function (Lei (vile-“biliary relatinn)
f . x > {0,1}.
Given a set of independently drawn examples
D : {<11:|,n'\ >.,., < 1,”,11", >} Where ti, : f[r,) e {0,1},
we would like to learn a ML hypothesis for the probability function 11(1) L"
PU (.1) i 1)l
For example, 110,) n 0.02 if P({<.'r,,1l, > it], n 1}) n 0.02.
Proposition: In this setting, 11A,,‘ I argmamwu Pip l 11) maximizes the Suln
Z;l‘[rl,th/I(f,)+(1*(lJl'H/(Ii/WLLJH.
Proof:
P(U\h):ll§’l‘1>‘[i,.ll,\Ii):ll§’l‘P(<l,\er,.h) m, l h)
It can be assumed that 'r, is independent of h, therefore:
P(D l h) n n;1,P(,l, l inn) - Pm

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
34
Proof (continued):
What we wanted to compute is hm : m1, : | if, h).
In a more general form:
i /i(i-,) if ii‘ I 1
“MM-h) ’ {1 /i(i,) if iz,:0
In a more Convcnicnt mathematical form: PM‘ \ 1,. ii) : hug/"(1 eh(i,))**<1i.
:>7UlL e ammﬂwiwﬁ'éi WWII*M'nDH'PLJi)
I umwwien “Iii/1(r'i)“'(l i /'(-ri))""' 1",’; PM)
, Winnie” Hz'iiliw'il e hum‘ "Y
I argmamhiH 2/1‘1n1i(i,)+(1eii,)zn(ie1i(i,>)]
,ei
Note: The quantity gjglwnmm 4 (1 imnu my is called erosee
entropy: the above hi“ minimizes this quantity.

***************Ending Page***************

***************Beginning Page***************
***************page number:35**************
13
3.3 The Expectation Maximization (EM) Algorithm
[Dempster el a1, 1977]
Find (local) Maximum Likelihood hypotheses when
data is only partially observable:
0 Unsupervised learning (lie. clustering):
the target value is unobservable
e Supervised learning:
some instance attributes arc unobservable
Some applications:
I Non-hierarchical clustering:
Estimate the means ofll Gausseans
I Learn Hidden lVlarkov klodels
0 Learn Probabilistic Context tree Grammars
I Train Radial Basis Function Networks
0 "main Bayesian Belief Networks

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
an
The General EM Problem
Given
I observed data X I {.r]. . . HLH}
independently generated using the parameterized
distributions/hypothcscs hl. l . . , h,,,
0 unobserved data Z I {21. . l . l 2",}
determine
i1 that (locally) maximizes P(X\h).

***************Ending Page***************

***************Beginning Page***************
***************page number:37**************
a7
The Essence of the EM Approach
Start with hm’7 an arbitrarily/conveniently chosen value of h.
Repeatedly
1. Use the observed data X and the current hypothesis
hm to estimate [the probabilities associated to the
values of] the unobserved variables Z, and further
on compute their expectations, EIZ].
2i The expected values of the unobserved variables Z
are used to calculate an improved hypothesis MM),
based on maximizing the mean of a log-likelihood
function: E[ln P(Y\/2)\X.h[‘>], where Y I {yh . . . _y,,,}
is the complete (observed and unobserved) data, i.e.
y‘ I (1,,21), for z‘:1_i,.,m.

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
The General EM Algorithm 38

Repeat the following two steps until convergence is reached:
Estimation (E) stcp:

Calculate the log likelihood function

Q(h\/1“))":’E[111P(Y\h>\Xv/I"‘l

where l’ : X LJ Z.
Maximization (M) step:

chlacc hypothesis hm by thc hypothesis MM) that maxi-

mizes this Q function.

NH“ <~ urginaxQ(/2\hw)

***************Ending Page***************


***************Beginning Page***************
***************page number:39**************
39
The EM algorithmic Schema
a idea: replace missing
vuhn's by mil-"mm ‘=0
value: ¢
o initialize parameters (o m
with “binary mm h —> E[ZIX, h 1
u rsﬂman" mmnng val»
he: baaenl on currmll
anmctcr values
. rcmimm WW7 ..\ l" P(X||'I)
Mew-s uaing the
complete data
- repeat the Previous m)_
quﬂbpsuntil I'm'nwy- H -argr;|ax EP(2|x;n‘") [In HX'ZIW
gmmi

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
1U
Methodology: How to apply an instance of the EM schema
W
‘W2 ‘=0
4129004“ ¢
w““ﬂw\“<'
h(‘)—> E[Zl x, hm]
E Step: apply
updating rule: (I)
H!
M Step: apply updaling rules (2)
h(x+1)_
h ;

***************Ending Page***************

***************Beginning Page***************
***************page number:41**************
11
Methodology: How to derive an instance of the EIVI schema
-< a (3m I Ba A urnmlu >
‘ ‘@an W. Mﬁmbimfw 1mm ”“""""‘
Em" |=0 (andmmenmnllnupormmanouwk)
9:36» ¢ \ variable! / dam
\ I

h'" E[2 | x, hm]
w [agilikellhnod o/Ihe hylmihexiv /
"obxzrvable" am parameters

A
m ln P(th) "rllle'll'llly“
[INK/ion
Q(h| n'")
h““'=ar max E In P X,Z|h
g h P1ZIX;h"') [\(_/)1
w/ u; mmpull 1h» Ing-Iikelihund
(4; Aulve mi; uplimiuniun pmblm 0f "WWW" 4m
luumlly “W ,Wmmmmnm. (2) apply expectalinn linearity
mm mlmmmo Iagrnnge" melhud/

***************Ending Page***************

***************Beginning Page***************
***************page number:42**************
12
Baum-Wclch Theorem
When Q is continuous, it can be shown that EM con-
verges t0 a stationary point (local maximum) 0f the
likelihood function P(X\h).

***************Ending Page***************

***************Beginning Page***************
***************page number:43**************
111
Two [particular] mixtures of Bernoulli distributions
LEW-""1" Z~Bern0u1li
A“ A“

1 u 1 u
X~""""""'1 X~"¢""'""'» X~Ecrnoullil 11412111011111“
1111 P1 1-0 29
WW; a 1-20

11/1 T/n
1 0
\' v13~ 11 I 1 B" zr 1
‘ N" """“‘ E H *7’) """“’ 3 X~7R11171111ll!(F/)i[li”)Ry'1r!1111lll(ZF/)

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
44
EM algorithm for solving [the general form of]
a mixture 0f two BCrnOulli distributions
lulllallull
llllhlllmll... Mm" ll l all" w» ll .mlllllll lll l l»
Mm pilmlll. l p sl Mm ll
z~mmaum (ml llmllv
lllllll l 7 0 1- 7 l lll 1 lull-l zvavls)
i lilll la": cind lug lllwllllllm damov obsewalzll: ll lml (vein Wllllclml
l o “ml:
X~liemnullil X~Bemoulliu will E llllllll l Z l. Cllzl.le.l,.l
m i WW, “ﬁll, w l,
W4 lll , 4J—W,,ﬁ Mull“ ,llmlll,“ WW‘ ‘HM
will M
lllll, >3; l.l".l
1 0 1’ ’ Zl'zlllt" ‘
q(l—ll : El": <17» “le
x ~ ﬁrspmllllllglpr Lynda] 1
(l n>lll,rllmlllll'q> WM" :izlilm l
Mum,”"ll.lll~ll.ll‘*"

***************Ending Page***************

***************Beginning Page***************
***************page number:45**************
EM algorithm for solving 4"
A mixture of vectors 0f independent and identically distributed (i.i.d.)
Bernoulli variables
EWDWWW
° Z~Hernaulli
‘W "m. "1,0 4mm mm!“ A n
§IX<L>LL3LL MU. rm’ mum MENU?‘
‘ um. ‘mo <59H\5T mm” x, Xi x,“ x. X. x."
I “"0 “*0 ' ‘ ‘ H ~n¢muum(e,g ~Bernuullx(ek)
gym “no “no '6an 45M"
w-m “mm WM
~‘—!
4,“,‘847055 H can x‘ x‘ x,“
@4 “yum
X ’ l'v nu! ~ W BHWH'UHH MY liwmmumw "n+ﬂ *T/ Irrvmmummml Ifl'vmmHvlrm "M
cr. “WM X. u“ mum “mm” Mummy", Chm; B. Du, 5M... “mm,
New", Bimechnulogy' M. 2e, m E, zuns. v.5. mm

***************Ending Page***************

***************Beginning Page***************
***************page number:46**************
4n
lnmahzale
mm we” BMW (Mir/)1" n. ‘mervﬂul w. l)j
Dem pmmeuh rm 5i mam a”,
Comm Itevahv
mu"! u Til (a. 'r mm nuns]
(sau péni cénd ‘ug'vemsnmhlatea daiebv observabﬂe nu max creste semmﬂcauv]
(m péni ma "Q" 0TH < E‘ M’; 441% < 5,
cu a hm in Mus)
execmi
stul E
pennu L : 1, r, cakmleazi
pm, wwwuiegumm
H‘ i H , , .
(54»,(1 *MP“ n HWPU i 921')“ Y‘
P5X;|':1*P:X:H\
Pa5u| M
s L W, a Q [1+1]
,gm : M 5‘ yg'“ Z w
2L. Pl?“ 2T7, Pf'é“
J n I, u
Returneazi 0/, ,uu‘ v

***************Ending Page***************

***************Beginning Page***************
***************page number:47**************
'17
A mixture of vectors of independent Bernoulli distributions,
applied m clustering of hand-written digits (MNIST)
Z~Cmegorical
m q 7 7 6 .7
l 2 K
4Q 950!
X X X X X X X X X
W’ 2. 3 :3 7
X (‘MUv 7015a‘: inwla, B Poems,
~er l
XI xl In

***************Ending Page***************

***************Beginning Page***************
***************page number:48**************
4x
The EM algorithm for solving this problem
lmuahzare
Embﬂm ovals" arbltvavcin \ntuvalul (o l} pcnuu pavamcnu 1n ,m 5| W‘. W“
(mm izevzmv
pentmv:0. 1' um Thxatinavzns]
(Sau pans rind lug'vewslmilltakea datelcr observable m. mal zle5£e gamma“)
(sen )
execuﬁ
Pasul E mm“ 1 H 5m 1 1: “We”:
n k ,1" m 7w
Mm mmﬂwfm A um 1' I‘
“h i 1‘ 0 ,w 7w‘
XFMJHMWS") ‘I MAP)‘ 0
Pasul M calculeazi
v M m
i .r H X
17W : W cu m Z 2:, “(11 ‘J, penuu k i l. ...1\.
1 A
1v‘ v
ik=iH ennuk |_ A‘
Z“ 1w P
m H n‘ penlm k *1. K
1m Hi“, pentm k i 1. A"
Returneali ‘n. . m slim, ,wl'ﬂ

***************Ending Page***************

***************Beginning Page***************
***************page number:49**************
19
E Bayesian Belief Networks
(also called Bayes Nets)
Interesting because:

0 The Naive Bayes assumption of conditional independence
of attributes is too restrictive.
(But it’s intractable without some such assumptions...)

0 Bayesian Belief networks describe conditional indepen-
dence among subsets of variables.

o It allows the combination of prior knowledge about
(in)depen(lencies among variables with observed training
data.

***************Ending Page***************

***************Beginning Page***************
***************page number:50**************
Conditional Independence do
Deﬁnition: X is conditionally independent of Y given Z if the
probability distribution governing X is independent of the
value of Y given a value of Z:
(V1,, y,, 1k) P(X :14)’ : y,. Z : 2k) I P(X : m2 : 21.)
More compactly, we write P(X\Y, Z) : P(X\Z)
Note: Naive Bayes uses conditional independence to justify
P(‘41_A2\V): P<A1\An/‘1P<AQW>: P<AW>P<A2|V>
Generalizing the above deﬁnition:
P(X1.Y.X;\Y1.Y.Y},,,Z| ...Z,,) : P(X1v..X;\Z| ...z,,)

***************Ending Page***************


***************Beginning Page***************
***************page number:51**************
51
A Bayes Net @ w
The network is deﬁned by
a A directed acyclic graph, represening a set of conditional independence
assertions:
Each node i representing a random variable i is asserted to be
conditionally independent of its nondescendants, given its immediate
predecessors‘
Example: 1’('1'/1uml:'7\Fvn'sH-Ulr’.ng/lin/ng] : 1’[Thum1v1lL/yllfnmy)
o A table of local conditional probabilities for each node/variable.

***************Ending Page***************

***************Beginning Page***************
***************page number:52**************
32

A Bayes Net (Cont’d)
represents the joint probability distribution over all
variables l] . Y2. . . .,Y,,:
This joint distribution is fully deﬁned by the graph,
plus the conditional probabilities:
P(1U1~~~,l/n) I P(Y1 I!!1~~>-YY;1 I y”) : HPU/ylpdrf'lltSO/J)

Iii

where Pm cuts(Y,) denotes immediate predecessors of Y,
in the graph.
In our example: P(Stm'm. lius'lwourGroup. . i . , Fo7'ﬁsfl"il‘€)

***************Ending Page***************

***************Beginning Page***************
***************page number:53**************
as
Inference in Bayesian Nets
Question: Given a Bayes net7 can one infer the probabilities of
values of one or more network variables, given the observed
values of (some) others?
Example:
Given the Bayes net P(L):O.4 ® P(F)d3 5
CDmPUte: \ P(S|L‘F)d].8
P(SI~L,F):O.5
(a) HS) P(S|L,~F)=0.6
(b) p(A_ 5') P(SI~L‘~F):D3
(b) PM) P(Al$)=0,7 ® P(GIS|=D8
P(A|~S):O.3 P(GI~S)A].2

***************Ending Page***************

***************Beginning Page***************
***************page number:54**************
34
Inference in Bayesian Nets (Cont’d)
Answer(s):

e If only one variable is of unknown (probability) value,
then it is easy to infer it

I In the general case, we can compute the probability dis-
tribution for any subset 0f network variables7 given the
distribution for any subset of the remaining variables.
But...

a The exact inference of probabilities for an arbitrary
Bayes net is an NP-bard problem!!

***************Ending Page***************

***************Beginning Page***************
***************page number:55**************
Inference in Bayesian Nets (Conﬂd)
In practice, We can succeed in many cases:

0 Exact inference methods work Well for some net structures.

0 Monte Carlo methods ‘"simulate’7 the network randomly
to calculate approximate solutions [Pradham 8L Dagurn,
1996i
(In theory even approximate inference of probabilities in
Bayes Nets can be NP-hard!! l Dagum Kc Luby, 1993])

***************Ending Page***************

***************Beginning Page***************
***************page number:56**************
an
Learning Bayes Nets (I)
There are several variants of this learning task
0 The network structure might he either known 0r unknown
(i.e., it has t0 be inferred from the training data).
0 The training examples might provide values of all network
variables7 or just for some of them.
The simplest case:
If the structure is known and we can observe the values
of all variables,
then it is easy to estimate the conditional probability table
entries (analogous t0 training a Naive Bayes classiﬁer).

***************Ending Page***************

***************Beginning Page***************
***************page number:57**************
a7
Learning Bayes Nets (II)
When
o the structure of the Bayes Net is known7 and
o the variables are only partially observable in the training
data
learning the entries in the conditional probabilities tables is
similar to (learning the weights of hidden units in) training a
neural network with hidden units:
e We can learn the net’s conditional probability tables using
the gradient ascent!
e Converge to the network h, that (locally) maximizes P(D|h).

***************Ending Page***************

***************Beginning Page***************
***************page number:58**************
Gradient Ascent for Bayes Nets T18
Let WW denote one entry in the conditional probability table
for the variable Y, in the network
uy,‘ I P(Y, I y,,\Parants(l/,) I the list a,‘ of values)
It can be shown (see the next two slides) that
i)liiP/,(D) i Z P;,(g/W MAM)
81(ng JED ug/k
therefore perform gradient ascent by repeatedly
1. update all uyjk using the 2. renormalize the 10ij to as-
training data D sure
“m k “@an 1)],(le/37Md) 2m,‘ : 1 and u g w,“ g 1
‘In; "/

***************Ending Page***************

***************Beginning Page***************
***************page number:59**************
59
Gradient Ascent for Bayes Nets: Calculus
0111MB) 0 alnmd) 1 MM)
,i I .iln P, d I ,i I if
0115M 0113M [[13] l( ) g 011:1,‘ éP/Kd) dug”

Summing over all values gm of Y,, and um of U, I Par'srLts(l/,):

01“ Mu) 1 a

i I if PM! y, numP/M, uuw)

0m- ,2; 1W) 0w,» 2 l ’ ’
l Z3
I Z l ZPnOllywuUMP”(y,,'\um)Ph(u,L/)
kn Phldl 0101/; 1W

Note that um, E Py,(y,/lu,k), therefore...

***************Ending Page***************

***************Beginning Page***************
***************page number:60**************
50
Gradient Ascent for Bayes Nets: Calculus (Cont’d)
611113,,(D) 1 0
i: iiPyl'w', ',.P,,
(710%- 2 P1,(d) aw!“ 1(1 \H/ HAW /L 1(llz)
l
: Zip/101W»MOP/1W4) (applying Bayes th.)
JED PW”
I Z 1 P/.(;v//-,uyk»\d)F’/.(11)Pz,(1m)
1ch PW’) P/KZ/U-YM)
Z Z 51(ywrmd)l’n(u,k) I Z PMywmd)
JED RAVI/NW») M, ELI/Mu“)
I Z H1(U11~“1L‘(i)
ch UM

***************Ending Page***************

***************Beginning Page***************
***************page number:61**************
m
Learning Bayes Nets (II7 Cont’d)
The EM algorithm can also be used.
Repeatedly:
1. Calculate/estimate from data the probabilities of unob-
served variables mw ,
assuming that the hypothesis h holds
2. Calculate a new h (i.e. new values of 11:1,‘) s0 to maximize
E[luP(Dl/1)],
where D now includes both the observed and the unob-
served variables.

***************Ending Page***************

***************Beginning Page***************
***************page number:62**************
m

Learning Bayes Nets (III)
When the structure is unknown, algorithms usually use
greedy search to trade off network complexity (add/substract
edges/nodes) against degree of ﬁt to the data.
Example: [Cooper & Herseovitz, 1992] the K2 algorithm:
When data is fully observable, use a score metric to choose
among alternative networksv
They report an experiment on (re-learning) a network with 37
nodes and 46 arcs describing anesthesia problems in a hospital
operating room. Using 3000 examples, the program succeeds
almost perfectly: it misses one arc and adds an arc which is
not in the original net‘

***************Ending Page***************

