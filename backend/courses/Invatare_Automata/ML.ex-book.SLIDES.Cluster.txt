***************Beginning Page***************
***************page number:1**************
1.
.
Clusterlng
Contains:
Hyerarchical clusterization: ex. 27, 33, 34, 6,
K-means algo. (Partitional clustering): ex. 40, 10, 12, 45, 51,
EM/GMM algo. (Model-based clustering): ex. 16, 18, 19, 20, 24, 64
from the 2023f version of the ML exercise book by L. Ciortuz et a1.

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
Hierarchical clustering

***************Ending Page***************


***************Beginning Page***************
***************page number:3**************
3.
Exemplifying the application 0f
hierarchical agglomerative clustering
(single-, complete- and average-linkage)
CMU, 2012 fall, Tom Mitchell, Ziv Bar-Joseph, HW4, pr. 2.21
extended by Liviu Ciortuz

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
4.
The table below is a distance matrix for 6 objects.
A B C D E F
A 0
B 0.12 0
C 0.51 0.25 0
D 0.84 0.16 0.14 0
E 0.28 0.77 0.70 0.45 0
F 0.34 0.61 0.93 0.20 0.67 0
Show the ﬁnal result of hierarchical clustering with single-,
complete- and average-linkage by drawing the corresponding den-
drograms.
N0t€2 To compute the average-linkage similarities, you may use the follow-
ing formula:
|X\d(X, Z) + |Y|d(Y, Z)
d X U Y, Z : —, 1
Where X, Y and X are mutually disjoint clusters. Prove this formula.

***************Ending Page***************


***************Beginning Page***************
***************page number:5**************
Solution: 5.
Single-linkage:
ABCDF E
A ABCDF 0
B C D E F E 0.28 0
AB 0
C 0.25 0 —
D 0.16 0.14 0 028 028
E 0.28 0.70 0.45 0 I I
F 0.34 0.93 0.20 0.67 0 0.26
0.24
AB CD E F
AB 0 0.22
CD 0.16 0 0.20 020
E 0.28 0.45 0 O18
F 0.34 0.20 0.67 0 I
0.10 0.16
ABCD E F Q14 014
ABCD 0
E 028 0 0.12 0.12
F 0.20 0.67 0 . , . . . . .
0 A B c D F E

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
Complete-linkage: 6.
ABF CDE
ABE 0
AB C D E F
AB 0 CDE 0.93 0
C 0.51 0 —
D 0.84 0.14 0 10
E 0.77 0.70 0.45 0 ' |
F 0.61 0.93 0.20 0.67 0 0.9 0-93
0.8
AB CD E F
AB 0 0-7 0.70
CD 0.84 0 06 Q61
E 0.77 0.70 0
F 0.61 0.93 0.67 0 0-5
0.4
ABF CD E 03
ABF 0 '
CD 0.93 0 0.2
E 0.77 0.70 0 0.1 Q12 0.14
O I -
A B F c D E

***************Ending Page***************


***************Beginning Page***************
***************page number:7**************
Average-linkage: 7.
AB 0 D E F d(AB,C) I d(A,C)-;d(B,C) I 051-5025 20.38
A; 02,8 0 d(AB,D) I d(A, D) -|2-d(B,D) I 084-5016 I 05
Z £5205 ‘31%5‘ 0215 0 d<AB»E> I —d<A’E) 2d”) I Lg?” I 0-525
F 0.475 0.93 0.20 0.67 0 d(AB,F) I d(A,F)J2rd(B,F) I 034-5061 I 0.475
AB CD E F d(AB,CD)<2 w I w I 0.44
ABCD ABOCD E F d(ABCD,E)(i) 2d(AB,E)1:2d(CD,E) 20525305752055
g 8:23 0%? 0 (“ABCDj F)@ 2d(AB, F) I 2d(CD, F) z 0.475 J; 0.565 :0_52

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
8.
Average-linkage (c0nt’d):
ABCDF E
4d ABCD E d F E 2.87
ABCDF 0 d(ABCDF, Efi) % I — I 0.574
E 0.574 0 5 5
0.54 0-55
0.52

0.48

0.42 0.44

0.36

0.30

0.24

0.18

0.12 0.12

0
A B C D F E

***************Ending Page***************


***************Beginning Page***************
***************page number:9**************
9.
Note: The proof given below for the (1) formula follows closely the one found in Prob-
lems and Solutions in Biological Sequence Analysis, Mark Borodovsky, Sfetlana Eki-
sheva, Cambridge University Press, 2006, pp. 192-193.
def. 1
A(X7Y) I w Z Z d(fv,y) (2)
1 || baggy
(2) 1
:>A(AUB,X) : — d(w,y)
<|A| + |B1>|X| 2;,
1
I — ﬁlmy) + (May)
<|AI + IB1> IXI <22 Z;
<2) 1
I —(\A\|X\A(A>X)+|B||X|A(B,X))
(IAI + lBl) IXI
1
I —(|A|A(A,X)+\B\A(B,X))
1A1+|B|
Therefore,
IXINX» Z) + \YINY, Z)
A(X UY, Z) : —.
1X l + 1Y1

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
10.
Hierachical [bottom-up] clustering
Ward’s metric
CMU, 2010 fall, Aarti Singh, HW3, pr. 4.1

***************Ending Page***************


***************Beginning Page***************
***************page number:11**************
11.

In this problem you will anlayze an alternative approach to quantify the
distance between two disjoint clusters, proposed by Joe H. Ward in 1963.
We will call it Ward’s metric.
Ward’s metric simply says that the distance between two disjoint clusters, X
and Y, is how much the Sllm Of squares will increase when we merge them.
More formally,

AOQY) I Z HIM — #XUYH2 — Z “951' — MXH2 — Z “331' — #Yll2 (3)

ZEZ'EXUY miEX 1Bi€Y
where ,uX is the centroid of cluster X (similarly, ,uy is the centroid of cluster
Y, and ,uXUy is the centroid of cluster XUY), and the notation at, E X denotes
a data point in the X cluster. By deﬁnition, here we will consider ,uX I
1

— Zmex xi, where nX is the number of elements in X.
TLX Z
A(X, Y) can be thought as the merging cost of combining clusters X and Y into
one cluster. That is, in agglomerative clustering those two clusters with the
lowest merging cost are merged, when Ward’s metric is used as a closeness
measure.

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
12.
a. Can you reduce the formula in equation (3) for A(X, Y) to a simpler form?
Give the simpliﬁed formula.
Hint: Your formula should be in terms of the cluster sizes (let’s denote them
as nX and ny) and the distance ||,uX — ,u,y||2 between cluster centroids ,LLX and
,uy only.

***************Ending Page***************


***************Beginning Page***************
***************page number:13**************
Solution 13.
NXQY) I Z Him- —#XUY‘|2 — Z H581- —#XHZ — Z Him- — Ml2
ZC¢€XUY $¢€X $¢€Y
I Z (513?; — #XLJY)2 — Z (332 — HX)2 — Z (516Z — [Lg/)2
wiGXLJY 13¢€X LL'Z'GY
I 51312 — ZHXUY ' Z 952' + Z 1“ng
ac- XLJY wiEXUY xiéXUY
05> X {QEX wZEX w EY xZEY 9:7;EY
6 2 n + n 2 n + n 2
(I) _ ( XILLX YHY) + ( XHX Y/lY) + ZManﬂX + Zuynyuy _ nxllig _ 12W?
71X —|— HY 71X + nY
_ ("XHX + "WM/)2 2 2
_ nX+nY +nXHX+nYHY
_(nXHX + "MM/)2 + ("XH%< + nY/lzy)(nX + RY) "XnY 2 2
I — I — _ 2 +
"X + ny 71X + ny (#X #XMY HY)
I wH _ H2
71X + ny HX HY -

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
14.
3 explicative notes
de . 1 no .
#ijcmzwi2>2$¢:|X‘/LX It nXpX (4)
{IMGX ZCZ'QX
def. 1 XmYzw 1 (
HXUY I — $1" I — 5131+ yz)
‘X w Z \XI + w Z Z
(4) 1 not. 1
I — X + Y z — n +n 5
(é) _ 2 Xﬁlfzw 2
MXUY Z £131" — ,LLXUY (nXUY MXUY) — nXUY MXUY — (71X + RY) MXUY
xiEXUY
(g) (nXHX + nY/iY)2 (6)
71X + RY .

***************Ending Page***************


***************Beginning Page***************
***************page number:15**************
l5.
b. Give an interpretation for Ward’s metric. What do you think it is trying
to achieve?
Hint: The simpliﬁed formula from above will be helpful to answer this part.
Solution:
With agglomerative hierarchical clustering, the sum of squares starts out at
zero (because every point is in its own cluster) and then grows as we merge
clusters. Ward’s metric aims to keep this growth in sum of squares as small
as possible. This is nice if we believe that the sum of squares (as a measure
of cluster coherence) should be small.
Notice that both nX and ny (in fact, [half of] their harmonic mean) show
up in A(X , Y) as computed at part a. The intuition is that given two pairs
of clusters whose centers are equally far apart, Ward’s method will prefer to
merge the smaller ones.

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
16.
c. Assume that you are given two pairs of clusters P1 and P2. The centers of
the two clusters in P1 are farther apart than the centers of the two clusters
in P2. Using Ward’s metric, does agglomerative clustering always choose to
merge the two clusters in P2 (those with less ‘distance’ between their centers)?
Why (not)? Justify your answer with a simple example.
Solution:
No, not always. Which pair will be merged also depends on the size of the
clusters. One simple counter example is where the size of the clusters in P1
are 1 and 99, respectively; and similarly 50 and 5O for P2.
_ Zany . .
Then the harmonic mean — of cluster s1zes 1s 2 - 0.99 for P1 and 2-25 for
TLX + RY
P2.
If the distance between the centers of the clusters in P1 is less than 25/0.99
: 25.(25) times the distance between the centers of the clusters in P2, then
A(P1) will be still smaller and so the clusters in P1 are merged.

***************Ending Page***************


***************Beginning Page***************
***************page number:17**************
17.
d. In clustering it is usually not trivial to decide what is the right number of
clusters the data falls into. Using Ward’s metric for agglomerative clustering,
can you come up with a simple heuristic to pick the number of clusters k7?
Solution:
Ward’s algorithm can give us a hint to pick a reasonable k: through the merging
cost. If the cost of merging increases a lot, it is probably going too far, and
losing a lot of structure.
So one possible heuristic is to keep reducing k until the cost jumps, and then
use the k right before the jump. In other words, pick the k just before the
merging cost takes off.

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
18.
Exemplifying the application 0f
hierarchical agglomerative clustering
using Ward’s metric
Liviu Ciortuz, 2018, using data from
Edinburgh, 2009 fall, C. Williams, V. Lavrenko, HW4, pr. 3

***************Ending Page***************


***************Beginning Page***************
***************page number:19**************
19.
La problema CMU, 2010 fall, Aarti Singh, HW3, pr. 4.1 am prezentat 0
funcgie de similaritate numitéi metrica luz' Ward. Potrivit acestei metrici,
distanga dintre douéi clustere disjuncte X §i Y se deﬁne§te astfel:
A(KY) I Z llwiwwm/ll2 — Z \lw¢—#xH2 — Z ||~"v@—/WH2 (7)
ZULEXUY miEX 932-EY
unde, spre exemplu, ,uX este centroidul [sau ,,centrul de greutate“ a1] clus-
terului X, iar 90¢ este 0 instan§5 genericé dintr-un cluster [0arecare, ﬁxat].
1
Prin deﬁnitie, aici vom considera ,uX I — Egg-ex mi, unde nX este numéirul de
71X Z
elemente din X. (Similar sunt deﬁniti centroizii ,uy §i ,LLXUy.)
Se poate aréita (vedeiﬁ tot problema CMU, 2010 fall, Aarti Singh, HW3, pr.
4.1) céi
anY 2
A X ,Y I — — . 8
( > nX+nYHHX HYH ()

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
20.
Observatiz':
1. Pentru perechi de clustere (X, Y) §i (X', Y’) astfel incét nX I nX/ §i ny I ny/
. . A anY nX/nY’ v v
— sau, mal general, atunm cand — I — — formula (8) arata ca
71X -|- 71y nX/ -|- ny/
la clusterlzare 1erarh1c5 este ,,fav0rlzat€1“ acea pereche pentru care centralzu
(MX §i ,uy, respectiv ,uX/ §i Myl) sunt mai apropiati.
2. Invers, dacéi ||,uX —,Lby|| I HMX’ —,uy/ H, atunci este favorizaté perechea pentru
. v anY . nX’nY’ . . v
care ponderea (adlca —, respectlv —) este mal mlca.
71X + RY nX/ + RY/
Aceste doué observatii v5 vor ajuta séi simpliﬁcaigi / reduceti foarte mult
calculele pe care ar trebui sﬁi 1e faceti pentru a rezolva urmétoarea ceringéi!

***************Ending Page***************


***************Beginning Page***************
***************page number:21**************
Aplicagi algoritmul de clusterizare ierarhicé aglomerativéi pe setul de date de la prob- 21'
lema Edinburgh, 2009 fall, C. Williams, V. Lavrenko, HW4, pr. 1,

A : (—4,—2),B : (—3,—2),C : (—2,—2),D : (—1,—2),E : (+1,—1)

F 1 (+1,+1),G ; (+2,+3),H; (+3,+2),I; (+3,+4),J ; (+4, +3)
utilizénd [de aceastéi datii] metrica lui Ward. Ca rezultat a1 clusterizérii, ve§i reprezenta
dendrograma sub formﬁ aplatizatii, folosind elipse (§i indici) pentru a indica clusterele
formate.

y
Commde rezultatul 0b§1nut alcl cu vreunul :
din rezultatele de la problema Edinburgh, 3 GI. J
2009 fall, C. Williams, V. Lavrenko, HW4,
pr.1? 2. i
Precizare: Dacé la 0 iteraigie a algoritmu- 1......- g g
lui de clusterizare distantgele (adicé simi- 5F f
laritéitile) dintre doué perechi de clustere i4 i3 —2 i1 5 1 1
au aceea§i valoare, ve§i considera (:5 prior- 0 1 2 3 4 X
itatea la alcétuirea noului cluster este dic- _1......-E
tatﬁ de ordinea alfabeticé. f
A B C D

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
22.
Soluigie
Iteratia 1:
Conform Observajiz'ez' 1 de mai sus, este suﬁcient s51 alegem 0 pereche de instanige aﬂate
la distaniga miniméi. Evident, perechile (AB), (B,C) §i (C, D) satisfac aceastéi condiigie.
Vom alege (A,B) datorité Precizdrz'z' din enun§. Agadar,
A B C D E F G H I J
A u
. . B -u

Acum, in matmcea de dzstante, vom C --u
completa (doar!) pozigiile situ- D ---u
ate imediat sub diagonala principaléi, E ----u
intrucét la iteratiile urmiitoare este F -----u
foarte probabil 05 vom avea nevoie G ------u
de ele.

H -------n

I --------Iil

J ---------II

***************Ending Page***************


***************Beginning Page***************
***************page number:23**************
23.
Pentru ﬁecare dintre iteragiile urméitoare, va ﬁ suﬁcient, in general, séi calculéim —
§i apoi sé comparém — distangele pentru douci perechz' de clustere (principalele doué
perechi!).
Iterafia 2:
2 —3.5 —2 2 2 —1.5 2 2 2 9
1
> 5 I M0}, {0})
—1.5
:> 02 I {0,0}, #02 :( _2 ).
Iterajia 3:
4 —3.5 —1.5 2 —2 2
A<{A»B}7{C>D}>—1H(_2)—(_2)H —H( 0 >H —4>1—A<{G}7{H}>
2 —1.5 1 2 2 —2.5 2 2 2 2 29
<{C}{}>3 _2) _1 3 _1) 3(65+>37534
29 5
F I 46 > 1 I A({G}7{H})
2.5
:> Cg : {G,H}, H03 : (2-5).

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
Iteratia 4: 24'
222222222 I Z H612) — (2W I Z HCEENF I §<222+222> I Z I 5 - 3 I2

> 1 I AW}, {JD

IQIMHMQIGQ-

Itemp'a 5:

2222222721» 22> I Z HG?) _ (312W I HCW I 2

I A({E}, {F}); vom utiliza din nou Preciza'rea din enuni;

I» C5 I {E,F}, #05 I ((1)).

Itemm 6:

WW» I 2 no» — (22> H2 I HG?) H2 I I

> 2 I A({G,H}, {1, J})

Observati §i <25 AGE, F}, {G7 H}) < A({E7F}, {0,17% intrucét lluE,F—#c,DH > \\#E,F—MG,H\\
I» 06 I {G,H,I,J}, #06 I (g).

***************Ending Page***************


***************Beginning Page***************
***************page number:25**************
25.
Iteratia7:

2-4 1 3 2 4 —2 2 4 52 1
A<{E7F}7{G,Hw-mH<O>—<3>1 —§H(_3)Hg-13——17§
> 4 I MA, B}, {0, 0})

—2.5
I>C7I{A,B,C,D}, #C7I( _2 ).
IteratiaS:

2-4 —2.5 1 2 4 —3.5 2 4
A<{A7B»@D}»{E»F}>——2+4 H< _2 )— (0)11 —§ 1K _2 )H —§<12-25+4>
2516.25:%@:@:213

3 3 4 3 3
1
>17§:A({E,F},{G,H,I,J})
:>Cg:{E,F,G,H,I,J}.
IteratiaQ:
C9:{A,B,C,D,E,F,G,H,I,J}.

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
26.
y
G
Concluzz'e: ‘I
Am 0b§inut acela§i rezultat 5C6
ca in cazul folosirii funcigiei = 5
de similaritate complete-
linkage (vedetji problema 4 3 2
Edinburgh, 2009 fall, C. ‘5 ‘5 f 2 5 4 x
Williams, V. Lavrenko, E
HW4, pr. 1). C8
A B C D
C7

***************Ending Page***************


***************Beginning Page***************
***************page number:27**************
27.
Exemplifying
the application of hierarchical divisive clustering
and the relationship between SliIlng-linkage hierarchies and
Minimum Spanning Trees (MSTs)
CMU, 2009 spring, Ziv Bar-Joseph, ﬁnal exam, pr. 9.3

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
28.
Hierarchical clustering may be bottom-up or top-down.
In this problem we will see whether a top-down clustering algo-
rithm can be exactly analogous to a bottom-up clustering algo-
rithm.
Consider the following top-down clustering algorithm:
1. Calculate the pairwise distance d(PZ-,Pj) between every two
objects H- and Pj in the set of objects to be clustered, and build a
complete graph on the set of objects with edge weights being the
corresponding distances.
2. Generate the Minimum Spanning Tree of the graph, i.e. choose
the subset of edges E’ with minimum sum of weights such that
G’ = (P, E’) is a single connected tree.
3. Throw out the edge with the heviest weight to generate two
disconnected trees corresponding to top level clusters.
4. Repeat the previous step recursively on the lower level clusters
to generate a top-down clustering on the set of n objects.

***************Ending Page***************


***************Beginning Page***************
***************page number:29**************
29.
a. Apply th1s algorltm on the dataset glven 1n
the nearby table, using the Euclidian distance.
b. Does this top-down algorithm perform analo-
gously to any bottom-up algorithm that you have -u-
encountered in class? Why? -uu

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
30.

Solution: y P6
a 12
Kruskal algorithm: 11
1. (P1,P2), cost 1, 10
2. (P4,P5), cost 2, 9 6V2
3. (P3,P5), cost 3, 8
4. (P2,P3), cost V17 7 P P
5. (P5,P6), cost 6\/§. 6 _3 3 5
Prim algorithm: 5 1/17 2
1. (P1,P2), cost 1, 4 P4;
2. (P2,P3), cost V17 3
3. (133.135), cost 3, 2 ‘1,11 1'2?
4. (P5,P4), cost 2, 1 1
5. (P5,P6), cost 6\/§. ‘ I ; ‘ I

0 1 2 3 4 5 6 7 s 9 10 11 12 x

***************Ending Page***************


***************Beginning Page***************
***************page number:31**************
3 1.
l C0
C4
C6
C3
P1 P2 P3 P4 P5 P6
(1,2) (2,2) (3,6) (6,4) (6,6) (12,12)

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
32.
Note: If there is only one MST for the given dataset, then both
Kruskal’s and Prim’s algorithm Will ﬁnd it. Otherwise, the two
algorithms can produce differents results.
One can see (both on this dataset and also in general) that
Kruskal’s algorithm is exactly analogous to the single-linkage
bottom-up clustering algorithm.
Therefore, there is indeed a bottom-up equivalent to the top-down
clustering algorithm presented in this exercise.

***************Ending Page***************


***************Beginning Page***************
***************page number:33**************
The K-means algorithm

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
34.
EXemplifinlg partitional clustering
using the K -means algorithm
T.U. Dresden, 2006 summer, Steffen Heldobler, Axel Grossmann, HW3

***************Ending Page***************


***************Beginning Page***************
***************page number:35**************
35.

Folositi algoritmul K-means §i distanta euclidiané pentru a grupa
urmiitoarele 8 instante din R2 in 3 clustere:

A(2,10), B(2,5), C(8,4), D(5,8), E(7,5), F(6,4), G(1,2), H(4,9).
Se vor lua drept centroizi initiali punctele A, D §i G.
a. Rulati prima iteratie a algoritmului K-means. Pe un grid de
valori 10 >< 10 veti marca instantele date, pozitiile centroizilor la
inceputul primei iteratii §i componenta ﬁecérui cluster la ﬁnalul
acestei iteratii. (Trasati mediatoarele segmentelor determinate de
centroizi, ca separatori ai clusterelor.)
b. Céte iteratii sunt necesare pentru ca algoritmul K-means séi
conveargfi? Desenati pe céte un grid rezultatul rulérii ﬁecérei
iteratii.

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
E E
'l] 1 E 3 4 5 E T" B Eil 'I'l]

***************Ending Page***************


***************Beginning Page***************
***************page number:37**************
To be used 6 6
Dataset: 3 3
C(874),D(5»8), 1ﬂl;;;;.;;;;m 1@;;;;;;;;;w
centroids: ?. ?
A, G, D. B 6

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
38.
Solution:
Iteration 0:
—u__
O O
#1 I (2, 10) 01 I {A}
O_ < , > ——— 0 _
M2 _ <5, 8) i» ——u- i 02 — {@QERH}
Mg I (1, 2) ———— cg I {5,0}
———-
———-
———-
Iteratz'on 1:
11% I #5’ I (2,10)
1_ 4+5+6+7+8 4+4+5+8+9 _(6 6) 0%:{A7H}
”2_ 5 ’ 5 _ ’ :>...i 021:{C,D,E,F}
2+1 5+2 Cl I 57G
#5 I (T?) I (1-5, 3-5) 3 { }

***************Ending Page***************


***************Beginning Page***************
***************page number:39**************
39.
Iteration 2:
2 _
M _ (3569521 0% I {A,D,H}
2 _

,ug I p; I (1.5, 3.5) 03 — {B’ G}

Iteration 3:
2 + 4 + 5 8 + 9 + 10

Hi’: (Ta?) :(11/379) C§:{A,D,H}:C12

Hg:(7713/3) :>...:> C§:{C,E,F}:CZQ I>St0p

u? I M3, I (1-5, 3-5)

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
11 I I I I I I I I I 40.
w ....'. . . @r . . . . . . ..
E
1 I I I I I I I I I
'l] 1 E 3 4 5 ‘El T" E- Eil 11]

***************Ending Page***************


***************Beginning Page***************
***************page number:41**************
41.
Exemplifying one property of K -means:
The clusterisation result depends on initialisation
CMU, 2006 spring, Carlos Guestrin, HW5, pr. 1

***************Ending Page***************

***************Beginning Page***************
***************page number:42**************
42.

a-b. Consider the data set in the following ﬁgures. The o symbols indicate

data points, while the crosses (><) indicate the current cluster centers. For

each one of these ﬁgures, show the progress of the K-means algorithm by
showing how the class centers move with each iteration until convergence.

For each iteration, indicate which data points will be associated with each

of the clusters, as well as the updated class centers. If during the cluster

update step, a cluster center has no points associated with it, it will not move.

Use / produce as many ﬁgures you need until convergence of the algorithm.

c. What does this imply about the behavior of the K-Means algorithm?

***************Ending Page***************


***************Beginning Page***************
***************page number:43**************
ALGORITMUL de clusterizare K-MEANS — Datele initiale ALGORITMUL de clusterizare K-MEANS - Datele initiale
1 1
' 0 ' 0
O . . O . .
o . o o . o . o o .
0 0 . 0 0 .
O O
O O
0 0 o 0
x x x X
X x
..‘ O ..‘ O
O O
o Q o Q
O O O O O O
O O
O O
O 1 0 1

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
ALGORITMUL de clusterizare K-MEANS — Datele initiale ALGORITMUL de clusterizare K-MEANS — lieratiaO
1 1
' 0 ' 0
Q . ' O ' '
0 ' 0 0 ' 0 ' 0 0 '
0 0 . 0 0 '
O O
O O
0 0 0 ¢
X X X X
Q’, Q Q.‘ Q
s’ s’
O O
, 0 0 0 0 0 0
Solut 10H‘ ' '
° o 0
O 1 0 1
a‘ . ALGORITMUL de clusterizare K-MEANS — lteratia1 ALGORITMUL de clusierizare K-MEANS - lieratia1
1 1
9 P . ' ' .
i , , \ 0 Q 0
\ ‘ . / \ /
\ ‘o , m a / o 0 0
\ ‘I , / \\ I . . . '
\ ‘ / / . ' \1 / / '
\ l 16/ /// \ \ */ ' x
\ 1 1 / / ‘ \ \ 7
\ 11:)1/ // \v/‘V 1 . ////’ .
\W/J/ . . // " .
\ H 1 / / ’
1 / ~ / / /
\H 1/ ‘ / / / /
\ﬂ/waxx 1 41/4:
NW 1 X ‘
4,111‘ 1
u \
1
I’ 11,11’ \ 1
/ 1‘ ‘v \ 1 .'
/ U‘ l '
0 ~ ~ 1 0 Q
O 1 O O O O O
Q Q
O 0
0 1 O 1

***************Ending Page***************


***************Beginning Page***************
***************page number:45**************
45.
ALGORITMUL de clusterizare K-MEANS — |teratia2 ALGORITMUL de clusterizare K-MEANS — |1eratia2
1 1
9 Q . ' - .
9 1 \ \ - ' '
\ \ /
\ p,’ 0 1 0/ ' 0' 0 0 .
\ . I / ' \\ \ . - »* / ' . z x
‘ ‘1/.’ \\\\ V\ . ///’
“Ml \\* //>V< \\ /// /
‘ H1/ / I \ / ‘ ' /x / ’ .
\ My!’ \ / /‘/
‘ "H/ 5 \ / / / /
\M / X \ / / /
MK \ / C ’
1: \ 4 /
111‘\\\ \
\\ \
/ \ \ \ a
' \\ ' \ ' .
/ “Q \ O
0 N \ \ 0 Q
O v O O O O
O O
O O
O 1 O 1
ALGORITMUL de clusterizare K-MEANS - lteratiaS ALGORITMUL de clusierizare K-MEANS — lieratia3
1 1
' 0 ' 0
' \ / 0 ' o
\ \ / \ O O
\ \ / \ /
0&3. 0/ . 0' 0 0 .
r” , ‘My 0 xo-i» ~14;
/ \ 1 \ / /
' \ / ‘ \\ ' / //
\\ X /’ 1 ' / / .
, /
X 0 0 , / g 0
\ / /
\\ / //
\\ / ’
/
>:< \\ 1/ ,’/
E \\ / /
\ / /
/
.\» ‘O \\ . ‘Q
/ \
I ﬁ \ Q §
Q \ O O O Q O
Q O
O 0
O 1 0 1

***************Ending Page***************

***************Beginning Page***************
***************page number:46**************
ALGORITMUL de clusterizare K-MEANS — lteratia4 ALGORITMUL de clusterizare K-MEANS — |teratia4
1 1
0 . . 0
\ / O O
0 \ \ \ / / \ 0 0 0
\ \ \ I
0 / ><0 x / 0 0 \ \ \ \
O \ , , / ~ \O
. \ * . \ \ \x
\ \ /
/ / 1‘ . , / .
0 0 I a 0
1 / / ’
I \ /
/
i l /
1 , /
1 l /
1 I / /
1 , /
/
l I /
1K‘ ‘ x‘
O \ ‘ O
O \ O O O O O
O O
O 0
O 1 0 1

***************Ending Page***************


***************Beginning Page***************
***************page number:47**************
ALGORITMUL de clusterizare K-MEANS — Datele initiale ALGORITMUL de clusterizare K-MEANS — lieratiaO
1 1
O . . O . '
¢ 0 ¢ 0
0 ' 0 0 ' 0 ' 0 0 '
o 0 . 0 '
O
O O
0 0 0 ¢
X x
.,—
x x x x
Q’, Q Q.‘ Q
s’ s’
Q O
, 0 0 0 0 0 0
Solut 10H‘ ' '
° o 0
O 1 0 1
b . ALGORITMUL de clusterizare K-MEANS — lteratia1 ALGORITMUL de clusierizare K-MEANS - lieratia1
1 1
O \ \ . . O . .
O \ \ \ ‘ \ \ X , / , Q 0 Q
\ \ \ \ \ / , f , z ,
' ,:<!I::\;>*;/Z g : I j 9i ¥ . . . . X ' .
'/ I: \\\\\\\\\\ ' // \\
\ \ \ \ ' / \\ '
5 0 ' ,’ \ 0 '
5 / \
X / \
\
1X /
Q \ O I Q O ‘ ‘I
Q O
O 0
0 1 0 1

***************Ending Page***************

***************Beginning Page***************
***************page number:48**************
48.
Solution [in Romanian]: c.

Este evident din acest exercitiu ca rezultatul algoritmului K-means depinde
de pozitionarea initiala a centroizilor. in cazul initializarii de la punctul a au
fost necesare 4 iteratii pana a se ajunge la convergenta, pe cand la punctul b
algoritmul a convers dupa doar o iteratie.

Mai este inca ceva important de remarcat: faptul ca la prima varianta de
initializare, punctul din dreapta jos, care este un outlier (rom., exceptie, caz
particular, aberatie) este pana la urma asociat clusterului format de punctele
din partea dreapta (sus), in vreme ce la cea de-a doua varianta de initializare
el constituie un cluster aparte/ “singleton”, obligand in mod indirect gruparile
de puncte din stanga-sus §i dreapta-sus sa formeze impreuna un singur cluster
(ccea ce, foarte probabil, nu era de dorit). Detectia outlier-elor este un capitol
important din invatarea automata. Ignorarea lor poate conduce la rezultate
eronate sau chiar aberante ale modelarilor.

***************Ending Page***************


***************Beginning Page***************
***************page number:49**************
49.
K-means as an optimisation algorithm:
The monotonicity of the J K criterion
[CMU, 2009 spring, Ziv Bar-Joseph, HW5, pr. 2.1]

***************Ending Page***************

***************Beginning Page***************
***************page number:50**************
. 50.
Algorltmul K-means (S. P. Lloyd, 1957)
Input: 951,...,;cn E Rd §i K E N*.
Output: 0 anumitéi K-partitjie pentru {$1, . . . ,azn}.
Procedural’:
[Initializare/Iteratz'a 0:] t <— O;
se ﬁxeazéi in mod arbitrar M1), . . . , #20 centroizii initiali ai clusterelor, §i
se asigneazé ﬁecare instanigéi £132- la centroidul cel mai apropiat, forménd astfel
clusterele Ci), . . . , C}?
[Corpul iterativﬂ Se executé iteratia ++ t:
Pasul 1: se calculeazéi noile pozitgii ale centroizilorza
1 _ —
H5" I T 296E061 332' pentru .7 I 19K;
le I 1 J
Pasul 2:
se reasigneazé ﬁecare xi la [clusterul cu] centroidul cel mai apropiat, adicéi
se stabile§te noua c0mp0nen§5 a clusterelor la iteratia t: Cf, . . . , Ck;
[Terminara] péné 05nd 0 anumité conditie este indeplinitii
(de exemplu: péné cénd pozitjiile centroizilor — sau: componenta clusterelor
— nu se mai modiﬁcéi de la 0 iteratie la alta).
a Formula aceasta corespunde folosirii distantei euclidiene. Pentru alte misuri de distangé, este posibil séi ﬁe
necesare alte formule.

***************Ending Page***************


***************Beginning Page***************
***************page number:51**************
51.
a. Demonstraigi c5, de la 0 iteragie la alta, algoritmul K-means
méiregte coeziu'nea de ansa'mblu a clusterelor. I.e., considerénd
funcgia
TL TL
def. def.
7f t _ t 2 _ t t
J(C 7H) — E ||$i — ILLCt(x7;)|| — E ($1 — New”) ' (551' — Howl-D,
izl 2'21
unde:
Ct I (610;, . . . ,Ck) este colectia de clustere (i.e., K-partitia) la momentul t,
pt : (,ui, ,ug, . . . , pk) este colecgia de centroizi ai clusterelor (K-conﬁguragia)
la momentul t,
Ct(w7;) desemneazéi clusterul la care este asignat elementul 33¢ la iteragia t,
operatorul - desemneazé produsul scalar a1 vectorilor din Rd,
arétaigi 05 J(Ct,,ut) Z J(Ct+1,,ut+1) pentru orice t.

***************Ending Page***************

***************Beginning Page***************
***************page number:52**************
52.
Ideea demonstraﬁei
Inegalitatea de mai sus rezulté din douéi inegalitéiigi (care corespund pa§ilor 1
§i 2 de la iteraigia t):
t t (1) t t+1 (2) t+1 t+1

J((7,u) 2 J(C,# )2 J(C ,u )
La prima inegalitate (cea corespunzétoare pasului 1) se poate considera c5
parametrul Ct este ﬁxat iar H este variabil, in vreme ce la a doua inegalitate
(cea corespunzétoare pasului 2) se consider-5 ,ut ﬁxat §i C variabil.
Prima inegalitate se poate 0b§ine insuménd 0 serie de inegalitégi, §i anume
céte una pentru ﬁecare cluster Cg. A doua inegalitate se demonstreazéi ime-
diat.

Ilustrarea acestei idei7 pe un exemplu particular:

Vezi urméitoarele 3 slide-uri
[Edinburgh, 2009 fall, C. Williams, V. Lavrenko, HW4, pr. 3]

***************Ending Page***************


***************Beginning Page***************
***************page number:53**************
53.
O O O O O
|—|-o—o-o-o-¢—|—¢-o-o-o-o—|
—20 —10 —5 O 5 10
lter. 0 / initialization
O O O O O O O O O O
WWW WW
—20 —1 O —5 O 5 1O —20 —1 O —5 0 5 1O
iter. 1 Step 1: re-positioning the centroids Step 2: re-computing the clusters
O O O O O O O O O O
N%H+H+—+¢Hm—l WW
—20 -10 -5 0 7/3 5 10 —20 -10 -5 0 7/3 5 10
iter. 2
O O O O O O O O O O
|—BI(—*H+—I++H+Q—I |—|9I(—0++¢—|++H+0—|
—20 -10 -5 0 22/75 10 —20 -10 -5 o 22/75 10
iter. 3
O O O O O O O O O O
l—H-¢9téHb—l—l—¢H%+¢-l |—H—.%Hb—l—l—+%0—l
—20 —10 —7-5 O 5 7 10 —20 —10 —7-5 0 5 7 1O

***************Ending Page***************

***************Beginning Page***************
***************page number:54**************
Pentru acest exemplu de aplicare a algoritmului K-means, scriem expre- 54'
siile numerice pentru valoarea criteriului J2(Ct, ,ut) pentru ﬁecare itera§ie
(t : 0, 1, 2,3).
0 + {(-9 - (-10))2 + . . . + (-5 - (-10))2
+2[(5 — (—10))2 + - - - + (9 — (—10))2]} 2
1. (-9 - (-20))2 + {(-8 - 7/3))2 + . . . (-5 - 7/3)2
+2[(5 — 7/3)2 + . . . —|— (9 — 7/3)2]} Z
2. (-9 - (-9))2 + . . . (-5 - (-9))2
+2[(5 - 22/7)2 + . . . + (9 - 22/32] 2
3. (-9 - (-7))2 + . . . (-5 - (-7))2
+2[(5- 7)2 +...+ (9- 7V]
Observajie: La prima vedere, este greu $5 dovedim aceste inegalitéfgi
(J2(Ct_1,,ut_1) Z J2(Ct,,ut), pentru t :Al,2,3) ...altfel deceit calculénd efectiv
valoarea expresiilor care se comparé. Inséi, introducénd ni§te termeni inter-
mediari, inegalitéigile acestea se vor demonstra intr-un mod foarte elegant...

***************Ending Page***************


***************Beginning Page***************
***************page number:55**************
55.
J2(Ct—1,Mt) J2(Ct,/1t)
0 + {(-9 - (—10))2 + . . . + (_5 - (_10))2
+2[<5 — <—10)>2 + . . . + (9 — (—10))21} z
1. 0 + {(—9 — 7/3)2 + (-8 — 7/3)2 + . . . + (—5 — 7/3))2 (—9 — (—20))2 + {(—8 — 7/3))2 + . . . + (—5 — 7/3)2
+2[(5 - 7/3)2 + . . . + (9 _ 7/391} 2 +2[(5 _ 7/3)2 + . . . + (9 _ 7/3)2]} 3
2. (_9- (—9))2 +{(—8-22/7))2 +...+(-5_22/7)2 (—9- (—9))2 +(—8— (—9))2+...+(—5— (-9))2
+2[(5 - 22/7)2 + . . . + (9 - 22/7)2]} 2 +2[(5 - 22/7)2 + . . . + (9 - 22/7)2] Z
(—9— (—7))2+---+(—5— (—7))2 (—9— (—7))2+---+(—5— (—7))2
+2[(5_7)2+...+(9_7)2] z +2[(5—7)2+...+(9-7)2]
Explicatii:
1. Inegalitiiigile pe orizontaléi (J2(Ct_1, ,ut) Z J2(Ct, ,ut), pentru t I 1, 2, 3) sunt u§0r de
demonstrat, pe baza c0resp0nden§ei termen cu termen. (Ele corespund eventualelor
mic§0réiri ale distantelor atunci cénd se face reasignarea instan§elor la centroizi.)
2. Restul inegalitéitilor (J2(Ct, pt) Z J2(Ct,,ut+1), pentru t I 0, 1, 2) se rezolvii printr-
0 metodé de optimizare simplé. De exemplu, pentru t I O este imediat cii funcigia
(—9 — :13)2 + (—8 — x)2 —|— . . . —|— (—5 — :13)2 —|— 2[(5 — 33‘)2 —|— . . . —|— (9 — x)2] i§i atinge minimul pentru
a: I 7/3, deci J2(C0,,u0) I f(—10)2 f(7/3) I J2(Co,p1).

***************Ending Page***************

***************Beginning Page***************
***************page number:56**************
Step by step (t I 1) 56'
J2(Ct_lﬂll't) J2(Ct>/*Lt)
I 0+{(-9-(-10))2+...+(-5-(-10))2
+2[<5 — <—10>>2 + . . . + (9 — (-10)>21} z
1. 0 + {(-9 - 7/3)2 + (-8 - 7/3)2 + . . . + (-5 - 7/3))2 (-9 - (-20))2 + {(-8 - 7/3))2 + . . . + (-5 - 7/3)2
+2[(5 - 7/3)2 + . . . + (9 - 7/9021} 2 +2[(5 - 7/3)2 + . . . + (9 - 7/3)2]} 2
Details to be ﬁlled
in class

***************Ending Page***************


***************Beginning Page***************
***************page number:57**************
Step by step (t I 2) 57‘
J2(Ct—1,p,t) J2(Ct7/J’t)
1. (-9- (—20))2+{(—s—7/3))2+...+(—5—7/3)2
+2[(5 — 7/3)2 + ~ ~ . + <9 — 7/3021} 2
2- (—9 — <—9>)2 + {<—8 — 22/7»2 + . ~ ' + <—5 — 22/7)2 (-9 — (—9))2 + (-8 — (—9))2 + - - - + <—5 — <—9>>2
+2[(5 — 22/7)2 + . . . + (9 — 22/7)2]} 2 +2[(5 — 22/7)2 + . . . + (9 — 22/7)2] 2
Details t0 be ﬁlled
in class / at home

***************Ending Page***************

***************Beginning Page***************
***************page number:58**************
Step by step (t I 3) 58‘
J2(Ct—1,p,t) J2(Ct7/J“t)
2. (-9— (-9))2 +(-s— (—9))2 +...+(—5— (—9))2
+2[(5 — 22/7)2 + . . . + (9 — 22/7)2] 3
3- (—9 — (—7))2 + - - - + (—5 — (—7))2 (—9 — (—7))2 + - - - + (—5 — (—7))2
+2[(5—7)2+...+(9—7)2] : +2[(5—7)2+...+(9—7)2]
Details t0 be ﬁlled
in class / at home

***************Ending Page***************


***************Beginning Page***************
***************page number:59**************
59.
Graphical illustration
by Gheorghe Balan, FII student, 2017 fall
m jscore llizariatilclri
2955
Ill
E
bi
HEB \
545 \X I-
199
3D \.
an 1.0 1.5 2.6 2.5 3.:1 3.5
iteration
For Edinburgh, 2009 fall, C. Williams, V. Lavrenko,
HW4, pr. 3

***************Ending Page***************

***************Beginning Page***************
***************page number:60**************
. . Q 7 60.
Graphlcal exempllﬁcatlon (cont d)
‘_.. ._. jscure vanatlon
‘if: "1,-1" J" g
© 2010-2012 Andrew {5% ﬁ“\\mmmmmmmmmmmnmm
Ng, Stanford University.
For Stanford, A. Ng, 2012 spring, HW9 (image segmentation and compression)

***************Ending Page***************


***************Beginning Page***************
***************page number:61**************
61.
Demonstragie, pentru cazul general
Observafie: Pentru convenientgé, ne vom limita la cazul d : 1. Extinderea demonstragiei
la cazul d > 1 nu comporté diﬁcultéigi.
Demonstrarea inegalitégii (1): J(Ct,/f)ZJ(Ct,,ut+1)
(Vezi pasul 1 a1 iteraii'iei t.)
Fixém j € {1, . ..,K}. Dacﬁ notﬁm cu C; : {552'17551'27- “ml-l}, unde Z 722' |C;|, atunci
l 2 K
MM) I Z (snip - #5) , deci J<Cw> I Z M15, #3)-
p:1 9:1
Dacéi se considerii C; ﬁxat, iar ,u; variabil, atunci putem minimiza imediat functia care
,,méisoaréi“ coezz'unea din interiorul clusterului C55
d f l l 1 l d f
fW) I' J(C§,u) I M2 — 42%) -#+ 2112 i argminﬂqw) I 7 2% I' x1511-
p:1 p=1 M p=1
Agadar, J(C§,,u) Z J(C;,,u;+1), pentru V11. in particular, pentru ,u : ‘u;- vom avea:
J(C;,/1§)ZJ(C;,MZ+1). Inegalitatea aceasta este valabiléi pentru toate clusterele j I
1, . . . , K. Dacii suméim toate aceste inegalitéiij, rezulté: J(Ct,,ut)ZJ(Ct,/f+1).

***************Ending Page***************

***************Beginning Page***************
***************page number:62**************
62.
Demonstrarea inegalitﬁtii (2): J(Ct,,ut+1) Z J(Ct+1,,ut+1)
(Vezi pasul 2 a1 iteratiei t.)
La acest pas, 0 instantéi oarecare xi, unde i G {1, . . . ,n}, este reasignaté de la
clusterul cu centroidul p§+1, la un alt centroid ,LLZH, dacé
|\wi—/L;/+1\|2 2 |\$1;—MZ+1\|2 <1> (w¢—#§/+1)2 z<wi—#§+1>2, pentru orice j’ I 1, . . . ,K.
in contextul iteratiei t, acest lucru implicé
t+1 2 t+1 2

(331' — HCt($i)) Z (5131' — 'LLCt+1(mZ~)> .
Suménd membru cu membru inegalitétile de acest tip obtinute pentru 2' I 1,—n,
rezulté: J(Ct,/f+1) Z J(Ct+1,,ut+1), ceea ce era de demonstrat.

***************Ending Page***************


***************Beginning Page***************
***************page number:63**************
63.
Interesting / Useful Remark (in English)

(by L. Ciortuz, following CMU, 2012 fall, E. Xing, A. Singh, HW3, pr. 1)
Following the result obtained above, the K-means clustering algorithm can be
seen as an optimisation algorithm. As input, we are given points 2:1, . . . ,asn €
Rd and an integer K > 1. The goal is to minimize the K-means objective
function / criterion, which in the case of using the Euclidian distance measure
is the Within-cluster sum of square distances

JWL) I Ellie- —ml-|\2,
2':1
Where ,u I (M1,. ..,,uK) are the cluster centers (,uj E Rd), and L I (l1, . . .,ln) are
the cluster assignments (ZZ- € {1, . . . ,K}). (Finding the exact minimum of this
function is computationally difficult.)

***************Ending Page***************

***************Beginning Page***************
***************page number:64**************
64.

Interesting / Useful Remark (cont’d)
Lloyd’s [K -means] algorithm, takes some initial cluster centers ,u, and pro-
ceeds as follows:
Step 1: Keeping ,u ﬁxed, ﬁnd cluster assignments L to minimize J(,Li, L). This
step only involves ﬁnding the nearest cluster center(s) for each input point
mi. Ties can be broken using arbitrary (but consistent) rules.
Step 2: Keeping L ﬁxed, ﬁnd ,u to minimize J(/i,L). This is a simple step
that only involves — in the case of using the Euclidian distance measure —
averaging points within a cluster.
Stopping criterion: If [this was not the ﬁrst iteration and] none of the values
in L changed from the previous iteration, go to the next step; otherwise repeat
from Step 1.
Termination: Return ,u and L.
Note: The initial cluster centers ,u given as input to the algorithm are often
picked randomly from 51:1, . . . ,ZEn. (In practice, we often repeat multiple runs
of Lloyd’s algorithm with different initializations, and pick the best resulting
clustering in terms of the K-means objective.)

***************Ending Page***************


***************Beginning Page***************
***************page number:65**************
65.
b. Ce putegi spune despre oprirea algoritmului K -means?
Termini oare acest algoritm intr-un numér ﬁnit de pagi, sau
(intrucét numérul de K-partiigii care pot ﬁ formate cu cele n
instanige de antrenament, x1, . . .,55n, este ﬁnit, §i anume K n) este
posibil ca el 55 reviziteze de 0 inﬁnitate de ori 0 K-conﬁguragie
anterioarii, ,u I (/11, . . . Mild?)

***************Ending Page***************

***************Beginning Page***************
***************page number:66**************
66.
Riispuns:
Dacé algoritmul reviziteazé 0 K-partiﬁe, atunci rezulté c5 pentru un anumit
t avem J(Ct_1,pt) I J(Ct,,ut+1). Este posibil ca acest fapt séi se intélmple, §i
anume atunci cénd:
— existé instan§e multiple (i.e., x2- I wj, de§i 2' I j),
— criteriul de oprire a1 algoritmului K-means este de forma
“pénéi cénd componeniga clusterelor nu se mai modiﬁcé”,
— se presupune c5, in cazul in care 0 instan§éi $1- este situaté la egalé distanigé
fatal de doi sau mai multi centroizi, ea poate ﬁ asignaté in mod aleatoriu la
oricare dintre ei.
A§a se intémplii in ewemplul din X3
ﬁgura aléturaté dacéi se consideré x 1 M1 x 2 M2 x4
cé la 0 iteratie t avem :62 I 0 € 40—|—0—|—0i
Cf §i x3 I O € C5, iar la iteragia —1 —1/2 0 1/2 1
urmétoare alegem ca $3 I 0 G Cf“
§i $2 I O E 05H §i, din nou, invers
la iteragia t —|— 2.

***************Ending Page***************


***************Beginning Page***************
***************page number:67**************
67.
Observatii

o Dacii se péistreazéi criteriul dat ca exemplu in enuntul problemei — adicé
se itereazﬁ péné cénd centroizii “stationeazii” — algoritmul se pqate opri
féiréi ca la ultima iteratie J(C,,u) s51 ﬁ atins minimul posibil. In cazul

2 2
1 1 1 2 1 2
eemluluidemaisus 0maea— 2-— —:1>—:2- — — .
X p ,v V 4—|— 4—|—4 3 (3>+<3>

0 Dacéi nu existéi instante multiple care $5 ﬁe situate la distante egale fatéi
de doi sau mai multi centroizi la 0 iteratie oarecare a algoritmului K -
means (precum sunt £132 §i x3 in ememplul de mai sus), sau dacéi se impune
restricjia ca in astfel de situatii instantele identice s5 ﬁe asignate la un
singur cluster, este evident c5 algoritmul K-means se 0pre§te intr-un
numiir ﬁnit de pa§i.

***************Ending Page***************

***************Beginning Page***************
***************page number:68**************
68.
Concluzii

o Algoritmul K -means exploreazéi — pornind de la o anumitéi ini§ializare
a celor K centroizi —, doar un subset din totalul de K" K-partiigii,
asiguréndu-ne insii c5 are loc proprietatea J(CO,,LL1) Z J(C'1,,u2) Z Z
J(Ct_1,,ut) Z J(Ct,/ﬁ+1), conform punctului a a1 acestei probleme.

o Atingerea minimului global a1 funcgiei J (C, ,u) — unde C este o vari-
abilé care parcurge mul§imea tuturor K -parti1;iilor care se pot forma cu
instantele {$1, . . . ,xn} — nu este garantatii pentru algoritmul K-means.
Valoarea funcgiei J care se obgine la oprirea algoritmului K -means este
dependentii de plasarea ini§ial€1 a centroizilor ,u, precum §i de modul con-
cret in care sunt alcﬁituite clusterele in cazul in care o instan§5 oarecare
se aﬂé la distangéi egalé de doi sau mai multi centroizi, dupéi cum am
ariitat in exemplul de mai sus.

***************Ending Page***************


***************Beginning Page***************
***************page number:69**************
69.
K -means algorithm:
The “approximate” maximization of the “distance” between
clusters
CMU, 2010 fall, Aarti Singh, HWS, pr. 5.2

***************Ending Page***************

***************Beginning Page***************
***************page number:70**************
70.
N ote: In this problem we will work with a version of the K -means algorithm which is slightly
modiﬁed w.r.t. the one given in the problem CMU, 2009 spring, Ziv Bar-Joseph, HW5, pr. 2.1,
where we have proved the monotonicity of the criterion J.
Let X z: {X1,X2, . . . ,Xn} be our sample points, and K denote the
number of clusters to use. We represent the cluster assignments
of the data points by an indicator matrix y € {O,1}”XK such that
vi]- I 1 means X,- belongs to cluster j. We require that each point
K

belongs to exactly one cluster, so 23:1 vi]- I 1.
[We already know that] the K-means algorithm “estimates” "y by minimiz-
ing the following “cohesion criterion” (or, “measure of distortion”,
or simply “sum of squares”):

n K

._ 2

1:1 9:1
where H - H denotes the vector 2-norm.
K -means alternates between estimating y and re-computing ,u,j’s.

***************Ending Page***************


***************Beginning Page***************
***************page number:71**************
71.
The K -means algorithm (...yet another version!)
0 Initialize 11,1419,“ .,;tK, and let C z: {1, . . . , K}.
0 While the value of J is still decreasing, repeat the following:
Step 1: Determine 'y by
17 HXi _ ,ujH2 S ||X¢ _ I'Lj/H27 Vj/ é C»
V117" <— .
0, otherW1se.
Break ties arbitrarily.
Step 2: Recompute ,uj using the updated y:
For each j € C, if 217:1 Vij > O set
,u- <_ 211%in
J 2?:1 Vij
Otherwise, don’t change ,uj.

***************Ending Page***************

***************Beginning Page***************
***************page number:72**************
72.
Let >7; denote the sample mean.
Consider the following three quantities:
n — 2
' Xi — X
Total variation: T(X) : M
n
n 2
-_ i' Xi — - .
Within-cluster variation: WAX) : W—HJH for j I 1, . . . , K
21:1 7117'
K Z" ..
Between-cluster variation: B(X) : Z (%%J) Haj — >2||2.
3:1
What is the relation between these three quantities?
Based on this relation, show that K -means can be interpreted as
minimizing a weighted average of within-cluster variations while
approximately(!) maximizing the between-cluster variation. Note
that the relation may contain an extra term that does not appear
above.

***************Ending Page***************


***************Beginning Page***************
***************page number:73**************
Solution 73'
To simplify the notation, we deﬁne nj I 212:1 'yZ-j.
We then have:
1 n _ 2 1 n K _ 2 1 K n _ 2
T(X) I E Z||X1— XH I E 22711||X1— X|| I H 22111HX1— X||
1:1 1:1 1:1 1:1 1:1
1 K n
I EZZWHX1_#1+”1 _’—‘||2 <9)
1:1 1:1
1 K n
I E Z 2111(||X1— H1112 + ||Hj — >_<||2 + 2(X1— Hj) - (Hj — 5O)
1:1 1:1
K 11 K _ K 11
1121-: 11-HX1-H-H2 "HM-XV 2 _
I Zﬁ++ZHT JFEZWFXWZW 1W1»
1:1 J 1:1 1:1 1:1
K n- 2 K t 2Tb 7.x.
: Z #Wj(X)—|-B(X) + E an (/117- —>_<) - (113- — 113-), Where [13- :' %.
1:1 1:1 9
W
l
n
(10)

***************Ending Page***************

***************Beginning Page***************
***************page number:74**************
74.
Notes
1. In (9), the quantities ,uj can be taken arbitrarily;
however, they will be thought of in the end — see the relation (10)
and also the Conclusion on the next slide — as the centroids of the
clusters 1,. . . , K, as computed at Step 2 [more exactly, at the previous
iteration] of the K -means algorithm.
2. The equality (10) on the previous slide holds because
wa — My‘) I (Z W951) — “ij I "1+ — "ij
1:1 1:1 J
2?: 777351; _
I "j <+—Hj :"j(#j—#j)
a

***************Ending Page***************


***************Beginning Page***************
***************page number:75**************
75.
Conclusion
We already know — see CMU, 2009 spring, Ziv Bar-Joseph, HW5,
1
pr. 2.1 — that K -means aims to minimize J, and consequently —J,
n
which coincides with the ﬁrst term in the expression we obtained
n .
for T(X), namely 2511 #Wj (X)
Since the total variation T(X) is constant, minimizing the ﬁrst
term is equivalent to maximizing the sum of the other two terms,
which is expected to be dominated by the between-cluster varia-
tion B(X) since a good ,uj should be close to ﬁj (after a certain
number of iterations), making the third term small in absolute
value.

***************Ending Page***************

***************Beginning Page***************
***************page number:76**************
76.
Graphical exempliﬁcation
by Gheorghe Balan, FII student, 2017 fall
jnter-clustere varia ' ~ . . inter-clustere variation
- 152.1
31313 \
m 0.0313 l m
Q 0.0651 \‘ Q
43.5 \ \
0.0060 \ 33.3
0 1 2 3 4 0 1 2 3
iteration iteration
For CMU, 2006 spring, C, Guestrin, HW5, pr. 1 For Edinburgh, 2009 fall, C. Williams, V. Lavrenko,
HW4, pr. 3

***************Ending Page***************


***************Beginning Page***************
***************page number:77**************
. . Q 77o
Graphlcal exempllﬁcatlon (cont’d)
intereclustere varlahon
:ssaz's - “““““‘m\
,‘U " A ‘liq-aka‘; 1‘ 15177.0 ’\
Bx -
V I‘ The." - 1.; ~in 151192 \‘
; . 2. 15071.2 .
‘ N .\ “seam . \‘
L: S: I‘ g 1499711 \‘
g; l“ 14983.1
V aim *1 14911 0 .\
ll I —
© 2010-2012 Andrew .\
Ng, Stanford University. ‘
For Stanford, A. Ng, 2012 spring, HW9 (image segmentation and compression)

***************Ending Page***************

***************Beginning Page***************
***************page number:78**************
78.
A kernelized version of K-means
[CMU, 2015 spring, T. Mitchell, N. Balcan, HW7, pr. 1.2-4]

***************Ending Page***************


***************Beginning Page***************
***************page number:79**************
79.
K -means with Euclidean distance metric assumes that each pair of clusters
is linearly separable. This may not be the case. A classical example is where
we have two clusters corresponding to data points on two concentric circles
in the R2 plane. We have seen that we can use kernels to obtain a non-linear
version of an algorithm that is linear by nature and K -means is no exception.
Recall that there are two main aspects of kernelized algorithms:
i. The solution is expressed as a linear combination of training examples;
ii. The algorithm relies only on inner products between data points rather
than their explicit representation.
We will show that these two aspects can be satisﬁed in K-means.
a. Let fyZ-j be an indicator that is equal to 1 if the asl- is currently assigned to
the jth cluster and O otherwise (1 g 2' g n and 1 g j g k). Show that the jth
cluster center uj can be updated as 221:1 ozijxi. Speciﬁcally, show how ozij can
be computed given all 'y’s.

***************Ending Page***************

***************Beginning Page***************
***************page number:80**************
80.
b. Given two data points $1 and x2, show that the square distance ||x1 — 902||2
can be computed using only (linear combinations of) inner products.
c. Given the results of parts a and b, show how to compute the square distance
||xi —uj ||2 using only (linear combinations of) inner products between the data
points 51:1, . . . ,xn.
d. [in Romanian] Considerand data o functie de ,,mapare“ (I) : Rd —> Rm
si functia-nucleu corespunzatoare K : Rd >< Rd e R, cu K(;v,y) : @(x) - @(y),
scrieti pseudo-codul algoritmului K -means kernel-izat.“ Concret, veti porni
cu anumite puncte initiale ca centroizi si veti folosi raspunsul de la punctul c
ca sa gasiti cel mai apropiat centroid pentru ﬁecare instanta, utilizand valorile
variabilelor-indicator 'yij. Apoi veti face uz repetat de raspunsul de la punctul
a pentru a reasigna instantele la centroizi si pentru a actualiza variabilele 'yZ-j.
a Aceasta varianta a algoritmului K -means cauta separatori liniari intre clusterele ce se vor forma cu
@(zcl), . . .,(I>(a:n) G Rm, insa foloseste efectiv doar instantele 932- G Rd (prin intermediul functiei-nucleu K) nu si
imaginile lor prin functia de mapare ((1)) in Rm.

***************Ending Page***************


***************Beginning Page***************
***************page number:81**************
81.
Solution
a. We have to write uj as 2?:1 aid-mi. It is kown (see for instance CMU,
1 n
2010 fall, Aarti Singh, HW3, pr. 5.2) that uj : <2n—) 21:1 yijxi :
i=1 'Yij
n Wig‘ 71g‘
22.: n—xZ-. Therefore we shall set 0%- : n—.
1 Zi’:1 7M , J Ei’:1 7W
Note that this result holds also when instead of 51:2- we would work with @(asi),
for alli: 1,...,n:
(211 Via‘ g J g 21/:1 WU
where 'yZ-j : 1 iff @(xz) belongs to the cluster j.
b. Ha: — :13’||2 déf' (:10 — an’) - (x — an’) I x - :1: +56’ ~31: — 23: - x’. The last equality is due
to the distributivity of the dot product () with respect to the subtraction /
sum of vectors.
Note that in fact this results holds in any vectorial space, in particular in the
feature space Rm.

***************Ending Page***************

***************Beginning Page***************
***************page number:82**************
82.
c. By substituting in the above result xi t0 a: and uj to at’, and subsequently
taking into account the form of uj developed at part a, we can write:
TL
llxi — Hjll2 I llﬂﬁi — Z Uzi/11717‘?
i’:1
TL TL TL
I $1‘ ‘551' + (Z 04m $1”) ' (Z 04M‘ $1”) — 2%“ (Z 04M $1")
i’:1 i”:1 i’:1
TL TL TL
I £131‘ ' SC»; + Z Z Obi/j Ody/j 331v ' SUZ'I/ — 2 Z OLi/j 331' ' £1717 (ll)
1/:1¢//:1 1":1

***************Ending Page***************


***************Beginning Page***************
***************page number:83**************
83.
Note that a similar formula can be obtained when considering @(xi) in the
place of mi and 22:1 ai/j<l>(x;) in the place of pj in the above formula. So, we
will have:
||@(f11¢) — Z ai/j<1>(w¢/)ll2 I @(371') ~<I>(w@-) + Z Z OW 0417C? @(ZW) ' (5%”)
27:1 17:1 il/Il
_/—/
H;-
1":1
i’:1i”:1
1/:1

***************Ending Page***************

***************Beginning Page***************
***************page number:84**************
84.
d. Here follows our pseudo-code for the kernelized K -means:

0 Initialize [possibly randomly] ,ul, . . . ,uK E Rd,
compute M1: @(ul), . . . ,u’K I @(HK) € Rm, and let C I {1, . . .,K}.

. While the value of J(’y,]J,/1,...,[,L/K) déf' 2;;1 2le Vij||<1>(xi) - “HP (which is
computed using (12)“ ) is still decreasing, repeat the following:

a Except for the ﬁrst time / iteration, when you shall use (11), where £131 must be substituted with @(mi) and uj
with [1,9, and subsequently use the function K to replace the dot products.

***************Ending Page***************


***************Beginning Page***************
***************page number:85**************
85.
1. Determine 'y by
1, ll‘I>(X@') — MHIZ £ll‘1>(Xi)— “glue W’ € C,
e Note that this inequality should be determined by using initially
l” (11) and then (12).
0, otherwise.
Break ties arbitrarily.
2. Recompute p;- using the updated 'y:
For 2' E {1,...,n} and j € {1,...,K},
'Yij
04¢ <— n—.
] 20:1 l1"?
For each j E C, if 2221715 > O set
/ n 2?:1 'l/ijq)(Xi)
p- e ail-(May) : n—.
3 ZZZ; J 21:1 7U
Otherwise, don’t change it}.

***************Ending Page***************

***************Beginning Page***************
***************page number:86**************
The EM algorithm
for Gaussean Mixture Models

***************Ending Page***************


***************Beginning Page***************
***************page number:87**************
87.
Exemplifying the application 0f a simple version of
EM /GMM
0n data from R
(0'1 I 0'2 I 1,7T1 I 7T2 I 1/2)
CMU, 2012 spring, Ziv Bar-Joseph, ﬁnal exam, pr. 3.1
enhanced by Liviu Ciortuz

***************Ending Page***************

***************Beginning Page***************
***************page number:88**************
88.

Suppose a GMM has two components with known variance and an equal prior
distribution 1 1

§NW1>1>+ §NW2>1>~
The observed data are £121 I 0.5 and 902 I 2, and the current estimates of #1
and ,ug are l and 2 respectively.
a. Execute the ﬁrst iteration of the EM algorithm.
Hint: Normal densities for the standardized variable ywzogazl) at 0, 0.5, 1, 1.5,
2 are 0.4, 0.35, 0.24, 0.13, 0.05 respectively.
b. Consider the log-likelihood function for the “observable” data,

d f ' d 2 2
£(,u1,,ug) glnP(:131,5102|,u,1,/.L2) mzep' ZlnP(a:,\u1,ug) I 2111 ZP($1,Zij|H17/l2) 7
1:1 71:1 Zij

where 2,",- 6 {0,1} and 23:1 zij : l for all 7L € {1, 2}.
Compute the values of E function at the beginning and also at the end of the
ﬁrst iteration of the EM algorithm.
What do you see?

***************Ending Page***************


***************Beginning Page***************
***************page number:89**************
Solution (81.) 89'
The E-step:
B.Th. P(£135|Z¢1 I 17H1)P(Zi1 I 1)
EZ, :PZ,:1a.~,-, I —
1
P(£E¢\Z¢1 I 17M1)'§
I I
P(a:i|Z,-1 I 1,441) ' 5 +P($12|Z¢2 I 17/42) ' 5
P($i,Zi1 I 1M1) .
: — for z e 1,2 .
P($¢|Z51 I 1M1) + P(335|Z¢2 I 1M2) { }
Therefore,
N 0.5; 1,1 N 0.5;0,1 0.35 35
N(0.5, 1,1) + N(0.5,2, 1) N(0.5,0, 1) + N(1.5,0, 1) 0.35+0.13 43
N 2,1,1 N 1,0,1 0.24 0.24 3
N(2, 1,1) +N(2,2,1) N(1,0, 1) +N(0,0, 1) 0.24+0.4 0.64 3
Similarly,
13
P(212 I 111mm) I P(Zn I 0|$17MI 1 — P(Zn I 1|IB1M1) I 4—8
5
P(Zz2 I 1‘a7271u) I P(Z21 I 0|:122,11): 1 — P(Z21 I 1|$27H1) I g

***************Ending Page***************

***************Beginning Page***************
***************page number:90**************
90.
The M-step:
2 2
Mel) _ ZZZ-:1 EVm'] 11% _ ZZZ-:1 P(Z¢j I 1|$i,u(t)) $1
' _ 2 — 2
J ZZZ-:1 EMA 21-21 P(Zij I 1m, PM)
Therefore,

35 3 13 5
<1>:—4—8'0'5+§'2:@e1009 and (1):—4—8.0'5+§2:1—%e154
“1 §+§ 106 ' “2 §+§ 86 '

48 8 48 8

***************Ending Page***************


***************Beginning Page***************
***************page number:91**************
Solution (b.) 91'
d f 2 2
“Mb/$2) 2' 21HP($i|/~L1»M2): 2111 ZP($i>Zij|H1,M2)
izl 71:1 Zij
2
I 21H ZP($11Z11,H1,H2) 'P(Zij|,u1,#2)
71:1 Zij 1/2
2 1 2 2
I 21H 521321211112) z Z -1n2+1n ZPm-mj I 1,17»
2:1 Zij 1:1 jzl
Zij P($¢1Z¢j,ﬁb1,#2)
1 0 1 < 1< >2)
z : z : —e —— $ —
11 7 12 ﬁ Xp 2 1 M1
1 1
Z11 I 0, 212 I 1 \/—2—7r exp(—§($1 — My)
1 1 2
Z21 I 1, Z22 I 0 \/—2—vr 6XP(—§($2 — N1) >
1 1
Z21 I 0, Z22 I 1 E exp(—§(x2 — My)

***************Ending Page***************

***************Beginning Page***************
***************page number:92**************
92.
Therefore,
a > 212+1<1 ( <1< >2>+ <1< >2)»
, :—n n—eX——x— eX——w—
M1H2 @1021/11 1321112
+1 ( 1 ( <1< >2>+ <1< >2)»
n—eX——x— eX——w—
ﬁ p 2 2 M1 p 2 2 M2
1 1
: —21n2 — ln(27r) + ln <exp(—§(w1 — ,ul)2) —|— exp(—§(a:1 — ,ug)2)>
1 2 1 2
+ln exp(—§(w2 —,u1) )+6Xp(—§(332 —,LL2) )
and
107 133
2,401,191) I 2(1,2) I -2.561833 g mil), Mg“) I z —, — I -2.462877,
106 86
meaning that the value of the log-likelihood function increases, which is in line
with the theoretical result concerning the correctness of the EM algorithm.

***************Ending Page***************


***************Beginning Page***************
***************page number:93**************
93.
Addenda (in Romanian)

Pentru [conﬁrmarea §i] ewtinde'rea acestui rezultat, prezentam in slide-ul
urmator rezultatele 0b§inute cu ajutorul unei implementari a algoritmului
EMza

— in graﬁcul din partea stanga sunt reprezentate valorile funcgiei de 10g-
verosimilitate 0b§inute de EM la ini§ializare §i apoi la ﬁnalul ﬁecareia din
primele 19 iteratii,

— in graﬁcul din partea dreapta avem reprezentarea sub forma de curbe de
izocontur a valorilor log-verosimilitagii in func§ie de cei doi parametri, ,ul §i
#2. Pe acest a1 doilea graﬁc a fost adaugat un ,,drum“ care pune in evideniga
succesiunea de valori pentru perechile (/11, n2) de-a lungul iteratiilor executate
de algoritmul EM.

a Aceasta implementare a fost realizata in 2018 de catre studentul Sebastian Ciobanu.

***************Ending Page***************

 
***************Beginning Page***************
***************page number:95**************
95.
Derivation of the EM algorithm for
a mixture of K uni-variate Gaussians:
the general CHSG (i.e., when all parameters 1T,,u,02 are free)
following Dahua Lin,
An Introduction t0 Echectation-Maazimization
(MIT, ML 6768 course, 2012 fall)

***************Ending Page***************

***************Beginning Page***************
***************page number:96**************
96.
Note
Z~Bern0ulli

WE
We will ﬁrst consider K I 2. 1 2
Generalization to K > 2 will be X~Gaussian1 X~Gaussian2
shown afterwards. (M1, (7%) (H2, O?

x

***************Ending Page***************


***************Beginning Page***************
***************page number:97**************
97.
Estimation (E) Step:
not. calcul
_ P(X¢:95iIsz217#7077T)'P(Zij:1Maﬁa”)
1 (33¢ — My)
_ v27mj p < 2092' j
— 1 (551' — #02) 1 < (5% — my)
—-eX —— '7T—|——-€X —— '7T
x/27r01 p < 20% 1 \/ 27TUQ p 20% 2
Therefore, for t > 0 we will have:
FF) ,eXp < (1'1- — M9152)
t_1 1:_1
10G) I U; ) 2(0; ))2
m t_1 t—1 t—1 1:—1
7T; )-€Xp (_($i —H(1 W) + 7T; ) -exp <_(332_1ng W)
t_1 t_1 t_1 t—1
a; > 2w; >>2 a; > 2w; >>2

***************Ending Page***************

***************Beginning Page***************
***************page number:98**************
98.
The likelihood 0f a “complete” instance (232-,z11, 21-2):
P(X¢ I $1,211 I Z117Z¢2 I Z¢2 \ H707”)
I P(X@' I $1‘ I Z11 I Zn; Z12 I Z¢27Mi70177w ' P(Z@'1 I 211,212 I 2121M170¢77T0
I —-eX —— ~11, where zi- I 1 and 21-1 : 0 for ’
\/ 27mj p ( 2032 J J j J 75 J
I —-/_z-Z ' exp —_ Z Z217—2 '7T17J17T222
2W 01 1 02 2 < 2 je{1,2} Uj
The log-likelihood 0f the same “complete” instance will be:
1nP(Xi I 332,211 I 2117212 I 222 \ M707”)
2 2 2
1 1 (Ii — H ‘)2

***************Ending Page***************


***************Beginning Page***************
***************page number:99**************
99.
Given the dataset X : {$1, . . . , Jan}, the log-likelihood function will be:
“luv 0-1 7T) déf. 1nP(X1 Z11 ZZ I ILL7 017T) zzzd 1HH P(XZ : £131‘, Z111 Z712 I ILL? 017T)
1:1
I ZlnPOQ I 331>Z111212 \ H701”)
1:1
1:1 3:1 1:1 1:1 J 1:1 3:1

***************Ending Page***************

***************Beginning Page***************
***************page number:100**************
l .
The expectation of the log-likelihood function: 00
TL n 2 1 n 2 ($2 _ ,LLj)2 n 2
1:1 1:1 1:1 9:1 J 1:1 9:1
Here above, the probability function w.r.t. which the expectation was com-
puted was left unspeciﬁed. Now we will make it explicit:
Q<H7017Tl H(t)70-(t)77r(t))nét.EZ|X7M(t)7U(t>,7T(t)|:11/l P(Xa Zla Z2 l [U7 U: 7TH
n 2
: g 111(21) - Z Z Em]- y Xi, 11(1), W, 11w] ln Uj
1:1 1:1
n 2
1 (5151' — M ‘)2
—§ Z Z ElZm' l X11/l(t)10(t)17T(t)lTj
1:1 1:1 J
n 2
1:1 1:1

***************Ending Page***************


***************Beginning Page***************
***************page number:101**************
101.
pg) 71g. E[Zij I XHLL“), 0-(t)17T(t)] i
QM 0,71 I u“), a“), 71(2) I
77, n 2 1 n 2 (x ,U )2 n 2
t t i — ' t
1:1 9:1 2:1 3:1 j 1:1 9:1
Since K I 2 and 711 —|— 7T2 I 1, we get
6201,0211 \ M“), 52,110”) I
n 2 n 2 2 n
n (t) 1 (t) ($1: — #1) (t) (t)
_§ 111271 — 22pm- 1n 09- — 5 g 2pm- 0—32- + Ella-1111711 +pZ-21n(l—7rl))

***************Ending Page***************

***************Beginning Page***************
***************page number:102**************
102.
Maximization (M) Step:
[For K I 2:]
lax” U 7T | Ha) (,(t) 7H”) I 0 (i) l 2713190) I é 27131175)‘:
6W1 7 7 ’ 7 7T1 1:1 21 1 — 7T1 7;:1 Z2
220E? I M21??? + 229E?) <I> 210$) I 7T1 2Q??? + PE?) 4:) 229$) 2W1
¢:1 i:1 i=1 1:1 i=1 v i:1
1 TL
i wit“) t 5 21);?
1:1
Taking into account that WYH) + WétH) I 1 and pg) +195? : 1 for 2' : 1, . . . , n,
1 n
t+1 1:
:> 7T; ) <— E 212919

***************Ending Page***************


***************Beginning Page***************
***************page number:103**************
103.
l QM U 7T | Mt) 0“) HO) — 0(1) 1 n (t) n (t)
57m ’ ’ ’ ’ — $22071 ($1—M1):0<:>Zpil (5131-_,u1):()
"L: 1:1
n (t) ,
i MY“) k Z:—P(f
21:1pz'l
n (t)
Similarly, #gw <_ 2:1—P2(5f
n t
21:11912
(It can be easily Shown that 21721105‘? > 0 and 221:1 pg) > 0-)

***************Ending Page***************

***************Beginning Page***************
***************page number:104**************
104.
i QM a 7T \ W a“) 77>) I 0 e —i id” + i ipWQ/Ji - 77172 I 0
801 7 7 7 7 011.21 11 0-? 7;:1 11 7
2 77, (7:) ._ (7+1) 2
:> (gym) e 27:11071 £117 ($1 )
21:11971
2 7? (t) __ (H1) 2
Similarly, (091)) <— —Zz:1p725f ($2 )
27:1191'2
Note: One could relatively easily prove that these solutions
(namely, 7T(t+1),/i(t+1),0'(t+1)) of the partial derivatives of the aum-
il'ia'ry function Q designate the values for which Q reaches its
maximum.

***************Ending Page***************


***************Beginning Page***************
***************page number:105**************
105.
Generalization to K > 2
In this case, the Bernoulli distribution is replaced by a categorical
one. The only one change needed in the above proof concerns
updating the parameters of this distribution.
Since 7T1 + + 71K : l, we must solve the following constraint
optimization problem:
max Q<7Ta Ill/7 UIWU)’ ILL(t)7 0-05))
7T,,u,(7
K
subject to 27g :1 and 7Tj Z 0, Vj I 1,...,K.
3:1
By letting asside the Z constraints, and using the Lagrangean
multiplier A € R, this problem becomes:
K
maX (QUE u, 0W), M“), 0(a) + M1 — Z 7%)) -
7T,,u,0
3:1

***************Ending Page***************

***************Beginning Page***************
***************page number:106**************
106.

ForjI1,...,K:
i Q(,u 0 7T l ,uw 0(15) 71-05)): O <I> ip<l>l I A (I) TIGJFD I l ipll).
871:7’ 7 7 a 1 1-21 1] 7le j A 2'21 Z]
Because 23111 7T§t+1> I 1, it follows that

K n n K n

_ (t) _ (t) _ _
)‘— 2219111 — ZZP11— 21—”'
3:1 1:1 1:1 3:1 1:1
1
Therefore,
(1+1) 1 n (t)

Note that indeed 7r;t+1) Z O, because the pg) terms designate some
probabilities (see E-step).

***************Ending Page***************


***************Beginning Page***************
***************page number:107**************
107.
T0 summarize:
E Step:
. (f) 2. (t) . (if)
19(5) n2‘ P(z-j = 1 | ‘fly-LN) (02W) 7m) I W
Z Z 7,7 , 7 t t
Z:l:1 N(97i | H; >(Uzz)(t)) ' 7T1
Where N 5132- 302. dzf — ~eX (_¥ .
M Step:
(15+1) 1 n (t)
7T1 k 52pm
1:1
n t
(t+1) 21:1 pgj)93i
“1' * n—<t>
22:1 pij
n t t+1
Uj n (t)
21:1 pij

***************Ending Page***************

***************Beginning Page***************
***************page number:108**************
108.
Example: Modelling the waiting and eruption times for
W,
(Yellowstone Park, USA)
Michael Eichler (University of Chicago, Statistics course (24600) - Spring 2004)
R code
Starting values:
p< —c(0.5, 40, 90, 20, 20) (0)
emstep<—function(Y, p) { 19(0) I 0.4 (O)
EZ<—P[1] * dnorm(Y, p[2], Sqrt(p[4]))/ I 40, I 4
(pm * dnormw. p[21,5qrt(p[4])) “in 0%0)
+(1-p[11) * dnormw, p[31.sqrt(p[51>>) H1 I 90, 01 I 4
p[l]<—mean(EZ)
p[2]<—sum(EZ * Y) / sum(EZ)
p[3]<—sum((1 - EZ)*Y) /sum(1-EZ)
p[4]<—sum(EZ * (Y - p[2])A2) / sum(EZ)
p[5]<—Sum((1 - EZ) * (Y - p[3])/\2) /sum(1 - EZ) r.‘
p l
} ‘l
emiteration<—function(Y, p, n:10) { E h I
for (i in (1:n)) { .1
p<—emstep(Y, p) g- 1F r k
} LL 1 F 1 Fl l
p .
} ‘ V 1 . 1
p<—c(0.5, 40, 90, 20, 20) I -l" k
p<—emiteration(Y, p, 20) d‘ I. Ii
p an as 52 5a 5-: m m a2 aa 91 1m
p<—emstep(Y, P) Wailqllna-b-alwa-anmpmmﬂ
P
}

***************Ending Page***************


***************Beginning Page***************
***************page number:109**************
109.
k pug) 1“le lugs} 01M; 02M}
1 0.3508 54.22 79.91 5.465 5.999
2 0.3539 54.38 79.94 5.671 6.013
3 0.3562 54.46 79.99 5.744 5.969
4 0.3578 54.51 80.02 5.787 5.935
5 0.3588 54.55 80.05 5.815 5.912
6 0.3595 54.57 80.06 5.834 5.897
7 0.3600 54.59 80.07 5.846 5.887
8 0.3603 54.60 80.08 5.855 5.880
9 0.3605 54.60 80.08 5.860 5.876
10 0.3606 54.61 80.09 5.864 5.873
11 0.3607 54.61 80.09 5.866 5.871
12 0.3608 54.61 80.09 5.868 5.870
13 0.3608 54.61 80.09 5.869 5.869
14 0.3608 54.61 80.09 5.870 5.869
15 0.3609 54.61 80.09 5.870 5.868
20 0.3609 54.61 80.09 5.871 5.868
25 0.3609 54.61 80.09 5.871 5.868

***************Ending Page***************

***************Beginning Page***************
***************page number:110**************
110.
Exemplifying
some methOdOlOgical iSSllES regarding the application of
the EM algorithmic schema
(using a simple ElVl/GMM algorithm on data from R (7T1 I 7T2 I 1/2))
CMU, 2007 spring, Eric Xing, ﬁnal exam, pr. 1.8

***************Ending Page***************


***************Beginning Page***************
***************page number:111**************
111.
A long time ago there was a village amidst hundreds of lakes. Two types of
ﬁsh lived in the region, but only one type in each lake.
These types of ﬁsh both looked exactly the same, smelled exactly the same
when cooked, and had the exact same delicious taste — except one was poi-
sonous and would kill any villager who ate it. The only other difference
between the ﬁsh was their effect on the pH (acidity) of the lake they occupy.
The pH for lakes occupied by the non-poisonous type of ﬁsh was distributed
according to a Gaussian with unknown mean (psafe) and variance (agafe) and
the pH for lakes occupied by the poisonous type was distributed according
to a different Gaussian with unknown mean (pdeadly) and variance (ageadly).
(Poisonous ﬁsh tended to cause slightly more acidic conditions).
Naturally, the villagers turned to machine learning for help. However, there
was much debate about the right way to apply EM to their problem. For each
of the following procedures, indicate whether it is an accurate implementa-
tion of Expectation-Maximization and will provide a reasonable estimate for
parameters ,u and 02 for each class.

***************Ending Page***************

***************Beginning Page***************
***************page number:112**************
112.
a.
Guess initial values of ,u and 02 for each class.
(1) For each lake, ﬁnd the most likely class of ﬁsh for the lake.
(2) Update the u and 02 values using their maximum likelihood estimates
based on these predictions.
Iterate (1) and (2) until convergence.
b.
For each lake, guess an initial probability that it is safe.
(1) Using these probabilities, ﬁnd the maximum likelihood estimates for the
,u and a values for each class.
(2) Use these estimates of ,u and a to reestimate lake safety probabilities.
Iterate (1) and (2) until convergence.
c.
Compute the mean and variance of the pH levels across all lakes.
Use these values for the ,u, and 02 value of each class of ﬁsh.
(1) Use the ,u and 02 values of each class to compute the belief that each lake
contains poisonous ﬁsh.
(2) Find the maximum likelihood values for ,u and 02.
Iterate (1) and (2) until convergence.

***************Ending Page***************


***************Beginning Page***************
***************page number:113**************
113.
Solution
a. It’ll do ok if we give sensible enough ,u and 02 initial values.
b. Ok, this is the same as a after the ﬁrst M-step.
(See the general EM algorithmic schema on the next slide.)
c. This will be stuck at the initial u and 02:
In the E-step we’ll get:
P - P 1
P(5af@‘$) : M : _
P(x\5afe) - P(safe) + P(5v|deadly) - P(deadly) 2
1
since on one side we assume P(safe) I P(p02'son) I 5 and on the other side
P(a:\safe) I P(:c|p02'son) because psafe I ,Lbdeadzy and 6?an I Ugeadly'
In the M-step ,u and 02 will not change since we are again letting them be
calculated from all lakes (weighted equally).

***************Ending Page***************

***************Beginning Page***************
***************page number:114**************
114.
The [general] EM algorithmic schema
t1!)
h“) —> E[Z | x, hm]
+th|— In P(th)
h(‘+1)= argnhwax EP(Z|X; hm) [In P(X,Zlh)]

***************Ending Page***************


***************Beginning Page***************
***************page number:115**************
115.

The EM algorithm for modeling
mixtures of multi-variate Gaussians

with diagonal covariance matrices

Utah University, Piyush Rai
ML course (CS5350/6350), 2009 fall,
lecture notes, Gaussian Miaztu're Models
[adapted by Liviu Ciortuz]

***************Ending Page***************

***************Beginning Page***************
***************page number:116**************
116.
Fie mixtura de distribuigii gaussiene
K
9mm) I Z @- N<x; W, 021),
3:1
unde
a: € Rd,
probabilitéigile a priori de selecigie 7Tj € 1R satisfac (ca de obicei) restrictiile 7rj Z O
. . K
pentru y : 1,...,K §1 ijlm : 1;
mediile gaussienelor j : 1, . . . ,K sunt vectorii #1 € Rd, iar
matricele de covariantéi ale acestor gaussine sunt identice, ba chiar au forma par-
ticularéi 021', cu 0 E R §i 0 > 0, matricea I ﬁind matricea identitate.
Se considerii instangele 5E1, . . . ,atn € Rd generate cu distribuigia probabilistéi de mai sus
(9mm).
in acest exercitjiu ve§i deduce regulile de actualizare din cadrul [pasului E §i a1 pasului M
a1] algoritmului EM care face estimarea parametrilor 7T nit (7T1, . . . ,7TK),,LL ng' (H1,-- -,HK)
§i 0.

***************Ending Page***************


***************Beginning Page***************
***************page number:117**************
117.
a. Se §tie c5 expresia funcigiei de densitate a distribugiei gaussiene multi-
variate (d-dimensionale) de medie ,u 6 Rd §i matrice de covarianigii E € RdXd
este
1 ( 1< FEW >>

—eXP ——$—H $—,LL a

(v 27W \/ IEI 2
unde a: §i ,u sunt considerati vectori-coloanii din Rd, iar operatorul T desem-
neazéi operagia de transpunere a vectorilor/matricelor.
Aducegi expresia de mai sus la forma cea mai simpléi pentru cazul E I 021.
V51 recomandiim s51 folosigi faptul céi ||x — /1J||2 : (x — #)T(a: — p) I (x — p) - (m — ,u),
unde operatorul - desemneazii produsul scalar a1 vectorilor.
Observajie: Prima din ultimele douéi egalitéti implicé un u§0r abuz (sau, mai
degrabii, 0 conventie) de notafie: 0 matrice realii de dimensiune 1 >< 1 este
identiﬁcaté cu un numiir real, care este chiar singurul ei element. Acelagi tip
de abuz/convenﬁe a intervenit §i in scrierea expresiei exp(. . .) de mai sus.

***************Ending Page***************

***************Beginning Page***************
***************page number:118**************
118.
Answer
2 —1 1
0
and also
‘2| I (02)d :> \/\E| I 0d since a > O.
Therefore,
1 1
Nx; ,EIJ2I : —ex (——a:— TE_1a:— )
(H ) (ﬁwmpfm (M)
1 1 T 1
— Wen) (5(ZUH) EQUAL»
1 1 2
I (—W6XP —ﬁ||x—/1H

***************Ending Page***************


***************Beginning Page***************
***************page number:119**************
119.

b. Vom asocia ﬁecﬁrei instan§e x1- un vector-indicator (mai precis,

un vector de variabile aleatoare ZZ- € {O,1}K, cu zz- n2 (22-1, . . .,zZ-K)

§i 22-]- : 1 dacﬁ §i numai dacé x1- a fost generat de gaussiana

NQC; ILLj7O-2I).

Pentru pasul E al algoritmului EM, ve§i demonstra mai intéi cii

media E[Zij] nét. E[Z¢j|$i;ﬁ,ﬂ,0], unde £131‘ I ($11,. . . ,ind) € Rd, ILL nét.
(M1,---,,LLK) E (Rd)K §i 0 6 R+, are valoarea P(zq;j I 1|;Ui;7r,,u,0), iar

apoi ve§i elabora formula de calcul a acestei probabilitéifgi, folosind
teorema lui Bayes.

***************Ending Page***************

***************Beginning Page***************
***************page number:120**************
120.
Answer
For the sake of simplicity, we will designate E[zij|:ci; 7m, 0] as E[zZ-j].
So,
Eizij] mg. Eizij|$d7Tﬁlei déf'O.P(ZZ'j :0|$i;7T,M,U)+1'P(Zij :1|961;7T7H,U)
I 13(21111 I llwimuﬂ)
Bayis F. P($Z-|zij I 1;7r,,u,0) -P(zZ-j I 1;7r,,u,0)
_ P(:cZ-;7T,,u,0)
— K
Zj’:1 PCUZIIZU’ I 1377-7111170) ' P(Zij’ I 1;7T,,LL,O')
i ( 1 || H2)
—eX —— mi— ' 7T'
i ( 2W0)d p 202 ILL] J
_ 1 1
K 2
'/_ —€X —— ZUZ'— '/ 7T'/
2, _1 (ﬂed 10(262“ My u) J
1
@- eXp Qﬁnwi — IMHQ)
I —1 (13>
K
2H w exp Qﬁuwi — #j’2>

***************Ending Page***************


***************Beginning Page***************
***************page number:121**************
121.

c. Arétaigi c5 expresia funcgiei de log-verosimilitate a datelor
,,complete“ in raport cu parametrii 7T, ,u §i 0 este

n K

1np(513, ZIW, ,u, a) : Z Z 212-an 7Tj +1nN(a:Z-;Iuj,021)),

1;:1 j=1
unde 5U n25. (x1, . . .,£Un) §i z ng' (21, . . . , Zn).
Deduceigi apoi expresia ,,func§iei auxiliare“

QM, M, awn W, A“) If E[1np<w,z\w,0>1,

cu precizarea c5 media aceasta este calculaté in raport cu
distribu§ia / distribugiile P(zZ-j|asi,7r(t), p(t),0(t)).

***************Ending Page***************

***************Beginning Page***************
***************page number:122**************
122.
Answer
The log-verosirnility of the complete data is
1n19<$>zlmll7® défi 1np<<£617 Z1): ' - ' 7 (ZUn, Zn)|7Tn H7 U) Liz'd. ll] Hp(mi7 Zilﬂ-v ILL7 0)
1:1
n mult. rule n
I ZIHPWmZiWMU) I Zlnpmlzwnma) 'P(Z@|W,M,U)
1:1 1:1 T
n K
I 22%‘ [1HN(%|#170)+1H7T1],
1:1 j:1
since zz- I (23-71, . . .,zZ-,j, . . .,zZ-,K), with zm- I 1 and zm-l I 0 for all j’ #j.
Furthermore, using the result we got at part a, we can write
2
lnp(:13,z\vr,p,0) : 23:223-3- [—§ ln(27r02) — FMS“ — ,ujII —|— lnwj] .
Finally, using the linearity of expectation,
Q(W,u,0\7r(t),u(t),v(t)) dif' Ellnp(w,Z|7T,mv)l
2 2
I ggEtzU-l [—§ln(27m )— ﬁIIacZ — ,LLjII + lnwj].

***************Ending Page***************


***************Beginning Page***************
***************page number:123**************
123.
Pentru Pasul M , in contextul precizat in enung, vom avea de
rezolvat urmétoarea problema” de optimizare:
(W(t+1),u(t+1),0(t+1)) I argmaX QUE u, 0W), M“), 0w)’ (14)
7T,/l,,0'
K
. .. (15+1) - _ - (H1) _
cu Test'mcgfzzle 7Tj Z O pentm j — 1, . . . , K 52 ij _ 1.

3:1
Aceastéi probleméi se rezolvéi optimizénd funcgia ei obiectiv in mod
separat in raport cu variabilele W711i §i 0.
d. Aplicagi metoda multiplicatorilor lui Lagrange pentru a re-
zolva problema de optimizare cu restrictii (14) in raport (doar) cu
variabilele 7T.

***************Ending Page***************

***************Beginning Page***************
***************page number:124**************
124.
Answer
For now, we will ignore the constraints 713- Z 0. Thus, the Lagrangean functional asso-
ciated to our optimisation problem is:
K
(W(t+1),u(t+1),v(t+l)) I arsmaﬂQW u, 617%”,th 0(6) — M1 — Z 71.1)), (15)
7r,,u,a,>\ -_
]_1
With A G R playing the role of Lagrangean multiplier. (After solving this optimisation
problem it will be seen that indeed wj Z 0 for all j I 1, . . . ,K.)
By taking the partial derivative of the Lagrangean functional with respect to 113- (with
j€ {1, . . .,K}) and then solving for it,
8 K
87f?‘ j=1
a n K d 2 1 2 n 1
a ZZElZZj] —§1n(27m ) — FHZEZ — ij +ln7rj + A I O (I) ZELZU]; I A
9 2:1 9:1 1:1 9
we will obtain the solution
1 TL
1+1
7T§ > : X ZEpij]. (16)
1:1

***************Ending Page***************


***************Beginning Page***************
***************page number:125**************
125.
Moreover, since these solutions should satisfy the constraint 23111 7r;t+l) : 1, it follows
that
1:1 1:1 1:1 1:1 1:1 1:1
1 11 K 1 11 11
XZZPW- :1|.1~1,11,11,0) :1<:> X21 :1<:> X : 1(:>A:n.
1:1 9:1 1:1
@—/
1
By replacing this value of A back into relation (16), we will get
1 TL
1e 1
@+:;me- (m
1:1
It is obvious that 7T§t+1> Z 0.

***************Ending Page***************

***************Beginning Page***************
***************page number:126**************
e. Optimizati functia Q (i.e., rezolvati problema de optimizare (14)) in raport cu vari- 126-
abilele ,u.
Indicatie: Urméitoarea formuléi de derivare vecto'm'ald (preluaté din documentul Matrix
Identities, de Sam Roweis, 1999) v5 poate ﬁ de folos:
§
(5g) 8—X(Xa + b)TC(Xa + b) : (C + CT)(Xa + b)aT
Answer:
Remember the notation ,u, I (p1, . . . HuK) € (Rd)K, With ,Ltj € Rd for j I 1, . . .,K.
(t) (1:) (t) _ 2 9 / 2
z: J I
I iEizi-i -iv m- - W ‘5%) —i imam - u-)(—1)
1.21 .7 202 PL] J 202 [LII]- .7 .7
1 n 1 TL n
I F ZEizinwi — My‘) I p KZEM-ﬂm) _ (XI-EMA) M1]
71:1 1'21 11:1
After equating this expression to zero (in fact, the column-vector (O, . . . , O)T € Rd), we
get the solution:
[MW-1) I 2?:1 Eizij133i (18)
j 21-11 E1215]

***************Ending Page***************


***************Beginning Page***************
***************page number:127**************
127.
f. Optimizati functia Q (i.e., rezolvati problema de optimizare (14)) in raport cu vari-
abila a.
Answer:
The partial derivative of Q(W,#,U|W(t),#(t), 0w) with respect to 0 is:
362W H U|7T(t) MU) 0(6)
60' 7 7 7 7
: 3 277:5:qu 111127102) - i111: 11-112 +1117?
(90 7:1 9:1 Z] 2 202 Z ‘7 "7
n K
— Z Ema-187 [-5 mm >- gut - mu 1
1:1 3:1
n K n K
d 20 —2 d 1
I 22%] [5 - F - F196: w] I ZZEW [; + gut — W]
1;:1 3:1 1;:1 9:1
1 n K n K 1 n K
I $th ZZEW +ZZE1ZZ-mm - MHZ) I gendﬁ + ZZEvZ-mm - Mu?)
1:1 3:1 1:1 3:1 1:1 3:1
1

***************Ending Page***************

***************Beginning Page***************
***************page number:128**************
128.
~ a (t) (t) (t) -
By equating 8—Q(7T,/L,0"7T ,u ,0 ) to O we get the solutlon:
0
1 n K
(t+1) 2 _ _ _, __ (H1) 2
<0 > - nd ZZELZWHIQJZ #j || 2 0- (19>
2:1 jzl
It is not too difficult to see that this solution leads to the maximization of
Q. (Note that 0 > O and the expression —nd02 —|— 217:1 29111 E[z@j]Ha:i — ,uj||2 as a
function of 02, is linear and decreasing.)

***************Ending Page***************


***************Beginning Page***************
***************page number:129**************
129.
g. Sumarizati rezultatele obtinute la punctele de mai sus (b si d-f), redactand in pseudo-
cod algoritmul EM pentru rezolvarea mixturii de gaussiene din enunt.
Answer:
By gathering the relations (13), (17), (18) and (19), we are now able to write the pseudo-
code of our algorithm:
0 Initialize the a priori probabilities 7r, the means ,u, and the variance 0'2.
0 Iterate until a certain termination condition is met:
Step E: Compute the expectation (i.e., a posterori probabilities) of z variables:
(t) . (t) t 2
(t) vigiE ,. _. (t) (t) (t) _ 7T3‘ N(33aluj >(U()) I)
pij — [ZZJIZMW nu ,0- l — K (t) (t) (t) 2
Zj/Il 7le N($;,uj, ,(0 ) I)
Step M: Compute new values for 71,,u and 0:
<t+1> _ 1 (t) <t+1> _ 2,2129” $1‘ (t+1) 2 _ 1 (t) , (t) 2
711 — g 2%‘ #1 — W (a ) — $221)” llwz —:uj ll
1:1 1:1 p19‘ izl jzl

***************Ending Page***************

***************Beginning Page***************
***************page number:130**************
Important Remark: a slight generalization 130'
Suppose that our mixture model is made of Gaussians with unrestricted di-
agonal covariance matrices, i.e.,

Z, ~ Categorical(p1,.. . ,pK)
(O'j,1)2 0 . . . 0
M331 2 :
XZ-IZZ'IJWV [ 5 ] 0 (0”) '
N ,d
J 0 0 (WV
It can be easily be proven (by going along the lines of the above proof) that
the only update relation that changes in the formulation of the EM algorithm
is (19). It becomes:
TL E Zi' $2, _ (‘15+1) 2
(0%“)? I W—’k”1k) 20 forj: 1...K and k: 1,...,d. (20)
This is indeed what we would expect, given on one hand the fact that the
Gaussian components are mutually independent, and on the other hand the
updating formulas [that we have already obtained] for the EM algorithm for
solving mixtures of uni-variate Gaussians [of unrestricted form].

***************Ending Page***************


***************Beginning Page***************
***************page number:131**************
131.
The EM algorithm for modeling
mixtures 0f multi-variate Gaussians
Stanford University, Prof. Andrew Ng
ML course, 2009, lecture notes, parts VIII and IX
[adapted by Liviu Ciortuz]

***************Ending Page***************

***************Beginning Page***************
***************page number:132**************
132.
Suppose that we are given the instances .901, . . .,:1:n G Rd (all seen
as column-vectors). We wish to model these data by specifying a
joint distribution p(:1:Z-,zz-) I p(xz|zz) p(zZ-). Here,
zz- N Categorical(7r),
K denotes the number of values that the zfs can take on, namely
wj nIt'pQZ- Ij) for j I 1,...,K, with 2217b‘: 1, and
the [conditional] distribution xllzl I j is a Gaussian of mean vector
uj and covariance matrix Ej.
Thus, our model posits that each x1- Was generated by randomly
choosing zz- from {1, . ..,K}, and then 2:2- Was drawn from one of
the K Gaussians, depending on zi. This is called the mixture of
[multi-var'iate] Gaussians model. Remember that
M > 1 1< YEW >
x' 0I—ex ——x— x—
Note that the z/s are latent random variables, meaning that
they’re hidden / unobserved.

***************Ending Page***************


***************Beginning Page***************
***************page number:133**************
133.
[Use the EM general scheme (see Tom Mitchell’s Machine Learning book,
1997, pag. 194-195) to] prove that the EM algorithm for estimating the pa-
rameters 7T, ,u and E of our mixture of multi-variate Gaussian distributions has
the following update rules:
E-step:
no - N z‘; I» El '
ww- :t- Em I Jl$i37Tl>Ml>Ell z —K <3” ” >7” <21)
Zl21N(w¢; M’, E’) 7n
M-step:
1 TL
1:1
2511 wig-$1-
J 21:1 will
Ej I 217:1 wij (113i; WWI/"i — #j)T_ (24)
21:1 we‘
where 7T’, ,u’ and E’ represent the values of our parameters at initialization,
and respectively the previous iteration of the EM algorithm.

***************Ending Page***************

***************Beginning Page***************
***************page number:134**************
. 134.
Hlnti You may ﬁnd useful the following formulas, from Matrix Identities,
by Sam Roweis, 1999 (http://www.cs.nyu.edu/Nroweis/notes/matrixid.pdf).

(16) (14H)T I (AT)_1
l
(2b) A_1\ I —
' |A|
6
(4a) 6—X|AXB\ : |AXB|(X_1)T : |AXB](XT)_1
ﬁ
(4b) 8—X 1H|Xl I (X_1)T I (XT)_1
8 T _ 8 T _
(5b) 8%XTAX z (A+AT)X
§ T T
8 T _ 8 T T _ T
(5e) 8X0, Xa_ 8X0, X a_aa
(5g) 8%(Xa + b)TC(Xa + b) : (C + CT)(Xa + b)aT

***************Ending Page***************


***************Beginning Page***************
***************page number:135**************
135.
Solution
The E-step is easy (use Bayes rule):
wij mg EM I jlfm; 7T’, u’, 3'] I WIZ- I jlwi; 7T’, u’, Z’) I
I p(33i|Zi I MAE’) 19(22- INT’) I NWMCE’) 7T}
Zfilpm-IZZ- I l; w, 2') m- : l; w’) Zfile; w, 2/) m

***************Ending Page***************

***************Beginning Page***************
***************page number:136**************
136.
We will now concentrate 0n the M-step:
According to the general EM scheme, we need t0 maximize, with respect t0
our parameters 71,11, E, the “auxiliary” function
/ / I def-
Q<7T1M>Ei7T1ll1Z) I Ep(zi:j|mi;7r’,11’,2’) lnp<£v 2;,11/9 217T)
n K
1:1 9:1
n K
I Z ZP<Z1Ij|$1;7T/1MI1EI)1H(P($1|Z1:j5M1E)P(Z1:31?»
1:1 9:1
I w1-1I1—-eXp(——(w1—M-)TE-_1(IB1—M'))'7T'
n K 1
I 22%- [—1n<<21>d/2|2111/2> — 5(11- M $51011- — 11>+1n111 <25)
1:1 9:1

***************Ending Page***************


***************Beginning Page***************
***************page number:137**************
137.
First, let’s derive the M-step update rule for 111, With Z I 1, . . . , K.
We have to maximize (25) with respect to ,ul, so let’s compute the corre-
sponding derivative:
n K
l 22w. _1n((27T)d/2|2.|1/2) _ 1(x- _ .)T E—1($. _ .) +1n7T.
(911; U 9 2 l pi? j 1 #9 J
1:1 9:1
I ——ZZw11—<11—111>T211111111)I ——Zw11—2<w1 —111>T2;1<w1 —111>
(9M1 1:1 3:1 2 2 1:1 (9M 1:1
(5_g) 111 __8 n2_1 E—1T _ '
— —§Zw118—2< j +< j > > (11-111)
1:1 m 1:1
(1e) 1 n n — — 1 n n —
I 52111921251121)1><1i_,1j>: 52101122211 (11-111)
1:1 1:1 V i=1 1:1
21'
I 21011 (Ef1$1— Eflm) I Zw112f1$1— Zwuzfl/ljz-
1:1 1:1 1:1
Setting this to zero and solving for ,ul therefore yields the update rule
m I —22711 l -
21:1 wil

***************Ending Page***************

***************Beginning Page***************
***************page number:138**************
138.
Secondly, we’ll derive the M-step updates to Ej, for j : 1, . . . , K.
Grouping together only the terms that depend on Ej in (25), we ﬁnd that we
need to maximize
22711111‘ “W — 5(931' —Hj) j (1%‘ —Hj)
1:1 3:1 9
(2b) n K 1
_ T _
I 22w” [1n \2, H1” - 5m - M) 2,- 1<mi - w]-
1;:1 9:1
We use the usual trick of working with the precision matrix A9- mg. 2951, where
Ej is assumed invertible.
When maximizing the above quantity with respect to Aj by taking derivatives,
we ﬁnd:

***************Ending Page***************


***************Beginning Page***************
***************page number:139**************
139.
8 n 1/2 1 T
@210” IMAM —5($i—#j) Aj<$i—#j)
1 n a 1 n a
I — wi‘—1n|A'|—- wi'— (wwmwwm
2; szj J 2; 98Aj1 J J J1
(4b),(5c) 1 n _ 1 n
I 5 Zwm'Aj 1 — 5 210M131 — WWW — My
1:1 1:1
1 _1 n 1 n T
I 5113- 21%" — 5 Zwijw — MOW — My‘) -
1'21 2'21
Setting this t0 zero and solving, we get:
21:1 wij

***************Ending Page***************

***************Beginning Page***************
***************page number:140**************
140.
Finally, let’s derive the M-step update for the parameters 7Tj.
Grouping together only the terms that depend on 7Tj in (25), we ﬁnd that we
need to maximize
n K
Z: Z wZ-j lIl 7Tj .
3:1 3:1
However, there is an additional constraint that the ﬁfs sum to 1, since they
represent the probabilities 7T3- I p(z3 I j;7r). To deal with the constraint that
K .
23:1 7Tj : 1, we construct the Lagranglan
n K K
L(7T)IZZ’LU¢lel7Tj-|-6 2715-1 7
1:1 3:1 3:1
where [3 is the Lagrange multiplier.
Note: We don’t need to worry about the constraint that 7Tj Z 0, because as
we’ll shortly see, the solution we’ll ﬁnd from this derivation will automatically
satisfy that anyway.

***************Ending Page***************


***************Beginning Page***************
***************page number:141**************
141.
Taking derivatives of £(7T), we ﬁnd:
(9 n w~ 1 n
—L 7T I i —|— I — w-- + .
2I1 1:1
- - . 21-11 we
Settlng th1s to zero and solv1ng, we get 7r]- I _—6.
By using the constraint 23- 7Tj I 1, and given the fact that Z]. wij I 1 since
wij nIt' p(zi I j\x¢;7r’,,u’, Z’), we easily ﬁnd
n K n
1:1 j:1 1:1
We therefore have our M-step derivation for the parameters 7Tj2
1 TL
7Tj I E Zwij,
1I1
and, obviously, wj Z 0.

***************Ending Page***************

***************Beginning Page***************
***************page number:142**************
142.
Remarks
1. Let’s contrast the update rules in the M-step with the formulas we would
have when the zis were known exactly (see the MLE of the parameters of
a single multi-variate Gaussean distribution, CMU, 2010 fall, Aarti Singh,
HWl, pr. 3.2.1):
1i H '}
7T‘ I — ZZ' I 7
J n 1:1 J
,LL' I 221:1 H23 I j}$¢
J 2?:1 1{z@- : j} 7
E‘ _ 22:1 1{Zi I j}(i'31 — WNW — MDT
j — n - 7
213:1 1{Zi I J}
with 1{zi I j} (“indicator functions”) indicating from which Gaussian each
datapoint had come.
They are identical, except that instead of the indicator functions 1{zi : j}
indicating from which Gaussian each datapoint had come, we now have the
wij 7S.

***************Ending Page***************


***************Beginning Page***************
***************page number:143**************
143.
Remarks (cont’d)
2. The EM-algorithm is reminiscent of the K-means clustering algorithm,
except that instead of the “hard” cluster assignments C(i), we have the “soft”
assignments wij.
3. Similar to K -means, the EM algorithm is also susceptible to local optima,
so reinitializing at several different initial parameters may be a good idea.
4. It’s clear that the EM algorithm has a very natural interpretation of
repeatedly trying to guess the unknown zfs.

***************Ending Page***************

***************Beginning Page***************
***************page number:144**************
144.
dis
Example. ‘ﬁlll'.‘.‘éﬁﬁggém 5
‘if-r‘: Ir III...-
M . . - .Mamméé 4
odelllng the waltlng and 4U hﬁﬁﬁmmﬁ 3
q a II- I- ll’ ll’ i I‘ -_- 1hr 1i
eruption times for 5H 'H*W E
- **:#:*:*:*ar ‘I'll-:4-
the Old Fazthful geyser, “m ‘W 1
‘ﬁr-'1.
(Yellowstone Park, USA) 9'1 u
Michael Eichler
University of Chicago . a
Statlstlcs course (24600), . ,_ __._ , -. ,,..,f._-;,:¢__;;.f,1__. 5 E?‘
o ‘in. I‘ :i i -+ - l '- :': I. - -" H,‘
Sprlng 2004 --"-}:=:-;.-.:_-:_?_.-:. t . . 4 ‘5
a $9
4n
50 2 _§
an 3f
minim, .TC' 1 <41?
g time . ED
(mm) 90
u
1m:

***************Ending Page***************


***************Beginning Page***************
***************page number:145**************
145.
A link between
K -means and EM / GMM (the multi-variate case)
CMU, 2008 fall, Eric Xing, HW4, pr. 2.2
(see also CMU, 2010 fall, Aarti Singh, HW4, pr. 1.2)

***************Ending Page***************


***************Beginning Page***************
***************page number:146**************
146.
Given N data points xi, (i = 17 . . .,N), K-means will group them
into K clusters by minimizing the distortion function
N K
J I Z Zwlm — MHZ»
1:1 3:1
where ,uj is the centroid of the j-th cluster, and
"yij : 1 if (1:2- belongs to the j-th cluster and 'yZ-j = 0 otherwise.
In this exercise, we will use the following procedure for K-means:
o Initialize [randomly] the cluster centroids ,uj, j : 1, . . . , K;
o Iterate until convergence:
— for every data point £13m update its cluster assignment:
yij I 1 ifj I arg mink ||xi — MHZ, and yij I O otherwise.
N
— for each cluster j, update its centroid: ,uj : 2211;“?
i:1 Vii

***************Ending Page***************

***************Beginning Page***************
***************page number:147**************
147.
Remember that in GMM, p(:1:) I 2511 7T], N(x|,LLj,Ej), where 7r]- is
the prior [probability] for the jth component, ,uj and Zj are the
mean and covariance matrix for the jth component respectively.
In the E-step of the EM algorithm, we will update
Z] _ z _

2211 wk MW 2k)
Now, suppose that
1L. E], :02], for some 0 > O, and for all k: l,...,K
ZZ. Wk §é 0 fOI‘ [<3 : l, . . . , K [LC: at any iteration of the EM algoritm], and
ZZZ. H513Z —,LLk/H 75 ||£1§7;—,LZ]§H fOI‘ any 116/ 7g k [at any iteration of the EM algoritm].
Under the above assumptions, prove that when 0 e 0+ we will
get p(z,-j I 1|35,-) —> 'yij, where "yij is the cluster assignment used in
K -means.

***************Ending Page***************


***************Beginning Page***************
***************page number:148**************
Answer: 1 148'
. _— ._ . 2
7U Mel-mm» 7T3 exp( 202sz m )
k::1 k: 1 Mk, k: Zkzl 77k exp <_?ﬂ$i _ MAP)
_ 1
— 1 Wk‘ 1 . . 2 . 2

+ 21¢?- ?j eXP FUW — My“ — 119% — Mkll )
Case 1:
If ||xZ — ,ujH : mink3 ||xi — Mk“, then for each k 753' we have Hzcz — /ch||2 — ||xZ — ,ukH2 < 0. Since

1
a —> 0+, it will follow that exp (F($@ _ W112 _ ‘15132 _ Mk2)) —> O. So, p(zZ-j : 1|xz) —> 1.
0
Case 2:
If11$1— MU ¢ mink llflh' — likll, then
o for all k: such that ||a2Z — ,LLjH < “1131' — pk!‘ it will follow (exactly as above) that
1
exp (wai — MHZ — Hwi — we) a 0;
1
o for all k: such that Ha:Z — ,uj|| > ||acZ — ,ukH we will have exp (2—2(agZ _ M112 _ Hg;Z _ Mk2))
0

—> +00.

1

ere ore, p(z j |x ) 1 + 00

***************Ending Page***************

