[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What does ANN stand for?
--InteriorSeparator--
Artificial Neural Networks
--InteriorSeparator--
easy
--InteriorSeparator--
1
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What type of activation function is used in the neural network described in pr.3 on page 3?
--InteriorSeparator--
Threshold (sign)
--InteriorSeparator--
medium
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to the text, which boolean functions can be represented by the units on the hidden level (1, 2, 3, and 4)?
--InteriorSeparator--
(right) x1 AND x2
(right) x1 AND NOT x2
(wrong) NOT x1 OR x2
(wrong) x1 XOR x2
--InteriorSeparator--
medium
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How many hidden layers are required to represent f(x1, x2) = 1 if x1, x2 >= 0 or x1, x2 < 0, and -1 otherwise, in the CMU HW5, pr. 4.1.2 version?
--InteriorSeparator--
Two
--InteriorSeparator--
medium
--InteriorSeparator--
15
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which activation functions are used in the neural network for approximating Lipschitz continuous function f(x) in CMU HW5, pr. 2.3?
--InteriorSeparator--
(right) Identity
(right) Step function
(wrong) Sigmoid
(wrong) ReLU
--InteriorSeparator--
hard
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the key idea for the solution to approximate the function f(x)?
--InteriorSeparator--
Covering the interval [C, D) with intervals of length ε/L.
--InteriorSeparator--
hard
--InteriorSeparator--
22
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to CMU, 2003 fall, T. Mitchell, A. Moore, midterm, pr. 6.21, for a sigmoid threshold unit with inputs x1, x2, and x3, what must be true of the weighted sum for the output to be greater than 0.5, if and only if (x1 AND x2) OR x3 is true?
--InteriorSeparator--
(right) The weighted sum is greater than 0
(wrong) The weighted sum is less than 0
(wrong) The weighted sum equals 0
(wrong) The weighted sum is less than -0.5
--InteriorSeparator--
medium
--InteriorSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
According to Tom Mitchell in Machine Learning, what methods can be used to find weights for a perceptron to represent a specific function?
--InteriorSeparator--
Linear or non-linear programming.
--InteriorSeparator--
medium
--InteriorSeparator--
34
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Does the Perceptron algorithm necessarily make the same number of mistakes on a set of examples S as it does on the same set of examples S' when presented in a different order?
--InteriorSeparator--
No
--InteriorSeparator--
medium
--InteriorSeparator--
36
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In Rosenblatt's Perceptron algorithm, what geometric relationship does the learned weight vector have to the hyperplane?
--InteriorSeparator--
The weight vector is normal (perpendicular) to the hyperplane.
--InteriorSeparator--
medium
--InteriorSeparator--
41
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Under what conditions is the convergence of Rosenblatt's Perceptron algorithm guaranteed?
--InteriorSeparator--
(right) Linearly separable data
(right) Bounded instances
(wrong) Non-linearly separable data
(wrong) Unbounded instances
--InteriorSeparator--
hard
--InteriorSeparator--
43
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In Rosenblatt's Perceptron algorithm with linearly separable data, does the number of iterations depend on the dimension of the space?
--InteriorSeparator--
No
--InteriorSeparator--
hard
--InteriorSeparator--
49
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the context of the gradient descent method, what is the learning rate (represented by η) and how should its value be chosen?
--InteriorSeparator--
A small, positive real number.
--InteriorSeparator--
medium
--InteriorSeparator--
54
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
According to [Hertz et al., 1991], what does the gradient descent-based training algorithm for the linear unit guarantee, if given a sufficiently small learning rate η?
--InteriorSeparator--
(right) Converges to a hypothesis that minimizes the sum of squared errors
(right) Convergence even when the training data contains noise
(wrong) Convergence only when the training data is linearly separable
(wrong) Convergence even if learning rate is large
--InteriorSeparator--
hard
--InteriorSeparator--
59
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Is the 0/1 loss function easily optimizable using gradient descent?
--InteriorSeparator--
No, because it is discontinuous and non-differentiable.
--InteriorSeparator--
medium
--InteriorSeparator--
61
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the log-sigmoid loss function, ℓ(y,f(x))?
--InteriorSeparator--
-log(σ(yf(x)))
--InteriorSeparator--
easy
--InteriorSeparator--
61
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the back-propagation algorithm, what does  δk  represent?
--InteriorSeparator--
δk = f’(netk)(tk – Ok)
--InteriorSeparator--
medium
--InteriorSeparator--
69
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
In the back-propagation algorithm, through which value does the weight wkj influence the error function E?
--InteriorSeparator--
(right) netk
(wrong) yj
(wrong) x_i
(wrong) wji
--InteriorSeparator--
medium
--InteriorSeparator--
72
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the effect of the regularization term added to the error function?
--InteriorSeparator--
It encourages smaller weights, leading to simpler models and preventing overfitting.
--InteriorSeparator--
medium
--InteriorSeparator--
85
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is the cross-entropy error function used for?
--InteriorSeparator--
(right) Training neural networks with sigmoidal units
(right) Determining probabilities for binary labels.
(wrong) For activation functions like ReLU only.
(wrong) Maximizing loss in discrete cases.
--InteriorSeparator--
hard
--InteriorSeparator--
94
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
When does the error function E*(w, ε) become equivalent to E(w)?
--InteriorSeparator--
When ε = 0
--InteriorSeparator--
medium
--InteriorSeparator--
95
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the main difficulty in back-propagation in training deep neural networks?
--InteriorSeparator--
Gradient vanishing/explosion.
--InteriorSeparator--
easy
--InteriorSeparator--
101
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the weights 2T1 initialized to be in the vanishing gradient problem?
--InteriorSeparator--
(right) |wj| < l (j = 1, . . . ,m)
(wrong) |wj| > l (j = 1, . . . ,m)
(wrong) |wj| = 0 (j = 1, . . . ,m)
(wrong) |wj| = infinity (j = 1, . . . ,m)
--InteriorSeparator--
hard
--InteriorSeparator--
101
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the rectified linear (ReL) activation function?
--InteriorSeparator--
σ(x) = max{0,x}
--InteriorSeparator--
easy
--InteriorSeparator--
101
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a way to help prevent the vanishing or exploding gradient?
--InteriorSeparator--
Layer-wise pre-training.
--InteriorSeparator--
hard
--InteriorSeparator--
102
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
In kernelized Perceptron, with what instances will the kernelized Perceptron work?
--InteriorSeparator--
(right) With the images of the instances.
(wrong) Will not work with kernels.
(wrong) With the original instances.
(wrong) With all the instances, images and original.
--InteriorSeparator--
medium
--InteriorSeparator--
111
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What does the expression 22;; 041K(:l:l,:1:Z-) represent in kernelized Perceptron?
--InteriorSeparator--
(right) Predicts a new instance x.
(wrong) Kernels in another space.
(wrong) Coefficient in trasdtum' space.
(wrong) A scalar function.
--InteriorSeparator--
hard
--InteriorSeparator--
113
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How is the kernel function K defined?
--InteriorSeparator--
K(x, y) = φ(x) ⋅ φ(y) where φ is a mapping from Rd to Rm
--InteriorSeparator--
hard
--InteriorSeparator--
111
--FlashCardSeparator--
}], role=model}, finishReason=STOP, avgLogprobs=-0.26943535935551743}]