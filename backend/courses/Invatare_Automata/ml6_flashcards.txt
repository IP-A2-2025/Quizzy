[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What is the core idea behind Bayesian Learning?
--InteriorSeparator--
Combining prior knowledge/probabilities with observed data.
--InteriorSeparator--
easy
--InteriorSeparator--
2
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the practical learning algorithms that Bayesian Methods provide?
--InteriorSeparator--
(right) Naive Bayes learning algorithm
(wrong) Decision Tree learning algorithm
(right) Expectation Maximization (EM) learning algorithm
(wrong) k-Nearest Neighbors algorithm
--InteriorSeparator--
medium
--InteriorSeparator--
2
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the formula for Bayes' Theorem?
--InteriorSeparator--
P(A|B) = P(B|A)P(A) / P(B)
--InteriorSeparator--
easy
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does P(h|D) represent in the context of hypothesis learning using Bayes' Theorem?
--InteriorSeparator--
The a posteriori probability of hypothesis h given data D.
--InteriorSeparator--
medium
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define the Maximum Likelihood (ML) hypothesis.
--InteriorSeparator--
The hypothesis that best explains the training data.
--InteriorSeparator--
easy
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
How is the Maximum A Posteriori (MAP) hypothesis defined?
--InteriorSeparator--
(right) argmax P(h|D)
(wrong) argmin P(D|h)
(right) argmax P(D|h)P(h) / P(D)
(wrong) argmin P(h|D)
--InteriorSeparator--
medium
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the drawback of the brute force MAP hypothesis learning algorithm?
--InteriorSeparator--
Requires computing all probabilities P(D|h) and P(h).
--InteriorSeparator--
medium
--InteriorSeparator--
8
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define the Bayes optimal classification.
--InteriorSeparator--
argmax Σ P(vj|hi)P(hi|D)
--InteriorSeparator--
hard
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a key property of the Gibbs Classifier related to Bayes Optimal Classifier?
--InteriorSeparator--
Expected error is no worse than twice the expected error of Bayes optimal.
--InteriorSeparator--
hard
--InteriorSeparator--
11
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the assumptions of the Naive Bayes Classifier?
--InteriorSeparator--
(right) Target function takes value from a finite set.
(right) Attributes are conditionally independent given the classification.
(wrong) Attributes are conditionally dependent given the classification.
(wrong) Small training data set is available
--InteriorSeparator--
medium
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the Naive Bayes classification rule?
--InteriorSeparator--
argmax P(vj) ∏ P(ai|vj)
--InteriorSeparator--
hard
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How does the Naive Bayes Classifier differ from previous learning algorithms discussed?
--InteriorSeparator--
It does no search through the hypothesis space.
--InteriorSeparator--
medium
--InteriorSeparator--
15
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the typical solution to handle unseen data in Naive Bayes classification?
--InteriorSeparator--
Redefine P(ai|vj) using m-estimates.
--InteriorSeparator--
medium
--InteriorSeparator--
18
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the context of text classification with Naive Bayes, what is the assumption made about word positions?
--InteriorSeparator--
Attributes (word positions) are identically distributed.
--InteriorSeparator--
medium
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the main idea behind the Minimum Description Length (MDL) principle?
--InteriorSeparator--
Prefer the shortest hypothesis.
--InteriorSeparator--
easy
--InteriorSeparator--
25
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
According to the Minimum Description Length (MDL) principle, what does -log2 P(h) represent?
--InteriorSeparator--
The length of h under the optimal code.
--InteriorSeparator--
medium
--InteriorSeparator--
25
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What does MDL trade off?
--InteriorSeparator--
(right) Hypothesis size
(right) Training errors
(wrong) Computational complexity
(wrong) Data size
--InteriorSeparator--
medium
--InteriorSeparator--
27
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What hypothesis does the MAP learner prefers based on description length?
--InteriorSeparator--
The hypothesis h that minimizes LCl(h) + LC2(D|h).
--InteriorSeparator--
hard
--InteriorSeparator--
26
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the maximum likelihood hypothesis (hML) when learning real valued functions with Gaussian noise?
--InteriorSeparator--
The one that minimizes the sum of squared errors.
--InteriorSeparator--
hard
--InteriorSeparator--
30
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a key assumption made when deriving the ML hypothesis as the least squared error hypothesis?
--InteriorSeparator--
Noise is drawn independently for each example.
--InteriorSeparator--
medium
--InteriorSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the scenarios where the Expectation Maximization (EM) algorithm is typically used?
--InteriorSeparator--
(right) Unsupervised learning where the target value is unobservable.
(right) Supervised learning where some instance attributes are unobservable.
(wrong) Supervised learning where all instance attributes are observable.
(wrong) Reinforcement learning where an agent learns through trial and error.
--InteriorSeparator--
medium
--InteriorSeparator--
35
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the general EM problem, what are we trying to maximize?
--InteriorSeparator--
P(X|h)
--InteriorSeparator--
hard
--InteriorSeparator--
36
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What are the two main steps in the EM algorithm?
--InteriorSeparator--
Estimation (E) step and Maximization (M) step.
--InteriorSeparator--
easy
--InteriorSeparator--
38
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What function is maximized in the Maximization (M) step of the EM algorithm?
--InteriorSeparator--
Q(h|h(t)) = E[ln P(Y|h) | X, h(t)]
--InteriorSeparator--
hard
--InteriorSeparator--
38
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does the Baum-Welch Theorem state about the EM algorithm?
--InteriorSeparator--
It converges to a stationary point (local maximum) of the likelihood function P(X|h) when Q is continuous.
--InteriorSeparator--
hard
--InteriorSeparator--
42
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Why are Bayesian Belief Networks interesting?
--InteriorSeparator--
They describe conditional independence among subsets of variables.
--InteriorSeparator--
medium
--InteriorSeparator--
49
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Define conditional independence of X and Y given Z.
--InteriorSeparator--
P(X|Y, Z) = P(X|Z)
--InteriorSeparator--
medium
--InteriorSeparator--
50
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How is a Bayesian Belief Network defined?
--InteriorSeparator--
By a directed acyclic graph and a table of local conditional probabilities.
--InteriorSeparator--
easy
--InteriorSeparator--
51
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does P(y1, ..., yn) = ∏ P(yi|Parents(Yi)) represent in the context of Bayesian Belief Networks?
--InteriorSeparator--
The joint probability distribution over all variables.
--InteriorSeparator--
hard
--InteriorSeparator--
52
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a challenge related to inference in Bayesian Networks?
--InteriorSeparator--
Exact inference is an NP-hard problem.
--InteriorSeparator--
medium
--InteriorSeparator--
54
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is required to apply the gradient ascent in Bayesian Belief Networks?
--InteriorSeparator--
To renormalize the wijk to assure  ∑ wijk = 1 and 0 ≤ wijk ≤ 1
--InteriorSeparator--
hard
--InteriorSeparator--
58
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Describe the main idea behind using the EM algorithm for learning Bayesian Belief Networks.
--InteriorSeparator--
Iteratively estimate probabilities of unobserved variables and then calculate new parameters to maximize the expected log-likelihood.
--InteriorSeparator--
hard
--InteriorSeparator--
61
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a common approach to learning Bayesian Belief Networks when the structure is unknown?
--InteriorSeparator--
Greedy search.
--InteriorSeparator--
medium
--InteriorSeparator--
62
}], role=model}, finishReason=STOP, avgLogprobs=-0.11526853917008739}]