[{content={parts=[{text=```
--FlashCardSeparator--
Single
--InteriorSeparator--
What is the conditional probability distribution of Y given X and parameter 'a' in a one-feature linear regression model?
--InteriorSeparator--
N(aX, σ²)
--InteriorSeparator--
easy
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following equations correctly represent the maximum likelihood problem for estimating 'a', assuming 'σ' is known? (Choose all that apply)
--InteriorSeparator--
(right) arg maxₐ Σᵢ -((Yᵢ - aXᵢ)² / (2σ²))
(right) arg minₐ Σᵢ (Yᵢ - aXᵢ)²
(wrong) arg maxₐ Πᵢ exp(-((Yᵢ - aXᵢ)² / (2σ²)))
(wrong) arg maxₐ Σᵢ(Yᵢ - aXᵢ)²
--InteriorSeparator--
hard
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the maximum likelihood estimate (aMLE) of parameter 'a' in linear regression with one parameter and no offset?
--InteriorSeparator--
aMLE = Σᵢ(XᵢYᵢ) / Σᵢ(Xᵢ²)
--InteriorSeparator--
medium
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the MAP estimate of 'a' given prior a ~ N(0, λ²) and σ = 1?
--InteriorSeparator--
aMAP = (Σᵢ(XᵢYᵢ)) / (Σᵢ(Xᵢ²) + (1/λ²))
--InteriorSeparator--
hard
--InteriorSeparator--
8
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What happens to aMLE and aMAP as the amount of training data (n) approaches infinity, assuming a fixed prior parameter λ?
--InteriorSeparator--
They become closer together.
--InteriorSeparator--
medium
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In linear regression with Gaussian noise (Yᵢ = Xᵢᵀw + b + εᵢ), what problem is equivalent to solving with the MLE method?
--InteriorSeparator--
Solving the Least Squares problem: arg min (Y - X'θ)ᵀ(Y - X'θ), where θ = (b, w)ᵀ
--InteriorSeparator--
medium
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the log-likelihood £(Y|θ) of the data in linear regression with Gaussian noise, assuming i.i.d. errors?
--InteriorSeparator--
£(Y|θ) = -n ln(√(2π)σ) - (1/(2σ²)) Σᵢ(Yᵢ - Xᵢᵀw - b)²
--InteriorSeparator--
medium
--InteriorSeparator--
14
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the optimal θ that maximizes the log-likelihood in general linear regression?
--InteriorSeparator--
θ = (X'ᵀX')⁻¹X'ᵀY
--InteriorSeparator--
hard
--InteriorSeparator--
16
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In nonlinear regression using higher order terms φ(Xᵢ), what is the optimal θ?
--InteriorSeparator--
θ' = (φ(X)ᵀφ(X))⁻¹φ(X)ᵀY
--InteriorSeparator--
medium
--InteriorSeparator--
18
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the problem that Ridge Regression aims to solve?
--InteriorSeparator--
Addresses issues from correlated predictor variables, complex models, and cases with more predictors than examples.
--InteriorSeparator--
medium
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the optimization problem solved by Ridge Regression?
--InteriorSeparator--
arg min ||Y - X'θ||² + λ||θ||²
--InteriorSeparator--
medium
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the solution to Ridge Regression?
--InteriorSeparator--
θ* = (X'ᵀX' + λI)⁻¹X'ᵀY
--InteriorSeparator--
hard
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
If θ has a prior distribution θ ~ N(0, τ²I), what is the posterior distribution of θ|Y?
--InteriorSeparator--
h(θ|Y) ∝ exp(-(1/(2σ²))||Y - X'θ||² - (1/(2τ²))θᵀθ)
--InteriorSeparator--
hard
--InteriorSeparator--
23
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In MAP estimation, how is λ related to σ and τ when solving the ridge regression problem?
--InteriorSeparator--
λ = σ²/τ²
--InteriorSeparator--
hard
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How does the regularization term in Ridge Regression alleviate the potential problem of overfitting?
--InteriorSeparator--
It penalizes large components in θ, shrinking θ and encouraging the model to avoid overfitting and adjusting to outlier data points.
--InteriorSeparator--
medium
--InteriorSeparator--
25
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the batch (steepest descent) update rule for linear regression with L2 regularization?
--InteriorSeparator--
θᵗ = θᵗ⁻¹ - η (Σᵢ(-2YᵢXᵢᵀ + 2XᵢᵀXᵢθᵗ⁻¹) + 2λθᵗ⁻¹)
--InteriorSeparator--
hard
--InteriorSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the sequential (online) update rule for linear regression with L2 regularization?
--InteriorSeparator--
θᵗ = θᵗ⁻¹ - η(-2YᵢXᵢᵀ + 2XᵢᵀXᵢθᵗ⁻¹ + 2λθᵗ⁻¹)
--InteriorSeparator--
medium
--InteriorSeparator--
30
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the update rule for Newton's method in a multi-dimensional setting?
--InteriorSeparator--
θʲ⁺¹ = θʲ - H⁻¹∇θl(θ)
--InteriorSeparator--
medium
--InteriorSeparator--
33
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How many iterations does Newton's method typically take to converge to θ when solving a least squares problem?
--InteriorSeparator--
One.
--InteriorSeparator--
medium
--InteriorSeparator--
35
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the MLE estimate in weighted least squares?
--InteriorSeparator--
θMLE = (XᵀWX)⁻¹XᵀWy
--InteriorSeparator--
hard
--InteriorSeparator--
41
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the advantage of using a weighted least-square estimator compared to a non-weighted version?
--InteriorSeparator--
Allows for downweighting of outlier data points by the inverse of the variance term.
--InteriorSeparator--
medium
--InteriorSeparator--
43
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the loss function to minimize for MLE with Laplace noise?
--InteriorSeparator--
J(β) = Σᵢ |Yᵢ - Xᵢᵀβ|
--InteriorSeparator--
medium
--InteriorSeparator--
44
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is an advantage of using the Laplace noise model compared to the Gaussian noise model?
--InteriorSeparator--
The Laplace distribution has heavier tails than the Gaussian distribution.
--InteriorSeparator--
medium
--InteriorSeparator--
45
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In linear regression, how does scaling features affect predictions?
--InteriorSeparator--
It does not change the regression predictions.
--InteriorSeparator--
medium
--InteriorSeparator--
47
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Is the least squares estimator biased or unbiased?
--InteriorSeparator--
Unbiased.
--InteriorSeparator--
medium
--InteriorSeparator--
51
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the covariance matrix of the least squares estimator?
--InteriorSeparator--
σ²(XᵀX)⁻¹
--InteriorSeparator--
medium
--InteriorSeparator--
53
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Is the Ridge estimator biased or unbiased?
--InteriorSeparator--
Biased.
--InteriorSeparator--
medium
--InteriorSeparator--
55
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In linear regression in R², what is the relationship between y, x, and the error term, e?
--InteriorSeparator--
y = c1x1 + c2x2 + e
--InteriorSeparator--
easy
--InteriorSeparator--
57
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In weighted regression in R², what is the solution (c) when the errors are independent Gaussian random variables?
--InteriorSeparator--
c = (XTWTWX)⁻¹XTWTWy
--InteriorSeparator--
hard
--InteriorSeparator--
59
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the conditional log-likelihood function for logistic regression?
--InteriorSeparator--
L(w) = Σᵢ(yᵢln(σ(wᵀxᵢ)) + (1 - yᵢ)ln(1 - σ(wᵀxᵢ)))
--InteriorSeparator--
medium
--InteriorSeparator--
63
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the partial derivative of the conditional log-likelihood function with respect to each parameter wj in logistic regression?
--InteriorSeparator--
∂L(w)/∂wj = Σᵢ(yᵢ - σ(wᵀxᵢ))xᵢⱼ
--InteriorSeparator--
medium
--InteriorSeparator--
67
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In logistic regression, what is the relationship between J(w) and  £(w) when yᵢ ∈ {-1, 1}
--InteriorSeparator--
-£(w)/n = J(w)
--InteriorSeparator--
hard
--InteriorSeparator--
77
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a key property of the Hessian matrix in logistic regression?
--InteriorSeparator--
It is negative semi-definite.
--InteriorSeparator--
hard
--InteriorSeparator--
82
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How does variable duplication affect logistic regression?
--InteriorSeparator--
Logistic regression will simply split the weight when facing the duplicated variable.
--InteriorSeparator--
medium
--InteriorSeparator--
87
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Describe the kernel trick idea.
--InteriorSeparator--
Map a feature vector in low dimensional space X into a higher dimensional space Z.
--InteriorSeparator--
medium
--InteriorSeparator--
89
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is P(Y=1|φ(X)) after applying the kernelization trick?
--InteriorSeparator--
σ(α₀ + Σⱼ αⱼK(X⁽ʲ⁾, X))
--InteriorSeparator--
hard
--InteriorSeparator--
92
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Does softmax only specify a posterior, or a joint distribution?
--InteriorSeparator--
Only a posterior.
--InteriorSeparator--
medium
--InteriorSeparator--
112
--FlashCardSeparator--
```}], role=model}, finishReason=STOP, avgLogprobs=-0.1567583851965515}]