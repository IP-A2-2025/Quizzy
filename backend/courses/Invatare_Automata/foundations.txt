***************Beginning Page***************
***************page number:1**************
1
Basic Statistics and Probability Theory
Based on
“Foundations of Statistical NLP”
C. Manning 84 H, Schiitzc, ch. 2, MIT Press, 2002
“Probability theory is nothing but common “me
reduced to calculatium"
Pierre simnn, IWaI-quis ‘19 Laplare (lwnszv)

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2
PLAN
1. Elementary Probability Notions:
0 Salnple Space, Event Space, and Probability Function
I Conditional Probability
0 Bayes’ Theorem
O Independence of Probabilistic Events
24 Random Variables:
I Discrete Variables anrl Continuous Variables
0 Mean, Variance and Standard Deviation
I Standard Distributions
0 Joint, Niarginal and and Conditional Distributions
I Independence 0i‘ Random Variable;

***************Ending Page***************

***************Beginning Page***************
***************page number:3**************
a
PLAN (conﬁd)

34 Limit Theorems

a Laws of Large Numbers

I Contra] Limit Theorems

4t Estimating the parameters of probabilistic models from
data

I Linximum Liknlihnod Eitiinntinn (NILE)

u Maximum A Posteriori (RIAP) Estimation

54 Elementary Information Theory

r Entropy; Conditional Entropy; Jnint Entropy

0 Information Gain / Niutual Information

- CrewEntrepy

u Iznlntive Entropy / Kullhack-Loihlcr (KL) Divergence

c Properties: bounds, chain rules. (non-)symmetries.
propel-tie; pertaining to independence

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
l
1. Elementary Probability Notions
o sample suaee= l'z (either discrete ul- continuous)
0 event: 115:2
e the certain event: i1
the impossible event: w
elementary event: any {e}, where e e S1
o event space: f : 2" (or a subspace oi 2" that contains ii and is eloseol
undel- eoniplement and Pountable union)
n probability function/distribution: P 1 F ~> [it l] such that:
rm.) e l
, the “countable ndditivity" property;
v74, ,AA disjoint events, Huh] I 2PM)
Consequence: fur a uniform distribution in a ﬁnite sarnple spaee;
Pi 4) i #fnwclmhle elementary events
' Mall elementary events

***************Ending Page***************

***************Beginning Page***************
***************page number:5**************
Conditional Probability
P A Fl I3

n HAM?) I jam)

Note: m4 l u) is called the a pusteriory probability of A. given B.
. The “multiplication” rule:

PM n n) : PM l mm?) : Pml 4)P(.-l)
. The “chain” rule:

m4, may. 1.4") a

P(A,)P(AZ l Agni l A, A» “PM” ‘AMA; .Anai)

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
ﬁ
o The “total probability” formula:
P(A) : P(‘-1\B)P(B)+ 13(‘4 \ ﬁB)P(ﬁl})
More generally:
if ,1 g m, and \1/ ¢ / n, n 1?, I v1, Him)
PM) I Z, PM \ BYJPUL)
o Bayes’ Theorem:
, i PM‘ 1;) P(I$)
m3 \ A) i iPM)
, i w
“r Pm ‘ A) i P(/HBJP(B)AP(A \ 0mm)
or

***************Ending Page***************

***************Beginning Page***************
***************page number:7**************
Independence 0f Probabilistic Events

e Independent nvnnti: PM m l?) : P(A)P[B)
Note: When Pm) # U, the. above deﬁnition is equivalem to
11.4 B) i 1-way

e Conditionally independent events:
PM m I? i c) : Pm i mm; i c), nSilnningT ni cnurin‘ tnnt
11(1) ¢ 0i
Note: wnen mam‘) ¢ n, the above deﬁnition is equivalent
tn miiiur) i 1mm),

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
R
2. Random Variables
2.1 Basic Deﬁnitions
Let n be a sample space, and
P: 2“ H [0,1] a probability function.
I A random variable of distribution 1’ is a function
X ﬂaw
o For now, let us consider n i ii
o The cumulative distribution function of X is F R A [n ac) deﬁned by
rm i mx g 1) i l’((w€ n \ xm g rh

***************Ending Page***************

***************Beginning Page***************
***************page number:9**************
2.2 Discrete Random Variables q
Deﬁnition: Lct P 2" A [0.1] be a probability innctinn, and x be a randcrn
variable of distribution P.
c If vﬂt<x1 is either linite or nnlinite countabla tlien
x is called a discrete random variable.
a For such a variable wc rlnﬁnc Hm probability mass function (plnf)
,7 12a lull asp“) "f 170; e i) “i! meal l ma) e ii).
(Obvibnsly, it follows tliat Z‘ mm, ])(.'Iv) a 1.)
Mean, Variance, and Standard Deviation:
c Expectation / mean 0f x:
mx) "é npc] : 2, rm) if x is n discrete i-nndnni variablc.
c Variance of X: Var(X) '2" Vm1)<\ I E(()< a 500)‘).
0 Standard deviation: r1 : v/vtm Y).
Cevarianee of ,\ and Y, twe random variables er distribution l’:
0 Cov[X.Y] : E[(X e E[X1)(l’ e E[Y])]

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
1F!
Exempliﬁcation:
. the Bernoulli distribution: mm 1,)
lnnnn: ,I. variance: 1/(1 in), entropy: Wm”, i (1 ill) 1%(1 ill)
o the Binomial distribution]: mi u [I] i (3', 1/’(1 *I))"7' for: in, . n
rnnan: Hp. variance: Wu , 1,)
Elnomlal pmbablllly mass luncﬂnll Blnnmlal cumullﬂve disﬁlbmlon lunctloll
g 5 {H'-
g : 5:33-2:22 .1 f,‘ .-
g . - p=n5_n:|n e , . ,-
'1‘ I I .
_ Q .. a ,
is a I \ \ T <= ~ ‘ .
5 g . - ﬁ r I I ,I
z g I I .' '. Z ‘ ‘ I
- i- a I I ,-
8 I ,\.-\ -. g r,- , i mum
a , ‘ _ . , ‘ _ . PM.”
.m- I.‘- - h . .MW.“
g cut-I‘ ~ad— z d-ﬂ'
I w in w w v w 20 an w

***************Ending Page***************

***************Beginning Page***************
***************page number:11**************
ll
2.3 Contlnuous Random Variables
Daﬁnitions=
Lat P 2" i [u l] be a probability runrtian, and
x [2 A 1R be a random variable of distribution P.
o 1f Vﬂl(/\] is unﬁnite non_eountable set, and
F, tlio euinulatiye distribution function of X is continuous_ tlien
X is called a continuous random variable.
(It follows, naturally, tlrat P(/\ a i) a U, for all .i e 1R.)
0 If thorn exists 1i lR H [0. ao) inch that Fm : 11x limb/l,
then x is called absolutely cuntinuous.
In suelr a case, n is called tire probability density function (pdf) of X.
o For I? g ‘R for which _/B,i(i)ili exists, P(X ‘(B1): [Billion
whore x112) '2‘ {a e s1 l Xlol e B}.
In partioular, 1*: 1)(r)rh a 1.
o Expeetation / mean of X: mx) ‘i’ E[X e f i,,i(.i)i1i,.

***************Ending Page***************


***************Beginning Page***************
***************page number:12**************
13
Exempliﬁcation:
,i' n)"
- Normal (Gaussean) distribution: 1mm”) i ﬁe T”
m

mean: ,i, variance: r12
o Standard Normal di>tributiom NUIU l)
- Remark:

For H.” Slllfh thm, HM ,p] > 5, nip, Binomial dish-ihutinns can he

approximated by Normal distributions.

***************Ending Page***************

***************Beginning Page***************
***************page number:13**************
Li
Gaussian Prohabiliiy densily 'unﬂion Gaussian cumumive dismhuiion 'unctinn
g q
-; : 1119;2an =
z N _“VQM,,
21110115“
V. 1 n 2 ‘ 4 V, a 1 .

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
2.4 Basic Properties of Random Variables u
Let P , 2" H [0.1] be a probability function.
,\ Q H JR" be a random discrete/continuous vuriuhle of distribution 1’,
0 If 1/ I?” > R'" is a ruurtinn, thnn ﬂu’) is a random vnrinhh».
If (7(X) is discrete, then E(r/[X)) : XII/[1)7!(.r).
If th) is continuous, then E[q(X]) : f,,(.~)p(i>ilr.
o If h is non-linear #t E(q(X)) i “5(a),
0 EuiX) : “5(X).
n mx + Y] : 1?(x)+ mm therefore E[Z:‘:lu!X,] : 21;, t,E[xx].
0 vmu) : "Ii/army
o vmxih) : VuﬂX).
t va) : E(X1) i E1(X]t
0 Cow/[XX] : E[XY] r E[X]E[Y].

***************Ending Page***************

***************Beginning Page***************
***************page number:15**************
15
2.5 Joint, Marginal and Conditional Distributions
Exempliﬁcation for the bi-variate case:
Let r2 be a sample space, P 2“ » [0.1] a probability flirtation, and
V , 12 A llv be a random variabla of distribution P.
One could naturally see v as a pair of two random variables ,\ ll a 1R and
Y a :2 » Rt (More precisely, \"(w) : (no) : (X(»J).Y[w])4)
n the joint prnf/pur of X and Y is deﬁned by
mam"! Wynn/1i 11(x a i.l/ a y) a mu s 12 l XM a i:Y[ul1*!/).
0 the marginal prnr/pulf functions of X and Y are:
for the discrete case:
PM") I 2,,1'(17l/)i WM I 2,1411 l/l
for tho continuous case:
PXU) i J;,l1(l l/J dy, l1» (l/J i .l, Pill/>111‘
n the conditional pmf/pdf of X given Y is:
, i I'\ t (Ml)
PXl) (I W) WW)

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
lﬁ
2.6 Independence of Random Variables
Deﬁnitions:
0 Lot X Y ho random variables of the Same typo (Ln. either tlisoroto or
continuous). and in, their joint prur/pdil
X and Y are said L0 be independent ir
lu l’(" u) I MU] 'I'l (u)
for all possible values i- and 4, of X and Y respectively.
n Similarly, let XJ’ and z he random variables of tho same typo, and p
their joint oral/pal:
X and Y are conditionally independent given Z ir
l/XilAl l/ l I) rlmm ii) [hm/l1)
for all possible values r. l/ and 1 or X. l' and Z respectively.

***************Ending Page***************

***************Beginning Page***************
***************page number:17**************
11
Properties of random variables pertaining t0 independence
0 If X.Y are independent‘ then
V01‘(X + Y) = Vur(X) + Varﬂ’).
e If x, i’ are independent. tiren
E(XY) : E[X)E(Y), ie Cov(X.Y] : u.
o 00va Y] e 0 p XJ' are independent.
e Tire covariance matrix earreepdnding td a vector of random variables
is eymnietrie and positive semi-(inﬁnite.
I If the covariance matrix of a multi-variate Gaussian distribution is
diagenaL tiien tire marginal distributions are independent.

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
IR
3. Limit Theorems
[ Sheldon Ross, A ﬁrst eourse in probability, 5th eels, was 1
"The mest important results in prebability theory are limit then-
rems, of these, the most important are."
laws of large numbers, eeneerned with stating conditions under
wliieli Lhe average er a sequenee of random variables converges (in
some sense) te the expected average:
central limit theorems, eoneerned with determining the conditions
under which the sum of a large number of random variables has a
probability distribution that is approximately normal."

***************Ending Page***************

***************Beginning Page***************
***************page number:19**************
10
Two Probability Bounds
Markov’s inequality:
If X is a random variable that takes only non'negative values,
then for any value u > 0,
. I: X
m 2/1) g L 1
1x
Chebyshev’s inequality:
If X is a random variable with ﬁnite mean p and variance '12,
then for any value 11 > o_
Pu X i/l \211) s ll,
u'
Note: As Cliebysliev’s inequality is valid for all distributions of the
random variable X_ we cannot expect the bound of the probability to
be very close m thc actual probability in most cases. (Sec ox. 2b, pag.
s91 in Russ‘ book.)

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
2F!
The weak law of large numbers
[ Bernoulli; Khintchine ]
Lni Xl X2. XH 1m a SOqllCHCQ (if inrlﬁpﬁndbnf and idnmimny
distributed random variables, each having a ﬁnite mean Em} i /i.
Then, fur any value F ~> 0,
.\' \'
PGMihi/WZr) HMSMX
H

***************Ending Page***************

***************Beginning Page***************
***************page number:21**************
2|
The central limit theorem
for i.i.d. random variables
[ Pierre Simon, Marquis de Laplace; Llapunoff in 1901-1902 ]
Let x, X2 _ lg, he a sequence of independent random variables,
each having ﬁnite mean ,1 and ﬁnite variance (12.
Then the distribution of
X, +, +XH i 71/1
a w
tends to he the standard normal (Gaussian) as n > ae.
That is, for ex < a < 0c.
Y X e l " 7,
P( " ‘ " Mg”) > /w~lnasn Ma
n \m ﬂ ex

***************Ending Page***************


***************Beginning Page***************
***************page number:22**************
22
The central limit theorem
for independent random variables
Let x.. xi... \,, be a sequenee of independent random variables having
respeetive means ,i, and variances (1,24
If
(a) the variables X, are unifnrmly bounded,
Le. for sulne A1 e iv Pu X, \< 11) :i fer all 1,
and
(b) ZZZ "F * cc,
then
H X, i i
1' W511 #1’(1i]asr|~i'>o
Vie, "I
where <1> is tlie eurnulative distributien funetien rer the standard normal
(Gaussian) disirihuﬂon.

***************Ending Page***************

***************Beginning Page***************
***************page number:23**************
23
The strong law of large numbers

Let thln .x,‘ be a sequence of independem and identically
distributed random variables< each having a ﬁnite mean E[X‘] : w.
Then, wi'h pmrmhuny 1,

X1 + + Xn

i A ,, as n ~> ee

n
That is,
P ( um (Xl i + .\',‘)/” e w) ,1

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
21
Other Probability Bounds
One-sided Chebyshev inequality:
If x is a random variable with mean o and ﬁnite variance a‘,
then fur any (1 :1 U.
a’
1* A ‘ < ,i
( 5")’ a1+u~’
Cornllary:
HEW :1" Var(/\] I "I, then for (1 > 0 ,
I,’
1’ x > < i
( ,;1+11J, nz+111
2
P(\’S/I*"] 2%
0 \ H’

***************Ending Page***************

***************Beginning Page***************
***************page number:25**************
25
Other Probability Bounds (c0nt’d)
Chernoff bounds:
If x is a random variable, then lull) "1’ 1;‘ w], is called the moment
generating function of X.
It can be shown that
P[X z u] 5 (Hum) for all I > n
P[X 5 u] g Hum) for all f <11
Chernoﬁ" bounds for the standard normal distribution:
If 2 is n s'mldarrl normal randnm varinhlm
then .\l(l) “2 5W] “2%”.
It can bB shown that
m2 211) 5M“ for all u > U
mz g h) g l *""'~’ for all H < ll

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
2G
Other Probability Bounds (c0nt’d)
Hoeffding bounds:
LeL X‘. .XH be some independent random variables, each X, being
bounded by we interval [1,, m].
If X’ ":” j‘ 27:, X‘, than n followi um
for any r 3 a
1 1
PU? i 15W 2 f) S (‘Xv *_2Hit
LU (In i w)“
V , 21H‘
’ ‘ ’ i ' > < \x if
1 “W x *1 ““( zigumw)
, , 2M1
. i L ‘ > < ‘Z ~ if
q PM PM ’ n ’ W< 2L‘("1*'11]2)

***************Ending Page***************

***************Beginning Page***************
***************page number:27**************
27
4. Estimation/inference of the parameters of
probabilistic models from data
(based on [Dnrbin et a1, Biological Slupumui Analysis. was],
p. 311-313, 319421)

A probabilistic model can be anything from a simple distribution
to a complex stochastic grammar with rnany iinplicit probability
distributions. Once the type of the model is chosen, the pararnee
tors have to ho inferred from doth.

We will ﬁrst consider the case of the categorical distribution. and
then we will present the diirerent strategies that can be used in
general.

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
2R
A case study: Estimation of the parameters of
a categorical distribution from data

Assume that the observations fur emmple. when rnlling a die about
which we don't know whether it is fair or not, or when counting the number
of times the amino acid i occurs in a column of a multiple sequenee align
ment i can be expressed as counts n‘ for each uuteume l (I e 1.! ..K).
and we want to estimate the probabilities H, of the underlying distributicuit
Case 1:
When we have plenty of data, it is natural tu use the maximum likeli-
hood (ML) selutien, Le. the observed frequency w’ e 2‘ :' 'v.

n i i

l J
Nutc: it is easy to shew that indeed P<7l MM”) > Pr” l0] for any 0 ¢ 0M‘;

H” l Um) i “Mm/W i "an i , ML mm
in 1’<~ ‘ 1/) eln My‘ simln a‘ e .t 2e hi 1/‘ >0

The inequality follows from the fact that the relative entropy is always
positive except when the two distributions are identitalt

***************Ending Page***************

***************Beginning Page***************
***************page number:29**************
Case 2: 2“
When the data is scarce, it is not clear what is the best estiinatei
Iii general, we should use prior knowledge, via Bayesian statistics.
For instance. one can usc the Dirichlet distribution with parainntnrs n.
if Pm i mm i in
PM i H) e it“)
It can be shuwn (see calculus on R. Durbin Et. al. BSA hODk. pag. 320)
that the posterior mean estimation (PME) ofthc parameters is
HFME "'J t/iz-u’ a V! 1w e i“ H"
i ( l J A + Z, (M
The (Us arc likc pseudncounts added to the real counts. (If WC think of the
(i'is as extra uhservations added tD the real Ones. this is precisely the lVIL
estimate!) This makes the Dirithlet regulariser very intuitive‘
HOW to use the pseudocounts: If it is fairly obvious that a certain residue,
let’s say i, is very common, than We should give it a very high pseudocciunt
1.‘; ir the residue j ls genel'aly i-ei-e, we sllollltl give ii a 10w pSelldOCOuIlt-

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
30
Strategies to be used in the general case
At The Maximum Likelihood (ML) Estimate
When we wish to infer the parameters t2 : (9,) for a model M from a set
of data D, the most obvious strategy is to maximise P[I7 l 1) \I] over nll
possible imlues of a. Formally:
9”‘ : ruemekPiD l e. M)
i
Note: Generally speaking, when we treat Pt, l l1] as a hrnetion or r (and
H is ﬁxed)_ we refer to it as n probabiliiy. When we treat P(1, l y) ns n
funetion of i/ (and I is ﬁxed), we call it a likelihood. Note that a likelihood
is not a probability distribution or density; it is simply a fnnotion of the
variable U.
A serious drawbaek of maximum likelihood is that it gives poor results
when data is soarte. The solution then is to introduce more prior knowl-
edge, using Bayes’ theorem. (In the Bayesian framework the parameters
are themselves seen as random variables!)

***************Ending Page***************

***************Beginning Page***************
***************page number:31**************
B. The Maximum A posterior: Probab1llty (MAP) Estimate
rim" "'J .hnnnrlwi mil *dilllliiYw
‘l ( ) *i Pin i M}
e .trgiliAYPU)‘ﬁ.1/)I)((/ i1)

t
The prior probability 1’(H i M) has to be chosen in some reasonable manner,
and this is the art of Bayesian estimation (although this freedom tu choose
a prior has made Bayesian statisties eentroversial at times...).
C. The Posterior Mean Estimator (PME)

9"“ : /HP[H i n was
where the integra! is over all probability vectors, i.e. all those that Sum to
one.
D. Yet anuther solution is to use the posterior probability Pts i all) to
sample from it (see [Dirrhin et a1, 199$], seetinn 11.4) and thereby lneate
reginns of high probability for the model parameters.

***************Ending Page***************


***************Beginning Page***************
***************page number:32**************
5. Elementary Information Theory 33
Deﬁnitions:
Let x and Y he discrete random variables.
n Entropy: H(X) ‘L’ gm) loglmiy) i , Linnlnnlm) i
FM ‘0511700-
cm. n p11] in n.“ W n." md nnnmn 7 n
a Speciﬁc Conditional entropy: my \ x I 1) ‘2' *2ch p(;/ \ .r) 1% p(!/ \
7,]
0 Average conditional entropy:
HO’ \ X) "5 X,<XP(')H(Y \ X *1)"'51"*Z,¢\ [M 11(1-1/)1vg11>(!/\ I).
n Joint entropy:
H(X. mg izwngn] 10g2p(1.1,)“°l‘ H(x)+11mxy‘ﬂ" 1-1(y)+1-1(xm‘
. Information gain (or: Mutual informaﬁnn):
[C(X Y) L" HLX) , mx \ Y] “"é" 1m") , 1m’ \ X)
‘"3" "(x Y) , mx \ Y) i my \ x) i moxx).

***************Ending Page***************

***************Beginning Page***************
***************page number:33**************
.Xd
Exempliﬁcation: Entropy of a Bernoulli Distribution
11m: [110ng (i ,iniw'i ,i)
ii 02 i, ii 0i in
p

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
si
Basic properties of
Entropy, Conditional Entropy, Joint Entropy and
Information Gain / Mutual Information
1 1
0 u g 11(1), .. 4),‘) g H (i. Hi) ilogzlt:
H u
H(X) i n iﬂ x is a constant random variable.
0 100m’) g u;
[C(X1Y) i 0 iff X and Y are independent;
10(x x) i H(X).
0 n(X i Y) g "(xi
H(X i Y) i H(X] iﬁ' X and Y are independent.
. H(X. i) g 1100+ HO’);
n(X Y) : nu’) i llfY) iﬂ‘ X and v are independent;
mx. HA) i H(X\A) + H(Y\X,A) (a conditional mini).
- achainrule: 1-1</\,.H .XH)’ H(X|)+H(X1\X|)+H +1-1tAnixin .AHL

***************Ending Page***************

***************Beginning Page***************
***************page number:35**************
.m
The Relationship between
Entropy, Conditional Entropy, Joint Entropy and
Information Gain
H(X,Y)
"()0 WY)

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
JG
Other deﬁnitions
o Let /\ be a discrete random variable. ,l its pnlf and l, another plnr
(usually a model er p),
Cross-entropy:
V . l
(‘Hm-ll I , Zl'(111°ligll(") : by. [10%; i]
M (IO)
0 Let X and Y be discrete random variables, and l, and '1 their re>pective
pmflr.
Relative entropy (0r, Kullhack-Leibler divergence):
. 11(4) [ ['05)]
null : ,llln», :F,lu, .
1H1) 22H) rpm r $.11“)
I (iqulemX)

***************Ending Page***************

***************Beginning Page***************
***************page number:37**************
.xr
The Relationship between
Entropy, Conditional Entropy, Joint Entropy, Information Gain,
Cross-Entropy and Relative Entropy (or KL divergence)
CH(X, m) H(X,Y) ch, pX )
7T
"(P9 "(PM
leoav) = Kuvan my)

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
as
Basic properties of
cross-entropy and relative entropy
Q (YII[X (1)2!)
n “71(1) H q) 3 ll for all p and r1:
limp H q) e n il-r p and q are identical.
o [Conscqucncni]
If A is a discrete randol'n variable, ,1 its plnf, and r] another pnri,
then CH<X.<l) 2 H(X) 2 0.
The ﬁrst of these two inequations is also known as Gibbs‘ inequation:
i 2i;11)110g217» S r Zia 10g.) 11»-
e Unlike H of a discrete il-ary variable, which is bounded by 1% n, there
is nu (general) upper bound ier CH. (l-lnwever, KL is upperuundﬂL)
r Unlike "(X Y], wliieli ie symmetric in its nrgrnnentr, CU and KL are
not! Therefore KL is NOT a distance metric! (See the next slide.)
, , 1 r i r)
- IGiX: i l : 1i um H my) I r Zr Em“ whee (221“; )

***************Ending Page***************

***************Beginning Page***************
***************page number:39**************
so
Remark
o The quantity
V](X. Y) i’ II[X.Y) 10<xy> : [[(XJ i [1(Y) 2]C[X.Y)
: H[X\Y)+H(Y\X]
known as variation of information, is a distance metric, iic. it is
nonengative, symmetric. implies indiscernability, and satisﬁes the tri-
angle inequality.
l
0 Consider i\1[]1.1/) e EU’ + q].
1 1
The function ‘150(1in : 51\’L(71H7\1)+ EKLmiiiu) is called me Jensen-
Shannon divergence.
One can prove that \/JSD(qu) deﬁne; a distance metric (ii-e Jensen-
Shannon distance).

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
4n
6. Recommended Exercises
- ham [Manning & Schﬁtzc, 2002 , ch. 2;]
Examples 1‘, 2, 4‘, 5, 7, s, 9
Exercises 2.1, 23, 2,4, 2.5
. From [Sheldon Ross, 1998 , ch. 8;]
Examples 2a, 2b, 33, 3b, 3c, 5a, 5b

***************Ending Page***************

***************Beginning Page***************
***************page number:41**************
ii
Addenda:
Other Examples of Probabilistic Distributions

***************Ending Page***************


***************Beginning Page***************
***************page number:42**************
42
Multinomial distribution:
generalises the binomial distribution to the case where there are A’ inde'
pendent outcomes with probabilities 0,, l e 1, WK such that 2le u, e ll
The probability of getting n‘ nccurrence of outcome l is given by
H‘ v
1’ e : ,ill" e'“.
("l l 115W) rel l
where n : n, + + n,“ and n: (1h 0,‘).
Note: The particular ease n : l represents the categorical distribu-
tion. This is a gcnni'alisatinn of tho Bernoulli distribution.
Example: The outcome of rolling a die H times is described by a categor-
ical distribution The probabilibies or each or the 6 outcomes are +1, .9,“
For a fair die, ll, Z , I 0h. and the probability of rolling it 12 times and
getting each outcome twice is:
'2' 1n 31H)‘
I X
(2')“ 5

***************Ending Page***************

***************Beginning Page***************
***************page number:43**************
43
Poisson distribution (or, Poisson law of small numbers):
M
AAA) : ‘I i -‘, with L -: N and parameter A :» 0.
Mean I variance I A.
Poisiﬂrl pwbubilily miis hmcﬁnn Poisson cumulnlive dislribulion lunDlion
g a .-"':"'"":l""
" 0 L: I ‘ ‘I 0"
"- g j ‘I I.’
a - A: 10 _ w ./ I.
4 X a .
‘ m a b
v f‘ \. / ,- o M
° . \- '-. g ‘ - ‘ ;,:4
/ '\ ."X "1. I‘, .1’ - )_:1D
g =1.I'0¢¢a$é¢0ua§allll g h..-
n 5 m 15 20 n 5 m '5 20
k k

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
H
Exponential distribution (a.k.a. negative sxpnnenlial disu-ihution):
[7il.)\) : x1 Y" f0!" , 1; 0 and pmnnmmr A > n.
Mean = A ', val'iHIlCe = A l.
Expononllll pmblblllly dllllily IUIICIIOII EXDWIOMIII cumulﬂlvn dimlhu'lon tuncﬂon
3 V
_ , A 5 Q
E ¥ i g
E ,:
g
_ ,1‘
n i z 1 . 5 q . 1 I . 5
Nan": Tho Expnnrmiai distribution n a pnnirulm' rm nhhr Gamma minimum“ (mkv L Z 1
in the next glide).

***************Ending Page***************

***************Beginning Page***************
***************page number:45**************
Gamma dlstrlbutlon:
N, m; : H ‘W fm' . 2 u and parameters A x n (shape) and H x n (scale).
Mean : ML variance : M7”.
Tm- gamma rum-nun i» H grm'ruliwuﬁnn of um rul'hlri'il runrﬁnn m n'al \‘iillll'x For my
,msmw real numhsx' ,_ r(| i I) 1m). (Thus, for inLegers rm] (n i nu)
Gamml mummy mum “MM Gamma wmulanv: MW“ mu.»-
‘ \ _ k-vﬂ n-zo '
_ K.“ a.“
_ Mn M5
T g MEMO .; =
K ~ E 4
_ MW,“
_ MW“
; r g _ HUME

***************Ending Page***************

***************Beginning Page***************
***************page number:46**************
HI
X2 distribution:
Wm me r u‘ 1 k| ,1 lg.
I! i» ubmimwl from Gamma (linrihulixm by ‘aking L i ml .mrl (I i 2.
NIcan : 1/‘ varianrc : 2m
cm 5.1”...“ mm...“ .1.....W..E...,,. 0.. w...“ Mm...“ 6.‘...Munmnum
g g
_ M
_ M
z \\ ‘ s _ M
\\\ : iii
Z \ g _ E'i

***************Ending Page***************

***************Beginning Page***************
***************page number:47**************
h‘
Laplace distribution:
y w m
m, ,1 y, i )7 H ,wm. w /kL
Mum I ,,, vuriimcr I 3H’.
LlpHOQ pmhabilily dlﬂliiy lunclmn LBPIBW CUMUIJIWE dusky NﬂCilﬂll
a V
s _ MM g
a g
_ ups M

***************Ending Page***************

***************Beginning Page***************
***************page number:48**************
JR
Student s dlstrlbutlon.
u \ ,
r ( J; ) 2 1 x 1
1'0 My i iv (I + L) 2 for I e 3 and 1/ > n (the “degree offreedom" paraln.)
Fm (7) 1’
[\(Iesn : 0 ﬁn- |/ > I. “LIMA-wise undeﬁned.
Variance : #2 for IA .y 2, x fur 1 < 1/ 5 z. utherwise undeﬁned.
The probability density l'unctinn and (he cumulative distribuLiDu function:
n40 10 .
uzs A - ":1 f
m c \ - M us
v u _ .
3m s E é d, 1
‘ ~ n0 i =
“ ‘5 , -.14:z
n m \ 01/ —m:5
m/ y. “0/ m"
7- '1 g 1 ‘ 4 '1 3 1 0
M. "m... WM ‘rm Pawn...“ ,5 Wm...“ .M b.1l..h-p.d. 1... n. “m... mmmum. b... .. n.
My. n.1,, m..." M it ., MC m... m mew. Mm n... M1 n. hum n, m...

***************Ending Page***************

***************Beginning Page***************
***************page number:49**************
4Q
Beta distribution: 15 U585“? _
- 1:“:3 I
guiluimdil 7 3-; g-;—
"(9 “ 1 m“. a)
where 5m. a) is the Beta function m ‘5
of arguments 41.1’? E 1m 5
1
, rmrw)
B “a : i
f" > m + 4) Us
with Fm Z <1, , 1,1 for any 1, e N‘.
U 0 n Z 0h 06 DB l

***************Ending Page***************

***************Beginning Page***************
***************page number:50**************
50
Dirichlet distribution:
I ,
D e : n’; H""‘6' “ H, i
< W 2W , i i L2H J
where
u i in, . in‘ with u, > n hm the parameters,
o, Batiify u g n‘ g i nnd Rum to 1, this being indicated by the delta innhtihn
term Aggy, i 1), and
the normalising factor can be expressed in terms of the gamma funttion:
t 11m)
z i ll'L /-/"' ‘a fl i9 i i
(In f ti t (2i Jr HEM
u
Mean nit/,1 i’,
Z] "i
For I; : 2, the Dirichlet nistrihntinn reduces m H“! Beth distribution.

***************Ending Page***************

***************Beginning Page***************
***************page number:51**************
ril
Remark:
Concerning the multinomial and Dirichlet distributions:
The algebraic expression for the parameters a, is similar in the two distri-
llutloiis.
However, the nlultinonlizll is a distribution over its exponents u,. whereas
the Dirichlet is a distribution over the numbers 1), that are exponentiatecli
The two distributions are said tu be conjugate distributions and their
elnse formal relationship leads in a harmonious interplay in many eatirna-
tioll problemsi
Similarly‘
the Beta distribution is the conjugate of the Bernoulli distribution, and
the Gamma distribution is the conjugate of the Poisson distribution.

***************Ending Page***************

