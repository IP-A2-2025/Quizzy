***************Beginning Page***************
***************page number:1**************
1.
Basic Statistics and Probability Theory
Based on
“Foundations of Statistical NLP”
C. Manning 81: H. Schiitze, ch. 2, MIT Press, 2002
“Probability theory is nothing but common sense
reduced to calculation.”
Pierre Simon, Marquis de Laplace (1749-1827)

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2.
PLAN
1. Elementary Probability Notions:
o Sample Space, Event Space, and Probability Function
o Conditional Probability
o Bayes’ Theorem
o Independence of Probabilistic Events
2. Random Variables:
0 Discrete Variables and Continuous Variables
o Mean, Variance and Standard Deviation
o Standard Distributions
0 Joint, Marginal and and Conditional Distributions
o Independence of Random Variables

***************Ending Page***************


***************Beginning Page***************
***************page number:3**************
3.
PLAN (cont’d)

3. Limit Theorems

o Laws of Large Numbers

o Central Limit Theorems

4. Estimating the parameters of probabilistic models from
data

o Maximum Likelihood Estimation (MLE)

o Maximum A Posteriori (MAP) Estimation

5. Elementary Information Theory

0 Entropy; Conditional Entropy; Joint Entropy

o Information Gain / Mutual Information

o Cross-Entropy

o Relative Entropy / Kullback-Leibler (KL) Divergence

0 Properties: bounds, chain rules, (non-)symmetries,
properties pertaining to independence

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
4.
1. Elementary Probability Notions
0 sample space: Q (either discrete or continuous)
o event: A Q Q
— the certain event: Q
— the impossible event: Q
— elementary event: any {w}, Where w € Q
o event space: F : 2Q (or a subspace of 2Q that contains (3 and is closed
under complement and countable union)
0 probability function/ distribution: P : .7: —> [0,1] such that:
— P(Q) I 1
— the “countable additivity” property:
V141, ..., Ak disjoint events, P(UAZ-) I ZP(A¢)
Consequence: for a uniform distribution in a ﬁnite sample space:
P ( A) _ #favorable elementary events
— #all elementary events

***************Ending Page***************


***************Beginning Page***************
***************page number:5**************
5.
Conditional Probability
P A Fl B

Note: P(A l B) is called the a posteriory probability of A, given B.
o The “multiplication” rule:

P(A H B) : P(A l B)P(B) : P(B | A)P(A)
0 The “chain” rule:

P(A1oA2o...oAn) I

P(A1)P(A2 l A1)P(A3 l A1,A2) . . . P(An I 141,142, . . . ,An_1)

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
6.

0 The “total probability” formula:

P(A) : P(A l B)P(B) —|— P(A l ﬁB)P(ﬁB)

More generally:

ifAQ UB7; and ViyéjBZ-HBJ- IQ), then
o Bayes’ Theorem:

_ PM l B) 13(3)
P(B | A) _ W
_ PM l B) P(B)
or PM; I A) — P(A l B)P(B) +P(A | ﬁB)P(ﬁB)
OI‘

***************Ending Page***************


***************Beginning Page***************
***************page number:7**************
7.
Independence of Probabilistic Events
o Independent events: P(A O B) I P(A)P(B)
Note: When P(B) 71$ O, the above deﬁnition is equivalent to
P(A\B) I P(A).
o Conditionally independent events:
P(Aﬁ B l C) I P(A l C)P(B l C), assuming, of course, that
P(C) 7A 0.
Note: When P(B O C’) yé O, the above deﬁnition is equivalent
to P(A\B, C) I P(AIC).

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
8.
2. Random Variables
2.1 Basic Deﬁnitions
Let Q be a sample space, and
P : 2Q —> [0,1] a probability function.
o A random variable of distribution P is a function
X : Q e R”
o For now, let us consider n : l.
o The cumulative distribution function of X is F : R —> [0, oo) deﬁned by
F(ac) :P(X g at) : P({w G Q | X(w) g a:})

***************Ending Page***************


***************Beginning Page***************
***************page number:9**************
2.2 Discrete Random Variables 9'
Deﬁnition: Let P : 2Q —> [0, l] be a probability function, and X be a random
variable of distribution P.
0 If Val (X) is either ﬁnite or unﬁnite countable, then
X is called a discrete random variable.
o For such a variable we define the probability mass function (pmf)
p 1 R —> [0,1] as p<£13>ngtp<X I 3;) d2" P({w e o | X(w) I jg).
(Obviously, it follows that 2$i€Val(X)p(at,-) I 1.)
Mean, Variance, and Standard Deviation:
o Expectation / mean of X:
E(X) 11g. E[X] I 2w xp(;c) if X is a discrete random variable.
0 Variance of X: Var(X) n2. Var[X] : E((X — E(X))2).
o Standard deviation: 0 : \/ Var(X).
Covariance of X and Y, two random variables of distribution P:
- Com, Y> I EKX — EleXY — Em]

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
. . 100
Exempllﬁcation:
o the Bernoulli distribution: b(r; 1,10)
mean: p, variance: p(1 — p), entropy: —plog2p — (1 — p) log2(1 — p)
o the Binomial distribution: b(r; mp) I CZ; p7“(1 — p)”_’“ for r I O, . . . ,n
mean: np, variance: np(1 — p)
Binomial probability mass function Binomial cumulative distribution function
o =o.5, =20 .° ' 0
o' A 0 p=0.5, n=40 o' o ‘ o
g I‘ l o,
E ‘LI-D O O ‘ \ co l I
t o' l \ ..\ ‘5 ° ‘ o
C A k
.\ o ... F: l / l
\k- 2 O. I. ‘II o .
Q o' l l I \ O l ‘ I
o ‘ o 1 / I.
8 I l \I.\ 0‘ N lo ‘ o 0 p=0.5,n=20
O o ‘ o. ‘ q ° o 1 .' ‘ p=o.7,n=20
o o’ i .do \ ° o' ‘- o 0 p=0.5,n=40
o 1o 20 so 4o o 10 20 so 4o
r r

***************Ending Page***************


***************Beginning Page***************
***************page number:11**************
11.
2.3 Contmuous Random Varlables
Deﬁnitions:
Let P : 2Q e [0,1] be a probability function, and
X : Q —> R be a random variable of distribution P.
0 If Val (X ) is unﬁnite non-countable set, and
F, the cumulative distribution function of X is continuous, then
X is called a continuous random variable.
(It follows, naturally, that P(X I 23) I 0, for all as € R.)
0 If there exists p : R —> [0, oo) such that F(a:) I ffoo p(t)dt,
then X is called absolutely continuous.
In such a case, p is called the probability density function (pdf) of X.
o For B Q R for which fB p(;z:)d:c exists, P(X_1(B)) I fB p(;z:)da:,
where X—1(B) “It {w € Q I X(w) € B}.
In particular, Ii: p(a;)d:13 I 1.
o Expectation / mean of X: E(X) “It E[X] I fatp(x)da:.

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
12.
Exempliﬁcation:
_ (w — m2
o Normal (Gaussean) distribution: N(:z:;,u,0) : +(2 202
\/27T0'

mean: ,u, variance: 02
o Standard Normal distribution: N(x;0,1)
o Remark:

For mp such that np(l — p) > 5, the Binomial distributions can be

approximated by Normal distributions.

***************Ending Page***************


***************Beginning Page***************
***************page number:13**************
13.
Gaussian probability density function Gaussian cumulative distribution function
o_ o.
— u = 0,0 = 0.2
— u = 0,0 = 1.0
00 — it = O, O = 5.0 (Q
O' — u = —2, 0 = 0.5 o
w. w.
3"?‘ o A o
><
é i.
Q 1
z. 6-
Z "2 <r_
O O
(x! N — u = O, O = 0.2
O d — p. = O, O = 1.0
— u = O, O = 5.0
— u = —2, O = 0.5
g O.
O
-4 -2 o 2 4 -4 -2 o 2 4
X X

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
2.4 Basic Properties of Random Variables 14'
Let P : 2Q —> [0,1] be a probability function,
X : Q e 1R” be a random discrete/ continuous variable of distribution P.
o If g : R” e Rm is a function, then 9(X) is a random variable.
If g(X) is discrete, then E(g(X)) I ng(x)p(513).
If 9(X) is continuous, then E(g(X)) I fg(a:)p(a:)d95.
o If g is non-linear 75> E(g(X)) I g(E(X)).
o E(aX) I aE(X).
o E(X —|— Y) I E(X) —|— E(Y), therefore E[2?:1 a,X,-] I 217:1 a,E[X,-].
o Var(aX) I a2 Var(X).
0 Var(X-|—a) I Var(X).
o VaT(X) I E(X2) — E2(X).
o Cov(X, Y) I E[XY] — E[X]E[Y].

***************Ending Page***************


***************Beginning Page***************
***************page number:15**************
15.
2.5 J omt, Marglnal and Condltlonal Distributlons
Exempliﬁcation for the bi-variate case:
Let Q be a sample space, P : 2Q —> [0,1] a probability function, and
V : Q —> R2 be a random variable of distribution P.
One could naturally see V as a pair of two random variables X : Q —> R and
Y : Q e R. (More precisely, V(w) I (any) I (X(w),Y(w)).)
0 the joint pmf/pdf of X and Y is deﬁned by
p(a:,y) “It pX,y(x,y) I P(X I x,Y I y) I P(w E Q l X(w) I 9:,Y(w) I y).
0 the marginal pmf/pdf functions of X and Y are:
for the discrete case:
thli) I Zyptn y), pyty) I prtfv, y)
for the continuous case:
px(w) I fypw, y) dy, 2M9) I LSWY, y) d3?
0 the conditional pmf/pdf of X given Y is:
I Fri/(513, y)
PX|Y<5U | y) py(y)

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
16.
2.6 Independence of Random Variables
Deﬁnitions:
0 Let X, Y be random variables of the same type (i.e. either discrete or
continuous), and 10ij their joint pmf/pdf.
X and Y are said to be independent if
lbw/(55731) I px($) ‘In/(31)
for all possible values :13 and y of X and Y respectively.
o Similarly, let X, Y and Z be random variables of the same type, and p
their joint pmf/pdf.
X and Y are conditionally independent given Z if
pX,Y\Z(33>y l Z) I PX|Z(55 l Z) 'PY\Z(?J l Z)
for all possible values 51:, y and z of X, Y and Z respectively.

***************Ending Page***************


***************Beginning Page***************
***************page number:17**************
17.
Properties of random variables pertaining to independence
0 If X,Y are independent, then
Var(X+Y) I Var(X) —|— Var(Y).
0 If X, Y are independent, then
E(XY) : E(X)E(Y), i.e. Co'v(X,Y) : 0.
o Cov(X, Y) : 0 75> X, Y are independent.
o The covariance matrix corresponding to a vector of random variables
is symmetric and positive semi-deﬁnite.
0 If the covariance matrix of a multi-variate Gaussian distribution is
diagonal, then the marginal distributions are independent.

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
18.
3. Limit Theorems
[ Sheldon Ross, A ﬁrst course in probability, 5th ed., 1998 ]
“The most important results in probability theory are limit theo-
rems. Of these, the most important are...
laws of large numbers, concerned with stating conditions under
which the average of a sequence of random variables converges (in
some sense) to the eXpected average;
central limit theorems, concerned with determining the conditions
under which the sum of a large number of random variables has a
probability distribution that is approximately normal.”

***************Ending Page***************


***************Beginning Page***************
***************page number:19**************
19.
Two Probability Bounds
Markov’s inequality:
If X is a random variable that takes only non-negative values,
then for any value a > O,
E X
P(X Z a) g L
a
Chebyshev’s inequality:
If X is a random variable with ﬁnite mean ,u and variance 02,
then for any value a > 0,
02
P(|X—#l2a)§?-
Note: As Chebyshev’s inequality is valid for all distributions of the
random variable X , we cannot expect the bound of the probability to
be very close to the actual probability in most cases. (See ex. 2b, pag.
397 in Ross’ book.)

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
20.
The weak law 0f large numbers
[ Bernoulli; Khintchine l
Let X1,X2,...,Xn be a sequence 0f independent and identically
distributed random variables, each having a ﬁnite mean E [X2] : ,u.
Then, for any value e > O,
X . . . Xn
P(|%—u‘ 26) e0asn—>oo.
n

***************Ending Page***************


***************Beginning Page***************
***************page number:21**************
21.
The central limit theorem
for i.i.d. random variables
[ Pierre Simon, Marquis de Laplace; Liapunoff in 1901-1902 l
Let X1, X2, . . . , Xn be a sequence of independent random variables,
each having ﬁnite mean ,u and ﬁnite variance 02.
Then the distribution of
X1+...+Xn—n,u
0 ﬁ
tends to be the standard normal (Gaussian) as n e oo.
That is, for —oo < a < oo,
X . . . Xn — 1 a
P<MS60 —>—/ e—$2/2d:13asneoo
0' W V271‘ —oo

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
22.
The central limit theorem
for independent random variables
Let X1,X2, . . .,Xn be a sequence of independent random variables having
respective means ,ui and variances 0?.
If
(a) the variables X2- are uniformly bounded,
i.e. for some M € R+ P(l X1- l< M) I l for all 2',
and
(b) 21:1 U? I 007
then
7'1 Xi — 1'
P Wga —><l>(a) asn—>oo
V 21:1 012
Where (l) is the cumulative distribution function for the standard normal
(Gaussian) distribution.

***************Ending Page***************


***************Beginning Page***************
***************page number:23**************
23.
The strong law 0f large numbers

Let X1, X2, . . . ,Xn be a sequence of independent and identically
distributed random variables, each having a ﬁnite mean E [X,] : ,u.
Then, with probability 1,

X1 + . . . + Xn

— —> ,u as n —> 00

n
That is,
P(lim(X1+...+Xn)/n:,u) :1
TL—>OO

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
24.
Other Probability Bounds
One-sided Chebyshev inequality:
If X is a random variable with mean 0 and ﬁnite variance 02,
then for any a > O,
2
0
P<X Z a) g m
Corollary:
If E[X] I ,u, Var(X) I 02, then for a > 0 2
P(X > + a) < U—
— ‘u — 0-2 _|_ a2
02
P X < — < —
( _ u a) _ 02 +02

***************Ending Page***************


***************Beginning Page***************
***************page number:25**************
25.
Other Probability Bounds (cont’d)
Chernoff bounds:
If X is a random variable, then M (t) 7g E [etX], is called the moment
generating function of X.
It can be shown that
P(X 2 a) g e_t“M(t) for all 15> 0
P(X g a) g e_mM(t) for all t < 0.
Chernoff bounds for the standard normal distribution:
If Z is a standard normal random variable,
then M(t) "it" EletX] cal?“ et2/2.
It can be shown that
P(Z Z a) g €_a2/2 for all a > 0
P(Z g a) g €_a2/2 for all a < 0.

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
26.
Other Probability Bounds (cont’d)
Hoeffding bounds:
Let X1, . . . 7Xn be some independent random variables, each X7; being
bounded by the interval [al-7 bi].
If X "2' izyzl Xi, then it follows that
for any t Z 0
_ _ 2 ZtZ
P<X — EiXi Z t) S QXP <—%>
21:1 (bi _ ai)
- - 2n2t2
P<EtX1— X z t) g exp (-—)
21:1(191' _ ai)2
_ _ 2712152
a’ P<\X — EtXn 2 t> g 2e><p (——).
thlwi — ai)2

***************Ending Page***************


***************Beginning Page***************
***************page number:27**************
27.
4. Estimation/ inference of the parameters of
probabilistic models from data
(based on [Durbin et al, Biological Sequence Analysis, 1998],
p. 311-313, 319-321)

A probabilistic model can be anything from a simple distribution
to a complex stochastic grammar with many implicit probability
distributions. Once the type of the model is chosen, the parame-
ters have to be inferred from data.

We will ﬁrst consider the case of the categorical distribution, and
then we will present the different strategies that can be used in
general.

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
28.
A case study: Estimation of the parameters of
a categorical distribution from data

Assume that the observations — for example, when rolling a die about
which we don’t know whether it is fair or not, or when counting the number
of times the amino acid 2' occurs in a column of a multiple sequence align-
ment — can be expressed as counts n, for each outcome z' (2' I 1,l...,K),
and we want to estimate the probabilities 9, of the underlying distribution.
Case 1:
When we have plenty of data, it is natural to use the maXimum likeli-
hOOd (ML) Solution, i.e. the observed frequency (Qt-ML : 2L ng' %.

. n -

j J
Note: it is easy to show that indeed P(n l 6ML) > P(n I d) for any 9 71$ 6ML.

PW | aML) _ 111(91MLW _ , QZML _ ML @ML

The inequality follows from the fact that the relative entropy is always
positive except when the two distributions are identical.

***************Ending Page***************


***************Beginning Page***************
***************page number:29**************
Case 2: 29'
When the data is scarce, it is not clear what is the best estimate.
In general, we should use prior knowledge, via Bayesian statistics.
For instance, one can use the Dirichlet distribution with parameters a.
_ PW l WW l 01)

It can be shown (see calculus on R. Durbin et. al. BSA book, pag. 320)
that the posterior mean estimation (PME) of the parameters is

aPME dei /8P(6 l mde I —”Z' + O”

Z N + 29- Odj
The 04's are like pseudocounts added to the real counts. (If we think of the
o/s as extra observations added to the real ones, this is precisely the ML
estimate!) This makes the Dirichlet regulariser very intuitive.
How to use the pseudocounts: If it is fairly obvious that a certain residue,
let’s say 1L, is very common, than we should give it a very high pseudocount
04,-; if the residue j is generaly rare, we should give it a low pseudocount.

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
30.
Strategies to be used in the general case
A. The Maximum Likelihood (ML) Estimate
When we wish to infer the parameters 9 : (9,) for a model M from a set
of data D, the most obvious strategy is to maximise P(D l €,M) over all
possible values of d. Formally:
9ML I argmaxP(D l Q, M)
0
Note: Generally speaking, when we treat P(5v l y) as a function of :13 (and
y is ﬁxed), we refer to it as a probability. When we treat P(x | y) as a
function of y (and x is ﬁxed), we call it a likelihood. Note that a likelihood
is not a probability distribution or density; it is simply a function of the
variable y.
A serious drawback of maximum likelihood is that it gives poor results
when data is scarce. The solution then is to introduce more prior knowl-
edge, using Bayes’ theorem. (In the Bayesian framework, the parameters
are themselves seen as random variables!)

***************Ending Page***************


***************Beginning Page***************
***************page number:31**************
B. The MaX1mum A poster10r1 Probablllty (MAP) Estimate
QMAP dgf' argle'naXPw l D,M) : arggnaxw
: argmaXP(D | €,M)P(€ | M)

6
The prior probability P(€ l M ) has to be chosen in some reasonable manner,
and this is the art of Bayesian estimation (although this freedom to choose
a prior has made Bayesian statistics controversial at times...).
C. The Posterior Mean Estimator (PME)

QPME I /9P(9 | D,M)d6
Where the integral is over all probability vectors, i.e. all those that sum to
one.
D. Yet another solution is to use the posterior probability PW l D,M) to
Sample from it (see [Durbin et al, 1998], section 11.4) and thereby locate
regions of high probability for the model parameters.

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
5. Elementary Information Theory 32-
Deﬁnitions:
Let X and Y be discrete random variables.1
def.
' Entr0py= H(X) I 2502908) logg M I — Em Ml") 10am) I
Epl—10g2 p(X)l-
Convention: if p(a:) I O then we shall consider 10(00) log2 19(00) I O.
0 Speciﬁc Conditional entropy: H(Y | X : a3) déf' — Egg/My | :13)log2 p(y |
o Average conditional entropy:
def. imed.
H(Y I X) I ZJJQXMI'WW l X :iv) I —Z$€X Zngp($7y)1Og2P(y l f6)-
o Joint entropy:
def. dem. dem.
H<X,Y> I —2:,yp<x,y> lOszptrw) I H<X>+H<Y1X> I H<Y>+H<X|Y>-
o Information gain (or: Mutual information):
IG(X;Y) déf' H(X) _ H(X y Y) “20" H(Y) _ H(Y ) X)
iméd' H(X, Y) _ H(X y Y) _ H(Y ) X) I IG(Y; X).

***************Ending Page***************


***************Beginning Page***************
***************page number:33**************
33.
Exempliﬁcation: Entropy 0f a Bernoulli Distribution
HQ?) I —P10g2p— (1—p)10g2(1—p)
0.0 0.2 0.4 0.6 0.8 1.0
p

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
. . 34.
Basu: propertles of
Entropy, Conditional Entropy, Joint Entropy and
Information Gain / Mutual Information
l l
o O§H(p1,...,pn) §H<—,...,—) :loan;
n n
H (X) : O iff X is a constant random variable.
. IG(X;Y) 2 0;
IG(X; Y) I O iﬁ X and Y are independent;
IG(X; X) I H(X).
o H(X | Y) g H(X)
H(X | Y) : H(X) iff X and Y are independent.
O H(X, Y) g H(X) + H(Y);
H(X, Y) I H(X) + H(Y) iff X and Y are independent;
H(X, Y|A) : H(X|A) —|— H(Y|X, A) (a conditional form).

***************Ending Page***************


***************Beginning Page***************
***************page number:35**************
35.
The Relationship between
Entropy, Conditional Entropy, Joint Entropy and
Information Gain
H(X5Y)
H(X) H(Y)

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
36.
Other deﬁnitions
o Let X be a discrete random variable, p its prnf and q another pmf
(usually a model of p).
Cross-entropy:
1
CH(X, q) I — 219(513) log2 q(:13) I Ep [10552 —]
M (AX)
o Let X and Y be discrete random variables, and p and q their respective
pmf’s.
Relative entropy (or, Kullback-Leibler divergence):
q($) p(X)
KLp q : — patlog—:E [log—
(H) gm 2W) p 2M)
I CH(X,q) — H(X).

***************Ending Page***************


***************Beginning Page***************
***************page number:37**************
37.
The Relationship between
Entropy, Conditional Entropy, Joint Entropy, Information Gain,
Cross-Entropy and Relative Entropy (or KL divergence)
CH(X, pY) H(X,Y) CH(Y, px)
0*”
a Z.
H(P X) H(P Y)
IG(X,Y) = KL(p XYII poY)

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
38.
Basic properties of
cross-entropy and relative entropy
o CH (X, q) Z 0
o KL(p H q) Z O for all p and q;
KL(p H q) I 0 iffp and q are identical.
o [Consequencez]
If X is a discrete random variable, p its pmf, and q another pmf,
then CH(X,q) Z H(X) Z 0.
The ﬁrst of these two inequations is also known as Gibbs’ inequation:
_ 2?:1 P110552 Pi g — 212% 10g2 (12"
o Unlike H of a discrete n-ary variable, which is bounded by log2 n, there
is no (general) upper bound for CH. (However, KL is upper-bounded.)
o Unlike H (X, Y), which is symmetric in its arguments, CH and KL are
not! Therefore KL is NOT a distance metric! (See the neXt slide.)
P 513 p y
- IG(X; Y) I mm n pm) I — 2w zype, y) logz (%)

***************Ending Page***************


***************Beginning Page***************
***************page number:39**************
39.
Remark
o The quantity
V1(X, Y) déf H(X, Y) - IG(X; Y) I H(X) + H(Y) _ 2IG(X; Y)
: H(X | Y)+H(Y | X)
known as variation of information, is a distance metric, i.e. it is
nonengative, symmetric, implies indiscernability, and satisﬁes the tri-
angle inequality.
1
o Consider M(p, q) : 5(1) + q).
1 1
The function JSD(qu) : §KL(pHM) —|— §KL(qHM) is called the Jensen-
Shannon divergence.
One can prove that \/ JSD(p\|q) deﬁnes a distance metric (the Jensen-
Shannon distance).

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
40.
6. Recommended Exercises
o From [Manning &: Schiitze, 2002 , ch. 2:]
Examples 1, 2, 4, 5, 7, 8, 9
Exercises 2.1, 2.3, 2.4, 2.5
o From [Sheldon Ross, 1998 , ch. 8:]
Examples 2a, 2b, 3a, 3b, 3c, 5a, 5b

***************Ending Page***************


***************Beginning Page***************
***************page number:41**************
41.
Addenda:
Other Examples 0f Probabilistic Distributions

***************Ending Page***************

***************Beginning Page***************
***************page number:42**************
42.
Multinomial distribution:
generalises the binomial distribution to the case where there are K inde-
pendent outcomes with probabilities (92-, i : 17 . . . , K such that 2511 61- : 1.
The probability of getting ni occurrence of outcome i is given by
n!
P d I —Hli <97”,

Where n:n1+...+nK, and Q: (61,...,@K).
Note: The particular case n I 1 represents the categorical distribu-
tiOIl. This is a generalisation of the Bernoulli distribution.
Example: The outcome of rolling a die n times is described by a categor-
ical distribution. The probabilities of each of the 6 outcomes are 61, . . . , Q6.
For a fair die, (91 : I 96, and the probability of rolling it 12 times and
getting each outcome twice is:

12! 1 12

— — I 3.4 >< 10—3

W (6)

***************Ending Page***************


***************Beginning Page***************
***************page number:43**************
43.
POISSOH dlStI'lbllthH (or, Poisson law 0f small numbers):
Ak
p(k; A) I F - e“, with k E N and parameter /\ > O.
Mean : variance I A.
Poisson probability mass function Poisson cumulative distribution function
g- 3 ooooggnooooo:'.ooo
o o O 7» = 1 o I" ‘a’
(q A it = 4 2 / i I.’
O 0 A = 10 o / °
‘x 2 LO, ‘ ./
¥ O
O O .
O ‘ o 0)‘: .‘m g / I. ‘ 7“ = 4
‘/ \ .0’ \_ ‘a . I‘ .l' o x = 10
g 000'.Oooo$6000000:l00 g 6000"
0 5 10 15 20 0 5 10 15 20
k k

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
44.
EXpOHentlal dIStI'lbUtIQH (a.k.a. negative exponential distribution):
p(a:; A) I AKA“; for :c Z 0 and parameter A > 0.
Mean I )Fl, variance I )CQ.
Exponential probability density function Exponential cumulative distribution function
5' — x=0.5
— k=1
— k=1.5 3
Q 5
“- v.
2 — 7»:0.5
— k=1
— x=1.5
o 1 2 a 4 5 o 1 2 3 4 5
X X
Note: The Exponential distribution is a particular case of the Gamma distribution (take k I 1
in the next slide).

***************Ending Page***************


***************Beginning Page***************
***************page number:45**************
' ' ' 45.
Gamma dlstrlbut1on:
€—:B/9
p(;1;; k, 9) z 38de for x Z 0 and parameters k: > O (shape) and € > O (scale).
Mean I k6, variance : [£92.
The gamma function is a generalisation of the factorial function to real values. For any
positive real number as, Ha: + 1) : xF(a:). (Thus, for integers F(n) I (n — 1)!.)
Gamma probability density function Gamma cumulative distribution function
_ k=1.0,6=2.0 ‘-
_ k=2.0,6=2.0
g- — k=3.0,6=2.0 g
_ k=5.0,6=1.0
_ k=9.0,6=0.5
g _k=7.5,6=1.0 =2
a k=0.5,6=1.0 3? °
s. w
Q. E’ _ k=1.0,6=2.0
g g. _ k=2.0,6=2.0
_ k=3.0,6=2.0
_ k=5.0,6=1.0
g- 2 _ k=9.0,6=0.5
_ k=7.5,6=1.0
k=0.5,6= 1.0
O 5 1O 15 20 O 5 1O 15 20
X X

***************Ending Page***************

***************Beginning Page***************
***************page number:46**************
46.
2 Q Q I .
X distribution.
1 1 11/2 /2 1 1
x;l/ I — — x” _ e_§m for a; > O and 1/ a ositive inte er.
p‘ ) 1W2) (2) — p g
It is obtained from Gamma distribution by taking k : u/Z and G : 2.
Mean I 1/, variance I 2v.
Chi Squared probability density function Chi Squared cumulative distribution function
o — k=1 F
— k=2
— k=4
— k=6
g >v<|
Q 5
2’ \\ CL F5 — k=1
— k=2
s- 2 — k=4
\\ — k=6
§ “=9
O 2 4 6 8 O 2 4 6 8
X X

***************Ending Page***************


***************Beginning Page***************
***************page number:47**************
47.
Laplace dlstrlbutlon:
lM — w!
1 —— .
p(a:;u,9) I ﬁe 9 ,w1th l9 > O.
Mean = u, variance I 262.
Laplace probability density function Laplace cumulative density function
o _ u=0,9=1 1-
_ u=0,6=2
<1; _ u=0,6=4 oo_
o _ u=—5,6=4 O
Q 5
N _ u=0,6=1
g o‘ _ u=0,6=2
_ u=0,6=4
— u=—5,6=4
—10 —5 0 5 10 —10 —5 0 5 10
X X

***************Ending Page***************

***************Beginning Page***************
***************page number:48**************
48.
’ Q O O .
Student s dlstrlbutlon.
I/ + 1 v + 1
F T 2 _T
a:
p(:1:;1/) : —u (1 -|— —) for a: E R and u > 0 (the “degree of freedom” param.)
\/I/7T F (—) V
2
Mean = 0 for u > 1, otherwise undeﬁned.
. 7/ .
Varlance : —2 for 1/ > 2, oo for 1 < 1/ g 2, otherw1se undeﬁned.
V _
The probability density function and the cumulative distribution function:
0.40 1,0 _
0.30 .I a, _ df=2 0'5
x '- —°“ vl
50.20 ,q
U 15 z {1.4 — df=1
' — df=2
0.10 . ._ 0;, J — df=5
D DDJ XE: 0 DJ d 00
‘ -4 -2 u 2 4 ‘ -4 -2 u 2 4
>1’ >1
Note [from Wiki]: The t-distribution is symmetric and bell-shaped, like the normal distribution, but it has
havier tails, meaning that it is more prone to producing values that fall far from its mean.

***************Ending Page***************


***************Beginning Page***************
***************page number:49**************
49.
Beta distribution: p.d.f.

Beta. diStI'iblltiOIl! 2'5 a=|3=05—
0a—1(1_0)ﬂ—1 2 giﬁ'giEIv ‘
P6304’5:—7 7' _. 3
( ) BM, 6) ;
Where B(a, [3) is the Beta function LL 1'5
0f arguments 04, 5 € R+ E i
more) ’
B , I ,
(04 6) F(Oz + ﬁ) 0.5 i w
with Hm) I (x — 1)! for any a: € N*. A I
0 0 0.2 0.4 0.6 0 1

***************Ending Page***************

***************Beginning Page***************
***************page number:50**************
50.
Dirichlet distribution:
1

1) e I —nl£ (99‘"16 K el- _ 1

( |OZ) Z(Od) 2-1 z (22:1 )
where
a I 041, . . . , 04K with CYZ' > O are the parameters,
61- satisfy 0 g 62- g 1 and sum to 1, this being indicated by the delta function
term 5(2162- — 1), and
the normalising factor can be expressed in terms of the gamma function:

._ 11-11(04)
Z I Hli 9%16 ._1 de:#
Mean of 0' 0%
Z. 23' 049' .

For K I 2, the Dirichlet distribution reduces to the Beta distribution.

***************Ending Page***************


***************Beginning Page***************
***************page number:51**************
51.
Remark:
Concerning the multinomial and Dirichlet distributions:
The algebraic expression for the parameters d, is similar in the two distri-
butions.
However, the multinomial is a distribution over its exponents 11,, whereas
the Dirichlet is a distribution over the numbers 9, that are exponentiated.
The two distributions are said to be conjugate diStI'ibutiOIlS and their
close formal relationship leads to a harmonious interplay in many estima-
tion problems.
Similarly,
the Beta distribution is the conjugate of the Bernoulli distribution, and
the Gamma distribution is the conjugate of the Poisson distribution.

***************Ending Page***************





