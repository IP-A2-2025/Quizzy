[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What is the definition of covariance between two random variables X and Y?
--InteriorSeparator--
Cov[X, Y] = E[(X - E[X])(Y - E[Y])]
--InteriorSeparator--
medium
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following statements regarding the correlation coefficient p[X, Y] are true?
--InteriorSeparator--
(right) -1 ≤ p[X, Y] ≤ 1
(wrong) p[X, Y] = 0 if X and Y are independent
(right) p[X, X] = 1
(wrong) p[X, Y] > 1
--InteriorSeparator--
medium
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
If X and Y are independent random variables, what is the value of Cov[X, Y]?
--InteriorSeparator--
0
--InteriorSeparator--
easy
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Given that X and Y are independent random variables, which of the following statements are true?
--InteriorSeparator--
(right) E[XY] = E[X]E[Y]
(right) Var[X + Y] = Var[X] + Var[Y]
(wrong) Cov[X, Y] ≠ 0
(wrong) E[X+Y] = E[X]E[Y]
--InteriorSeparator--
medium
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
State Markov's inequality.
--InteriorSeparator--
For X ≥ 0 with E[X] = μ, P{X ≥ t} ≤ μ/t, for t > 0.
--InteriorSeparator--
medium
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
State Chebyshev's inequality.
--InteriorSeparator--
For any random variable X with mean E[X] = μ and variance Var[X] = σ², P{|X - μ| ≥ t} ≤ σ²/t², for t > 0.
--InteriorSeparator--
medium
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What does Chebyshev's inequality imply about a random variable with a small variance?
--InteriorSeparator--
(right) The probability that the variable takes values far from the mean is low.
(wrong) The probability that the variable takes values far from the mean is high.
(wrong) The variable's distribution is heavily skewed.
(wrong) The mean is always zero.
--InteriorSeparator--
medium
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the necessary and sufficient conditions for equality to hold in Markov's inequality?
--InteriorSeparator--
(right) P{X = 0} + P{X = t} = 1
(wrong) P{X = 0} = 1
(wrong) P{X = t} = 1
(wrong) P{X = 0} + P{X = t} = 0
--InteriorSeparator--
hard
--InteriorSeparator--
18
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the necessary and sufficient conditions for equality to hold in Chebyshev's inequality?
--InteriorSeparator--
(right) P{X = μ - t} + P{X = μ} + P{X = μ + t} = 1
(wrong) P{X = μ} = 1
(wrong) P{X = μ - t} + P{X = μ + t} = 1
(wrong) P{X = μ - t} = P{X = μ} = P{X = μ + t} = 1/3
--InteriorSeparator--
hard
--InteriorSeparator--
22
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
State Chernoff's inequality (upper tail) for a sum of independent Bernoulli random variables.
--InteriorSeparator--
If (Xi) are independent Bernoulli random variables with parameter pi, and X = ΣXi, μ = E[X], then P{X > (1 + δ)μ} < (e^δ/(1+δ)^(1+δ))^μ, for δ > 0.
--InteriorSeparator--
hard
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
State Hoeffding's inequality.
--InteriorSeparator--
If X1, X2, ..., Xn are independent bounded random variables: ai ≤ Xi ≤ bi, ai, bi ∈ R, and X = ΣXi, then P{X - E[X] > a} < exp(-2a^2 / Σ(bi - ai)^2), for all a > 0.
--InteriorSeparator--
hard
--InteriorSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does it mean for two random variables X and Y to be independent?
--InteriorSeparator--
For any two sets A and B, P{(X ∈ A) ∩ (Y ∈ B)} = P{X ∈ A} * P{Y ∈ B}.
--InteriorSeparator--
easy
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Given the following table of probabilities, are the events X and Y independent?
Y
-3  2 4
X   1  1/6 1/3 1/6
3   1/6 1/6 0
--InteriorSeparator--
(wrong) Yes
(right) No
--InteriorSeparator--
hard
--InteriorSeparator--
32
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
For any constant a,b, and c. Which of the following is true about covariance?
--InteriorSeparator--
(right) cov[aX+bY+c, Z] = a*cov[X,Z]+b*cov[Y, Z]
(wrong) cov[aX+bY+c, Z] = cov[X,Z]*cov[Y, Z]
(wrong) cov[aX+bY+c, Z] = a+cov[X,Z]+b+cov[Y, Z]
--InteriorSeparator--
medium
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the relationship between variance and covariance?
--InteriorSeparator--
Var[X] = Cov[X,X]
--InteriorSeparator--
medium
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
A coin is tossed n times and X is the number of heads. Using Chernoff bounds, how do you evaluate the probability that X is greater than the mean?
--InteriorSeparator--
(right) P{X > (1 + δ)μ} < exp(-δ^2*μ/3)
(wrong) P{X > (1 + δ)μ} > exp(-δ^2*μ/3)
(wrong) P{X > (1 + δ)μ} < exp(-δ*μ/3)
--InteriorSeparator--
hard
--InteriorSeparator--
27
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Provide the formula for Var[X+Y] in terms of Var[X], Var[Y], and Cov[X,Y].
--InteriorSeparator--
Var[X + Y] = Var[X] + 2Cov[X, Y] + Var[Y]
--InteriorSeparator--
medium
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Provide the formula for correlation in terms of covariance and standard deviations.
--InteriorSeparator--
p(X, Y) = Cov[X, Y] / (StDev[X] * StDev[Y])
--InteriorSeparator--
medium
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the properties for the function f(t) = e^t
--InteriorSeparator--
(right) f'(t) = f''(t) = e^t
(right) Convex on R+
(wrong) Concave on R+
(wrong) f(t) = 0 for all t
--InteriorSeparator--
hard
--InteriorSeparator--
48
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How can Chernoff inequality be used for a number of independent Bernoulli variables?
--InteriorSeparator--
Can show that the sum of a number of independent Bernoulli variables decreases exponentially as we move (to the right) from the average of this sum
--InteriorSeparator--
medium
--InteriorSeparator--
25
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
A random variable X >= 0 has mean and variance both equal to 20. Using Markov, what can we say about P{X >= 40}?
--InteriorSeparator--
P{X >= 40} <= 1/2
--InteriorSeparator--
medium
--InteriorSeparator--
40
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
A random variable X >= 0 has mean and variance both equal to 20. Using Chebyshev, what can we say about P{X >= 40}?
--InteriorSeparator--
(right) P{X >= 40} < 4/9
(wrong) P{X >= 40} <= 1/2
(wrong) P{X >= 40} < 1/4
--InteriorSeparator--
hard
--InteriorSeparator--
40
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
State the Corollary of Hoeffding's inequality.
--InteriorSeparator--
P{|X - E[X]| > ε} < 2exp(-2ε^2 / Σ(bi - ai)^2)
--InteriorSeparator--
hard
--InteriorSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is an alternative to evaluating the characteristics of a binomial variable?
--InteriorSeparator--
(right) Using Chernoff inequality
(wrong) Using the CDF
(wrong) Using the PMF
(wrong) Calculating the PDF
--InteriorSeparator--
medium
--InteriorSeparator--
27
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
True or False: Markov inequality needs independent random variables
--InteriorSeparator--
False
--InteriorSeparator--
hard
--InteriorSeparator--
28
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
True or False: Chebyshev inequality needs independent random variables
--InteriorSeparator--
False
--InteriorSeparator--
hard
--InteriorSeparator--
28
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
True or False: Chernoff inequality needs independent random variables
--InteriorSeparator--
True
--InteriorSeparator--
hard
--InteriorSeparator--
28
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
If two events are independent, then which statements are always correct?
--InteriorSeparator--
(right) They are not correlated
(wrong) They are correlated
(wrong) They have the same mean
--InteriorSeparator--
hard
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In the context of Hoeffding's inequality, what does 'a' represent?
--InteriorSeparator--
a is a margin for the probability that X deviates from E[X]
--InteriorSeparator--
hard
--InteriorSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Hoeffding’s inequality builds upon which concept?
--InteriorSeparator--
(right) Bounded random variables
(wrong) Unbounded random variables
(wrong) Normally distributed data
--InteriorSeparator--
hard
--InteriorSeparator--
29
--FlashCardSeparator--
}], role=model}, finishReason=STOP, avgLogprobs=-0.17387889883371704}]