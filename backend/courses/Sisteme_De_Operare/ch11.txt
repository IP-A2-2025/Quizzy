***************Beginning Page***************
***************page number:1**************
Chapter 11: Mass-Storage
Systems
—

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
WChapter 11 : Mass-Storage Systems
I Overview oi Mass Storage Structure
I HDD Scheduling
I NVM Scheduling
I Error Detection and Correction
I Storage Device Management
I Swap-Space Management
I Storage Atiachmeni
I RAID Structure

***************Ending Page***************

***************Beginning Page***************
***************page number:3**************
A I I
W Objectwes
I Describe the physical structure oi secondary storage devices and
the effect of a device's structure on its uses
I Explain the perlorrnarice characteristics of mass-storage devices
I Evaluate l/O scheduling algorithms
I Discuss operating-system services provided for mass storage,
including RAID
Wm", 5"..." CW,“ 7 w mm H 1 “mm-z Gllvln m we slut‘

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
A
W OverVIew of Mass Storage Structure
I Bulk of secondary storage for modern computers is hard disk drives
(HDDs) and nonvolatile memory (NVM) devices
I HDDs spin platters ol magnetically-coated material under moving read-
write heads
l Drives rotate at 60 to 250 limes per second
' Transfer rate ls rate at which data llow between drive and oompuler
' Positioning time (randomacoess time) is time to move disk arm to
desired cylinder (seek time) and time for desired sector to rotate
under the disk head (rotational latency)
' Head crash results lrorn disk head making contact with the disk
surface -- Thars bad
I Disks can be removable
am",a"..mam.amaa.m m Emma admnmqniam

***************Ending Page***************

***************Beginning Page***************
***************page number:5**************
“19$? Moving-head Disk Mechanism
(rank! | spindls
4~>
F! a‘
g $1.1?\
cylinder c 4H} l i Intimate
; ; ma
\ _ ___ I
m,
m/
Malian

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
M Hard Dlsk Drlves
I Planers range 1mm .85" to 14" (histories
' Commonly 3.5", 2.5", and 1Y8‘ . v > '
I Range from sues \o 3TB perdrive _ \_ ﬁ'
I Peﬂormance / /,.,' ~ = W7“ .‘
AG)“ ¢ ‘ ~.
' Transler Rate — theoretical — 6 Gbls J " -*
Q EWemive Transier Rale — real — _ v
1Gb/sec
O Seek lime from 3mslo12ms i 9ms
common for desktop drives
O Average seek \Zme measured or
calculated based on 1/3 of ‘racks
0 La\ency based on spindle speed
'1/(RPM/60):60/RPM
° Average lalency : ‘A lalency
0mm", 5"..." Cum,“ , m mm H e Ellhvuhllz. SIM“ m em, em

***************Ending Page***************

***************Beginning Page***************
***************page number:7**************
r‘
“W Hard Dlsk Performance
I Access Latency = Average access time = average seek time +
average lalency
° For lastest disk 3ms + 2ms = 5ms
' For slow disk 9ms + 5.56ms = 14.56ms
I Average l/O time = average access time + (amount to transfer/
transfer rate) + controller overhead
I For example to transfer a 4K5 block 0n a 7200 RPM disk with a
Ems average seek lime, le/sec lransler raie with a ,1ms
controller overhead =
' 5ms + 4.17ms + Uv1rns + transfer time =
. Transler time 1 4K5 I 1617/5 " BGb l GB " 1GB I 10241KB 1 32
/ (1024i) = 0.031 ms
0 Average l/O time for 4K5 block = 9.27ms + .031ms =
9.301ms
om.wsy....ncm.m.em~am M mm.‘ amnmqniam

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
A
~43?) The Flrst Commerclal Dlsk Drlve
_ \ \
L 1956
FAA x IBM RAMDAC computer
‘1/, ’ included the IBM Mudel
\ ' _ f ~ 350 disk storage system
1‘ - v . t 5M (7 blt) characters
HR 1 50 x 24” platters
.é ﬂ ;& é? ' " Access time = < 1 second
*4' ‘ _ f‘
My ._ ,
,MM..__Y_ _ _ ‘
om",5,...mcwmkmm H. “mm-z W“ m W181."

***************Ending Page***************

***************Beginning Page***************
***************page number:9**************
A
‘4);’ Nonvolatile Memory Devices

I If disk-drive like, then called solid-state disks (SSDs)

I Other iorrns include USB drives (thumb drive, ﬂash dnve), DRAM
disk replacements, surface-mounted on motherboards‘ and main
storage in devices like smarlphones

I Can be more reliable than HDDs

I More expensive per MB

I Maybe have shorter life span i need carelul management

I Less capacity

I But much faster

I Busses can be loo slow -> connect directly to POI for example

I No moving parts, so no seek time or rotational latency

***************Ending Page***************


***************Beginning Page***************
***************page number:10**************
ﬁ
“w Nonvolatlle Memory DeVIces
I Have characteristics that present
challenges
I Read and written lﬂ “page” increments
(think sector) but can‘t overwrite in place
0 Must ﬁrst be erased, and erases
happen in larger "block" increments
. Cari only be erased a limited number 0f
times belore worn out — ~ 100,000
. Liie span measured in drive writes per
day (DWPD)
t A 1TB NAND drive with rating of
5DWPD is expected to have 5TB per
day written within warrantee period
without tailing
om".5,...WWHWW n in sit-m.‘ “Mum-“1W.

***************Ending Page***************

***************Beginning Page***************
***************page number:11**************
Q
._ NAND Flash Controller Algonthms
I With no overwrite, pages end up with mix of valid and invalid data
I T0 lrack which logical blocks are valid‘ controller mainlains flash
translation layer (FTL) table
I Also implements garbage collection lo lree invalid page space
I Allocates overprovisioning to provide working space for GO
I Each cell has lilespan, so wear leveling needed to write equally to all cells
valid valid w V i-
n“! page
i an i valid
pig! r1‘; nag!
NAND block with valid and invalid pages
Ovlmmiiyilum Manama.“ ll H alumna gammaaqnwh

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
A .
MW Volatile Memory
I DRAM frequentiy used as mass-storage devide
' Nul teenniesily sewndsry storage because volalile, bul can nsve me
systems, be used like very lasl secondary storage
I RAM drives (wiin many names, including RAM disks) present as raw block
devices, eemmeniy file system formatted
I Computers have buffering, caching ina RAM, se why RAM drives?
' Caches I buﬂers aliocated / managed by programmer, operaling syslem,
hardware
' RAM drives under U59? contra‘
0 Found in an majoroperating sysiems
' Linux Nev/ram, macOS dlskutil (O create (hem, Linux /tmp of
me system type tmpfs
I Used as high speed temporary storage
' Programs could share bulk daie, quickly, by rsading/wriling te RAM drive
0mm, eye..." emu“ e "P sum u t; Ellhvuhllz. e-inn ."d ems emu

***************Ending Page***************

***************Beginning Page***************
***************page number:13**************
7 P -
.1.»
W’ Magnetlc Tape
WmWWW-“Wmdn-Madmmmm
m m Ma MM .md m m.“ mm m m
m w m m my M mm We TM m M
Mu.“ mmmmmmmmmWM
"hm/M 5mm WWWmmwmmm
WHEN“.
w...“ MLvom-p-nmmnwmm-Mm. A

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
t‘
“3?; Dlsk Attachment
I Host-attached storage accessed tnrougn l/o ports talking to IIO busses
I Several busses available, including advanced technology attachment
(ATA), serial ATA (5AM; eSATA, serial attached SCSI (5A5), universal
serial bus (USE), and fibre channel (FC)Y
I Most common is SATA
I Because NVM much taster than HDD, new tast intertace for NVM Called NVM
express (NVMe), connecting directly to PCI bus
I Data trensters on a bus carried nut by special electronic processors called
controllers (or host-bus adapters, HBAs)
. Hust controller on the computer end ot the bus, device contmller on device
end
l Computer places commend on nosl controller, using memory-mapped l/O
pons
t Host controller sends messages to device controller
t Data transierred via DMA between device and computer DRAM
0mm in..." “We m sum l. is sum-s em .o mien‘

***************Ending Page***************

***************Beginning Page***************
***************page number:15**************
t‘
W Address Mapplng
I Disk drives are addressed as large l-dirnensional arrays or logical
blocks, where (he logical block is (he smallest uni! 0' transfer
' Low-level formatting creates logical blocks on physical media
I The l-dimensional array of logical blocks iS mapped irltO the sectors
or the disk sequentially
' Sector 0 is the ﬁrst sector of the first track on the outermost
cylinder
0 Mapplrlg proceeds in order through thal track, lherl the rest of the
lracks in that cylinder, and lherl through the rest ol the cylinders
lrom oulermusl to innermost
Q Logical to physical address should be easy
> Except ror bad sectors
> Non-constant # of sectors per track via constant angular
velocity
Ovimmi Syn-m Cam,“ e In.‘ arm ii ti Ellhluahlm. rain“ ."a am. em

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
A
way-‘i HDD Schedullng
I The operating system is responsible for using hardware
efﬁciently — for the disk drives, this means having a last
access time and disk bandwidth
I Minimize seek time
I Seek time = seek distance
I Disk bandwidth is the total number oi bytes transferred,
divided by the total time between [he first request for service
and the completion of the last transfer
um“. m..." “We w 5.1....“ it w “mm-z W“ .n. W1 W.

***************Ending Page***************

***************Beginning Page***************
***************page number:17**************
A
MW Disk Scheduling (Cont.)
I There are many seuroes oldisk l/o request
. OS
' system prooesses
0 Users processes
I l/o request Includes input or eutpul mode, eisk address, memory address,
number (ll SGCIOFS IO transfer
I os maintains queue ol rsquesls, per dlsk ur device
I Idle disk can immediately work 0|’! IIO [EQUESL busy diSK means work must
0 Optimization algorithms only make sense wnen a queue exists
I In the pest, operating system responsible for queue management, disk
drive head SCVledUIiﬂg
. NOW, built iﬂlO (he storage devices, controllers
' Just pruvlde LBAs, handle sorting of requests
r Some oi the algorithms tney use described next

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
A
W D|sk Schedullng (Cont.)
I Note that drive controllers have small buffers and can manage a
queue of l/O requests (of varying “depth")
I Several algorithms exist l0 schedule the servicing 01 disk l/O
requests
I The analysis is true for one or many platters
I We illustrate scheduling algorithms with a request queue (0-199)
98,183,37,122,14,124,65,67
Head painter 53
0mmWNWWPM,“ l. l, Slim-a mum-aim“

***************Ending Page***************


***************Beginning Page***************
***************page number:19**************
w FCFS
Illustration shows total head movement of 64D cylinders
queue : 98, 183,37,122,14,124.65,67
head starts at 53
O 14 37 536567 98 122124 183199
1_1—1_1_u—1—u—1_1
T
0mm", 5m... mum. 71M mm qu “mm-z SIM“ m w": mm

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
W” SCAN
I The disk arm starls at one end of the disk, and moves toward the
other end, servicing requests until it gets to the other end of the
disk, where the head movement is reversed and servicing
continues.
I SCAN algorithm Somelimes called the elevator algorithm
I Illustration shows total head movement ol 208 cylinders
I But note that if requesls are uniformly dense, largest density at
other end of disk and those wait the longest
Wm. m..." Mum.’ w mm tut mm.‘ W“ .n. W1 W.

***************Ending Page***************

***************Beginning Page***************
***************page number:21**************
“6} ' SCAN (Cont.)
queue i 98,183, 37, 122, 14, 124, 65, 67
head starts at 53
O 14 37 536567 98 122124 183199
|_1_|_1_u—1_u—1_|
‘1
0mm", 5"..." “mm. 7 w mm 112; “mm-z SIM“ m w": emu

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
W” c SCAN
I Provides a more uniform wait time than SCAN
I The head moves from one end ol the disk to the other, serVicing
requests as it goes
' When it reaches the other end, however, it immediately
returns t0 the beginning of the disk, without servicing any
requests on the return trip
I Treats the cylinders as a circular list that wraps around trom the
last cylinder to the ﬁrst one
I Total number of cylinders?
om“, 5"..." Cum,“ , w mm tIZJ “mm-z s-M“ m w": slut‘

***************Ending Page***************

***************Beginning Page***************
***************page number:23**************
W C-SCAN (Cont.)
queue 98.183.37. 122. 14, 124, 65, S7
head starls a1 53
O 14 37 536567 98 122124 183199
)_|_1_1_u—1_u—1_¢
r
0mm", 5"..." Cum,“ 7 w mm 1m “mm-z SIM“ m a...“ emu

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
y‘
~43?) Selecting a Disk-Scheduling Algorithm
I SSTF Is common and has a natural appeal
I SCAN and C'SCAN perform beller for systems trial place a heavy lead orl lhe disk
' Less slarvallon‘ bill sllll Deselble
I Ta avoid siarvallon Linux implements deadline scheduler
' Malnlalrls separate read and write queues. gives read priorily
> Because processes more likely lo block on read lharl write
' lrnplerrlerlls lqur queues: 2 x read and 2 x wrlle
> 1 read and 1 wrrle queue serled in LBA order‘ essentially lmplernentirlg OSCAN
> 1 read and 1 wrile queue sorled ll'l FCFS order
> All l/O requesls sent in balm sowed in lhalqueue‘s order
> Aﬂer each balch‘ checks If any requesls in FCFS older than conﬁgured age
(delauli SOOms)
- Ii an‘ LBA queue cnnlalrling lhal request is selected ier next batch el l/O
I lrl RHEL 7 eled NOOP and completely lair queueing scheduler (c FQ) also
available, deleulle vary by slorsgs device

***************Ending Page***************

***************Beginning Page***************
***************page number:25**************
R .
w)?“ NVM Scheduling
I NO dlsk heads or rclaﬂonai ialency but still room (or Oplimilalion
I In RHEL 7 NOOP (no scheduling) i5 used but adjacent LEA requests are
Combined
' NVM b95131 Iandcm l/Ov HDD at Sequeniial
0 Throughput can be similar
' lnpullOulpul operations per second (IUPS) much higher with NVM
(hundreds 0f ihousands VS hundreds)
I But write ampliﬁcation [one write, causing garbage collection and many
read/writes) can decrease ihe perkmnance advantage
0mm", 5"..." Cum,“ , w mm "a “mm-z s-M“ m w": emu

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
y‘

MW Error Detection and Correction
I Fundamentai aspect of many pans of computing (memory, networking, storage)
I Error detection determines ii irrere a problem has occurred (for example a bit

ﬂipping)
' II delecied, can hall [he operation
0 Detection rrequerruy done via parily bit
I Parity one form or checksum e uses modular arithmetic to oompule, store,
compare values 0f ﬁxed-length words
' Anoihererror-deiection melhod common in networking is cyclic
redundancy check (CRC) which uses hash function to deiect muliipie-bil
errors
I Error-correction code (ECC) not only dereeis, but can correct some errors
' Soﬂ errors correctable, hard errors detected bu‘ not curreded

***************Ending Page***************

***************Beginning Page***************
***************page number:27**************
ﬁ
“W Storage DeVIce Management
I Low-level ronnattinq, or physical formatting i Dividing a disk into
sectors that the disk controller can read and write
' Each sector can hold header information. plus data, plus error
correction code teccl
' Usually 512 bytes at data but can he selectable
I To use a disk to hold ﬁles, the operating system still needs to record its
owrl data structures on the disk
. Partition the disk irlto one or more groups or cylinders, each treated
as a logical disk
i Logical formatting or ‘making a ﬁle system"
0 To increase efﬁciency mostﬁle systems group blocks into clusters
t Disk IIO done in blocks
> File l/O done in clusters

***************Ending Page***************


***************Beginning Page***************
***************page number:28**************
A
W'Storage Device Management (cont.)
I Root partition contains the OS, other partitions can hold other
Oses, other file systems, or be raw
' Mounted at boot time
. Other partitions can mount automatically or manually
I At mount time, ﬁle system consistency checked
0 ls all metadata Correct?
t Ii not, ﬁx it, try again
> ll yes, add lo mount table, allow access
I Boot block can point to boot volume or boot loader set of blocks that
contain enough code IO know hDW to load the kernel from the ﬁle
system
. Or a boot management program for multi-OS booting
om",5,...mcm.m.e.n.i.,.m “I, Siam-a Mummiam

***************Ending Page***************

***************Beginning Page***************
***************page number:29**************
t‘
“W DeVIce Storage Management (Cont)
I Raw disk access for apps that
want to do their own block
management, keep OS out 01 the
wa databases for exam le a" m
y‘ . . . . p ) Imi-
I Boot block |n|t|al|zes system -\
pavtmont \ pamlmn
i The bootstrap is stored in \
ROM. ﬁrmware WW -
i Bootstrap loader program palm“ - W'Pamw"
stored in boot blocks of boot
I Methods such as sector sparing _
used to handle bad blocks Bootlng from secondary
storage In Wlndows
om.nm..mcm.mpmam t. 3., “mm Mummism

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
t‘
W Swap-Space Management
I Used (or moving entire processes (swapping), or pages (paging), irom
DRAM to secondary storage when DRAM not large enough for all processes
I Operating system provides swap space management
l Secondary storage slower than DRAM, so important to optimize
performance
' Usually multiple swap spaces possible , decreasing l/O load on any
given device
' Best t0 have dedicated devices
° Can be in raw partition or a ﬁle within a ﬁle system (for convenienoe of
adding)
° Data structures for swapping on Linux systems:
>i Mo we e
p Page a
‘M
III--
nr swiD m-
0mm in..." Ema.’ m a.“ l. :1 Elam.‘ a.“ .n. w‘. all‘

***************Ending Page***************

***************Beginning Page***************
***************page number:31**************
t‘
W Storage Attachment
I Computers access storage in three ways
Q host—attached
Q network-attached
Q cloud
I Host attached access through local IIO ports, using one of
several technologies
Q To attach many devices, use storage busses such as USB.
lirewire, thunderbolt
Q High-end systems use ﬁbre channel (FC)
t High-speed serial architecture using ﬁbre or copper
cables
t Multiple hosts and storage devices can connect to the FC
fabric
om", 5"..." “We m a.“ u u mm.‘ a.“ m w‘. W“

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
A
“$9 Network-Attached Storage
I Network-attached storage (NAS) is storage made avallable over
a network rather than over a local Connection (such as a bus)
. Remotely attaching to ﬁle systems
I NFS and CIFS are common protocols
I Implemented via remote procedure calls (RPCs) between host
and storage over typically TCP or UDP on IP network
I iSCSl protocol uses IP network to carry the SCSI protocol
Q Remotely attaching to devices (blocks)
m
m
Wm, 5"..." “We m» 5.1.th l. :3 Ell-m.‘ a.“ .n. w‘. gm

***************Ending Page***************

***************Beginning Page***************
***************page number:33**************
ﬁe
say) Cloud Storage
I Similar lo NAS, provides access to slorage across a network
' Unllke NAS, accessed over the lntemel or a WAN to remote
data center
I NAS presented as just another ﬁle system, while cloud storage is
API based, With programs using the APls lo provide access
. Examples include Dropbox, Amazon S3, Microsoft OneDrive,
Apple iCloud
. Use APls because of latency and failure scenarios (NAS
protocols wouldn't work well)
0mm in..." Emmy m a.“ u u mm.‘ a.“ .n. w‘. Wit

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
r‘
MW Storage Array
I Can just attach disks, or arrays oi disks
I Avoids the NAS drawback 01 using network bandwidth
I Storage Array has controller(s), provides features to attached
host(s)
Q Ports to connect hosts to array
' Memory, oontrolling software (sometimes NVRAM, etc)
Q A few to thousands of disks
. RAID, hot spares, hot swap (discussed later)
. Shared storage -> more efﬂciency
' Features found in some file systems
> Snaphots, clones, thin provisioning, replication,
deduplioalion, etc
om",Syihmcmmiamam it as gimme “humanism

***************Ending Page***************

***************Beginning Page***************
***************page number:35**************
W‘ Storage Area Network
I Common in large storage environments
I Mulliple hosis aﬂached to multiple storage arrays i ﬂexible
M M
SW Chum
SIR
CBMEY
was web oomem
lihvavy pvmidnr
0mm 5M... CW,“ 7 w mm u as “mm; W“ m 6.“, ﬁlm:

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
t‘
“W Storage Area Network (Cont.)
I SAN l5 one or more storage arrays
' Connected to one or more Fibre
Channel switches or InfiniBand
(IB) network
I Hosts also attach to the switches ‘1 _ l l
I Storage made available via LUN _________:_>
Masking from speciﬁc arrays to alum-i‘ g ‘
speciﬁc servers I ""r"'""::‘ 4 v
I Easy to add or remove storage, add ‘ "53::
new host and allocate it storage ---II_II.I?.v
I Wny have separate storage m“
networks and communications A Storage Array
networks?
. Consider iSCSl, FCOE
awash..."awnaam {1:1 amt-a Mmaamiem;

***************Ending Page***************


***************Beginning Page***************
***************page number:37**************
K‘
‘$9 RAID Structure
I RAID — redundant array oi inexpensive disks
' multiple disk drives provides reliability via redundancy
I Increases the mean time to failure
I Mean time to repair — exposure lime when another lailure could
cause data loss
I Mean time to data loss based on above factors
I "mirrored disks 'ail independently, OOnSider disk with 1300.000
mean time to failure and 10 hour mean time to repair
. Mean time to data loss is 100, 0002 l (2 *10) = 500 *106 hours,
0r 57,000 years!
I Frequently combined with NVRAM to improve write performance
I Several improvements in disk»use techniques involve the use of
multiple disks working cooperatively
gamma“... Mammalian.“ u u Ellhvuhllz. GIIvln-nnslgneﬁluq

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
(as
My’ RAID (Cont.)
I Disk striping uses a group of disks as one storage unit
I RAID is arranged into six different levels
I RAID schemes improve performance and improve the reliability er
the storage system by storing redundant data
' Mirroring or shadowing (RAID 1) keeps duplicate of each
disk
' Striped mirrors (RAID 1+0) or mirrored stripes (RAID 0+1)
provides high periormance and high reliability
v Block interleaved parity (RAID A, 5, s) uses much less
redundancy
I RAID within a storage array can still fail il the array fails, so
automatic replication of the data between arrays i8 COmmOn
I Frequently‘ a small number 0f hot-spare disks are leﬂ
unallocated, automatically replacing a tailed disk and having data
rebuilt onto them
OM",rrihmmmsmam h r, Err-arr.‘ alarm-him“

***************Ending Page***************

***************Beginning Page***************
***************page number:39**************
“w ,, RAID Levels
IIII
“mm.”
IIIIIIII
mm..-
IIIII
ka...‘
IIIII
magma...»
IIIIII
mum.“
IIIIII
IIIIII
IIIIII
IIIIII
IIIIII
IIIIII

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
|‘7'r RAID(0+1)and(1+0)
a a 1M5 a

***************Ending Page***************

***************Beginning Page***************
***************page number:41**************
r‘
“)9 Other Features
I Regardless of where RAID implemented, other useful features
can be added
I Snapshot is a view of ﬁle system before a set of changes take
place (Le, al a point in time)
0 More in Ch 12
I Replication is automatic duplication 0f writes between separate
sites
. For redundancy and disaster recovery
. Can be synchronous or asynchronous
I Hoi spare disk is unused, automatically used by RAID production
if a disk lails to replace the lailed disk and rebuild the RAID set if
possible
° Decreases mean time to repair
0mm in..." “may m a.“ I“, mm.‘ a.“ .n. w“. W“

***************Ending Page***************

***************Beginning Page***************
***************page number:42**************
r‘
MW Extenslons
I RAID alone does not prevent or detect
daia Corrupiion 0r other errors, just disk
failures
I Solaris ZFS adds checksums 0f all data
and metadata
mammal
I Checksums kepi wiih pointer to object, to m as.“
detect if object is the right one and
whether it changed
I Can deieci and correct data and metadata
cormmio" mm
I ZFS also removes volumes, partitions
' Disks allocated in pools m‘ “M
Q Filesystems with a pool share that ZFS checksums all
pool, use and release space like metadata and data
mallocO and freeﬂ memory
allocate / release calls A
mammal“... mmmpmmm lua mm“; wounds-gnaw“

***************Ending Page***************

***************Beginning Page***************
***************page number:43**************
W Traditional and Pooled Storage
ii ii ii
(a) Tvadmonal volumes and me systems
EEEEE
1h] ZFS and Domed slomqs

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
y‘
“W Object Storage

I Genera-Mme compullng, me syslems m sulﬁclenl (or very large some

I Anulher approach e star‘ WM a smrage pool and place omects in m
0 omen Just a cnnlalner M data
' No vyay 10 nevigexe me pew 10 m1 objects (no dveﬂory siruqures, (ew

servlces

I Computereonenled‘ nolusereonemea

I Typmal sequence
0 Create an ohlecl WIthm me pew, reoewe an oh|ect ID
' Access oblecl via mm \n
. DQIGIG DbjSCl VIE "131 ID

I Owed swrage management soﬂware like Hadoop file system (HDF$| and

Ceph delermms where m smre objecls, manages prolecﬁon
' TypicaHy by swring N copies, acvoss N sysiems, in me object siorage clusKer
' Horizontally scalable
. CHINE!“ addressable, unstructured
we.“ in..." “We w em Hts elemme We m w": W‘

***************Ending Page***************

***************Beginning Page***************
***************page number:45**************
End of Chapter 11
—

***************Ending Page***************

