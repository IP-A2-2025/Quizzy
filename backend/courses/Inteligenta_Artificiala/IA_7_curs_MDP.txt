***************Beginning Page***************
***************page number:1**************
IA 2023/2024
<E|> <ﬁ>4IE>4IE> IE OQO

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
Continut
lntroducere
Procese de decizie Markov
Value iteration
Policy iteration
E1 ﬁ ' _= :E ‘ORG
Cm 7 'A 2023/2024 2/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:3**************
Tnvétare cu Tntﬁrire (Reinforcement learning)
Agentul trebuie $5 Tnvete un comportament, féré a avea un instructor
V Agentul are o sarciné de Tndeplinit
V Efectueazé o serie de acfciuni
> Primeste feedback din partea mediului: cét de bine a actionat pentru
a-$i Tndeplini sarcina.
Agentul primegte o recompensé pozitivﬁ dacﬁ Tndeplinegte bine
sarcina, altfel o recompensé negativé.
Aceasté modalitatea de Tnvéfcare se numeste Tnvétare cu Tntérire.
Curs 7 IA 2023/2024 3/41

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
Modelul de interactiune
> Agentul efectueazé actiuni
P Mediul Ti prezinté agentului situatii numite stéri $i acordé recompense
Agent
state reward action
x, I’ u!
I rH-I
s”. Enwronment
Traiectorie / episod (50,.20, r0,51,al, r1, ...)
Cue 7 ‘A 2023/2024 4/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:5**************
Invétare cu Tntﬁrire
> Scopul: de a determina agentul 55 actioneze a.T. 555i maximizeze
recompensele
P Agentul trebuie 55 identiﬁce secventa de actiuni ce conduce Ia
Tndeplinirea sarcinii
P Date de antrenare: (S,A, R) Stare, Actiune, Recompensé
Cm 7 ‘A 2023/2024 5/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
Apllcatll: TD-Gammon
predicted probability
0| winning: Vr
TD error, VM- v, —~éo
- - - - - - hidden units 40*30
0*?‘ 0 0 ( }
backgammon posiiion (198 input uniis)
O refcea neuronalé antrenati cu TD-learning (intrarea: pozitia, ie$irea: 0
estimare a valorii pentru acea pozifcie)
Invaté din simuléri (joacé jocuri Tmpotriva |ui Tnsu$i)
Cm 7 ‘A 2023/2024 6/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:7**************
Apl Icatl |: robotlca
AIBO: Tnvafcé' 55 meargé '04 Minitaur quadrupedal robot: Tnvafcé
(utilizénd Policy gradient) $5 mea rgé (actor-critic deep RL)
a? <7 m “in... “mm h ‘If um M “m. in in‘! 221%;Erggrgm'ﬁﬂﬁ WAT-‘LTﬁfﬂﬁ RZT-EWQHYSJK.“
ZEmgmhnm-wmlmnﬁu-nLB-nn.
https://sites.goog1e . com/view/
https : //www. cs . utexas . edu/users/ minitaur-locomotion/
AustinVilla/?p=research/1earned_walk
Cm 7 ‘A 2023/2024 7 / 41

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
Deep Reinforcement Learning
AlphaGo (Google DeepIVIind, 2015): programul a Tnvétat 55 joace jocurile
Atari 2600 urmé'rind doar afisajul 5i scorul
> 2016: a c§$tigat cu 4-1 impotriva lui Lee Sedol, jucétor de GO
profesionist cu 9 dan, premiu: 1 000 000$
> 2017: a cé$tigat Tmpotriva lui Ke Jie, cel mai bun jucétor de GO din
lume
AlphaGo Zero: a invéfcat $5 joace féré informatii din jocuri ale persoanelor
umane (doar pe baza regulilor jocului) $i a Tnvins AlphaGo cu 100-0 (oct.
2017)
AlphaZero a invins AlphaGo Zero cu 60-40 $i a ajuns dupé’ doar 9 ore de
antrenare Ia un nivel superior tuturor programelor de $ah $i GO existente
(dec. 2017)
Curs 7 IA 2023/2024 8/41

***************Ending Page***************


***************Beginning Page***************
***************page number:9**************
Conti n ut
Procese de decizie Markov
Value iteration
Policy iteration
El 5' i E :5 QQQ
Cm 7 ‘A 2023/2024 9/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
Decizii secventiale
0.3
1 2 3 4
> Mediu determinist
F (sus, sus, dreapta, dreapta, dreapta)
> Mediu stochastic
P Model de tranzitii P(s’|s, a): probabilitatea de a ajunge din starea s Tn
starea s’ efectuénd actiunea a
> Actiunea obtine efectul dorit cu probabilitatea 0.8
F Agentul primeste o recompensé: -0.04 pentru stérile nonterminale;
+/-1 pentru stérile terminale

***************Ending Page***************


***************Beginning Page***************
***************page number:11**************
Deterministic vs. stochastic
Deterministic Grid World Siochastic Grid World
III
HEB
E N s W
E N s w
i:| 15' 2 E :5 V’) Q "v
Curs 7 IA 2023/2024 11/41

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
Presupunerea Markov
> Starea curenté st depinde de un istoric finit al stérilor anterioare
> Proces Markov de ordin Tntéi: starea curenté’ st depinde doar de
starea anterioaré stil
P($t‘5t—1,- - '150) I P(5t‘5f—1)
Cm 7 ‘A 2023/2024 12/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:13**************
Proces de decizie Markov (Markov Decision Process)
Proces de decizie Markov: o problema de decizie secventiala pentru un
mediu stochastic cu un model de tranzitie Markov $i recompense aditive
> Sta'ri s E S (starea initiala so), actiuni a € A
> Modelul de tranzitii P(s/]s7 a)
V Functia de recompensa R(s)
Cum arata o solutie? Trebuie 55 speciﬁce ce actiune $5 execute agentul Tn
ﬁecare stare (politica Tl‘).
Sunt probleme de cautare nedeterministe.
Putem utiliza alg. de cautare (expect/max).
Cm 7 ‘A 2023/2024 13/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
Exemplu
> Un robot (ma$in§) vrea 55 célétoreascé departe, repede.
P Stéri: Cool, Warm, Overheated. Actiuni: Slow, Fast. Dacé merge
mai repede, prime$te 0 recompensé dublé.
0.5 +1
\ Fast 1'0
+1 Slow f ‘32 '10
0.5 <
Slow / \ Warm \ “xiii;
G V V Fast 0.5 +2 Q
1-0 +1 COO‘ 0'5 overheat/ed’
+2
Cm 7 ‘A 2023/2024 14/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:15**************
Exemplu: arborele de céutare
f2}
1AA 1/‘ \JK b
is b‘ is It‘ is‘ b
Curs7 U 5' IA 2;)23/20; :5 E74?

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
Arbori de céutare pentru MDP
> Fiecare stare proiecteazi un arbore de céutare.
s
I‘ s, a
5,,as'
L '5
s stare, (s,a,s’) tranzitie, P(s’]s, a), R(s,a,s’)
> Tn problemele de céutare, scopul este de a identiﬁca o secventé'
optimé.
Tn MDP, scopul este de a identifica o politicé’ optimé 7T* (strategie)
P 7T : S —> A
71(5) este actiunea recomandaté Tn starea s
Cm 7 ‘A 2023/2024 16/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:17**************
MDP

> Utilitate: suma recompenselor pentru o secvenfcé de stﬁri.
Recompensa este c§$tigu| imediat, pe termen scurt; utilitatea este
c§$tigu| total, pe termen lung.

> Calitatea unei politici: utilitatea a$teptat§ a secvenfcelor posibile de
stéri.
Mediu stochastic: putem avea o secvenfcé diferité de stéri cénd
executﬁm aceea$i politicé din starea initialé.

> Politica optimé 7r* maximizeazé utilitatea a$teptat§.

Cm 7 ‘A 2023/2024 17/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
MDP
Exemplu: politica optimé 5i valorile stérilor
IIII IIII
I IE I IE
IIII II
1 2 a 4 1 2 a 4
Curs 7 IA 2023/2024 18/41

***************Ending Page***************


***************Beginning Page***************
***************page number:19**************
Utilitéfci
Orizont finit
7 Uh([50,51, . . . , 5N+k]) : Uh([50, 51, . . . ,SNl),V/( > 0
Dupé momentul N, nimic nu mai conteazé.
V Politica optimé nu este stationaré: actiunea optimalé pentru o
anumitﬁ stare se poate schimba Tn timp.
Exemplu:
, IIII
z IIIH
' IIII
P N I 3 —> trebuie 55 ri$te (sus)
> N : 100 —> poate alege solutia mai sigur‘a’ (stﬁnga)
Cm 7 ‘A 2023/2024 19/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
Utilitéfci
Orizont inﬁnit
> Nu existé un termen limité ﬁx
> Politica optimé este stationaré
V a. Recompense aditive
Uh([$0, 517 $2, . . .1) I R($0) + R($1) + R(52) + - - -
P b. Recompense actualizate (discounted)
2
Uh([so7 51, 52, . . .]) : R(50) + 7R(51) + W R(52) + . ..
'y € [0,1] factorul de actualizare (discount factor) indic5 faptul c5
recompensele viitoare conteazé mai putin decét cele imediate.
Cm 7 ‘A 2023/2024 2O/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:21**************
Recompense actualizate
uh([50,51752, . ..]) I R(50)+’YR(51)+’YZR(52)+...
V Preferé recompensele curente, nu cele de mai térziu
> Valorile recompenselor scad exponential
Exemplu 1: 'y : 0.5
U([1,2,3]): 1>1< 1+0.5>x<2+0.25>|<3
U([1,2,3]) < U([37271])
Exemplu 2: Quiz
Curs 7 IA 2023/2024 21/41

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
Orizont infinit - evaluare
Trebuie $5 ne asigurém c5 utilitatea unei secvenfce posibil inﬁnite este
ﬁnité.
P Abordarea 1. Dacé recompensele sunt mérginite $i "y < l atunci:
OO OO
Uh([501517521---]): ZVt/ﬂst) S ZVtRmaX I RmaX/(l — V)
t=0 t:0
> Abordarea 2. Dac5 mediul contine stéri terminale 5i se garanteazé
faptul c5 agentul va atinge una din ele (avem o politicé adecvaté,
proper policy), putem utiliza 'y : 1
Cm 7 ‘A 2023/2024 22/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:23**************
Utilitatea asteptaté
P Fiecare politicﬁ genereazé secvente multiple de stﬁri, datorité'
incertitudinii tranzitiilor P(s’|s, a)
> Fie St o variabilﬁ aleatoare: starea Tn care ajunge agentul la
momentul t executénd politica 7r; 50 : s.
Utilitatea asteptaté obtinuté prin executia politicii 7T din starea s:
OO
U”(s) I E [Z vtR(5t)]
t=0
( I valoarea a$teptat§ a sumei recompenselor actualizate, obtinute
pentru toate secventele posibile de stéri)
Cm 7 ‘A 2023/2024 23/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
Evaluarea unei politici
> Politica optimé
7r: I argmax7r U”(s)
> UH (s) utilitatea adevératé a unei stéri: valoarea a$teptat§ a sumei
recompenselor actualizate dacé agentul executé’ o politicé optimé
> Principiul Maximum Expected Utility: alege actiunea care
maximizeazé utilitatea asteptaté’ a stérii urmétoare
7r*(s) I argmaxa€A(s) Z P(s’\s, a)U(s’)
SI
Cm 7 ‘A 2023/2024 24 / 41

***************Ending Page***************


***************Beginning Page***************
***************page number:25**************
Exemplu
Fie 'y : 1 5i R(s) : —0.04.
2 IE
1 2 3 4
Aproape de starea ﬁnalé utilitéfcile sunt mai mari pentru cé este nevoie de
mai putini pa$i cu recompensé negativé pentru atingerea stérii respective.
Cue 7 ‘A 2023/2024 25 / 41

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
Ecuatia Bellman
U(s) : R(s) + *ymaxaeA(s) Z P(s’ls, a)U(s’)
5/
Ecuatia Bellman (1957): utilitatea unei stéri este recompensa imediaté
pentru acea stare, R(s), plus utilitatea asteptaté maximé a stérii
urmétoare.
Cm 7 ‘A 2023/2024 26/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:27**************
Exemplu
Utilitatea stérii (1,1):
U(1, 1) I —0.04 + ymax[0.8U(1, 2) + 0.1U(2, 1) + 0.1U(1,1), (Up)
O.9U(171) + 0.1U(1, 2), (Left)
O.9U(171) + 0.1U(2,1), (Down)
0.8U(2,1) + O.1U(1, 2) + 0.1U(1,1)] (Right)
Cea mai buné' actiune: Up.
Curs 7 IA 2023/2024 27/41

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
Rezolvarea unui proces de decizie Markov
> n stéri posibile
> n ecuafcii Bellman, una pentru ﬁecare stare
> n ecuatii cu n necunoscute: U(s)
> Nu se poate rezolva ca sistem de ecuatii liniare din cauza functiei max
Cm 7 ‘A 2023/2024 28/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:29**************
Conju n ut
Procese de decizie Markov
Value iteration
El 5' i E :5 Q Q0
Cm 7 ‘A 2023/2024 29/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
I. lterarea valorilor (Value iteration)
Calculeazé utilitatea fiecérei stéri $i identiﬁcé actiunea optimﬁ Tn ﬁecare
stare
Algoritm pentru calcularea politicii optime:
P lnitializeazﬁ utilitﬁfcile cu valori arbitrare
V Actualizeazé utilitatea ﬁecérei stéri din utilitéfcile vecinilor
U,-+1(s) I R(s) + Vmaxa Z P(S"5, a) u,(s’)
5/
V Repeté pentru ﬁecare s simultan, péné la atingerea unui echilibru
Cm 7 ‘A 2023/2024 3O/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:31**************
Iterarea valorilor: pseudocod
function VALUE-ITERATION(mdp, e) returns a utility function
inputs: mdp, an MDP with states S, actions 14(5), transition model P[s" | s, a),
rewards R[s), discount qr
e, the maximum error allowed in the utility of any state
local variables: U, U’, vectors of utilities for states in S, initially zero
6, the maximum change in the utility of any state in an iteration
repeat
U ¢— U’; 5P0
for each state s in S do
f ‘_ f f
U [5] R[s) + qr £3,312 P{s | 5,11) U[s]
if | U15] — U[s]| > 6 then 64— | U’[s] — U[s]|
until 6 < 6(1 — 11);’?
return U

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
Iterarea valorilor
Value Iteration. for estimating r.- .1: m
Algorithm parameter: a. small threshold 6 > 0 determining accuracy of estimation
Initialize V(s), for all 5 E 5+, arbitrarily except that V(te'rminal) : 0
Loop:
| A e 0
| Loop for each s E S:
| 'n e V(s)
| V(s) P maxn 28,1 p(s', r| s, a) [r + 7V(s')]
| A P max(A, |'u i V(s)|)
until A < 6
Output a deterministic policy, 1r w 7r," such that
1r(s) I argmaxa Es,” p(s', r| s, a) [r + ﬁll/(5')]
Din Sutton&Bart0. Reinforcement Learning: an introduction
Functia utilitate U <—> V functie valoare

***************Ending Page***************


***************Beginning Page***************
***************page number:33**************
Iterarea valorilor
> Exemplu: Cursa
P Demo: https://courses.grainger.illinois.edu/cs440/fa2018/
lectures/mdp- value-demo . pdf
cl a , E :5 @qcv

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
Probleme ale algoritmului Iterarea valorilor
> Tncet: 0(52A) per iteratie
> Valoarea “max” la ﬁecare stare se modiﬁcﬁ rar
> Politica converge adesea cu mult Tnaintea valorilor
Cm 7 ‘A 2023/2024 34/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:35**************
Conjui n ut
Procese de decizie Markov
Policy iteration
El 5' i E :5 Q Q0
Cm 7 ‘A 2023/2024 35/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
II. Iterarea politicilor
> Dacé ﬁxﬁm politica, avem o singuré actiune per stare
> Dace”; o actiune este Tn mod evident mai buné decét toate celelalte, nu
avem nevoie de valorile exacte ale utilitéfcilor
> Algoritmul alterneazé’ urmétorii pasi:
P 1. Evaluarea politicii: daté 0 politicé 1n, calculeazé U,- : U7” utilitéfcile
stérilor pe baza politicii 1r,-
> 2. Tmbunétﬁtirea politicii: calculeazé o noué politicé 7r;+1, pe baza
utilitétilor U,-
Repeté ace$ti pa$i péné' cénd politica converge
Cm 7 ‘A 2023/2024 36/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:37**************
1. Evaluarea politicii
Actiunea pentru fiecare stare e ﬁxata de politica; Ia iteratia i, politica 7r,-
specifica actiunea 7T,'(S) Tn starea s.
Ecuatii Bellman simplificate
(1,-(5) I R($) + v2 PMS’ m($))UI-($’)
5/

V Pentru n stari, sistem de n ecuatii liniare cu n necunoscute

V Se poate rezolva exact Tn O(n3) sau Tn mod aproximativ

> Aplic‘a’m Value iteration

u,+1(5) I 12(5) + v Z PMS, menu-(5')
5/
Cue 7 ‘A 2023/2024 37 / 41

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
Evaluarea politicii
Exemplu: 7T,'(171) : Up, 7T,'(1,2) : Up
Ecuatiile Bellman simpliﬁcate:
U,‘(1,1): —0.04 + O.8U,'(1,2) + 0.1U,'(1,1) + 0.1U,-(2,1)
U,-(1, 2) I —0.04 + O.8U,-(1, 3) + 0.2U,-(1, 2)
Curs 7 IA 2023/2024 38/41

***************Ending Page***************


***************Beginning Page***************
***************page number:39**************
2. lmbunétéfcirea politicii
> Valorile U(s) se cunosc
> Calculeazé pentru fiecare s, actiunea optim5
27(5) I maxa 2 P(s’|s, a)U(s')
5/
V Dada’ ‘27(5) $5 7r,-(s), actualizeazé politica: 7r,-+1(s) + a7(s)
Se pot actualiza doar pérfcile "promitétoare" ale spatiului de céutare.
Cm 7 ‘A 2023/2024 39 / 41

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
Iterarea politicilor: pseudocod
function POLICY-ITERATION(mdp) returns a policy
inputs: mdp, an MDP with states S, actions 14(5), transition model P[s" | s, a)
local variables: U , a vector of utilities for states in S, initially zero
7T, a policy vector indexed by state, initially random
repeat
U <— POLICY-E VALUATION (1r, U, mdp}
unchanged? ‘— true
for each state s in S do
- f f f f
1f Egg-:5} E P(s |s, a) U[s] > Z P(s IS,?T[S]) U[s ] then do
S S
rr[s] <— argmax Z P{s" | s, a) U[s"]
a G A[s} s;
unchanged ? ¢— false
until unchanged‘?
return ir
Demo: https : //cs . Stanford . edu/people/karpathy/reinforcej s/gridwor1d_dp . html

***************Ending Page***************


***************Beginning Page***************
***************page number:41**************
Bibliografie
V Russel&Norvig. Artiﬁcial Intelligence: a modern approach. Ch 17.
Making complex decisions
P Sutton&Barto. Reinforcement Learning: an introduction. Ch 3.
Finite Markov Decision Processes, Ch 4. Dynamic Programming
http://incompleteideas.net/book/RLbook2020.pdf
Curs 7 IA 2023/2024 41/41

***************Ending Page***************





