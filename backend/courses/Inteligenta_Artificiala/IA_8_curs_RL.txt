***************Beginning Page***************
***************page number:1**************
IA 2023/2024
<E|> <ﬁ>4IE>4IE> IE OQO

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
Conjunut
lntroducere
lnvétarea pasivé
Bazaté pe model: ADP
Férﬁ model: TD-learning
Invétarea activé
Q-learning
Deep Q-learning
Concluzii
4E1r<ﬁ>4ifb4IE> :E QQG
CUrs8 —

***************Ending Page***************


***************Beginning Page***************
***************page number:3**************
Reinforcement learning
V Proces de decizie Markov > Tnvétare cu Tntérire

V Multimea de stéri S, > Se bazeazé pe procese de
multimea de actiuni A decizie Markov, dar:

F Modelul de tranzitii P(5’|5, a) P Modelul de tranzitii este
este cunoscut necunoscut

V Functia de recompensé R(s) P Functia de recompensé este
este cunoscuté’ necunoscuté

> Calculeazﬁ o politicé’ optimé > Tnvaté o politicé’ optimé'

Curs 8 IA 2023/2024 3/41

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
Tipuri de Tnvﬁtare cu Tntérire
Pasivé/activé
V Pasivé: agentul executé o politicé' fixé $i o evalueazé
'—d
$7
ﬁﬁm
4%
Agentul nu are control asupra actiunilor sale; roboticﬁ
> Activé: agentul Tsi actualizeazé politica pe misuré ce Tnvaté

***************Ending Page***************


***************Beginning Page***************
***************page number:5**************
Tipuri de Tnvéfcare cu Tntérire
Bazaté pe model/féré model
P Bazatﬁ pe model: Tnvaté modelul de tranzitii $i recompense $i TI
folose$te pentru a descoperi politica optimi
> Firs“; model: descoperﬁ politica optimﬁ féré a Tnvéta modelul
Cm 8 ‘A 2023/2024 5/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
Continut
Tnvétarea pasivﬁ
Bazaté pe model: ADP
Féré’ model: TD-Iearning
El r51 i E :5 ‘)QQ
Cm 8 ‘A 2023/2024 6 / 41

***************Ending Page***************


***************Beginning Page***************
***************page number:7**************
Invétarea pasivé
> Politica este ﬁxé: Tn starea s executé Tntotdeauna actiunea 7T(S)
> Scopul: Tnvaté cét de bun5 este politica 7r
> Tnvaté’ utilitatea U”(s)
cum?
executé politica $i Tnvaté din experienté
F abordare similaré cu pasul (1) de evaluare a politicii din cadrul
algoritmului lterarea politici/or; diferenta: nu cunoa$tem modelul de
tranzitii P(s’|s, a) 5i nici R(s)
lnv§fcarea pasivﬁ este o modalitate de explorare a mediului.

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
Invétarea pasivé
QHHHH
2 IE
' I":
1 2 3 4
V Agentul executé o serie de Tncercéri (trials)
(171)—.04 W (172)-404 W (173)—.04 W (1,2)—.04 W (1,3)-.04 W
(273L011 W (373L011 W (47 3)+1
(1, 1)-.04 W (1,2)-.04 W (173)-04 W (2,3)-.04 W (373)-04 W
(3, 2)<04 W (3, 3%.04 W (47 3)+1
P Politica este aceea$i, dar mediul este nedeterminist
> Scopul este 55 Tnvete utilitatea a$teptat5 U“(s) I E [220 7tR(St)]
Cm 8 ‘A 2023/2024 8/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:9**************
Continut
lnvétarea pasivé
Bazati pe model: ADP
El r51 i E :5 QQG
Cm 8 ‘A 2023/2024 9 / 41

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
Invétarea bazaté’ pe model
Programarea dinamicﬁ adaptivﬁ (ADP)
1. Tnvétﬁm modelul de tranzitii
Estimém P(s’|s,7r(s)) 5i R(s) din Tncercéri.
Utilizém un tabel de probabilitéfci (cét de des apare rezultatul unei
actiuni 5i estimém probabilitatea de tranzitie)
Exemplul 1: actiunea Right este executati de 3 ori Tn starea (1,3) $i
Tn 2 cazuri starea rezultantﬁ este (2,3)
:> P((2,3)|(1,3), Right) : 2/3
2. Rezolvé'm IVIDP
Curs 8 IA 2023/2024 10/41

***************Ending Page***************


***************Beginning Page***************
***************page number:11**************
. . V . V
Programarea dinamica adaptiva (ADP)
1. lnv5tarea modelului empiric
Exem pl u:
Input Policy TE Observed (s, a, s’, R) Transitions Learned Model
Episode 1 Episode 2 T(s, a, s’)
'2' B, east, C, -1 B, east, C, <1 lg‘:- east, 5)): 31;‘;
C, east, D, -1 C, east, D, -1 ' e35‘! = -
E D, exit, x, +10 D, exit, x, +10 NC‘ e35“ M4125
CE
u Episode 3 Episode 4 R(s, a, s’)
E, north, C, -1 E, north, C, ~1 "(3- east c1 i'l
C, east, D, -1 C, east, A, -1 2:32:31'30
mums-IF 1 o, exit, x, +10 A, exit, x, -1o ‘
Cuts 8 ‘A 2023/2024 11/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
Programarea dinamicé’ adaptivﬁ
2. Utilizém programarea dinamicé pentru rezolvarea procesului de
decizie Markov.
Probabilitéfcile $i recompensele Tnvétate se introduc Tn ecuatiile
Bellman (politica ﬁxé).
U”($) : R($) + "Y Z P($’|$, 7r(QM/"(5')
s/
Se rezolvé’ sistemul de ecuatii liniare cu necunoscutele U”(s).
ADP este ineﬁcienté dacé spatiul stérilor este mare
V sistem de ecuafcii liniare de ordin n
V jocul de table: 1050 ecuatii cu 1050 necunoscute
Cm 8 ‘A 2023/2024 12/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:13**************
Conju n ut
lnvétarea pasivé
Féré’ model: TD-Iearning
El r51 i E :5 Q Q0
Cm 8 ‘A 2023/2024 13/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
Invétare féré model
1. Estimarea directﬁ a utilitﬁtii
> Utilitatea unei stﬁri este recompensa totalé' a$teptat5 de la acea stare
Tnainte (reward-to-go)
Exemplu: (1,1)__04 W (1,2)__04 W (1,3)__04 W (1,2)__04 W (173)—.04 W
(2» 3)2_04 W (3,3)104 W (47 3)+1
(372)-04 W (373)-04 W (473)+1
(171)—.04 W (271L011 W (371)—.04 W (3,2)—.04 W (4,2)_1
Prima Tncercare produce:
> Tn starea (1,1) recompensa totalé 0.72 (1 - .04 x 7)
F Tn starea (1,2) doué recompense totale 0.76 5i 0.84
P Tn starea (1,3) doué recompense totale 0.80 $i 0.88
> Utilitatea estimaté: media valorilor e$antionate
> U(1,1) : 0.72, U(1,2) : 0.80, U(1,3) : 0.84 etc.
Curs 8 IA 2023/2024 14/41

***************Ending Page***************


***************Beginning Page***************
***************page number:15**************
Estimarea directﬁ a utilitﬁfcii
> Presupune c5 utilitéfcile sunt independente (fals)
Nu tine cont de faptul c5 utilitatea unei stéri depinde de utilitétile
stirilor succesoare (constréngerile date de ecuafciile Bellman)
V céutarea Tntr-un spatiu mult mai mare
P convergenta este foarte lenté
> Avem toate episoadele dinainte
Cm 8 ‘A 2023/2024 15/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
2. lnvﬁtarea diferenfcelor temporale (Temporal Differences)
> Combini avantajele celor doui abordéri anterioare (Estimarea directé’
a utilit5tii $i Programarea dinamica” adapt/V5)
P actualizeazé doar stérile direct afectate
F satisface aproximativ ecuatiile Bellman
> Scopul: estimarea utilitétilor U”(s), date episoadele generate
utilizﬁnd politica 7T.
Actiunile sunt decise de politlca 7T.
> Utilitétile sunt ajustate dupé fiecare tranzifcie observaté.
Exemplu:
V Dupé prima Tncercare: estimé'rile U”(1,3) I 0.84, U’T(27 3) I 0.92.
> Fie tranzitia (1,3) —> (2,3) Tn a doua Tncercare.
Constréngerea daté de ecuatia Bellman impune ca
uﬁ(1,3) : i004 + U”(2,3) : 0.88 (v : 1).
P Estimarea initialé U”(1,3) : 0.84 este mai micé —> se actualizeazé
Cm 8 ‘A 2023/2024 16/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:17**************
Invétarea diferentelor temporale

P Ecuatia diferentelor temporale utilizeazé diferenfca utilitétilor Tntre

stéri succesive:
U"($) <— U”($) + a(R($) + WW5’) — U”($))

a rata de Tnvéfcare

V Actualizarea implicé doar succesorul s’, pe cénd conditiile de echilibru
(ec. Bellman) implicé toate stérile urmétoare posibile

P IVletoda aplicé o serie de corectii pentru a converge

V Obs: metoda nu are nevoie de un model de tranzifcii P pentru a
realiza actualizérile

Cm 8 ‘A 2023/2024 17/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
Diferente tem pora le: pseudocod
function PASSIVE-TD-AGENTQOBTCBPU returns an action
inputs: percept, a percept indicating the current state 5" and reward signal r’
persistent: 7r: a ﬁxed policy
U, a table of utilities, initially empty
N5, a table of frequencies for states, initially zero
s, a, r, the previous state, action, and reward, initially null
if s’ is new then U[s'] <— r’
if s is not null then
increment N, [s]
U[s] {— U[s] + 0(N5 [5]) (r + qr U[s’] — U [5])
if s’.TERM1NAL? then s, a, r ‘—nu]l else s, a, r <— s’,1r[s"], r’
return a
Demo: https : //cs . Stanford . edu/people/karpathy/reinforcej s/gridwor1d_td . html

***************Ending Page***************


***************Beginning Page***************
***************page number:19**************
Tnvétarea diferentelor temporale: exemplu
States Observed Transitions
I
Hun MEI "g
Asume:y:1,u:1/2
Curs a IA 2023/2024 19/41

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
Invétarea diferentelor temporale
V Rata de Tnvéfcare a determiné viteza de convergenté la utilitatea realé
> Valoarea medie a U”(s) va converge Ia valoarea corecté
P suﬁciente Tncercéri, tranzitiile rare apar rar
V daci a este 0 functie care scade pe misuré ce nr. de vizitéri ale unei
stﬁri creste, atunci U”(s) converge la valoarea corecti
P functia a(n) I l/n sau a(n) : 1/(1 + n) G (O, 1]

***************Ending Page***************


***************Beginning Page***************
***************page number:21**************
. . . V . V
leerente temporale vs. Programare dlnamlca adaptlva
> TD nu are nevoie de model, ADP este bazaté pe model
> TD utilizeazé doar succesorul observat pentru actualizare 5i nu toti
succesorii
> TD converge mai lent, dar executé calcule mai simple
u '5 <1 E
i] [(11] 10!] HR] 4:111 511i] U It] 41!] {£1 M] “11]
Numhcnal'lricm Nmnbcrnl'lricll.~
> TD poate ﬁ vézut ca o aproximare a ADP
Cue 8 ‘A 2023/2024 21/41

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
Conjunut
lnvétarea activé
Q-learning
Deep Q-learning
cl 51 i E :2 QQG
Cm 8 ‘A 2023/2024 22 / 41

***************Ending Page***************


***************Beginning Page***************
***************page number:23**************
Invétarea pasivé vs. Tnvétarea activé
> Agentul pasiv are o politicé fixi vs. agentul activ trebuie $5 decidﬁ
actiunile
V Agentul pasiv Tnvafcé’ (probabilitéfcile tranzitiilor $i) utilitéfcile stérilor 5i
alege actiunile optime
vs.
Agentul activ T$i actualizeazé politica pe mésuré ce Tnvaté’
P scopul este 55 Tnvete politica optimﬁ
P Tnsé, functia utilitate nu este cunoscuté decét aproximativ
Cm 8 ‘A 2023/2024 23/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
Exploatare vs. explorare
Dilema exploatare-explorare a agentului
> 55 T$i maximizeze utilitatea, pe baza cuno$tinfcelor curente, sau
> 55 T$i Tmbunétﬁfceascé cuno$tinfce|e
Este necesar un compromis Tntre
> exploatare
F agentul opre$te Tnvétarea 5i executé actiunile date de politicé
> explorare
> agentul Tnvaté Tncercénd actiuni noi
Cm 8 ‘A 2023/2024 24/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:25**************
Dilema exploatare - explorare: solutii
Metoda 6-greedy
V Fie 6 € [0,1]
> Actiunea urmétoare selectaté va ﬁ:
F 0 actiune aleatoare, cu probabilitatea e
P actiunea optimé, cu probabilitatea 1 — 6
> Implementare
> initial e I 1 (explorare)
F cénd se termini un episod de Tnvétare, e scade (de ex. cu 0.05) - cre$te
progresiv rata de exploatare
V e nu scade niciodaté sub un prag, de ex. 0.1
> agentul are mereu o $ans§ de explorare, pentru a evita optimele locale

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
Conju n ut
lnvétarea activé
Q-learning
El r51 _ _= :E ‘D ‘KG
Cm 8 ‘A 2023/2024 26 / 41

***************Ending Page***************


***************Beginning Page***************
***************page number:27**************
Algoritmul Q-Learning (Watkins, 1989)

> Algoritmul Q-Learning Tnvaté o functie actiune-valoare Q(s, a) (Q
quality).
Q(s, a) valoarea asociaté realizérii actiunii a Tn starea s.
Relafcia dintre utilitéfci $i valorile Q: U(s) : maxé,Q(s7 a).

> Ecuatiile adevﬁrate Ia echilibru cénd valorile Q sunt corecte

Q(s, a) : R(s) + 7 Z P(s’|s, a)maxa/Q(s’, a’)
s/
Acestea pot ﬁ utilizate Tntr-un proces iterativ care calculeazé valorile
Q exacte.
Cm 8 ‘A 2023/2024 27/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
Algoritmul Q-Learning

> Un agent TD care Tnvafcé’ o functie Q nu are nevoie de un model
probabilist P(s’|s, a) (Tnv5tare fire? model).

> Pentru ﬁecare e$antion (s, 3,5’, r), se actualizeazé' valoarea Q.
Ecuafcia de actualizare pentru TD Q-Learning:

Q(s, a) I Q(s, a) + a(R(s) + "ymaxa/Q(s', a’) — Q(s7 a))
(executénd actiunea a Tn starea s rezulté s’)
Coeficientul de Tnvétare a determiné' viteza de actualizare a
estimérilor; de obicei, a G (0,1)
Cm 8 ‘A 2023/2024 28/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:29**************
Algoritmul Q-Learning: pseudocod
function Q-LEARNING-AGENT(percept) returns an action
inputs: percept, a percept indicating die current state s’ and reward signal r’
persistent: Q, a table of action values indexed by state and action, initially zero
N3“, a table of frequencies for state—action pairs, initially zero
s, a, r, the previous state, action, and reward, initially null
ifTERMlNAL‘?(s) then Q[s, None] ‘— r’
if s is not null then
increment N5u[s, a]
Qis, a] ‘— Qis, a] + “(NM-[5, “DU + 't mm’ Qtsi 03] — Qist at)
s, a, r ¢— s’, argmsxa; f[Q[s", [1"], Nsﬂ[s", (11), r’
return a
f functie de exp/orare
> Q-learning converge la o politicé optimé'
P Q-Learning este mai lent decﬁt ADP

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
Q-Iearnlng: exemplu
Paenian is in an unknown MDP where there are three state-‘s [A, B, C] and two aetions [St0p, G0]. W'e are given
the following samples generated from taking actions in the unknown MDP. For the following problems, assume
'y : l and r1 : 0.5.
(a) \Ne run Q-learning on the following samples:
HEI-
I-IIEI
Iii-I-
IEI-Ili
\Nhat are the estimates for the following Q-values as obtained by Q-learning? All Q-values are initialized
to D.
Q(C, Stop) :1 Q(C, Go) z?
cl 51 i E :2 Q ‘1x (‘v

***************Ending Page***************


***************Beginning Page***************
***************page number:31**************
SARSA
> Ecuatia de actualizare
Q($, a) I Q($, a) + Q(R($) + 7Q(5’» a’) — Q($, a))

(5’,a’) perechea (starea urmétoare, actiunea urmétoare)
SARSA utilizeazé abordarea TD: se actualizeazé tabelul Q dupé
ﬁecare pas péné cand solutia converge/nr. max. de iteratii.

> Exemplu aplicatie: Windy Gridwor/d
http://WWW.incompleteideas.net/b00k/ebook/n0de64.html

Curs 8 IA 2023/2024 31/41

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
Conju n ut
lnvétarea activé
Deep Q-learning
El r51 i E :5 Q Q0
Cm 8 'A 2023/2024 32 / 41

***************Ending Page***************


***************Beginning Page***************
***************page number:33**************
Deep Reinforcement Learning
Utilizeazé’ o retea neuralé (profundé) pentru a aproxima valorile Q
> intrare: o stare
ie$ire: o estimare a lui Q, Pentru fiecare actiune posibilé
— ~-
_——
QLearning
Deep Q Learning
Cm 8 'A 2023/2024 33 / 41

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
Deep Reinforcement Learning
V Considerém ec. de actualizare a valorii Q (derivaté din ec. Bellman):
Q(s7 a) : Q(s7 a) + a[r + vmaxa/Q(s’, a’) — Q(s7 a)]
: (1 — a)Q(s, a) + r + "ymaxa/Q(s', a’)
> Functia de cost: eroarea medie patraticé dintre valorea Q prezisé 5i
valorea fcinté Q* (nu se cunoa$te).
Valoarea tinté: target(s’) I r + "ymaxa/Q(s’, a’)
Minimizém /oss(s, 3,5’) : (Q(s7 a) — target(s’))2.
P Utilizém metoda Gradient descent pentru a optimiza functia de cost
Descriere: https : //deep1earningmath . org/deep-reinforcement-learning . html
Demo: https://cs.stanford.edu/people/karpathy/reinforcejs/
Cm 8 ‘A 2023/2024 34/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:35**************
Deep Q-learning

Probleme:
V e$anti0anele sunt corelate —> reteaua nu poate generaliza
V target(s’) este o estimare —> convergentﬁ lentﬁ/alg. nu e stabil

Solutii:
> e-greedy policy
> experience replay: memorﬁm experientele (s, a, r,s’) $i le folosim

pentru antrenare (mini-batch)
Cm 8 ‘A 2023/2024 35 / 41

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
Double Deep Q-network
V valoarea tinti se modiﬁcé' Ia ﬁecare iteratie; solutie: o retea separaté
pentru a estima valoarea tinté
[_I‘]
Target Freeman
Farallng updal: m ﬁery
Q’ Q
Target Network I Prediction Nerwork
> la ﬁecare C iteratii, parametrii din reteaua de predictie sunt copiati Tn
reteaua tinté
Cm 8 ‘A 2023/2024 36 / 41

***************Ending Page***************


***************Beginning Page***************
***************page number:37**************
Function approximation
Ug(s) I 9113(5) + 6215(5) + . . . 0,,fn(s)
f1, . . . f” atribute
RL Tnvaté valorile parametrilor 6 : 91, . ..9,, a.i. functia de evaluare 09
aproximeazé functia utilitate.
> actualizeazé parametrii dupé ﬁecare Tncercare
V utilizeazé o functie de eroare 5i calculeazé gradientii Tn raport cu 6
A 2
51(5) I (‘10(5) — “j($)) /2
uJ-(s) recompensa totalﬁ observaté’ din starea s pentru Tncercareaj
6E-(s) A 509(5)
6'<—6'—Otji:9'+06 u- s —U s i
V putere de generalizare (st‘a'ri vizitate —> stéri nevizitate)
Cm 8 ‘A 2023/2024 37/ 41

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
Alte abordari

> Metode de tip policy gradient: Tnvata' o politica’ parametrizata (fara a

decide actiunea pe baza valorilor).
7r(a\s, 6) : Pr{At : alSt : s, 6t : 6}

Scopul: identiﬁcarea parametrilor 6? a.i. 55 maximizam performanta
J(6). Parametrii sunt actualizati utilizﬁnd Gradient Ascent
Politica poate ﬁ stochasticé.

> Metode de tip actor-critic: Tnvata atét politica cét $i valorile.
ChatGPT utilizeaza Proximal policy optimization:
https z //openai . com/research/openai—baselines—ppo.

Cm 8 ‘A 2023/2024 38/ 41

***************Ending Page***************


***************Beginning Page***************
***************page number:39**************
Conjunut
Introducere
Invéfcarea pasivé
Bazaté pe model: ADP
Férﬁ model: TD-learning
Invétarea activé
Q-learning
Deep Q-learning
Concluzii
<E1><ﬁ>4if>4IE> :E QQG
Curs8 —

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
Concluzu
V Invétarea cu Tntérire este necesaré pentru agentii care evolueazé Tn
medii necunoscute
> lnvétarea pasivé presupune evaluarea unei politici date
> Invitarea activé presupune Tnvétarea unei politici Optime
Cm 8 ‘A 2023/2024 4° / 41

***************Ending Page***************


***************Beginning Page***************
***************page number:41**************
Bibliografie
> Artificial Intelligence: A modern Approach. Ch. 21. Reinforcement
Learning
V Sutton&Barto. Reinforcement Learning. An introduction
http://incompleteideas.net/book/RLbook2020.pdf
Curs 8 IA 2023/2024 41/41

***************Ending Page***************





