***************Beginning Page***************
***************page number:1**************
IA 2023/2024
<E|> <ﬁ>4IE>4IE> IE OQO

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
Continut
lntroducere
Perceptronul
Regula de antrenare a perceptronului
Gradient Descent $i Regula delta
Retele neuronale multi-strat
Backpropagation
<E1><ﬁ>4if>4IE> :E QQG
CUrs6 —

***************Ending Page***************


***************Beginning Page***************
***************page number:3**************
Istoric

V l\/lcCulloch&Pitts '43 propun primul model matematic al unui neuron
artificial
Nu poate Tnvéta, parametrii se stabilesc analitic

> Minsky '51 primul circuit electronic construit ca o retea neuronalﬁ
artificialé (subcircuite ce functioneazé ca ni$te neuroni interconectati)

V Rosenblatt '58 dezvolta Perceptronul, prima retea neuronalé
functionalé

> Hinton '06 pune bazele Deep Neural Network

Cm 6 ‘A 2023/2024 3 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
Retele neuronale artiﬁciale
> Sunt inspirate din modul de structurare 5i functionare a creierului
Nudeu 5mm Terminaﬂilaaxonului V
(torpul (elulei \ \\
Dendrite 12:3‘?

\ Axon l
Incercarea de a reproduce inteligenta (comportamentul unui neuron
biologic).

Cue 6 ‘A 2023/2024 4/54

***************Ending Page***************


***************Beginning Page***************
***************page number:5**************
Retele neuronale artlfluale
Un neuron se conecteazé’ cu alti neuroni prin intermediul dendritelor.
Neuronii comunicé’ Tntre ei prin intermediul sinapselor (excitatorii sau
inhibitorii). Neuronul se poate activa $i produce un semnal electric care e
transmis mai departe prin axon.

/ Hecmcal slgml

_,"' 2/ swam“ (calm)
U’ ’

3 ‘ \‘Synavsx \‘Amn _

Interconectarea neuronilor asiguré puterea de calcul.
Cm 6 ‘A 2023/2024 5 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
Retele neuronale artlfluale
Unitate functionala (neuron artificial): un model computational simpliﬁcat
al neuronului
> semnale de intrare
> ponderi sinaptice ata$ate conexiunilor
> prag de activare
> ie$ire
Analogii
. . V .. . V m w
RN blologlca RN artIﬁCIala 0 OSYnapse
_ axon from a neuron
corpul celulel neuron \woxﬂ
_ _ V . dendrite
dendrite Intrarl ceubody f(2m+b)
- - 10111 l
axon |e5|re Zmﬁb
_ V ’ l outputaxon
SI n a psa pon dere activation
11/ J} function
2 2
Cm 6 ‘A 2023/2024 6 / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:7**************
Retele neuronale artificiale
> Un ansamblu de unitéti functionale (neuroni) interconectate
V Antrenarea presupune determinarea parametrilor retelei, utilizénd date
de antrenare
> Sunt sisteme adaptive de tip "cutie neagré" care extrag un model
printr-un proces de Tnvétare.
Cm 6 ‘A 2023/2024 7 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
IVIetode de Invatare

F Supervizatz”; (clasiﬁcare, regresie)
F Exemple de antrenare etichetate
F Scop: estimarea parametrilor care minimizeazé eroarea (diferenta Tntre

réspunsurile corecte $i cele produse de retea)

F Nesupervizaté (clusterizare, asociere, reducerea dimensionalitﬁtii)
F Date de antrenare care nu sunt etichetate
F Scop: obtinerea de informatii

***************Ending Page***************


***************Beginning Page***************
***************page number:9**************
Aplicatii: Clasificare
Daté 0 multime de instante (atribute, etichete), 55 se identiﬁce clasa la
care apartine o instanté nou‘a’.
(supervizaté)
Exemplu: identificarea speciei din care face parte o floare de iris

> atribute: lungime $i litime sepaIe/petale

V clase: Iris versicolor, Iris setosa, Iris virginica

Versicolor Setoscl irginicu
Cm 6 ‘A 2023/2024 9 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
Apllcatu: RegreSIe
55 se determine relatia dintre doué sau mai multe variabile, dat un set de
date de antrenament (aproximarea unei functii)
a.‘
15 _ __
n I .‘: . o
10 . "' “.Q't- .
"n. ' ‘I ‘"- .:I
Gd -10 10 20 3O 4U 50 50
Diferenta dintre clasiﬁcare 5i regresie: tipul ie$irii (discret vs. continuu)
Cm 6 ‘A 2023/2024 1° / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:11**************
Continut
Perceptronul
Regula de antrenare a perceptronului
Gradient Descent 5i Regula delta
cl 51 t E :2 QQG
Cue 6 ‘A 2023/2024 11 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
Perceptronul (Rosenblatl, 1958)
Intrare: un vector de valori reale x,-
Calculeazé o combinatie liniaré a acestora.
9 IW1
GHQ (3%
. . Activation
I I function
6 IW"
inputs weights
W1, . . . w” ponderi (const. reale) ata$ate conexiunilor; W,- contributia
intr§rii x,- la rezultat
Cm 6 ‘A 2023/2024 12 / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:13**************
Perceptronul
Intrare: un vector de valori reale x,-
Calculeazé 0 combinatie liniarﬁ a acestora.
Returneazé 1, dacé rezultatul e mai mare decét un prag (—vv0), -1 altfel.
1 daca WO+W1X1+...W,,X,,>O
0(X1,...,x,,) I (1)
—1 altfel
w I"; 10:’
r3 “'2 W9
. n n
. w ‘Enwixi 0: lifgéwl-xpﬂ
" -1 Olhtrwiie
\n
Invéfcarea unui perceptron: alegerea ponderilor W0, . . . , W”.
Cm 6 ‘A 2023/2024 13/ 54

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
Perceptron
Notatie simplificaté: o intrare constanté’ x0 I 1.
27:0 WiX,‘ > 0, sau W ' Y > O.
0W) I {(7.} - Y)
Functia de activare treapté
iv ~1 ‘ ‘I
l daca > O I . _|_
my) I y — I, \
O altfel ‘.1
v-l
Functia de activare SGmn Slch\1ncl|on Slcp Funcuon
i H i +1
f( ) 1 daca y Z O
y : < >.\ < >
—1 altfel ‘ ‘
v-l 1-1
Sign Function Linear Funclmn
Perceptron: un neuron artificial care utilizeazé functia de activare
treapté/semn
Cm 6 ‘A 2023/2024 14/ 54

***************Ending Page***************


***************Beginning Page***************
***************page number:15**************
Puterea de reprezentare a perceptronilor
V Scopul: $5 clasificém intrérile x1, . . .xn Tn 2 clase
V Perceptronul: un hiperplan care Tmparte spatiul vectorilor de intrare
(n-dimensional) Tn douﬁ regiuni
Ecuatia hiperplanului: W - Y : O
I2 J‘2
+
+
- + -
+
X1 I:
_ _ - +
. . . (a) b
SeparabI/e lln/ar ( )
(pentru un perceptron cu doué intréri)
Curs 6 IA 2023/2024 15/54

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
Puterea de reprezentare a perceptronilor
Un perceptron poate ﬁ utilizat pentru a reprezenta functii booleene.
Pentru a reprezenta functia AND, setém ponderile, spre ex.
W0 I —1.5, W1 I W2 I 1
k A

‘0m mum "a 14 ‘

0 1 l0 ‘M +\ +l

1 0 n __

1 1 1 *(

1m] mu! I5 j‘ ""Ql J “
Cm 6 ‘A 2023/2024 16/ 54

***************Ending Page***************


***************Beginning Page***************
***************page number:17**************
Puterea de reprezentare a perceptronilor
Functia XOR (1 (i) x1 75 X2) nu poate ﬁ reprezentaté de un singur
perceptron.
l: A

--| Q

Ill‘

O l l

l O 1

l 1 O

Q F
Orice functie booleané poate ﬁ reprezentatﬁ de o retea de unitéti
interconectate.
Cm 6 ‘A 2023/2024 17/ 54

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
Conjunut
Perceptronul
Regula de antrenare a perceptronului
El 5' i E :5 ‘)QQ
Cm 6 ‘A 2023/2024 18/ 54

***************Ending Page***************


***************Beginning Page***************
***************page number:19**************
Regula de antrenare a perceptronului

> Tnvétarea ponderilor: identiﬁcé vectorul de ponderi a.i. perceptronul
$5 returneze iesirea corecté pentru fiecare exemplu de antrenare.

> Genereazﬁ ponderi aleatoare,
calculeazé ie$irea pentru ﬁecare exemplu de antrenare,
modificé ponderile atunci cénd clasiﬁcé gresit un exemplu.
Repeté acest procedeu pénﬁ cénd perceptronul clasiﬁcé corect
exemplele de antrenare.

Cm 6 ‘A 2023/2024 19 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
Regula de antrenare a perceptronului
Ponderile sunt modiﬁcate conform regulii de antrenare a perceptronului:
W,- k W; + AW,-

unde

Aw,- : 17(t — o)x,-
t este ie$irea dorité pentru exemplul de antrenare, o este ie$irea generaté
de perceptron, 17 rata de fnvﬁtare (const. pozitivé)

Cm 6 ‘A 2023/2024 2° / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:21**************
Intuitie (regula de antrenare a perceptronului)

V Dacé exemplul este clasiﬁcat corect t — o I 0; AW,- I 0 —> ponderile
nu sunt actualizate

> Dacé perceptronul returneazﬁ -1 cénd ie$irea corecté' este +1 $i
17 : 0.1,x,- : 0.8, atunci Aw,- : 0.1(1 — (-1))0.8 : 0.16

> Dace”; perceptronul returneazé +1 cand ie$irea corecti este -l, atunci
ponderea scade

Curs 6 IA 2023/2024 21/54

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
Regula de antrenare a perceptronului
Atunci cénd exemplele de antrenare sunt separabile liniar $i 17 suﬁcient de
mic,
procedura converge (considerénd un nr. finit de aplicéri a regulii de
antrenare a perceptronului)
la un vector de ponderi care clasiﬁcé toate exemplele de antrenare.

Cm 6 ‘A 2023/2024 22 / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:23**************
Continut
Perceptronul
Gradient Descent $i Regula delta
El r51 i E :5 QQG
Cm 6 ‘A 2023/2024 23 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
Regula delta
Regula de antrenare a perceptronului poate e$ua dacﬁ exemplele nu sunt
separabile liniar.
Regula delta: utilizeazé Gradient descent pentru a céuta Tn spatiul
vectorilor de ponderi.
Considerém o unitate liniaré pentru care iesirea este 0(7) : W ~ 7.
Eroarea de antrenare pentru un vector de ponderi W:
—> _ 1 i 2
E(W) — 2 2W 0d)
dED
unde D multimea datelor de antrenare, td ie$irea dorité pentru exemplul d,
0d ie$irea unité'tii liniare pentru d.
Cm 6 ‘A 2023/2024 24 / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:25**************
Vlzuallzarea spatlulul de |poteze
Suprafata erorii are forma parabolica, cu un minim global.
25
T .
2m “
‘H
\\\\ \“t“‘
. \\\ \\ “ $ §
15\ mw\\\\\\\\\\\\\\\\“‘:¢:¢¢
w n
10 \$§§\§\\\\\\\\\\\‘a\¢‘e‘0.0’
1 wwwmawwvww
'\\“§‘§§\\\\\\‘§\\“}\n\‘\\\“¢%e%¢¢0,o
5 \\\\\\\‘§1\§§\\\\\\\\\\‘\\\\“.¢¢§00'0
\\\\\:$\\\‘:e\“\“:\‘-w\‘v%%
-2
u %
1
.1 3 2
W0 W1
Gradient descent: modificé Tn mod repetat vectorul de ponderi. La fiecare
pas, vectorul este modiﬁcat Tn directia care produce cea mai abrupté
coborére. Acest proces continuﬁ pﬁna la atingerea erorii minime globale.
Cm 6 ‘A 2023/2024 25 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
Gradient descent
Gradientul specificé directia care produce cea mai abrupté ascensiune Tn E.
6E 6E 6E
van) : [i,i,...,i]
6W0 5W1 6W”
Regula de antrenare pentru Gradient descent: W <— W + AW, unde
AW : —nVE(Vv>), n este rata de fnvé'tare (const. pozitivé).
6E
Wi I Wi + AWI, AW; I —?7i
6W,‘
Curs 6 IA 2023/2024 26 /54

***************Ending Page***************


***************Beginning Page***************
***************page number:27**************
Gradient descent
6E 6 l 2
i I if t _
6W,‘ (SW; 2 Z( d 0d)
deD
1 6
I 5 Z (TWIW — Od)2
dGD
1 6
: i 2 t — i t —
22 (d od)6Wi(d 0d)
deD
5 —> —>
Z( d 0d)6Wi( d W Xd)
deD
6E
67/,- Z 2W — 0d)(_Xid)
dED
xid componenta x,- a exemplului de antrenare d.
Actualizarea ponderii cu AW,- : nZdeD(td — Od)X,'d.
Curs 6 IA 2023/2024 27/54

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
Gradient descent
GRAIHENT-DESCENTUmii-iing .exampies, q)
Each Imining example is a pair of thefonn ii’. r), where f is the vector of input values. and
a‘ is the target output value. n is the learning rare fag, .05).
. Initialize each w; to some small random value
I. Until the termination condition is mei, Do
u Initialize each Aw; to zem.
I F0!‘ each (I. a‘) in rminingxxampiﬁ. DO
u Input the instance 3r’ to the unit and compute the output 0
c For each linear unit weight nu, D0
Aw,- <— Aw; + 3U ——0}x; (T41)
a Fer each Linear unit weight 1.0;, Do
w; <- w,- + Aw; {T42}

***************Ending Page***************


***************Beginning Page***************
***************page number:29**************
Stochastic gradient descent
Problemele algoritmului Gradient descent:

> convergenta lenta

V existenta mai multor minime locale
Stochastic gradient descent: actualizarea ponderilor incremental, calculénd
eroarea pentru fiecare exemplu individual

AW,- : n(t — o)x,-
unde t valoarea dorita, o iesirea reala, X,- a i-a intrare pentru exemplul de
antrenare
Ecuatia T4.l este Tnlocuita cu W,- <— W,- + n(t — o)x,-.
Regula de antrenare AW,- : 17(t — o)xi se mai numeste regula delta/
regula LIVIS (least-mean-square)/reguIa Adaline.
Cm 6 ‘A 2023/2024 29 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
Continut
Retele neuronale multi-strat
Backpropagation
El r51 i E :5 ‘)QG
Cm 6 ‘A 2023/2024 3° / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:31**************
Retele neuronale multl-strat
O refcea neuronalé’ cu propagare Tnainte (feed-forward) cu
> un strat de intrare
> unul sau mai multe straturi ascunse
V un strat de ie$ire
Input Layer Hidden Layers Output Layer
/ a
O
I
I
> Semnalele de intrare sunt propagate Tnainte prin straturile retelei
> Calculele se realizeazé Tn neuronii din straturile ascunse $i din stratul
de ie$ire
Cm 6 ‘A 2023/2024 31/ 54

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
Retele neuronale multi-strat
Pot exprima suprafete de decizie neliniare.
Exemplu: Retea antrenatﬁ 55 recunoascé Tntre 10 vocale ("hid").
1000
. all-id
$1»- .- .2:
mu hid § who'd hood t ‘a HM
‘\tm 5:’ m 7;‘ I i 1' l'k ' 32::
\‘QQaQ’ woo ‘ ~M°¢
Fl § F2
m0 $00 LNG 110°
PI- (3*!
Semnalul vocal este reprezentat de doi parametri numerici, obtinufci din
analiza spectrala a sunetului. Punctele din graﬁcul din dreapta sunt
exemplele de testare.
Cm 6 ‘A 2023/2024 32 / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:33**************
Proprietatea de aproximare universalﬁ
> O retea neuronalﬁ cu un strat ascuns, cu un nr. posibil infinit de
neuroni, poate aproxima orice functie realé continué
V Un strat suplimentar poate Tnsé reduce foarte mult nr. de neuroni
necesari Tn straturile ascunse
Cm 6 ‘A 2023/2024 33 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
Unitate sigmoid
x] w; m=l
X2 w? WU
1 ' 9 » Q? '
. w Ml=i§onIi o=ﬁl|rrleﬂ="—i_"?
r: 1+2
o : 0(Vv> - Y), unde 0(y) I é
1 + e—Y
a functia sigmoidé; derivata diff?) I 0(y) - (1 — a(y))
Curs 6 IA 2023/2024 34/54

***************Ending Page***************


***************Beginning Page***************
***************page number:35**************
Functii de activare neliniaré’
> Functia sigmoidﬁ (logisticé)
f(X) I i f'(X) I f(X)(1 — 7‘(XD
> Functia sigmoidé bipolaré (tangenta hiperbolicé)
f(x) I iii f’(x) I 1 _ {(XV
V Functia ReLU (Rectiﬁed Linear Unit)
f(x):{0 daca><<07 f’(x):{0 dacax<0
x dacaxZO 1 dacaxZO
‘ 2 Sigmoid TanH U ReLU
“is 4 7; q 2 4 ML. .4 .2 D 2 ‘- 5% N‘ '2 n 2 4 6
Cm 6 ‘A 2023/2024 35 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
Perceptron multi-strat: proprietéti

> Un perceptron cu un singur strat are acelea$i limitéri chiar dacé'
folose$te o functie de activare neliniaré

> Un perceptron multi-strat cu functii de activare Iiniare este echivalent
cu un perceptron cu un singur strat

P o combinatie liniari de functii Iiniare este tot 0 functie Iiniaré
ex: f(x):2x+1, g(y):y-3, g(f(x)):(2x+l)-3:2x-2
Cm 6 ‘A 2023/2024 36 / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:37**************
Continut
Retele neuronale multi-strat
Backpropagation
El r51 i E :5 ‘)QG
Cm 6 ‘A 2023/2024 37 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
Algoritm u | Backpropagation
> Rumelhart, Hinton& Williams, '86
> Tnvaté ponderile Tntr-o retea multi-strat. Folose$te Gradient descent
pentru a minimiza eroarea pﬁtraticﬁ Tntre ie$irea retelei 5i valorile
dorite.
> Deoarece avem retele cu mai multe unititi de ie$ire, redeﬁnim E
—> _1 _ 2
E(W) — 2 Z Z (tkd Okd)
dGD k€outputs
unde outputs multimea de unitéti de ie$ire, tkd $i okd valoarea dorité’,
respectiv, de ie$ire asociaté cu unitatea de ie$ire k $i exemplul de
antrenare d.
Cm 6 ‘A 2023/2024 38 / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:39**************
Algoritm u | Backpropagation
Are doué faze:
> Reteaua prime$te vectorul de intrare $i propagﬁ semnalul Tnainte, strat
cu strat, péné se genereazé ie$irea
V Semnalul de eroare este propagat Tnapoi, de la stratul de ie$ire c‘a’tre
stratul de intrare, ajusténdu-se ponderile retelei
Cm 6 ‘A 2023/2024 39 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
Baa/(propagation
Pa$ii algoritmului:
> Initializarea: alege numérul de intrﬁri, unitéti ascunse 5i de ie$ire;
initializeazé ponderile $i pragurile cu valori aleatorii mici
F Tn general, pot fi valori din intervalul [-0.1, 0.1]
> Activarea
V se activeazé reteaua prin aplicarea vectorului de antrenare 7
> se calculeazé iesirile neuronilor din stratul ascuns
P se calculeazé ie$iri|e neuronilor din stratul de iesire 0 I cr(7v> ~ 7)
Cm 6 ‘A 2023/2024 4° / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:41**************
Ba ckpropaga tion
layer 1 layer 2 layer 8
. "P34
“ﬁg. mil-h is the weight frem the km neuron
.6Mb" in the [l i 1)“I layer to the ju' neuron
‘%.‘ﬁ. in the In‘ layer
‘W’
> lesirile neuronilor din stratul ascuns
n
Oh : 0(2 Whixi)
[:0
P le$iri|e neuronilor din stratul de iesire
m
0k : (7(2 Wkioi)
i=0
C. 51 e E :2 i (I; e

***************Ending Page***************

***************Beginning Page***************
***************page number:42**************
Baa/(propagation
Actualizeazé fiecare pondere vvj; proportional cu rata de Tnvétare 17,
valoarea de intrare Xj,‘ 5i eroarea 61-.
V Pentru neuronii de ie$ire
V se calculeazé gradientii de eroare ai neuronilor din stratul de ie$ire
Pentru unitatea de ie$ire k,
6k : (tk — Ok)O/<(1 — 0k)
> Pentru neuronii din stratul ascuns
> se calculeazé gradientii de eroare ai neuronilor din stratul ascuns
Pentru unitatea ascunsé h, se Tnsumeazé erorile 5k pentru fiecare
unitate de ie$ire influentaté de h, ponderate cu th (ponderea de Ia
stratul ascuns h Ia stratul de ie$ire k):
6h I 0;,(1 — oh) Z th5l<
kEoutputs

***************Ending Page***************


***************Beginning Page***************
***************page number:43**************
Actualizarea ponderilor
Pentru fiecare exemplu de antrenare d: Wj,‘ I Wj; + AVI/ji,Avu/j,- : —17%,
jl
unde Ed este eroarea pentru exemplul de antrenare d
Em) :1 Z (rk — 002
2
k€outputs
> Ponderile unei unitifci de ie$ire
AWJI I 7751Xﬁ> 51 I (17- Oj)01(1 — OJ)
V Ponderile unui neuron ascuns
AWJI I 775F917 51 I 01(1 — 01) Z 5ka1
k€ Downstream(j)
Cm 6 ‘A 2023/2024 43 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:44**************
Baa/(propagation
Pentru refceIe feed-forward cu numér arbitrar de straturi,
6r I Or(]- _ or) 2 Wsrés
sE/ayer m+1
6, pentru unitatea r din stratul m este calculati din valorile 5 de la
urmétorul strat m + 1
Cm 6 IA 2023/2024 44 / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:45**************
Derlvarea
n xi,- = the ith input to unit j
I wj, : the weight associated with the ith input to unit j
I net,- = E,- wﬁxﬁ (the weighted sum of inputs for unit j)
a oj = the output computed by unit j
I I!- = the target output for unit j
I r: z the sigmoid function
n outputs = the set of units in the ﬁnal layer of the network
a D0wnstream( j) = the set of units whose immediate inputs include the
output of unit j
Utilizém regula de Tnléntuire:
6E0’ I 6Ed 6netj
6 W],- 6 netj 6 Wj,‘
(2)
6 Ed
6netj J

***************Ending Page***************

***************Beginning Page***************
***************page number:46**************
Derlva rea
Case l: Training Rule for Output Unit Weights. Just as wﬂ- can inﬂuence the
rest of the network only through nzrj, net]- can inﬂuence the network only through
oi, Therefore. we can invoke the chain rule again to write
LE1 = mar (4.23)
ﬁner; Ba, Brief]
To begin, consider just the ﬁrst term in Equation (4,23)
BE 3 l
_i = ﬁ- 20t>0102
30; 30]‘ 2 keoumuu
The derivatives 517m ~or)2 will be zero for all output units k exoept when k t j.
We therefore drop the summation over output units and simply set k = ji
‘ 3E4 _ B l . 2
3—aj — 3-0159,- —|1_;)
1 6(2: — 0-)
= -21.- . g
2 (1 0,) Ba;
= -(r, -Dj) (4.24)
Next consider lhe second term in Equation (4.23)‘ Since o,- = when)‘ the
derivative #30:]; is just the derivative of the sigmoid function, which we have
already noted is equal to rr(neIJ-)[1 — o“(ner,-)). Therefore,
30; _ ﬁn(ne|,)
anet, n ﬁner,-
= a,(l — 0;) (4.25)
Substituting expressions (4.24) and (4.25) into (4.23), we obtain
851
m = *(tj i aj) nJ-(l faj) (4.26)
Fll, UAIC Curs 6 IA 2023/2024 46/54

***************Ending Page***************


***************Beginning Page***************
***************page number:47**************
Derlva rea
Case 2: Training Rule for Hidden Unit Weights. In the case where j is an
intﬁﬂ'lZI-L 0r hidden unit in the network, the derivation of the training rule fur wil-
musl take into account the indirect ways in which wﬁ can inﬂuence the network
outputs and hence Edi For this reason. we will ﬁnd il useful tu refer to die
set nf all units immediately downstream of unit j in the network GS.‘ all units
whose direct inputs include the output of unit j). We denote this set of units by
Downstrenm( j). Notice that net; can inﬂuence the network outputs (and therefore
Ed‘) only through the units in Downstreamﬁ). Therefore, we can write
3E4 2 Z BEd ﬁner‘
ﬁner! gaunmmrmmulanﬂk ﬁner]
3 t
= Z _5k 2
ksDawnslrramU) 3””1
Bnett a0,-
= Z "5* T?
kenowmrmmq) 91' M’?
= E >8‘ wideal'
renmmtmmg) "95'
= Z ——8* wk] [III-(I 20].)
keDowmlrequ)
(4.28)
Rem-ranging tern-is and using 8] to denote >33Tit, we have
a,- eoju -0,) Z at w”
lebowmlreamﬁ)
and
Awh- = rr 51' X11
Fll, UAIC Curs 6 IA 2023/2024 47/54

***************Ending Page***************

***************Beginning Page***************
***************page number:48**************
Stochastic Grad/en t Descent
BACKPRDPAGATIUNUrﬂiﬂing-lxﬂmptes. fl. mm um", madden)
Each training ample 1‘: a pair ofrhefonn (i, F ), where E ir the vector afrletwark input
values. and‘ l' is :he vector of target nemark output values.
q i1 riw learning rale (agu .05). n.,. i: rhe number 0f wwrk inputs. "hiddm the mmber of
mils in the hidden layer, and um :he number ofoutpul unim
The mpmfmm unit r imu unit j is demredxj.-, and :he Weighlﬁ'om um‘: i m um‘: j is ignored
wjl.
n Cream a feediforwald network with n... inpms. Rhiddzn hiﬂdcn units. and HM output units.
- Initialize all network weights m small random numbers (cg, beTWeen —.05 and ‘05).
u UnLil the lemu'nalinn evndition is met, Do
I Fnr each (Tc, F) 1'11 Jrainl'ngxxumples. D0
Propagate the input forward rkraugh the network:
l. Input [he instance i to die network and compute the output a, of evcry uni! u in
tha network.
Propagate Ike errnn backward rhmugh the network-
2. For each net-Wm]: ougmt unit k. calculate its :rrur term 15k
5: 4- 0:!11 — 00m. — 0k) (T43)
3. Fur each hidden uni! h, calculale ils error rm 6;,
6h 4- Ill-(1 -0r¢] Z 10th . (1-4-4)
kermlpm
4. Updane each network weigh: w];
wil- <- wj-l + Aw“-
whu'e
Ami.‘ : 1161.1], (T45)
Fll, UAIC Curs 6 IA 2023/2024 48/54

***************Ending Page***************


***************Beginning Page***************
***************page number:49**************
Baa/(propagation
> Se itereazé peste toate exemplele (vectori) de antrenare (o epoci)
> Antrenarea retelei continué’ péné cénd eroarea medie pétraticﬁ ajunge
sub un prag acceptabil sau pénﬁ cénd se atinge un nr. maxim de
epoci de antrenare
Exemplu: din Artiﬁcial Intelligence. A Guide to Intelligent Systems.
Cm 6 ‘A 2023/2024 49 / 54

***************Ending Page***************

***************Beginning Page***************
***************page number:50**************
Backpropaga tion
> Convergenta: algoritmul Backpropagation converge c5tre un minim
local
> Invafcare incrementala vs. Tnvafcare pe lot (batch learning)
P batch learning: ponderile se actualizeazé o singuré data, dupa

prezentarea tuturor vectorilor din grup
Avantaj: rezultatele antrenarii nu mai depind de ordinea Tn care sunt
prezentati vectorii de antrenare

***************Ending Page***************


***************Beginning Page***************
***************page number:51**************
Varianta cu "moment" a algoritmului Backpropagation
V Ajustarea ponderilor de Ia epoca curenté se calculeazé pe baza
gradientului precum $i a ajustérilor de la epoca anterioaré
AWJ'K") I 7751><ji+ OlAvw/(n — 1)

unde Awﬂ-(n) ajustarea ponderii la epoca n, O g a < 1 const.
momentum (inertie)

Why Momentum Real/y Works, https://disti11.pub/20l7/momentum/

Curs 6 IA 2023/2024 51/54

***************Ending Page***************


***************Beginning Page***************
***************page number:52**************
Overflttl n g
Error versus weigh; updates (example l)
0.01
0.009 . Training set error °
Validation set error +
0.000 _
0.007 1+
{E 0.006
0.005
0.004 '
0.003
0.002
0 5000 10000 15000 20000
Number of weight updates
Solutii: weight decay (include o penalitate Tn functia de eroare), k-fold
cross-validation
Cm 6 ‘A 2023/2024 52 / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:53**************
Proiectarea retelelor neuronale: Etape
> Arhitecturé: numﬁr de nivele $i de unité'ti pe ﬁecare nivel, topologie
(mod de interconectare), functii de activare
Arhitecturi: unidirectionale vs. recurente
> Antrenare: determinarea valorilor ponderilor
> Validare: testarea modelului pe date de test
https://playground.tensorflow.org/
Cm 6 ‘A 2023/2024 53 / 54

***************Ending Page***************


***************Beginning Page***************
***************page number:54**************
Bibliografie

V T. |\/|. Mitchell, Machine Learning, Ch. 4 Artificial Neural Networks,
lVchraW-Hill Science, 1997

> S. Russell, P. Norvig, Artificial Intelligence: A Modern Approach, Ch.
18.7 Artiﬁcial Neural Networks, Prentice Hall, 1995

> M. Neqnevitsky. Artificial Intelligence. A Guide to Intelligent
Systems, Ch. 6. Multilayer neural networks, 2005

Curs 6 IA 2023/2024 54/54

***************Ending Page***************


