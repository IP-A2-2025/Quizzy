[{content={parts=[{text=```
--FlashCardSeparator--
Single
--InteriorSeparator--
What is the primary goal of Reinforcement Learning?
--InteriorSeparator--
To learn a behavior without an instructor.
--InteriorSeparator--
Easy
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does the agent receive as feedback from the environment in Reinforcement Learning?
--InteriorSeparator--
Rewards, either positive or negative, based on its actions.
--InteriorSeparator--
Easy
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following elements are involved in the interaction model in Reinforcement Learning?
--InteriorSeparator--
(right) Agent
(right) Environment
(wrong) Supervisor
(wrong) Database
--InteriorSeparator--
Easy
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the agent expected to maximize in Reinforcement Learning?
--InteriorSeparator--
Rewards.
--InteriorSeparator--
Easy
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What data is used for training an agent in Reinforcement Learning?
--InteriorSeparator--
(right) State
(right) Action
(wrong) Policy
(wrong) Value
--InteriorSeparator--
Easy
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the purpose of the transition model P(s'|s, a) in a stochastic environment?
--InteriorSeparator--
It gives the probability of moving from state s to state s' by taking action a.
--InteriorSeparator--
Medium
--InteriorSeparator--
10
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a Markov process of the first order?
--InteriorSeparator--
A process where the current state depends only on the previous state.
--InteriorSeparator--
Medium
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the key components of a Markov Decision Process (MDP)?
--InteriorSeparator--
(right) States (S)
(right) Actions (A)
(right) Transition Model (P(s'|s, a))
(right) Reward Function (R(s))
--InteriorSeparator--
Medium
--InteriorSeparator--
13
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the main difference between a deterministic search and an MDP?
--InteriorSeparator--
In a deterministic search, the goal is to find an optimal sequence of actions. In an MDP, the goal is to find an optimal policy.
--InteriorSeparator--
Medium
--InteriorSeparator--
16
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the difference between reward and utility in the context of MDPs?
--InteriorSeparator--
Reward is the immediate gain, while utility is the total gain over the long term.
--InteriorSeparator--
Medium
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is a characteristic of an optimal policy in a finite horizon MDP?
--InteriorSeparator--
(right) It is not necessarily stationary.
(wrong) It always prioritizes short-term rewards.
(wrong) It always avoids risk.
(wrong) It remains the same over time.
--InteriorSeparator--
Medium
--InteriorSeparator--
19
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does the discount factor (gamma) represent in discounted rewards?
--InteriorSeparator--
It indicates that future rewards are worth less than immediate rewards.
--InteriorSeparator--
Medium
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Why is it important to ensure that the utility of an infinite sequence is finite?
--InteriorSeparator--
To allow the algorithm to converge to a solution.
--InteriorSeparator--
Hard
--InteriorSeparator--
22
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does U*(s) represent in the context of MDPs?
--InteriorSeparator--
The expected utility of a state when following an optimal policy.
--InteriorSeparator--
Hard
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the Bellman equation used for?
--InteriorSeparator--
To express the utility of a state as the immediate reward plus the discounted expected utility of the next state.
--InteriorSeparator--
Hard
--InteriorSeparator--
26
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Why can't Bellman equations be solved directly as a system of linear equations?
--InteriorSeparator--
(right) Because of the 'max' function.
(wrong) Because the number of equations is less than the number of unknowns.
(wrong) Because they are non-deterministic.
(wrong) Because the rewards are always negative.
--InteriorSeparator--
Hard
--InteriorSeparator--
28
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the goal of Value Iteration?
--InteriorSeparator--
To calculate the utility of each state and identify the optimal action in each state.
--InteriorSeparator--
Medium
--InteriorSeparator--
30
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the stopping criterion for the Value Iteration algorithm?
--InteriorSeparator--
When the maximum change in the utility of any state in an iteration is less than a threshold.
--InteriorSeparator--
Hard
--InteriorSeparator--
31
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a key problem with the Value Iteration algorithm?
--InteriorSeparator--
It is slow (O(n^2 A) per iteration).
--InteriorSeparator--
Hard
--InteriorSeparator--
34
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which steps does Policy Iteration alternate between?
--InteriorSeparator--
(right) Policy Evaluation
(right) Policy Improvement
(wrong) Value Calculation
(wrong) Reward Maximization
--InteriorSeparator--
Medium
--InteriorSeparator--
36
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How does Policy Iteration simplify the Bellman equation during Policy Evaluation?
--InteriorSeparator--
By fixing the action for each state according to the current policy.
--InteriorSeparator--
Hard
--InteriorSeparator--
37
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the goal of Policy Improvement step?
--InteriorSeparator--
To calculate the optimal action for each state, given the current utility values.
--InteriorSeparator--
Hard
--InteriorSeparator--
39
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What triggers an update to the policy in Policy Iteration?
--InteriorSeparator--
When a different action maximizes the expected utility more than the current action defined by the policy.
--InteriorSeparator--
Hard
--InteriorSeparator--
40
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the key differences between Value Iteration and Policy Iteration?
--InteriorSeparator--
(right) Value Iteration repeatedly updates utilities, while Policy Iteration alternates between policy evaluation and improvement.
(wrong) Value Iteration converges faster than Policy Iteration.
(wrong) Policy Iteration requires more computational resources per iteration.
(wrong) Value Iteration always finds the optimal policy.
--InteriorSeparator--
Hard
--InteriorSeparator--
36, 30
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
If the environment contains terminal states and a proper policy exists, what value can be used for gamma?
--InteriorSeparator--
1
--InteriorSeparator--
Medium
--InteriorSeparator--
22
--FlashCardSeparator--
```}], role=model}, finishReason=STOP, avgLogprobs=-0.1457031875832687}]