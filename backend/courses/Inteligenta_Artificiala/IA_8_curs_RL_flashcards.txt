[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What are the two main types of Reinforcement Learning discussed based on agent interaction?
--InteriorSeparator--
Passive and Active
--InteriorSeparator--
easy
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
In reinforcement learning, what information is required to calculate an optimal policy?
--InteriorSeparator--
(right) Model of transitions
(right) Reward function
(wrong) Initial state
(wrong) Agent's name
--InteriorSeparator--
medium
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In passive learning, what is the agent's primary goal?
--InteriorSeparator--
To learn how good a fixed policy is.
--InteriorSeparator--
medium
--InteriorSeparator--
7
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the primary difference between model-based and model-free reinforcement learning?
--InteriorSeparator--
Model-based learns the transition and reward model; model-free learns the optimal policy without learning the model.
--InteriorSeparator--
medium
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What does ADP (Adaptive Dynamic Programming) involve?
--InteriorSeparator--
(right) Learning the model of transitions and rewards.
(wrong) Directly learning the optimal policy.
(right) Solving the MDP using the learned model.
(wrong) Ignoring the environment.
--InteriorSeparator--
medium
--InteriorSeparator--
10
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a limitation of ADP (Adaptive Dynamic Programming) when dealing with large state spaces?
--InteriorSeparator--
It becomes inefficient due to the need to solve a large system of linear equations.
--InteriorSeparator--
hard
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the main disadvantage of directly estimating utility in model-free learning?
--InteriorSeparator--
It doesn't account for the dependencies between state utilities as defined by the Bellman equations.
--InteriorSeparator--
hard
--InteriorSeparator--
15
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does the Temporal Difference (TD) learning method do?
--InteriorSeparator--
Combines advantages of direct utility estimation and adaptive dynamic programming.
--InteriorSeparator--
medium
--InteriorSeparator--
16
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In Temporal Difference learning, what does the learning rate (alpha) control?
--InteriorSeparator--
The speed of convergence to the real utility.
--InteriorSeparator--
medium
--InteriorSeparator--
20
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is one key difference between TD learning and ADP?
--InteriorSeparator--
TD does not require a model, while ADP is model-based.
--InteriorSeparator--
medium
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the exploration-exploitation dilemma in active learning?
--InteriorSeparator--
The trade-off between maximizing utility based on current knowledge and improving knowledge by trying new actions.
--InteriorSeparator--
medium
--InteriorSeparator--
24
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the purpose of the epsilon-greedy method in reinforcement learning?
--InteriorSeparator--
To balance exploration and exploitation.
--InteriorSeparator--
medium
--InteriorSeparator--
25
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In Q-learning, what does the Q(s, a) function represent?
--InteriorSeparator--
The value associated with performing action 'a' in state 's'.
--InteriorSeparator--
medium
--InteriorSeparator--
27
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Why does a TD agent learning a Q-function not require a probabilistic model P(s'|s, a)?
--InteriorSeparator--
Because it's a model-free learning approach.
--InteriorSeparator--
medium
--InteriorSeparator--
28
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
According to the pseudocode provided, what is the condition to update the Q value in the Q-LEARNING-AGENT algorithm?
--InteriorSeparator--
If the previous state is not null.
--InteriorSeparator--
hard
--InteriorSeparator--
29
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is SARSA?
--InteriorSeparator--
A TD learning method that updates the Q-table after each step until the solution converges.
--InteriorSeparator--
medium
--InteriorSeparator--
31
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In Deep Reinforcement Learning, what does the neural network approximate?
--InteriorSeparator--
The Q values for each possible action.
--InteriorSeparator--
medium
--InteriorSeparator--
33
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the target value used in Deep Q-Learning's loss function?
--InteriorSeparator--
r + gamma * max_a'(Q(s', a'))
--InteriorSeparator--
hard
--InteriorSeparator--
34
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In Deep Q-Learning, what is 'experience replay' used for?
--InteriorSeparator--
To memorize experiences and use them for training, helping the network to generalize better.
--InteriorSeparator--
hard
--InteriorSeparator--
35
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What problem does the Double Deep Q-Network address?
--InteriorSeparator--
The instability caused by the target value changing at each iteration.
--InteriorSeparator--
hard
--InteriorSeparator--
36
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the main goal of function approximation in reinforcement learning?
--InteriorSeparator--
To generalize from visited states to unvisited states.
--InteriorSeparator--
hard
--InteriorSeparator--
37
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a key characteristic of policy gradient methods?
--InteriorSeparator--
They learn a parameterized policy without explicitly deciding actions based on values.
--InteriorSeparator--
hard
--InteriorSeparator--
38
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the key takeaways about reinforcement learning?
--InteriorSeparator--
(right) It is necessary for agents evolving in unknown environments
(right) Passive learning involves evaluating a given policy
(wrong)  It can only be applied to simple problems
(wrong)  It is always model based
--InteriorSeparator--
medium
--InteriorSeparator--
40
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the key distinction between passive and active reinforcement learning?
--InteriorSeparator--
Passive learning evaluates a fixed policy, while active learning learns an optimal policy.
--InteriorSeparator--
medium
--InteriorSeparator--
40
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are some solutions to Deep Q-learning problems such as correlated samples?
--InteriorSeparator--
(right) e-greedy policy
(right) experience replay
(wrong) SARSA
(wrong) ADP
--InteriorSeparator--
hard
--InteriorSeparator--
35
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Why is it important for the agent to always have a chance to explore in the epsilon-greedy method?
--InteriorSeparator--
To avoid local optima.
--InteriorSeparator--
hard
--InteriorSeparator--
25
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following is true regarding TD learning?
--InteriorSeparator--
(wrong) requires a model of the environment.
(right) Uses only the observed successor for updating.
(right) Executes simpler calculations than ADP.
(wrong) Converges faster than ADP.
--InteriorSeparator--
hard
--InteriorSeparator--
21
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are steps in Adaptive Dynamic Programming (ADP)?
--InteriorSeparator--
(right) Learn the model of transitions
(right) Solve MDP
(wrong) Execute the fixed policy
(wrong) Evaluate the policy without transitions
--InteriorSeparator--
medium
--InteriorSeparator--
10
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In active learning, what must the agent do?
--InteriorSeparator--
Decide on actions.
--InteriorSeparator--
easy
--InteriorSeparator--
23
--FlashCardSeparator--

}], role=model}, finishReason=STOP, avgLogprobs=-0.17961032319321812}]